{"repo": "smdabdoub/phylotoast", "path": "phylotoast/biom_calc.py", "func_name": "MRA", "original_string": "def MRA(biomf, sampleIDs=None, transform=None):\n    \"\"\"\n    Calculate the mean relative abundance percentage.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :param transform: Mathematical function which is used to transform smax to another\n                      format. By default, the function has been set to None.\n\n    :rtype: dict\n    :return: A dictionary keyed on OTUID's and their mean relative abundance for a given\n             number of sampleIDs.\n    \"\"\"\n    ra = relative_abundance(biomf, sampleIDs)\n    if transform is not None:\n        ra = {sample: {otuID: transform(abd) for otuID, abd in ra[sample].items()}\n              for sample in ra.keys()}\n    otuIDs = biomf.ids(axis=\"observation\")\n    return mean_otu_pct_abundance(ra, otuIDs)", "language": "python", "code": "def MRA(biomf, sampleIDs=None, transform=None):\n    \"\"\"\n    Calculate the mean relative abundance percentage.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :param transform: Mathematical function which is used to transform smax to another\n                      format. By default, the function has been set to None.\n\n    :rtype: dict\n    :return: A dictionary keyed on OTUID's and their mean relative abundance for a given\n             number of sampleIDs.\n    \"\"\"\n    ra = relative_abundance(biomf, sampleIDs)\n    if transform is not None:\n        ra = {sample: {otuID: transform(abd) for otuID, abd in ra[sample].items()}\n              for sample in ra.keys()}\n    otuIDs = biomf.ids(axis=\"observation\")\n    return mean_otu_pct_abundance(ra, otuIDs)", "code_tokens": ["def", "MRA", "(", "biomf", ",", "sampleIDs", "=", "None", ",", "transform", "=", "None", ")", ":", "ra", "=", "relative_abundance", "(", "biomf", ",", "sampleIDs", ")", "if", "transform", "is", "not", "None", ":", "ra", "=", "{", "sample", ":", "{", "otuID", ":", "transform", "(", "abd", ")", "for", "otuID", ",", "abd", "in", "ra", "[", "sample", "]", ".", "items", "(", ")", "}", "for", "sample", "in", "ra", ".", "keys", "(", ")", "}", "otuIDs", "=", "biomf", ".", "ids", "(", "axis", "=", "\"observation\"", ")", "return", "mean_otu_pct_abundance", "(", "ra", ",", "otuIDs", ")"], "docstring": "Calculate the mean relative abundance percentage.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :param transform: Mathematical function which is used to transform smax to another\n                      format. By default, the function has been set to None.\n\n    :rtype: dict\n    :return: A dictionary keyed on OTUID's and their mean relative abundance for a given\n             number of sampleIDs.", "docstring_tokens": ["Calculate", "the", "mean", "relative", "abundance", "percentage", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/biom_calc.py#L70-L92", "partition": "train"}
{"repo": "smdabdoub/phylotoast", "path": "phylotoast/biom_calc.py", "func_name": "raw_abundance", "original_string": "def raw_abundance(biomf, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Calculate the total number of sequences in each OTU or SampleID.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: List\n    :param sampleIDs: A list of column id's from BIOM format OTU table. By default, the\n                      list has been set to None.\n\n    :type sample_abd: Boolean\n    :param sample_abd: A boolean operator to provide output for OTUID's or SampleID's. By\n                       default, the output will be provided for SampleID's.\n\n    :rtype: dict\n    :return: Returns a dictionary keyed on either OTUID's or SampleIDs and their\n             respective abundance as values.\n    \"\"\"\n    results = defaultdict(int)\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating raw total abundances: The sampleIDs provided \"\n                \"do not match the sampleIDs in biom file. Please double check the \"\n                \"sampleIDs provided.\\n\")\n    otuIDs = biomf.ids(axis=\"observation\")\n\n    for sampleID in sampleIDs:\n        for otuID in otuIDs:\n            abd = biomf.get_value_by_ids(otuID, sampleID)\n            if sample_abd:\n                results[sampleID] += abd\n            else:\n                results[otuID] += abd\n    return results", "language": "python", "code": "def raw_abundance(biomf, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Calculate the total number of sequences in each OTU or SampleID.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: List\n    :param sampleIDs: A list of column id's from BIOM format OTU table. By default, the\n                      list has been set to None.\n\n    :type sample_abd: Boolean\n    :param sample_abd: A boolean operator to provide output for OTUID's or SampleID's. By\n                       default, the output will be provided for SampleID's.\n\n    :rtype: dict\n    :return: Returns a dictionary keyed on either OTUID's or SampleIDs and their\n             respective abundance as values.\n    \"\"\"\n    results = defaultdict(int)\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating raw total abundances: The sampleIDs provided \"\n                \"do not match the sampleIDs in biom file. Please double check the \"\n                \"sampleIDs provided.\\n\")\n    otuIDs = biomf.ids(axis=\"observation\")\n\n    for sampleID in sampleIDs:\n        for otuID in otuIDs:\n            abd = biomf.get_value_by_ids(otuID, sampleID)\n            if sample_abd:\n                results[sampleID] += abd\n            else:\n                results[otuID] += abd\n    return results", "code_tokens": ["def", "raw_abundance", "(", "biomf", ",", "sampleIDs", "=", "None", ",", "sample_abd", "=", "True", ")", ":", "results", "=", "defaultdict", "(", "int", ")", "if", "sampleIDs", "is", "None", ":", "sampleIDs", "=", "biomf", ".", "ids", "(", ")", "else", ":", "try", ":", "for", "sid", "in", "sampleIDs", ":", "assert", "sid", "in", "biomf", ".", "ids", "(", ")", "except", "AssertionError", ":", "raise", "ValueError", "(", "\"\\nError while calculating raw total abundances: The sampleIDs provided \"", "\"do not match the sampleIDs in biom file. Please double check the \"", "\"sampleIDs provided.\\n\"", ")", "otuIDs", "=", "biomf", ".", "ids", "(", "axis", "=", "\"observation\"", ")", "for", "sampleID", "in", "sampleIDs", ":", "for", "otuID", "in", "otuIDs", ":", "abd", "=", "biomf", ".", "get_value_by_ids", "(", "otuID", ",", "sampleID", ")", "if", "sample_abd", ":", "results", "[", "sampleID", "]", "+=", "abd", "else", ":", "results", "[", "otuID", "]", "+=", "abd", "return", "results"], "docstring": "Calculate the total number of sequences in each OTU or SampleID.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: List\n    :param sampleIDs: A list of column id's from BIOM format OTU table. By default, the\n                      list has been set to None.\n\n    :type sample_abd: Boolean\n    :param sample_abd: A boolean operator to provide output for OTUID's or SampleID's. By\n                       default, the output will be provided for SampleID's.\n\n    :rtype: dict\n    :return: Returns a dictionary keyed on either OTUID's or SampleIDs and their\n             respective abundance as values.", "docstring_tokens": ["Calculate", "the", "total", "number", "of", "sequences", "in", "each", "OTU", "or", "SampleID", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/biom_calc.py#L95-L135", "partition": "train"}
{"repo": "smdabdoub/phylotoast", "path": "phylotoast/biom_calc.py", "func_name": "transform_raw_abundance", "original_string": "def transform_raw_abundance(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Function to transform the total abundance calculation for each sample ID to another\n    format based on user given transformation function.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :param fn: Mathematical function which is used to transform smax to another format.\n               By default, the function has been given as base 10 logarithm.\n\n    :rtype: dict\n    :return: Returns a dictionary similar to output of raw_abundance function but with\n             the abundance values modified by the mathematical operation. By default, the\n             operation performed on the abundances is base 10 logarithm.\n    \"\"\"\n    totals = raw_abundance(biomf, sampleIDs, sample_abd)\n    return {sid: fn(abd) for sid, abd in totals.items()}", "language": "python", "code": "def transform_raw_abundance(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Function to transform the total abundance calculation for each sample ID to another\n    format based on user given transformation function.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :param fn: Mathematical function which is used to transform smax to another format.\n               By default, the function has been given as base 10 logarithm.\n\n    :rtype: dict\n    :return: Returns a dictionary similar to output of raw_abundance function but with\n             the abundance values modified by the mathematical operation. By default, the\n             operation performed on the abundances is base 10 logarithm.\n    \"\"\"\n    totals = raw_abundance(biomf, sampleIDs, sample_abd)\n    return {sid: fn(abd) for sid, abd in totals.items()}", "code_tokens": ["def", "transform_raw_abundance", "(", "biomf", ",", "fn", "=", "math", ".", "log10", ",", "sampleIDs", "=", "None", ",", "sample_abd", "=", "True", ")", ":", "totals", "=", "raw_abundance", "(", "biomf", ",", "sampleIDs", ",", "sample_abd", ")", "return", "{", "sid", ":", "fn", "(", "abd", ")", "for", "sid", ",", "abd", "in", "totals", ".", "items", "(", ")", "}"], "docstring": "Function to transform the total abundance calculation for each sample ID to another\n    format based on user given transformation function.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :param fn: Mathematical function which is used to transform smax to another format.\n               By default, the function has been given as base 10 logarithm.\n\n    :rtype: dict\n    :return: Returns a dictionary similar to output of raw_abundance function but with\n             the abundance values modified by the mathematical operation. By default, the\n             operation performed on the abundances is base 10 logarithm.", "docstring_tokens": ["Function", "to", "transform", "the", "total", "abundance", "calculation", "for", "each", "sample", "ID", "to", "another", "format", "based", "on", "user", "given", "transformation", "function", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/biom_calc.py#L138-L155", "partition": "train"}
{"repo": "smdabdoub/phylotoast", "path": "phylotoast/biom_calc.py", "func_name": "arcsine_sqrt_transform", "original_string": "def arcsine_sqrt_transform(rel_abd):\n    \"\"\"\n    Takes the proportion data from relative_abundance() and applies the\n    variance stabilizing arcsine square root transformation:\n\n    X = sin^{-1} \\sqrt p\n    \"\"\"\n    arcsint = lambda p: math.asin(math.sqrt(p))\n    return {col_id: {row_id: arcsint(rel_abd[col_id][row_id])\n                     for row_id in rel_abd[col_id]} for col_id in rel_abd}", "language": "python", "code": "def arcsine_sqrt_transform(rel_abd):\n    \"\"\"\n    Takes the proportion data from relative_abundance() and applies the\n    variance stabilizing arcsine square root transformation:\n\n    X = sin^{-1} \\sqrt p\n    \"\"\"\n    arcsint = lambda p: math.asin(math.sqrt(p))\n    return {col_id: {row_id: arcsint(rel_abd[col_id][row_id])\n                     for row_id in rel_abd[col_id]} for col_id in rel_abd}", "code_tokens": ["def", "arcsine_sqrt_transform", "(", "rel_abd", ")", ":", "arcsint", "=", "lambda", "p", ":", "math", ".", "asin", "(", "math", ".", "sqrt", "(", "p", ")", ")", "return", "{", "col_id", ":", "{", "row_id", ":", "arcsint", "(", "rel_abd", "[", "col_id", "]", "[", "row_id", "]", ")", "for", "row_id", "in", "rel_abd", "[", "col_id", "]", "}", "for", "col_id", "in", "rel_abd", "}"], "docstring": "Takes the proportion data from relative_abundance() and applies the\n    variance stabilizing arcsine square root transformation:\n\n    X = sin^{-1} \\sqrt p", "docstring_tokens": ["Takes", "the", "proportion", "data", "from", "relative_abundance", "()", "and", "applies", "the", "variance", "stabilizing", "arcsine", "square", "root", "transformation", ":"], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/biom_calc.py#L158-L167", "partition": "train"}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/lookup-word.py", "func_name": "f2lookup", "original_string": "def f2lookup(f, lookup):\n    \"\"\"\n    find and replace elements in lookup within file\n    \"\"\"\n    lookup = {i: r for i, r in [l.strip().split('\\t')[0:2] for l in lookup]}\n    for line in f:\n        line = line.strip().split()\n        for i, w in enumerate(line):\n            if w in lookup:\n                line[i] = lookup[w]\n        yield ' '.join(line)", "language": "python", "code": "def f2lookup(f, lookup):\n    \"\"\"\n    find and replace elements in lookup within file\n    \"\"\"\n    lookup = {i: r for i, r in [l.strip().split('\\t')[0:2] for l in lookup]}\n    for line in f:\n        line = line.strip().split()\n        for i, w in enumerate(line):\n            if w in lookup:\n                line[i] = lookup[w]\n        yield ' '.join(line)", "code_tokens": ["def", "f2lookup", "(", "f", ",", "lookup", ")", ":", "lookup", "=", "{", "i", ":", "r", "for", "i", ",", "r", "in", "[", "l", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "[", "0", ":", "2", "]", "for", "l", "in", "lookup", "]", "}", "for", "line", "in", "f", ":", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "for", "i", ",", "w", "in", "enumerate", "(", "line", ")", ":", "if", "w", "in", "lookup", ":", "line", "[", "i", "]", "=", "lookup", "[", "w", "]", "yield", "' '", ".", "join", "(", "line", ")"], "docstring": "find and replace elements in lookup within file", "docstring_tokens": ["find", "and", "replace", "elements", "in", "lookup", "within", "file"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/lookup-word.py#L10-L20", "partition": "train"}
{"repo": "smdabdoub/phylotoast", "path": "bin/diversity.py", "func_name": "print_MannWhitneyU", "original_string": "def print_MannWhitneyU(div_calc):\n    \"\"\"\n    Compute the Mann-Whitney U test for unequal group sample sizes.\n    \"\"\"\n    try:\n        x = div_calc.values()[0].values()\n        y = div_calc.values()[1].values()\n    except:\n        return \"Error setting up input arrays for Mann-Whitney U Test. Skipping \"\\\n               \"significance testing.\"\n    T, p = stats.mannwhitneyu(x, y)\n    print \"\\nMann-Whitney U test statistic:\", T\n    print \"Two-tailed p-value: {}\".format(2 * p)", "language": "python", "code": "def print_MannWhitneyU(div_calc):\n    \"\"\"\n    Compute the Mann-Whitney U test for unequal group sample sizes.\n    \"\"\"\n    try:\n        x = div_calc.values()[0].values()\n        y = div_calc.values()[1].values()\n    except:\n        return \"Error setting up input arrays for Mann-Whitney U Test. Skipping \"\\\n               \"significance testing.\"\n    T, p = stats.mannwhitneyu(x, y)\n    print \"\\nMann-Whitney U test statistic:\", T\n    print \"Two-tailed p-value: {}\".format(2 * p)", "code_tokens": ["def", "print_MannWhitneyU", "(", "div_calc", ")", ":", "try", ":", "x", "=", "div_calc", ".", "values", "(", ")", "[", "0", "]", ".", "values", "(", ")", "y", "=", "div_calc", ".", "values", "(", ")", "[", "1", "]", ".", "values", "(", ")", "except", ":", "return", "\"Error setting up input arrays for Mann-Whitney U Test. Skipping \"", "\"significance testing.\"", "T", ",", "p", "=", "stats", ".", "mannwhitneyu", "(", "x", ",", "y", ")", "print", "\"\\nMann-Whitney U test statistic:\"", ",", "T", "print", "\"Two-tailed p-value: {}\"", ".", "format", "(", "2", "*", "p", ")"], "docstring": "Compute the Mann-Whitney U test for unequal group sample sizes.", "docstring_tokens": ["Compute", "the", "Mann", "-", "Whitney", "U", "test", "for", "unequal", "group", "sample", "sizes", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/diversity.py#L54-L66", "partition": "train"}
{"repo": "smdabdoub/phylotoast", "path": "bin/diversity.py", "func_name": "print_KruskalWallisH", "original_string": "def print_KruskalWallisH(div_calc):\n    \"\"\"\n    Compute the Kruskal-Wallis H-test for independent samples. A typical rule is that\n    each group must have at least 5 measurements.\n    \"\"\"\n    calc = defaultdict(list)\n    try:\n        for k1, v1 in div_calc.iteritems():\n            for k2, v2 in v1.iteritems():\n                calc[k1].append(v2)\n    except:\n        return \"Error setting up input arrays for Kruskal-Wallis H-Test. Skipping \"\\\n               \"significance testing.\"\n    h, p = stats.kruskal(*calc.values())\n    print \"\\nKruskal-Wallis H-test statistic for {} groups: {}\".format(str(len(div_calc)), h)\n    print \"p-value: {}\".format(p)", "language": "python", "code": "def print_KruskalWallisH(div_calc):\n    \"\"\"\n    Compute the Kruskal-Wallis H-test for independent samples. A typical rule is that\n    each group must have at least 5 measurements.\n    \"\"\"\n    calc = defaultdict(list)\n    try:\n        for k1, v1 in div_calc.iteritems():\n            for k2, v2 in v1.iteritems():\n                calc[k1].append(v2)\n    except:\n        return \"Error setting up input arrays for Kruskal-Wallis H-Test. Skipping \"\\\n               \"significance testing.\"\n    h, p = stats.kruskal(*calc.values())\n    print \"\\nKruskal-Wallis H-test statistic for {} groups: {}\".format(str(len(div_calc)), h)\n    print \"p-value: {}\".format(p)", "code_tokens": ["def", "print_KruskalWallisH", "(", "div_calc", ")", ":", "calc", "=", "defaultdict", "(", "list", ")", "try", ":", "for", "k1", ",", "v1", "in", "div_calc", ".", "iteritems", "(", ")", ":", "for", "k2", ",", "v2", "in", "v1", ".", "iteritems", "(", ")", ":", "calc", "[", "k1", "]", ".", "append", "(", "v2", ")", "except", ":", "return", "\"Error setting up input arrays for Kruskal-Wallis H-Test. Skipping \"", "\"significance testing.\"", "h", ",", "p", "=", "stats", ".", "kruskal", "(", "*", "calc", ".", "values", "(", ")", ")", "print", "\"\\nKruskal-Wallis H-test statistic for {} groups: {}\"", ".", "format", "(", "str", "(", "len", "(", "div_calc", ")", ")", ",", "h", ")", "print", "\"p-value: {}\"", ".", "format", "(", "p", ")"], "docstring": "Compute the Kruskal-Wallis H-test for independent samples. A typical rule is that\n    each group must have at least 5 measurements.", "docstring_tokens": ["Compute", "the", "Kruskal", "-", "Wallis", "H", "-", "test", "for", "independent", "samples", ".", "A", "typical", "rule", "is", "that", "each", "group", "must", "have", "at", "least", "5", "measurements", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/diversity.py#L69-L84", "partition": "train"}
{"repo": "smdabdoub/phylotoast", "path": "bin/diversity.py", "func_name": "write_diversity_metrics", "original_string": "def write_diversity_metrics(data, sample_ids, fp=None):\n    \"\"\"\n    Given a dictionary of diversity calculations (keyed by method)\n    write out the data to a file.\n    \"\"\"\n    if fp is None:\n        fp = \"./diversity_data.txt\"\n\n    with open(fp, \"w\") as outf:\n        out = csv.writer(outf, delimiter=\"\\t\")\n        out.writerow([\"SampleID\", \"Group\", \"Calculation\"])\n        for group, d in data.iteritems():\n            for sid, value in d.iteritems():\n                out.writerow([sid, group, value])", "language": "python", "code": "def write_diversity_metrics(data, sample_ids, fp=None):\n    \"\"\"\n    Given a dictionary of diversity calculations (keyed by method)\n    write out the data to a file.\n    \"\"\"\n    if fp is None:\n        fp = \"./diversity_data.txt\"\n\n    with open(fp, \"w\") as outf:\n        out = csv.writer(outf, delimiter=\"\\t\")\n        out.writerow([\"SampleID\", \"Group\", \"Calculation\"])\n        for group, d in data.iteritems():\n            for sid, value in d.iteritems():\n                out.writerow([sid, group, value])", "code_tokens": ["def", "write_diversity_metrics", "(", "data", ",", "sample_ids", ",", "fp", "=", "None", ")", ":", "if", "fp", "is", "None", ":", "fp", "=", "\"./diversity_data.txt\"", "with", "open", "(", "fp", ",", "\"w\"", ")", "as", "outf", ":", "out", "=", "csv", ".", "writer", "(", "outf", ",", "delimiter", "=", "\"\\t\"", ")", "out", ".", "writerow", "(", "[", "\"SampleID\"", ",", "\"Group\"", ",", "\"Calculation\"", "]", ")", "for", "group", ",", "d", "in", "data", ".", "iteritems", "(", ")", ":", "for", "sid", ",", "value", "in", "d", ".", "iteritems", "(", ")", ":", "out", ".", "writerow", "(", "[", "sid", ",", "group", ",", "value", "]", ")"], "docstring": "Given a dictionary of diversity calculations (keyed by method)\n    write out the data to a file.", "docstring_tokens": ["Given", "a", "dictionary", "of", "diversity", "calculations", "(", "keyed", "by", "method", ")", "write", "out", "the", "data", "to", "a", "file", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/diversity.py#L106-L119", "partition": "train"}
{"repo": "smdabdoub/phylotoast", "path": "bin/diversity.py", "func_name": "handle_program_options", "original_string": "def handle_program_options():\n    \"\"\"Parses the given options passed in at the command line.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Calculate the alpha diversity\\\n                                     of a set of samples using one or more \\\n                                     metrics and output a kernal density \\\n                                     estimator-smoothed histogram of the \\\n                                     results.\")\n    parser.add_argument(\"-m\", \"--map_file\",\n                        help=\"QIIME mapping file.\")\n    parser.add_argument(\"-i\", \"--biom_fp\",\n                        help=\"Path to the BIOM table\")\n    parser.add_argument(\"-c\", \"--category\",\n                        help=\"Specific category from the mapping file.\")\n    parser.add_argument(\"-d\", \"--diversity\", default=[\"shannon\"], nargs=\"+\",\n                        help=\"The alpha diversity metric. Default \\\n                             value is 'shannon', which will calculate the Shannon\\\n                             entropy. Multiple metrics can be specified (space separated).\\\n                             The full list of metrics is available at:\\\n                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\\\n                             Beta diversity metrics will be supported in the future.\")\n    parser.add_argument(\"--x_label\", default=[None], nargs=\"+\",\n                        help=\"The name of the diversity metric to be displayed on the\\\n                        plot as the X-axis label. If multiple metrics are specified,\\\n                        then multiple entries for the X-axis label should be given.\")\n    parser.add_argument(\"--color_by\",\n                        help=\"A column name in the mapping file containing\\\n                              hexadecimal (#FF0000) color values that will\\\n                              be used to color the groups. Each sample ID must\\\n                              have a color entry.\")\n    parser.add_argument(\"--plot_title\", default=\"\",\n                        help=\"A descriptive title that will appear at the top \\\n                        of the output plot. Surround with quotes if there are\\\n                        spaces in the title.\")\n    parser.add_argument(\"-o\", \"--output_dir\", default=\".\",\n                        help=\"The directory plots will be saved to.\")\n    parser.add_argument(\"--image_type\", default=\"png\",\n                        help=\"The type of image to save: png, svg, pdf, eps, etc...\")\n    parser.add_argument(\"--save_calculations\",\n                        help=\"Path and name of text file to store the calculated \"\n                        \"diversity metrics.\")\n    parser.add_argument(\"--suppress_stats\", action=\"store_true\", help=\"Do not display \"\n                        \"significance testing results which are shown by default.\")\n    parser.add_argument(\"--show_available_metrics\", action=\"store_true\",\n                        help=\"Supply this parameter to see which alpha diversity metrics \"\n                             \" are available for usage. No calculations will be performed\"\n                             \" if this parameter is provided.\")\n    return parser.parse_args()", "language": "python", "code": "def handle_program_options():\n    \"\"\"Parses the given options passed in at the command line.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Calculate the alpha diversity\\\n                                     of a set of samples using one or more \\\n                                     metrics and output a kernal density \\\n                                     estimator-smoothed histogram of the \\\n                                     results.\")\n    parser.add_argument(\"-m\", \"--map_file\",\n                        help=\"QIIME mapping file.\")\n    parser.add_argument(\"-i\", \"--biom_fp\",\n                        help=\"Path to the BIOM table\")\n    parser.add_argument(\"-c\", \"--category\",\n                        help=\"Specific category from the mapping file.\")\n    parser.add_argument(\"-d\", \"--diversity\", default=[\"shannon\"], nargs=\"+\",\n                        help=\"The alpha diversity metric. Default \\\n                             value is 'shannon', which will calculate the Shannon\\\n                             entropy. Multiple metrics can be specified (space separated).\\\n                             The full list of metrics is available at:\\\n                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\\\n                             Beta diversity metrics will be supported in the future.\")\n    parser.add_argument(\"--x_label\", default=[None], nargs=\"+\",\n                        help=\"The name of the diversity metric to be displayed on the\\\n                        plot as the X-axis label. If multiple metrics are specified,\\\n                        then multiple entries for the X-axis label should be given.\")\n    parser.add_argument(\"--color_by\",\n                        help=\"A column name in the mapping file containing\\\n                              hexadecimal (#FF0000) color values that will\\\n                              be used to color the groups. Each sample ID must\\\n                              have a color entry.\")\n    parser.add_argument(\"--plot_title\", default=\"\",\n                        help=\"A descriptive title that will appear at the top \\\n                        of the output plot. Surround with quotes if there are\\\n                        spaces in the title.\")\n    parser.add_argument(\"-o\", \"--output_dir\", default=\".\",\n                        help=\"The directory plots will be saved to.\")\n    parser.add_argument(\"--image_type\", default=\"png\",\n                        help=\"The type of image to save: png, svg, pdf, eps, etc...\")\n    parser.add_argument(\"--save_calculations\",\n                        help=\"Path and name of text file to store the calculated \"\n                        \"diversity metrics.\")\n    parser.add_argument(\"--suppress_stats\", action=\"store_true\", help=\"Do not display \"\n                        \"significance testing results which are shown by default.\")\n    parser.add_argument(\"--show_available_metrics\", action=\"store_true\",\n                        help=\"Supply this parameter to see which alpha diversity metrics \"\n                             \" are available for usage. No calculations will be performed\"\n                             \" if this parameter is provided.\")\n    return parser.parse_args()", "code_tokens": ["def", "handle_program_options", "(", ")", ":", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Calculate the alpha diversity\\\n                                     of a set of samples using one or more \\\n                                     metrics and output a kernal density \\\n                                     estimator-smoothed histogram of the \\\n                                     results.\"", ")", "parser", ".", "add_argument", "(", "\"-m\"", ",", "\"--map_file\"", ",", "help", "=", "\"QIIME mapping file.\"", ")", "parser", ".", "add_argument", "(", "\"-i\"", ",", "\"--biom_fp\"", ",", "help", "=", "\"Path to the BIOM table\"", ")", "parser", ".", "add_argument", "(", "\"-c\"", ",", "\"--category\"", ",", "help", "=", "\"Specific category from the mapping file.\"", ")", "parser", ".", "add_argument", "(", "\"-d\"", ",", "\"--diversity\"", ",", "default", "=", "[", "\"shannon\"", "]", ",", "nargs", "=", "\"+\"", ",", "help", "=", "\"The alpha diversity metric. Default \\\n                             value is 'shannon', which will calculate the Shannon\\\n                             entropy. Multiple metrics can be specified (space separated).\\\n                             The full list of metrics is available at:\\\n                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\\\n                             Beta diversity metrics will be supported in the future.\"", ")", "parser", ".", "add_argument", "(", "\"--x_label\"", ",", "default", "=", "[", "None", "]", ",", "nargs", "=", "\"+\"", ",", "help", "=", "\"The name of the diversity metric to be displayed on the\\\n                        plot as the X-axis label. If multiple metrics are specified,\\\n                        then multiple entries for the X-axis label should be given.\"", ")", "parser", ".", "add_argument", "(", "\"--color_by\"", ",", "help", "=", "\"A column name in the mapping file containing\\\n                              hexadecimal (#FF0000) color values that will\\\n                              be used to color the groups. Each sample ID must\\\n                              have a color entry.\"", ")", "parser", ".", "add_argument", "(", "\"--plot_title\"", ",", "default", "=", "\"\"", ",", "help", "=", "\"A descriptive title that will appear at the top \\\n                        of the output plot. Surround with quotes if there are\\\n                        spaces in the title.\"", ")", "parser", ".", "add_argument", "(", "\"-o\"", ",", "\"--output_dir\"", ",", "default", "=", "\".\"", ",", "help", "=", "\"The directory plots will be saved to.\"", ")", "parser", ".", "add_argument", "(", "\"--image_type\"", ",", "default", "=", "\"png\"", ",", "help", "=", "\"The type of image to save: png, svg, pdf, eps, etc...\"", ")", "parser", ".", "add_argument", "(", "\"--save_calculations\"", ",", "help", "=", "\"Path and name of text file to store the calculated \"", "\"diversity metrics.\"", ")", "parser", ".", "add_argument", "(", "\"--suppress_stats\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Do not display \"", "\"significance testing results which are shown by default.\"", ")", "parser", ".", "add_argument", "(", "\"--show_available_metrics\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Supply this parameter to see which alpha diversity metrics \"", "\" are available for usage. No calculations will be performed\"", "\" if this parameter is provided.\"", ")", "return", "parser", ".", "parse_args", "(", ")"], "docstring": "Parses the given options passed in at the command line.", "docstring_tokens": ["Parses", "the", "given", "options", "passed", "in", "at", "the", "command", "line", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/diversity.py#L122-L168", "partition": "train"}
{"repo": "jay-johnson/network-pipeline", "path": "network_pipeline/scripts/packets_redis.py", "func_name": "recv_msg", "original_string": "def recv_msg(body,\n             message):\n    \"\"\"recv_msg\n\n    Handler method - fires when a messages is consumed from\n    the ``FORWARD_QUEUE`` queue running in the ``FORWARD_BROKER_URL``\n    broker.\n\n    :param body: message body\n    :param message: message object can ack, requeue or reject\n    \"\"\"\n\n    log.info((\"callback received msg \"))\n\n    agg.handle_msg(\n        body=body,\n        org_message=message)", "language": "python", "code": "def recv_msg(body,\n             message):\n    \"\"\"recv_msg\n\n    Handler method - fires when a messages is consumed from\n    the ``FORWARD_QUEUE`` queue running in the ``FORWARD_BROKER_URL``\n    broker.\n\n    :param body: message body\n    :param message: message object can ack, requeue or reject\n    \"\"\"\n\n    log.info((\"callback received msg \"))\n\n    agg.handle_msg(\n        body=body,\n        org_message=message)", "code_tokens": ["def", "recv_msg", "(", "body", ",", "message", ")", ":", "log", ".", "info", "(", "(", "\"callback received msg \"", ")", ")", "agg", ".", "handle_msg", "(", "body", "=", "body", ",", "org_message", "=", "message", ")"], "docstring": "recv_msg\n\n    Handler method - fires when a messages is consumed from\n    the ``FORWARD_QUEUE`` queue running in the ``FORWARD_BROKER_URL``\n    broker.\n\n    :param body: message body\n    :param message: message object can ack, requeue or reject", "docstring_tokens": ["recv_msg"], "sha": "4e53ae13fe12085e0cf2e5e1aff947368f4f1ffa", "url": "https://github.com/jay-johnson/network-pipeline/blob/4e53ae13fe12085e0cf2e5e1aff947368f4f1ffa/network_pipeline/scripts/packets_redis.py#L21-L37", "partition": "train"}
{"repo": "jay-johnson/network-pipeline", "path": "network_pipeline/scripts/packets_redis.py", "func_name": "consume_network_packet_messages_from_redis", "original_string": "def consume_network_packet_messages_from_redis():\n    \"\"\"consume_network_packet_messages_from_redis\n\n    Setup a ``celery_connectors.KombuSubscriber`` to consume meessages\n    from the ``FORWARD_BROKER_URL`` broker in the ``FORWARD_QUEUE``\n    queue.\n    \"\"\"\n    # end of recv_message\n    # Initialize KombuSubscriber\n    sub = KombuSubscriber(\n        name,\n        FORWARD_BROKER_URL,\n        FORWARD_SSL_OPTIONS)\n\n    # Now consume:\n    seconds_to_consume = 10.0\n    heartbeat = 60\n    serializer = \"application/json\"\n    queue = FORWARD_QUEUE\n\n    sub.consume(\n        callback=recv_msg,\n        queue=queue,\n        exchange=None,\n        routing_key=None,\n        serializer=serializer,\n        heartbeat=heartbeat,\n        time_to_wait=seconds_to_consume)\n\n    log.info(\"end - {}\".format(name))", "language": "python", "code": "def consume_network_packet_messages_from_redis():\n    \"\"\"consume_network_packet_messages_from_redis\n\n    Setup a ``celery_connectors.KombuSubscriber`` to consume meessages\n    from the ``FORWARD_BROKER_URL`` broker in the ``FORWARD_QUEUE``\n    queue.\n    \"\"\"\n    # end of recv_message\n    # Initialize KombuSubscriber\n    sub = KombuSubscriber(\n        name,\n        FORWARD_BROKER_URL,\n        FORWARD_SSL_OPTIONS)\n\n    # Now consume:\n    seconds_to_consume = 10.0\n    heartbeat = 60\n    serializer = \"application/json\"\n    queue = FORWARD_QUEUE\n\n    sub.consume(\n        callback=recv_msg,\n        queue=queue,\n        exchange=None,\n        routing_key=None,\n        serializer=serializer,\n        heartbeat=heartbeat,\n        time_to_wait=seconds_to_consume)\n\n    log.info(\"end - {}\".format(name))", "code_tokens": ["def", "consume_network_packet_messages_from_redis", "(", ")", ":", "# end of recv_message", "# Initialize KombuSubscriber", "sub", "=", "KombuSubscriber", "(", "name", ",", "FORWARD_BROKER_URL", ",", "FORWARD_SSL_OPTIONS", ")", "# Now consume:", "seconds_to_consume", "=", "10.0", "heartbeat", "=", "60", "serializer", "=", "\"application/json\"", "queue", "=", "FORWARD_QUEUE", "sub", ".", "consume", "(", "callback", "=", "recv_msg", ",", "queue", "=", "queue", ",", "exchange", "=", "None", ",", "routing_key", "=", "None", ",", "serializer", "=", "serializer", ",", "heartbeat", "=", "heartbeat", ",", "time_to_wait", "=", "seconds_to_consume", ")", "log", ".", "info", "(", "\"end - {}\"", ".", "format", "(", "name", ")", ")"], "docstring": "consume_network_packet_messages_from_redis\n\n    Setup a ``celery_connectors.KombuSubscriber`` to consume meessages\n    from the ``FORWARD_BROKER_URL`` broker in the ``FORWARD_QUEUE``\n    queue.", "docstring_tokens": ["consume_network_packet_messages_from_redis"], "sha": "4e53ae13fe12085e0cf2e5e1aff947368f4f1ffa", "url": "https://github.com/jay-johnson/network-pipeline/blob/4e53ae13fe12085e0cf2e5e1aff947368f4f1ffa/network_pipeline/scripts/packets_redis.py#L41-L70", "partition": "train"}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/strip_masked.py", "func_name": "parse_masked", "original_string": "def parse_masked(seq, min_len):\n    \"\"\"\n    parse masked sequence into non-masked and masked regions\n    \"\"\"\n    nm, masked = [], [[]]\n    prev = None\n    for base in seq[1]:\n        if base.isupper():\n            nm.append(base)\n            if masked != [[]] and len(masked[-1]) < min_len:\n                nm.extend(masked[-1])\n                del masked[-1]\n            prev = False\n        elif base.islower():\n            if prev is False:\n                masked.append([])\n            masked[-1].append(base)\n            prev = True\n    return nm, masked", "language": "python", "code": "def parse_masked(seq, min_len):\n    \"\"\"\n    parse masked sequence into non-masked and masked regions\n    \"\"\"\n    nm, masked = [], [[]]\n    prev = None\n    for base in seq[1]:\n        if base.isupper():\n            nm.append(base)\n            if masked != [[]] and len(masked[-1]) < min_len:\n                nm.extend(masked[-1])\n                del masked[-1]\n            prev = False\n        elif base.islower():\n            if prev is False:\n                masked.append([])\n            masked[-1].append(base)\n            prev = True\n    return nm, masked", "code_tokens": ["def", "parse_masked", "(", "seq", ",", "min_len", ")", ":", "nm", ",", "masked", "=", "[", "]", ",", "[", "[", "]", "]", "prev", "=", "None", "for", "base", "in", "seq", "[", "1", "]", ":", "if", "base", ".", "isupper", "(", ")", ":", "nm", ".", "append", "(", "base", ")", "if", "masked", "!=", "[", "[", "]", "]", "and", "len", "(", "masked", "[", "-", "1", "]", ")", "<", "min_len", ":", "nm", ".", "extend", "(", "masked", "[", "-", "1", "]", ")", "del", "masked", "[", "-", "1", "]", "prev", "=", "False", "elif", "base", ".", "islower", "(", ")", ":", "if", "prev", "is", "False", ":", "masked", ".", "append", "(", "[", "]", ")", "masked", "[", "-", "1", "]", ".", "append", "(", "base", ")", "prev", "=", "True", "return", "nm", ",", "masked"], "docstring": "parse masked sequence into non-masked and masked regions", "docstring_tokens": ["parse", "masked", "sequence", "into", "non", "-", "masked", "and", "masked", "regions"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/strip_masked.py#L13-L31", "partition": "train"}
{"repo": "christophertbrown/bioscripts", "path": "ctbBio/strip_masked.py", "func_name": "strip_masked", "original_string": "def strip_masked(fasta, min_len, print_masked):\n    \"\"\"\n    remove masked regions from fasta file as long as\n    they are longer than min_len\n    \"\"\"\n    for seq in parse_fasta(fasta):\n        nm, masked = parse_masked(seq, min_len)\n        nm = ['%s removed_masked >=%s' % (seq[0], min_len), ''.join(nm)]\n        yield [0, nm]\n        if print_masked is True:\n            for i, m in enumerate([i for i in masked if i != []], 1):\n                m = ['%s insertion:%s' % (seq[0], i), ''.join(m)]\n                yield [1, m]", "language": "python", "code": "def strip_masked(fasta, min_len, print_masked):\n    \"\"\"\n    remove masked regions from fasta file as long as\n    they are longer than min_len\n    \"\"\"\n    for seq in parse_fasta(fasta):\n        nm, masked = parse_masked(seq, min_len)\n        nm = ['%s removed_masked >=%s' % (seq[0], min_len), ''.join(nm)]\n        yield [0, nm]\n        if print_masked is True:\n            for i, m in enumerate([i for i in masked if i != []], 1):\n                m = ['%s insertion:%s' % (seq[0], i), ''.join(m)]\n                yield [1, m]", "code_tokens": ["def", "strip_masked", "(", "fasta", ",", "min_len", ",", "print_masked", ")", ":", "for", "seq", "in", "parse_fasta", "(", "fasta", ")", ":", "nm", ",", "masked", "=", "parse_masked", "(", "seq", ",", "min_len", ")", "nm", "=", "[", "'%s removed_masked >=%s'", "%", "(", "seq", "[", "0", "]", ",", "min_len", ")", ",", "''", ".", "join", "(", "nm", ")", "]", "yield", "[", "0", ",", "nm", "]", "if", "print_masked", "is", "True", ":", "for", "i", ",", "m", "in", "enumerate", "(", "[", "i", "for", "i", "in", "masked", "if", "i", "!=", "[", "]", "]", ",", "1", ")", ":", "m", "=", "[", "'%s insertion:%s'", "%", "(", "seq", "[", "0", "]", ",", "i", ")", ",", "''", ".", "join", "(", "m", ")", "]", "yield", "[", "1", ",", "m", "]"], "docstring": "remove masked regions from fasta file as long as\n    they are longer than min_len", "docstring_tokens": ["remove", "masked", "regions", "from", "fasta", "file", "as", "long", "as", "they", "are", "longer", "than", "min_len"], "sha": "83b2566b3a5745437ec651cd6cafddd056846240", "url": "https://github.com/christophertbrown/bioscripts/blob/83b2566b3a5745437ec651cd6cafddd056846240/ctbBio/strip_masked.py#L33-L45", "partition": "train"}
{"repo": "smdabdoub/phylotoast", "path": "bin/network_plots_gephi.py", "func_name": "get_relative_abundance", "original_string": "def get_relative_abundance(biomfile):\n    \"\"\"\n    Return arcsine transformed relative abundance from a BIOM format file.\n\n    :type biomfile: BIOM format file\n    :param biomfile: BIOM format file used to obtain relative abundances for each OTU in\n                     a SampleID, which are used as node sizes in network plots.\n\n    :type return: Dictionary of dictionaries.\n    :return: Dictionary keyed on SampleID whose value is a dictionarykeyed on OTU Name\n             whose value is the arc sine tranfsormed relative abundance value for that\n             SampleID-OTU Name pair.\n    \"\"\"\n    biomf = biom.load_table(biomfile)\n    norm_biomf = biomf.norm(inplace=False)\n    rel_abd = {}\n    for sid in norm_biomf.ids():\n        rel_abd[sid] = {}\n        for otuid in norm_biomf.ids(\"observation\"):\n            otuname = oc.otu_name(norm_biomf.metadata(otuid, axis=\"observation\")[\"taxonomy\"])\n            otuname = \" \".join(otuname.split(\"_\"))\n            abd = norm_biomf.get_value_by_ids(otuid, sid)\n            rel_abd[sid][otuname] = abd\n    ast_rel_abd = bc.arcsine_sqrt_transform(rel_abd)\n    return ast_rel_abd", "language": "python", "code": "def get_relative_abundance(biomfile):\n    \"\"\"\n    Return arcsine transformed relative abundance from a BIOM format file.\n\n    :type biomfile: BIOM format file\n    :param biomfile: BIOM format file used to obtain relative abundances for each OTU in\n                     a SampleID, which are used as node sizes in network plots.\n\n    :type return: Dictionary of dictionaries.\n    :return: Dictionary keyed on SampleID whose value is a dictionarykeyed on OTU Name\n             whose value is the arc sine tranfsormed relative abundance value for that\n             SampleID-OTU Name pair.\n    \"\"\"\n    biomf = biom.load_table(biomfile)\n    norm_biomf = biomf.norm(inplace=False)\n    rel_abd = {}\n    for sid in norm_biomf.ids():\n        rel_abd[sid] = {}\n        for otuid in norm_biomf.ids(\"observation\"):\n            otuname = oc.otu_name(norm_biomf.metadata(otuid, axis=\"observation\")[\"taxonomy\"])\n            otuname = \" \".join(otuname.split(\"_\"))\n            abd = norm_biomf.get_value_by_ids(otuid, sid)\n            rel_abd[sid][otuname] = abd\n    ast_rel_abd = bc.arcsine_sqrt_transform(rel_abd)\n    return ast_rel_abd", "code_tokens": ["def", "get_relative_abundance", "(", "biomfile", ")", ":", "biomf", "=", "biom", ".", "load_table", "(", "biomfile", ")", "norm_biomf", "=", "biomf", ".", "norm", "(", "inplace", "=", "False", ")", "rel_abd", "=", "{", "}", "for", "sid", "in", "norm_biomf", ".", "ids", "(", ")", ":", "rel_abd", "[", "sid", "]", "=", "{", "}", "for", "otuid", "in", "norm_biomf", ".", "ids", "(", "\"observation\"", ")", ":", "otuname", "=", "oc", ".", "otu_name", "(", "norm_biomf", ".", "metadata", "(", "otuid", ",", "axis", "=", "\"observation\"", ")", "[", "\"taxonomy\"", "]", ")", "otuname", "=", "\" \"", ".", "join", "(", "otuname", ".", "split", "(", "\"_\"", ")", ")", "abd", "=", "norm_biomf", ".", "get_value_by_ids", "(", "otuid", ",", "sid", ")", "rel_abd", "[", "sid", "]", "[", "otuname", "]", "=", "abd", "ast_rel_abd", "=", "bc", ".", "arcsine_sqrt_transform", "(", "rel_abd", ")", "return", "ast_rel_abd"], "docstring": "Return arcsine transformed relative abundance from a BIOM format file.\n\n    :type biomfile: BIOM format file\n    :param biomfile: BIOM format file used to obtain relative abundances for each OTU in\n                     a SampleID, which are used as node sizes in network plots.\n\n    :type return: Dictionary of dictionaries.\n    :return: Dictionary keyed on SampleID whose value is a dictionarykeyed on OTU Name\n             whose value is the arc sine tranfsormed relative abundance value for that\n             SampleID-OTU Name pair.", "docstring_tokens": ["Return", "arcsine", "transformed", "relative", "abundance", "from", "a", "BIOM", "format", "file", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/network_plots_gephi.py#L33-L57", "partition": "train"}
{"repo": "jay-johnson/network-pipeline", "path": "network_pipeline/build_training_request.py", "func_name": "build_training_request", "original_string": "def build_training_request(\n        csv_file=ev(\n            \"CSV_FILE\",\n            \"/tmp/cleaned_attack_scans.csv\"),\n        meta_file=ev(\n            \"CSV_META_FILE\",\n            \"/tmp/cleaned_metadata.json\"),\n        predict_feature=ev(\n            \"PREDICT_FEATURE\",\n            \"label_value\"),\n        ignore_features=[\n            \"label_name\",\n            \"ip_src\",   # need to make this an int\n            \"ip_dst\",   # need to make this an int\n            \"eth_src\",  # need to make this an int\n            \"eth_dst\"   # need to make this an int\n        ],\n        seed=None,\n        test_size=float(ev(\n            \"TEST_SIZE\",\n            \"0.20\")),\n        preproc_rules=None):\n    \"\"\"build_training_request\n\n    :param csv_file: csv file built with prepare_dataset.py\n    :param meta_file: metadata file built with prepare_dataset.py\n    :param predict_feature: feature (column) to predict\n    :param ignore_features: features to remove from the csv\n                            before the split of test + train\n                            data\n    :param seed: integer to seed\n    :param test_size: percent of records to split into test\n                      vs train\n    :param preproc_rules: future preprocessing rules hooks\n    \"\"\"\n\n    last_step = \"not started\"\n    res = {\n        \"status\": INVALID,\n        \"err\": \"\",\n        \"csv_file\": csv_file,\n        \"meta_file\": meta_file,\n        \"meta_data\": None,\n        \"seed\": None,\n        \"test_size\": test_size,\n        \"predict_feature\": predict_feature,\n        \"features_to_process\": [],\n        \"ignore_features\": ignore_features,\n        \"X_train\": None,\n        \"X_test\": None,\n        \"Y_train\": None,\n        \"Y_test\": None\n    }\n\n    try:\n\n        last_step = (\"building seed={}\").format(\n                        seed)\n\n        log.debug(last_step)\n\n        use_seed = seed\n        if not use_seed:\n            use_seed = 9\n\n        res[\"seed\"] = np.random.seed(use_seed)\n\n        last_step = (\"Loading csv={}\").format(\n                        csv_file)\n\n        log.info(last_step)\n\n        if not os.path.exists(csv_file):\n            res[\"status\"] = ERROR\n            res[\"err\"] = (\"Unable to find csv_file={}\").format(\n                            csv_file)\n            log.error(res[\"err\"])\n            return res\n        # end of checking for a valid csv file on disk\n\n        if not os.path.exists(meta_file):\n            res[\"status\"] = ERROR\n            res[\"err\"] = (\"Unable to find meta_file={}\").format(\n                            meta_file)\n            log.error(res[\"err\"])\n            return res\n        # end of checking for a valid metadata file on disk\n\n        # load csv file into pandas dataframe\n        df = pd.read_csv(csv_file)\n\n        features_to_process = []\n        meta_data = {}\n\n        try:\n            last_step = (\"opening metadata={}\").format(\n                            meta_file)\n            log.debug(last_step)\n            meta_data = json.loads(\n                open(meta_file, \"r\").read()\n            )\n            res[\"meta_data\"] = meta_data\n            if \"post_proc_rules\" in meta_data:\n                if \"drop_columns\" in meta_data[\"post_proc_rules\"]:\n                    log.debug((\"Found drop_columns={}\")\n                              .format(\n                                meta_data[\"post_proc_rules\"][\"drop_columns\"]\n                              ))\n                    for ign in meta_data[\"post_proc_rules\"][\"drop_columns\"]:\n                        ignore_features.append(ign)\n        except Exception as e:\n            res[\"error\"] = (\"Failed building ignore_features: \"\n                            \"ignore_features={} meta={} meta_data={} \"\n                            \"last_step='{}' ex='{}'\").format(\n                                ignore_features,\n                                meta_file,\n                                meta_data,\n                                last_step,\n                                e)\n            log.error(res[\"error\"])\n            res[\"status\"] = ERROR\n            return res\n        # end of trying to lookup the meta data file\n        # for non-int/float features to ignore\n\n        last_step = (\"metadata={} df has \"\n                     \"columns={} ignore={}\").format(\n                        meta_file,\n                        df.columns.values,\n                        ignore_features)\n\n        log.info(last_step)\n\n        for feature in df.columns.values:\n            keep_it = True\n            for ign in ignore_features:\n                if feature == ign:\n                    keep_it = False\n            if keep_it:\n                if feature != predict_feature:\n                    features_to_process.append(feature)\n        # end of for all features to process\n\n        last_step = (\"Done post-procecessing \"\n                     \"Predicting={} with features={} \"\n                     \"ignore_features={} records={}\").format(\n                        predict_feature,\n                        features_to_process,\n                        ignore_features,\n                        len(df.index))\n\n        log.info(last_step)\n\n        res[\"predict_feature\"] = predict_feature\n\n        res[\"ignore_features\"] = []\n        for k in ignore_features:\n            if k not in res[\"ignore_features\"]:\n                res[\"ignore_features\"].append(k)\n        res[\"features_to_process\"] = []\n        for k in features_to_process:\n            if k not in res[\"features_to_process\"]:\n                if k != predict_feature:\n                    res[\"features_to_process\"].append(k)\n\n        # split the data into training\n        (res[\"X_train\"],\n         res[\"X_test\"],\n         res[\"Y_train\"],\n         res[\"Y_test\"]) = train_test_split(\n                            df[features_to_process],\n                            df[predict_feature],\n                            test_size=test_size,\n                            random_state=res[\"seed\"])\n\n        last_step = (\"Done splitting rows={} into \"\n                     \"X_train={} X_test={} \"\n                     \"Y_train={} Y_test={}\").format(\n                        len(df.index),\n                        len(res[\"X_train\"]),\n                        len(res[\"X_test\"]),\n                        len(res[\"Y_train\"]),\n                        len(res[\"Y_test\"]))\n\n        log.info((\"Success: {}\")\n                 .format(last_step))\n\n        res[\"err\"] = \"\"\n        res[\"status\"] = VALID\n    except Exception as e:\n        res[\"status\"] = ERROR\n        res[\"err\"] = (\"Failed build_training_request \"\n                      \"step='{}' with ex='{}'\").format(\n                        last_step,\n                        e)\n        log.error((\"build_training_request: {}\")\n                  .format(res[\"err\"]))\n    # end of try/ex\n\n    return res", "language": "python", "code": "def build_training_request(\n        csv_file=ev(\n            \"CSV_FILE\",\n            \"/tmp/cleaned_attack_scans.csv\"),\n        meta_file=ev(\n            \"CSV_META_FILE\",\n            \"/tmp/cleaned_metadata.json\"),\n        predict_feature=ev(\n            \"PREDICT_FEATURE\",\n            \"label_value\"),\n        ignore_features=[\n            \"label_name\",\n            \"ip_src\",   # need to make this an int\n            \"ip_dst\",   # need to make this an int\n            \"eth_src\",  # need to make this an int\n            \"eth_dst\"   # need to make this an int\n        ],\n        seed=None,\n        test_size=float(ev(\n            \"TEST_SIZE\",\n            \"0.20\")),\n        preproc_rules=None):\n    \"\"\"build_training_request\n\n    :param csv_file: csv file built with prepare_dataset.py\n    :param meta_file: metadata file built with prepare_dataset.py\n    :param predict_feature: feature (column) to predict\n    :param ignore_features: features to remove from the csv\n                            before the split of test + train\n                            data\n    :param seed: integer to seed\n    :param test_size: percent of records to split into test\n                      vs train\n    :param preproc_rules: future preprocessing rules hooks\n    \"\"\"\n\n    last_step = \"not started\"\n    res = {\n        \"status\": INVALID,\n        \"err\": \"\",\n        \"csv_file\": csv_file,\n        \"meta_file\": meta_file,\n        \"meta_data\": None,\n        \"seed\": None,\n        \"test_size\": test_size,\n        \"predict_feature\": predict_feature,\n        \"features_to_process\": [],\n        \"ignore_features\": ignore_features,\n        \"X_train\": None,\n        \"X_test\": None,\n        \"Y_train\": None,\n        \"Y_test\": None\n    }\n\n    try:\n\n        last_step = (\"building seed={}\").format(\n                        seed)\n\n        log.debug(last_step)\n\n        use_seed = seed\n        if not use_seed:\n            use_seed = 9\n\n        res[\"seed\"] = np.random.seed(use_seed)\n\n        last_step = (\"Loading csv={}\").format(\n                        csv_file)\n\n        log.info(last_step)\n\n        if not os.path.exists(csv_file):\n            res[\"status\"] = ERROR\n            res[\"err\"] = (\"Unable to find csv_file={}\").format(\n                            csv_file)\n            log.error(res[\"err\"])\n            return res\n        # end of checking for a valid csv file on disk\n\n        if not os.path.exists(meta_file):\n            res[\"status\"] = ERROR\n            res[\"err\"] = (\"Unable to find meta_file={}\").format(\n                            meta_file)\n            log.error(res[\"err\"])\n            return res\n        # end of checking for a valid metadata file on disk\n\n        # load csv file into pandas dataframe\n        df = pd.read_csv(csv_file)\n\n        features_to_process = []\n        meta_data = {}\n\n        try:\n            last_step = (\"opening metadata={}\").format(\n                            meta_file)\n            log.debug(last_step)\n            meta_data = json.loads(\n                open(meta_file, \"r\").read()\n            )\n            res[\"meta_data\"] = meta_data\n            if \"post_proc_rules\" in meta_data:\n                if \"drop_columns\" in meta_data[\"post_proc_rules\"]:\n                    log.debug((\"Found drop_columns={}\")\n                              .format(\n                                meta_data[\"post_proc_rules\"][\"drop_columns\"]\n                              ))\n                    for ign in meta_data[\"post_proc_rules\"][\"drop_columns\"]:\n                        ignore_features.append(ign)\n        except Exception as e:\n            res[\"error\"] = (\"Failed building ignore_features: \"\n                            \"ignore_features={} meta={} meta_data={} \"\n                            \"last_step='{}' ex='{}'\").format(\n                                ignore_features,\n                                meta_file,\n                                meta_data,\n                                last_step,\n                                e)\n            log.error(res[\"error\"])\n            res[\"status\"] = ERROR\n            return res\n        # end of trying to lookup the meta data file\n        # for non-int/float features to ignore\n\n        last_step = (\"metadata={} df has \"\n                     \"columns={} ignore={}\").format(\n                        meta_file,\n                        df.columns.values,\n                        ignore_features)\n\n        log.info(last_step)\n\n        for feature in df.columns.values:\n            keep_it = True\n            for ign in ignore_features:\n                if feature == ign:\n                    keep_it = False\n            if keep_it:\n                if feature != predict_feature:\n                    features_to_process.append(feature)\n        # end of for all features to process\n\n        last_step = (\"Done post-procecessing \"\n                     \"Predicting={} with features={} \"\n                     \"ignore_features={} records={}\").format(\n                        predict_feature,\n                        features_to_process,\n                        ignore_features,\n                        len(df.index))\n\n        log.info(last_step)\n\n        res[\"predict_feature\"] = predict_feature\n\n        res[\"ignore_features\"] = []\n        for k in ignore_features:\n            if k not in res[\"ignore_features\"]:\n                res[\"ignore_features\"].append(k)\n        res[\"features_to_process\"] = []\n        for k in features_to_process:\n            if k not in res[\"features_to_process\"]:\n                if k != predict_feature:\n                    res[\"features_to_process\"].append(k)\n\n        # split the data into training\n        (res[\"X_train\"],\n         res[\"X_test\"],\n         res[\"Y_train\"],\n         res[\"Y_test\"]) = train_test_split(\n                            df[features_to_process],\n                            df[predict_feature],\n                            test_size=test_size,\n                            random_state=res[\"seed\"])\n\n        last_step = (\"Done splitting rows={} into \"\n                     \"X_train={} X_test={} \"\n                     \"Y_train={} Y_test={}\").format(\n                        len(df.index),\n                        len(res[\"X_train\"]),\n                        len(res[\"X_test\"]),\n                        len(res[\"Y_train\"]),\n                        len(res[\"Y_test\"]))\n\n        log.info((\"Success: {}\")\n                 .format(last_step))\n\n        res[\"err\"] = \"\"\n        res[\"status\"] = VALID\n    except Exception as e:\n        res[\"status\"] = ERROR\n        res[\"err\"] = (\"Failed build_training_request \"\n                      \"step='{}' with ex='{}'\").format(\n                        last_step,\n                        e)\n        log.error((\"build_training_request: {}\")\n                  .format(res[\"err\"]))\n    # end of try/ex\n\n    return res", "code_tokens": ["def", "build_training_request", "(", "csv_file", "=", "ev", "(", "\"CSV_FILE\"", ",", "\"/tmp/cleaned_attack_scans.csv\"", ")", ",", "meta_file", "=", "ev", "(", "\"CSV_META_FILE\"", ",", "\"/tmp/cleaned_metadata.json\"", ")", ",", "predict_feature", "=", "ev", "(", "\"PREDICT_FEATURE\"", ",", "\"label_value\"", ")", ",", "ignore_features", "=", "[", "\"label_name\"", ",", "\"ip_src\"", ",", "# need to make this an int", "\"ip_dst\"", ",", "# need to make this an int", "\"eth_src\"", ",", "# need to make this an int", "\"eth_dst\"", "# need to make this an int", "]", ",", "seed", "=", "None", ",", "test_size", "=", "float", "(", "ev", "(", "\"TEST_SIZE\"", ",", "\"0.20\"", ")", ")", ",", "preproc_rules", "=", "None", ")", ":", "last_step", "=", "\"not started\"", "res", "=", "{", "\"status\"", ":", "INVALID", ",", "\"err\"", ":", "\"\"", ",", "\"csv_file\"", ":", "csv_file", ",", "\"meta_file\"", ":", "meta_file", ",", "\"meta_data\"", ":", "None", ",", "\"seed\"", ":", "None", ",", "\"test_size\"", ":", "test_size", ",", "\"predict_feature\"", ":", "predict_feature", ",", "\"features_to_process\"", ":", "[", "]", ",", "\"ignore_features\"", ":", "ignore_features", ",", "\"X_train\"", ":", "None", ",", "\"X_test\"", ":", "None", ",", "\"Y_train\"", ":", "None", ",", "\"Y_test\"", ":", "None", "}", "try", ":", "last_step", "=", "(", "\"building seed={}\"", ")", ".", "format", "(", "seed", ")", "log", ".", "debug", "(", "last_step", ")", "use_seed", "=", "seed", "if", "not", "use_seed", ":", "use_seed", "=", "9", "res", "[", "\"seed\"", "]", "=", "np", ".", "random", ".", "seed", "(", "use_seed", ")", "last_step", "=", "(", "\"Loading csv={}\"", ")", ".", "format", "(", "csv_file", ")", "log", ".", "info", "(", "last_step", ")", "if", "not", "os", ".", "path", ".", "exists", "(", "csv_file", ")", ":", "res", "[", "\"status\"", "]", "=", "ERROR", "res", "[", "\"err\"", "]", "=", "(", "\"Unable to find csv_file={}\"", ")", ".", "format", "(", "csv_file", ")", "log", ".", "error", "(", "res", "[", "\"err\"", "]", ")", "return", "res", "# end of checking for a valid csv file on disk", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_file", ")", ":", "res", "[", "\"status\"", "]", "=", "ERROR", "res", "[", "\"err\"", "]", "=", "(", "\"Unable to find meta_file={}\"", ")", ".", "format", "(", "meta_file", ")", "log", ".", "error", "(", "res", "[", "\"err\"", "]", ")", "return", "res", "# end of checking for a valid metadata file on disk", "# load csv file into pandas dataframe", "df", "=", "pd", ".", "read_csv", "(", "csv_file", ")", "features_to_process", "=", "[", "]", "meta_data", "=", "{", "}", "try", ":", "last_step", "=", "(", "\"opening metadata={}\"", ")", ".", "format", "(", "meta_file", ")", "log", ".", "debug", "(", "last_step", ")", "meta_data", "=", "json", ".", "loads", "(", "open", "(", "meta_file", ",", "\"r\"", ")", ".", "read", "(", ")", ")", "res", "[", "\"meta_data\"", "]", "=", "meta_data", "if", "\"post_proc_rules\"", "in", "meta_data", ":", "if", "\"drop_columns\"", "in", "meta_data", "[", "\"post_proc_rules\"", "]", ":", "log", ".", "debug", "(", "(", "\"Found drop_columns={}\"", ")", ".", "format", "(", "meta_data", "[", "\"post_proc_rules\"", "]", "[", "\"drop_columns\"", "]", ")", ")", "for", "ign", "in", "meta_data", "[", "\"post_proc_rules\"", "]", "[", "\"drop_columns\"", "]", ":", "ignore_features", ".", "append", "(", "ign", ")", "except", "Exception", "as", "e", ":", "res", "[", "\"error\"", "]", "=", "(", "\"Failed building ignore_features: \"", "\"ignore_features={} meta={} meta_data={} \"", "\"last_step='{}' ex='{}'\"", ")", ".", "format", "(", "ignore_features", ",", "meta_file", ",", "meta_data", ",", "last_step", ",", "e", ")", "log", ".", "error", "(", "res", "[", "\"error\"", "]", ")", "res", "[", "\"status\"", "]", "=", "ERROR", "return", "res", "# end of trying to lookup the meta data file", "# for non-int/float features to ignore", "last_step", "=", "(", "\"metadata={} df has \"", "\"columns={} ignore={}\"", ")", ".", "format", "(", "meta_file", ",", "df", ".", "columns", ".", "values", ",", "ignore_features", ")", "log", ".", "info", "(", "last_step", ")", "for", "feature", "in", "df", ".", "columns", ".", "values", ":", "keep_it", "=", "True", "for", "ign", "in", "ignore_features", ":", "if", "feature", "==", "ign", ":", "keep_it", "=", "False", "if", "keep_it", ":", "if", "feature", "!=", "predict_feature", ":", "features_to_process", ".", "append", "(", "feature", ")", "# end of for all features to process", "last_step", "=", "(", "\"Done post-procecessing \"", "\"Predicting={} with features={} \"", "\"ignore_features={} records={}\"", ")", ".", "format", "(", "predict_feature", ",", "features_to_process", ",", "ignore_features", ",", "len", "(", "df", ".", "index", ")", ")", "log", ".", "info", "(", "last_step", ")", "res", "[", "\"predict_feature\"", "]", "=", "predict_feature", "res", "[", "\"ignore_features\"", "]", "=", "[", "]", "for", "k", "in", "ignore_features", ":", "if", "k", "not", "in", "res", "[", "\"ignore_features\"", "]", ":", "res", "[", "\"ignore_features\"", "]", ".", "append", "(", "k", ")", "res", "[", "\"features_to_process\"", "]", "=", "[", "]", "for", "k", "in", "features_to_process", ":", "if", "k", "not", "in", "res", "[", "\"features_to_process\"", "]", ":", "if", "k", "!=", "predict_feature", ":", "res", "[", "\"features_to_process\"", "]", ".", "append", "(", "k", ")", "# split the data into training", "(", "res", "[", "\"X_train\"", "]", ",", "res", "[", "\"X_test\"", "]", ",", "res", "[", "\"Y_train\"", "]", ",", "res", "[", "\"Y_test\"", "]", ")", "=", "train_test_split", "(", "df", "[", "features_to_process", "]", ",", "df", "[", "predict_feature", "]", ",", "test_size", "=", "test_size", ",", "random_state", "=", "res", "[", "\"seed\"", "]", ")", "last_step", "=", "(", "\"Done splitting rows={} into \"", "\"X_train={} X_test={} \"", "\"Y_train={} Y_test={}\"", ")", ".", "format", "(", "len", "(", "df", ".", "index", ")", ",", "len", "(", "res", "[", "\"X_train\"", "]", ")", ",", "len", "(", "res", "[", "\"X_test\"", "]", ")", ",", "len", "(", "res", "[", "\"Y_train\"", "]", ")", ",", "len", "(", "res", "[", "\"Y_test\"", "]", ")", ")", "log", ".", "info", "(", "(", "\"Success: {}\"", ")", ".", "format", "(", "last_step", ")", ")", "res", "[", "\"err\"", "]", "=", "\"\"", "res", "[", "\"status\"", "]", "=", "VALID", "except", "Exception", "as", "e", ":", "res", "[", "\"status\"", "]", "=", "ERROR", "res", "[", "\"err\"", "]", "=", "(", "\"Failed build_training_request \"", "\"step='{}' with ex='{}'\"", ")", ".", "format", "(", "last_step", ",", "e", ")", "log", ".", "error", "(", "(", "\"build_training_request: {}\"", ")", ".", "format", "(", "res", "[", "\"err\"", "]", ")", ")", "# end of try/ex", "return", "res"], "docstring": "build_training_request\n\n    :param csv_file: csv file built with prepare_dataset.py\n    :param meta_file: metadata file built with prepare_dataset.py\n    :param predict_feature: feature (column) to predict\n    :param ignore_features: features to remove from the csv\n                            before the split of test + train\n                            data\n    :param seed: integer to seed\n    :param test_size: percent of records to split into test\n                      vs train\n    :param preproc_rules: future preprocessing rules hooks", "docstring_tokens": ["build_training_request"], "sha": "4e53ae13fe12085e0cf2e5e1aff947368f4f1ffa", "url": "https://github.com/jay-johnson/network-pipeline/blob/4e53ae13fe12085e0cf2e5e1aff947368f4f1ffa/network_pipeline/build_training_request.py#L17-L216", "partition": "train"}
{"repo": "smdabdoub/phylotoast", "path": "bin/iTol.py", "func_name": "find_otu", "original_string": "def find_otu(otuid, tree):\n    \"\"\"\n    Find an OTU ID in a Newick-format tree.\n    Return the starting position of the ID or None if not found.\n    \"\"\"\n    for m in re.finditer(otuid, tree):\n        before, after = tree[m.start()-1], tree[m.start()+len(otuid)]\n        if before in [\"(\", \",\", \")\"] and after in [\":\", \";\"]:\n            return m.start()\n    return None", "language": "python", "code": "def find_otu(otuid, tree):\n    \"\"\"\n    Find an OTU ID in a Newick-format tree.\n    Return the starting position of the ID or None if not found.\n    \"\"\"\n    for m in re.finditer(otuid, tree):\n        before, after = tree[m.start()-1], tree[m.start()+len(otuid)]\n        if before in [\"(\", \",\", \")\"] and after in [\":\", \";\"]:\n            return m.start()\n    return None", "code_tokens": ["def", "find_otu", "(", "otuid", ",", "tree", ")", ":", "for", "m", "in", "re", ".", "finditer", "(", "otuid", ",", "tree", ")", ":", "before", ",", "after", "=", "tree", "[", "m", ".", "start", "(", ")", "-", "1", "]", ",", "tree", "[", "m", ".", "start", "(", ")", "+", "len", "(", "otuid", ")", "]", "if", "before", "in", "[", "\"(\"", ",", "\",\"", ",", "\")\"", "]", "and", "after", "in", "[", "\":\"", ",", "\";\"", "]", ":", "return", "m", ".", "start", "(", ")", "return", "None"], "docstring": "Find an OTU ID in a Newick-format tree.\n    Return the starting position of the ID or None if not found.", "docstring_tokens": ["Find", "an", "OTU", "ID", "in", "a", "Newick", "-", "format", "tree", ".", "Return", "the", "starting", "position", "of", "the", "ID", "or", "None", "if", "not", "found", "."], "sha": "0b74ef171e6a84761710548501dfac71285a58a3", "url": "https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/bin/iTol.py#L17-L26", "partition": "train"}
{"repo": "LEMS/pylems", "path": "lems/parser/LEMS.py", "func_name": "LEMSFileParser.parse_on_entry", "original_string": "def parse_on_entry(self, node):\n        \"\"\"\n        Parses <OnEntry>\n\n        @param node: Node containing the <OnEntry> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        event_handler = OnEntry()\n\n        self.current_event_handler = event_handler\n        self.current_regime.add_event_handler(event_handler)\n\n        self.process_nested_tags(node)\n\n        self.current_event_handler = None", "language": "python", "code": "def parse_on_entry(self, node):\n        \"\"\"\n        Parses <OnEntry>\n\n        @param node: Node containing the <OnEntry> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        event_handler = OnEntry()\n\n        self.current_event_handler = event_handler\n        self.current_regime.add_event_handler(event_handler)\n\n        self.process_nested_tags(node)\n\n        self.current_event_handler = None", "code_tokens": ["def", "parse_on_entry", "(", "self", ",", "node", ")", ":", "event_handler", "=", "OnEntry", "(", ")", "self", ".", "current_event_handler", "=", "event_handler", "self", ".", "current_regime", ".", "add_event_handler", "(", "event_handler", ")", "self", ".", "process_nested_tags", "(", "node", ")", "self", ".", "current_event_handler", "=", "None"], "docstring": "Parses <OnEntry>\n\n        @param node: Node containing the <OnEntry> element\n        @type node: xml.etree.Element", "docstring_tokens": ["Parses", "<OnEntry", ">"], "sha": "4eeb719d2f23650fe16c38626663b69b5c83818b", "url": "https://github.com/LEMS/pylems/blob/4eeb719d2f23650fe16c38626663b69b5c83818b/lems/parser/LEMS.py#L1131-L1146", "partition": "train"}
{"repo": "LEMS/pylems", "path": "lems/parser/LEMS.py", "func_name": "LEMSFileParser.parse_on_event", "original_string": "def parse_on_event(self, node):\n        \"\"\"\n        Parses <OnEvent>\n\n        @param node: Node containing the <OnEvent> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        try:\n            port = node.lattrib['port']\n        except:\n            self.raise_error('<OnEvent> must specify a port.')\n            \n        event_handler = OnEvent(port)\n\n        self.current_regime.add_event_handler(event_handler)\n\n        self.current_event_handler = event_handler\n        self.process_nested_tags(node)\n        self.current_event_handler = None", "language": "python", "code": "def parse_on_event(self, node):\n        \"\"\"\n        Parses <OnEvent>\n\n        @param node: Node containing the <OnEvent> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        try:\n            port = node.lattrib['port']\n        except:\n            self.raise_error('<OnEvent> must specify a port.')\n            \n        event_handler = OnEvent(port)\n\n        self.current_regime.add_event_handler(event_handler)\n\n        self.current_event_handler = event_handler\n        self.process_nested_tags(node)\n        self.current_event_handler = None", "code_tokens": ["def", "parse_on_event", "(", "self", ",", "node", ")", ":", "try", ":", "port", "=", "node", ".", "lattrib", "[", "'port'", "]", "except", ":", "self", ".", "raise_error", "(", "'<OnEvent> must specify a port.'", ")", "event_handler", "=", "OnEvent", "(", "port", ")", "self", ".", "current_regime", ".", "add_event_handler", "(", "event_handler", ")", "self", ".", "current_event_handler", "=", "event_handler", "self", ".", "process_nested_tags", "(", "node", ")", "self", ".", "current_event_handler", "=", "None"], "docstring": "Parses <OnEvent>\n\n        @param node: Node containing the <OnEvent> element\n        @type node: xml.etree.Element", "docstring_tokens": ["Parses", "<OnEvent", ">"], "sha": "4eeb719d2f23650fe16c38626663b69b5c83818b", "url": "https://github.com/LEMS/pylems/blob/4eeb719d2f23650fe16c38626663b69b5c83818b/lems/parser/LEMS.py#L1148-L1167", "partition": "train"}
{"repo": "LEMS/pylems", "path": "lems/parser/LEMS.py", "func_name": "LEMSFileParser.parse_on_start", "original_string": "def parse_on_start(self, node):\n        \"\"\"\n        Parses <OnStart>\n\n        @param node: Node containing the <OnStart> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        event_handler = OnStart()\n\n        self.current_regime.add_event_handler(event_handler)\n\n        self.current_event_handler = event_handler\n        self.process_nested_tags(node)\n        self.current_event_handler = None", "language": "python", "code": "def parse_on_start(self, node):\n        \"\"\"\n        Parses <OnStart>\n\n        @param node: Node containing the <OnStart> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        event_handler = OnStart()\n\n        self.current_regime.add_event_handler(event_handler)\n\n        self.current_event_handler = event_handler\n        self.process_nested_tags(node)\n        self.current_event_handler = None", "code_tokens": ["def", "parse_on_start", "(", "self", ",", "node", ")", ":", "event_handler", "=", "OnStart", "(", ")", "self", ".", "current_regime", ".", "add_event_handler", "(", "event_handler", ")", "self", ".", "current_event_handler", "=", "event_handler", "self", ".", "process_nested_tags", "(", "node", ")", "self", ".", "current_event_handler", "=", "None"], "docstring": "Parses <OnStart>\n\n        @param node: Node containing the <OnStart> element\n        @type node: xml.etree.Element", "docstring_tokens": ["Parses", "<OnStart", ">"], "sha": "4eeb719d2f23650fe16c38626663b69b5c83818b", "url": "https://github.com/LEMS/pylems/blob/4eeb719d2f23650fe16c38626663b69b5c83818b/lems/parser/LEMS.py#L1169-L1183", "partition": "train"}
{"repo": "LEMS/pylems", "path": "lems/parser/LEMS.py", "func_name": "LEMSFileParser.parse_parameter", "original_string": "def parse_parameter(self, node):\n        \"\"\"\n        Parses <Parameter>\n\n        @param node: Node containing the <Parameter> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when the parameter does not have a name.\n        @raise ParseError: Raised when the parameter does not have a\n        dimension.\n        \"\"\"\n\n        if self.current_component_type == None:\n            self.raise_error('Parameters can only be defined in ' +\n                             'a component type')\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<Parameter> must specify a name')\n\n        try:\n            dimension = node.lattrib['dimension']\n        except:\n            self.raise_error(\"Parameter '{0}' has no dimension\",\n                             name)\n\n        parameter = Parameter(name, dimension)\n\n        self.current_component_type.add_parameter(parameter)", "language": "python", "code": "def parse_parameter(self, node):\n        \"\"\"\n        Parses <Parameter>\n\n        @param node: Node containing the <Parameter> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when the parameter does not have a name.\n        @raise ParseError: Raised when the parameter does not have a\n        dimension.\n        \"\"\"\n\n        if self.current_component_type == None:\n            self.raise_error('Parameters can only be defined in ' +\n                             'a component type')\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<Parameter> must specify a name')\n\n        try:\n            dimension = node.lattrib['dimension']\n        except:\n            self.raise_error(\"Parameter '{0}' has no dimension\",\n                             name)\n\n        parameter = Parameter(name, dimension)\n\n        self.current_component_type.add_parameter(parameter)", "code_tokens": ["def", "parse_parameter", "(", "self", ",", "node", ")", ":", "if", "self", ".", "current_component_type", "==", "None", ":", "self", ".", "raise_error", "(", "'Parameters can only be defined in '", "+", "'a component type'", ")", "try", ":", "name", "=", "node", ".", "lattrib", "[", "'name'", "]", "except", ":", "self", ".", "raise_error", "(", "'<Parameter> must specify a name'", ")", "try", ":", "dimension", "=", "node", ".", "lattrib", "[", "'dimension'", "]", "except", ":", "self", ".", "raise_error", "(", "\"Parameter '{0}' has no dimension\"", ",", "name", ")", "parameter", "=", "Parameter", "(", "name", ",", "dimension", ")", "self", ".", "current_component_type", ".", "add_parameter", "(", "parameter", ")"], "docstring": "Parses <Parameter>\n\n        @param node: Node containing the <Parameter> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when the parameter does not have a name.\n        @raise ParseError: Raised when the parameter does not have a\n        dimension.", "docstring_tokens": ["Parses", "<Parameter", ">"], "sha": "4eeb719d2f23650fe16c38626663b69b5c83818b", "url": "https://github.com/LEMS/pylems/blob/4eeb719d2f23650fe16c38626663b69b5c83818b/lems/parser/LEMS.py#L1185-L1214", "partition": "train"}
{"repo": "LEMS/pylems", "path": "lems/parser/LEMS.py", "func_name": "LEMSFileParser.parse_property", "original_string": "def parse_property(self, node):\n        \"\"\"\n        Parses <Property>\n\n        @param node: Node containing the <Property> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when the property does not have a name.\n        @raise ParseError: Raised when the property does not have a\n        dimension.\n        \"\"\"\n\n        if self.current_component_type == None:\n            self.raise_error('Property can only be defined in ' +\n                             'a component type')\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<Property> must specify a name')\n\n        try:\n            dimension = node.lattrib['dimension']\n        except:\n            self.raise_error(\"Property '{0}' has no dimension\",\n                             name)\n                             \n        default_value = node.lattrib.get('defaultvalue', None)\n        \n        property = Property(name, dimension, default_value=default_value)\n\n        self.current_component_type.add_property(property)", "language": "python", "code": "def parse_property(self, node):\n        \"\"\"\n        Parses <Property>\n\n        @param node: Node containing the <Property> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when the property does not have a name.\n        @raise ParseError: Raised when the property does not have a\n        dimension.\n        \"\"\"\n\n        if self.current_component_type == None:\n            self.raise_error('Property can only be defined in ' +\n                             'a component type')\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<Property> must specify a name')\n\n        try:\n            dimension = node.lattrib['dimension']\n        except:\n            self.raise_error(\"Property '{0}' has no dimension\",\n                             name)\n                             \n        default_value = node.lattrib.get('defaultvalue', None)\n        \n        property = Property(name, dimension, default_value=default_value)\n\n        self.current_component_type.add_property(property)", "code_tokens": ["def", "parse_property", "(", "self", ",", "node", ")", ":", "if", "self", ".", "current_component_type", "==", "None", ":", "self", ".", "raise_error", "(", "'Property can only be defined in '", "+", "'a component type'", ")", "try", ":", "name", "=", "node", ".", "lattrib", "[", "'name'", "]", "except", ":", "self", ".", "raise_error", "(", "'<Property> must specify a name'", ")", "try", ":", "dimension", "=", "node", ".", "lattrib", "[", "'dimension'", "]", "except", ":", "self", ".", "raise_error", "(", "\"Property '{0}' has no dimension\"", ",", "name", ")", "default_value", "=", "node", ".", "lattrib", ".", "get", "(", "'defaultvalue'", ",", "None", ")", "property", "=", "Property", "(", "name", ",", "dimension", ",", "default_value", "=", "default_value", ")", "self", ".", "current_component_type", ".", "add_property", "(", "property", ")"], "docstring": "Parses <Property>\n\n        @param node: Node containing the <Property> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when the property does not have a name.\n        @raise ParseError: Raised when the property does not have a\n        dimension.", "docstring_tokens": ["Parses", "<Property", ">"], "sha": "4eeb719d2f23650fe16c38626663b69b5c83818b", "url": "https://github.com/LEMS/pylems/blob/4eeb719d2f23650fe16c38626663b69b5c83818b/lems/parser/LEMS.py#L1216-L1247", "partition": "train"}
{"repo": "LEMS/pylems", "path": "lems/parser/LEMS.py", "func_name": "LEMSFileParser.parse_index_parameter", "original_string": "def parse_index_parameter(self, node):\n        \"\"\"\n        Parses <IndexParameter>\n\n        @param node: Node containing the <IndexParameter> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when the IndexParameter does not have a name.\n        \"\"\"\n\n        if self.current_component_type == None:\n            self.raise_error('IndexParameters can only be defined in ' +\n                             'a component type')\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<IndexParameter> must specify a name')\n\n\n        index_parameter = IndexParameter(name)\n\n        self.current_component_type.add_index_parameter(index_parameter)", "language": "python", "code": "def parse_index_parameter(self, node):\n        \"\"\"\n        Parses <IndexParameter>\n\n        @param node: Node containing the <IndexParameter> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when the IndexParameter does not have a name.\n        \"\"\"\n\n        if self.current_component_type == None:\n            self.raise_error('IndexParameters can only be defined in ' +\n                             'a component type')\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<IndexParameter> must specify a name')\n\n\n        index_parameter = IndexParameter(name)\n\n        self.current_component_type.add_index_parameter(index_parameter)", "code_tokens": ["def", "parse_index_parameter", "(", "self", ",", "node", ")", ":", "if", "self", ".", "current_component_type", "==", "None", ":", "self", ".", "raise_error", "(", "'IndexParameters can only be defined in '", "+", "'a component type'", ")", "try", ":", "name", "=", "node", ".", "lattrib", "[", "'name'", "]", "except", ":", "self", ".", "raise_error", "(", "'<IndexParameter> must specify a name'", ")", "index_parameter", "=", "IndexParameter", "(", "name", ")", "self", ".", "current_component_type", ".", "add_index_parameter", "(", "index_parameter", ")"], "docstring": "Parses <IndexParameter>\n\n        @param node: Node containing the <IndexParameter> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when the IndexParameter does not have a name.", "docstring_tokens": ["Parses", "<IndexParameter", ">"], "sha": "4eeb719d2f23650fe16c38626663b69b5c83818b", "url": "https://github.com/LEMS/pylems/blob/4eeb719d2f23650fe16c38626663b69b5c83818b/lems/parser/LEMS.py#L1250-L1272", "partition": "train"}
{"repo": "LEMS/pylems", "path": "lems/parser/LEMS.py", "func_name": "LEMSFileParser.parse_tunnel", "original_string": "def parse_tunnel(self, node):\n        \"\"\"\n        Parses <Tunnel>\n\n        @param node: Node containing the <Tunnel> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when the Tunnel does not have a name.\n        \"\"\"\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<Tunnel> must specify a name')\n        try:\n            end_a = node.lattrib['enda']\n        except:\n            self.raise_error('<Tunnel> must specify: endA')\n        try:\n            end_b = node.lattrib['enda']\n        except:\n            self.raise_error('<Tunnel> must specify: endB')\n        try:\n            component_a = node.lattrib['componenta']\n        except:\n            self.raise_error('<Tunnel> must specify: componentA')\n        try:\n            component_b = node.lattrib['componentb']\n        except:\n            self.raise_error('<Tunnel> must specify: componentB')\n\n\n        tunnel = Tunnel(name, end_a, end_b, component_a, component_b)\n\n        self.current_structure.add_tunnel(tunnel)", "language": "python", "code": "def parse_tunnel(self, node):\n        \"\"\"\n        Parses <Tunnel>\n\n        @param node: Node containing the <Tunnel> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when the Tunnel does not have a name.\n        \"\"\"\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<Tunnel> must specify a name')\n        try:\n            end_a = node.lattrib['enda']\n        except:\n            self.raise_error('<Tunnel> must specify: endA')\n        try:\n            end_b = node.lattrib['enda']\n        except:\n            self.raise_error('<Tunnel> must specify: endB')\n        try:\n            component_a = node.lattrib['componenta']\n        except:\n            self.raise_error('<Tunnel> must specify: componentA')\n        try:\n            component_b = node.lattrib['componentb']\n        except:\n            self.raise_error('<Tunnel> must specify: componentB')\n\n\n        tunnel = Tunnel(name, end_a, end_b, component_a, component_b)\n\n        self.current_structure.add_tunnel(tunnel)", "code_tokens": ["def", "parse_tunnel", "(", "self", ",", "node", ")", ":", "try", ":", "name", "=", "node", ".", "lattrib", "[", "'name'", "]", "except", ":", "self", ".", "raise_error", "(", "'<Tunnel> must specify a name'", ")", "try", ":", "end_a", "=", "node", ".", "lattrib", "[", "'enda'", "]", "except", ":", "self", ".", "raise_error", "(", "'<Tunnel> must specify: endA'", ")", "try", ":", "end_b", "=", "node", ".", "lattrib", "[", "'enda'", "]", "except", ":", "self", ".", "raise_error", "(", "'<Tunnel> must specify: endB'", ")", "try", ":", "component_a", "=", "node", ".", "lattrib", "[", "'componenta'", "]", "except", ":", "self", ".", "raise_error", "(", "'<Tunnel> must specify: componentA'", ")", "try", ":", "component_b", "=", "node", ".", "lattrib", "[", "'componentb'", "]", "except", ":", "self", ".", "raise_error", "(", "'<Tunnel> must specify: componentB'", ")", "tunnel", "=", "Tunnel", "(", "name", ",", "end_a", ",", "end_b", ",", "component_a", ",", "component_b", ")", "self", ".", "current_structure", ".", "add_tunnel", "(", "tunnel", ")"], "docstring": "Parses <Tunnel>\n\n        @param node: Node containing the <Tunnel> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when the Tunnel does not have a name.", "docstring_tokens": ["Parses", "<Tunnel", ">"], "sha": "4eeb719d2f23650fe16c38626663b69b5c83818b", "url": "https://github.com/LEMS/pylems/blob/4eeb719d2f23650fe16c38626663b69b5c83818b/lems/parser/LEMS.py#L1275-L1309", "partition": "train"}
{"repo": "LEMS/pylems", "path": "lems/parser/LEMS.py", "func_name": "LEMSFileParser.parse_path", "original_string": "def parse_path(self, node):\n        \"\"\"\n        Parses <Path>\n\n        @param node: Node containing the <Path> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error('<Path> must specify a name.')\n\n        description = node.lattrib.get('description', '')\n\n        self.current_component_type.add_path(Path(name, description))", "language": "python", "code": "def parse_path(self, node):\n        \"\"\"\n        Parses <Path>\n\n        @param node: Node containing the <Path> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error('<Path> must specify a name.')\n\n        description = node.lattrib.get('description', '')\n\n        self.current_component_type.add_path(Path(name, description))", "code_tokens": ["def", "parse_path", "(", "self", ",", "node", ")", ":", "if", "'name'", "in", "node", ".", "lattrib", ":", "name", "=", "node", ".", "lattrib", "[", "'name'", "]", "else", ":", "self", ".", "raise_error", "(", "'<Path> must specify a name.'", ")", "description", "=", "node", ".", "lattrib", ".", "get", "(", "'description'", ",", "''", ")", "self", ".", "current_component_type", ".", "add_path", "(", "Path", "(", "name", ",", "description", ")", ")"], "docstring": "Parses <Path>\n\n        @param node: Node containing the <Path> element\n        @type node: xml.etree.Element", "docstring_tokens": ["Parses", "<Path", ">"], "sha": "4eeb719d2f23650fe16c38626663b69b5c83818b", "url": "https://github.com/LEMS/pylems/blob/4eeb719d2f23650fe16c38626663b69b5c83818b/lems/parser/LEMS.py#L1312-L1327", "partition": "train"}
{"repo": "LEMS/pylems", "path": "lems/parser/LEMS.py", "func_name": "LEMSFileParser.parse_record", "original_string": "def parse_record(self, node):\n        \"\"\"\n        Parses <Record>\n\n        @param node: Node containing the <Record> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if self.current_simulation == None:\n            self.raise_error('<Record> must be only be used inside a ' +\n                             'simulation specification')\n\n        if 'quantity' in node.lattrib:\n            quantity = node.lattrib['quantity']\n        else:\n            self.raise_error('<Record> must specify a quantity.')\n\n        scale = node.lattrib.get('scale', None)\n        color  = node.lattrib.get('color', None)\n        id  = node.lattrib.get('id', None)\n\n        self.current_simulation.add_record(Record(quantity, scale, color, id))", "language": "python", "code": "def parse_record(self, node):\n        \"\"\"\n        Parses <Record>\n\n        @param node: Node containing the <Record> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if self.current_simulation == None:\n            self.raise_error('<Record> must be only be used inside a ' +\n                             'simulation specification')\n\n        if 'quantity' in node.lattrib:\n            quantity = node.lattrib['quantity']\n        else:\n            self.raise_error('<Record> must specify a quantity.')\n\n        scale = node.lattrib.get('scale', None)\n        color  = node.lattrib.get('color', None)\n        id  = node.lattrib.get('id', None)\n\n        self.current_simulation.add_record(Record(quantity, scale, color, id))", "code_tokens": ["def", "parse_record", "(", "self", ",", "node", ")", ":", "if", "self", ".", "current_simulation", "==", "None", ":", "self", ".", "raise_error", "(", "'<Record> must be only be used inside a '", "+", "'simulation specification'", ")", "if", "'quantity'", "in", "node", ".", "lattrib", ":", "quantity", "=", "node", ".", "lattrib", "[", "'quantity'", "]", "else", ":", "self", ".", "raise_error", "(", "'<Record> must specify a quantity.'", ")", "scale", "=", "node", ".", "lattrib", ".", "get", "(", "'scale'", ",", "None", ")", "color", "=", "node", ".", "lattrib", ".", "get", "(", "'color'", ",", "None", ")", "id", "=", "node", ".", "lattrib", ".", "get", "(", "'id'", ",", "None", ")", "self", ".", "current_simulation", ".", "add_record", "(", "Record", "(", "quantity", ",", "scale", ",", "color", ",", "id", ")", ")"], "docstring": "Parses <Record>\n\n        @param node: Node containing the <Record> element\n        @type node: xml.etree.Element", "docstring_tokens": ["Parses", "<Record", ">"], "sha": "4eeb719d2f23650fe16c38626663b69b5c83818b", "url": "https://github.com/LEMS/pylems/blob/4eeb719d2f23650fe16c38626663b69b5c83818b/lems/parser/LEMS.py#L1329-L1350", "partition": "train"}
{"repo": "LEMS/pylems", "path": "lems/parser/LEMS.py", "func_name": "LEMSFileParser.parse_event_record", "original_string": "def parse_event_record(self, node):\n        \"\"\"\n        Parses <EventRecord>\n\n        @param node: Node containing the <EventRecord> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if self.current_simulation == None:\n            self.raise_error('<EventRecord> must be only be used inside a ' +\n                             'simulation specification')\n\n        if 'quantity' in node.lattrib:\n            quantity = node.lattrib['quantity']\n        else:\n            self.raise_error('<EventRecord> must specify a quantity.')\n\n        if 'eventport' in node.lattrib:\n            eventPort = node.lattrib['eventport']\n        else:\n            self.raise_error('<EventRecord> must specify an eventPort.')\n\n\n        self.current_simulation.add_event_record(EventRecord(quantity, eventPort))", "language": "python", "code": "def parse_event_record(self, node):\n        \"\"\"\n        Parses <EventRecord>\n\n        @param node: Node containing the <EventRecord> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if self.current_simulation == None:\n            self.raise_error('<EventRecord> must be only be used inside a ' +\n                             'simulation specification')\n\n        if 'quantity' in node.lattrib:\n            quantity = node.lattrib['quantity']\n        else:\n            self.raise_error('<EventRecord> must specify a quantity.')\n\n        if 'eventport' in node.lattrib:\n            eventPort = node.lattrib['eventport']\n        else:\n            self.raise_error('<EventRecord> must specify an eventPort.')\n\n\n        self.current_simulation.add_event_record(EventRecord(quantity, eventPort))", "code_tokens": ["def", "parse_event_record", "(", "self", ",", "node", ")", ":", "if", "self", ".", "current_simulation", "==", "None", ":", "self", ".", "raise_error", "(", "'<EventRecord> must be only be used inside a '", "+", "'simulation specification'", ")", "if", "'quantity'", "in", "node", ".", "lattrib", ":", "quantity", "=", "node", ".", "lattrib", "[", "'quantity'", "]", "else", ":", "self", ".", "raise_error", "(", "'<EventRecord> must specify a quantity.'", ")", "if", "'eventport'", "in", "node", ".", "lattrib", ":", "eventPort", "=", "node", ".", "lattrib", "[", "'eventport'", "]", "else", ":", "self", ".", "raise_error", "(", "'<EventRecord> must specify an eventPort.'", ")", "self", ".", "current_simulation", ".", "add_event_record", "(", "EventRecord", "(", "quantity", ",", "eventPort", ")", ")"], "docstring": "Parses <EventRecord>\n\n        @param node: Node containing the <EventRecord> element\n        @type node: xml.etree.Element", "docstring_tokens": ["Parses", "<EventRecord", ">"], "sha": "4eeb719d2f23650fe16c38626663b69b5c83818b", "url": "https://github.com/LEMS/pylems/blob/4eeb719d2f23650fe16c38626663b69b5c83818b/lems/parser/LEMS.py#L1352-L1375", "partition": "train"}
{"repo": "LEMS/pylems", "path": "lems/parser/LEMS.py", "func_name": "LEMSFileParser.parse_regime", "original_string": "def parse_regime(self, node):\n        \"\"\"\n        Parses <Regime>\n\n        @param node: Node containing the <Behaviour> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            name = ''\n\n        if 'initial' in node.lattrib:\n            initial = (node.lattrib['initial'].strip().lower() == 'true')\n        else:\n            initial = False\n\n        regime = Regime(name, self.current_dynamics, initial)\n        old_regime = self.current_regime\n        self.current_dynamics.add_regime(regime)\n        self.current_regime = regime\n\n        self.process_nested_tags(node)\n\n        self.current_regime = old_regime", "language": "python", "code": "def parse_regime(self, node):\n        \"\"\"\n        Parses <Regime>\n\n        @param node: Node containing the <Behaviour> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            name = ''\n\n        if 'initial' in node.lattrib:\n            initial = (node.lattrib['initial'].strip().lower() == 'true')\n        else:\n            initial = False\n\n        regime = Regime(name, self.current_dynamics, initial)\n        old_regime = self.current_regime\n        self.current_dynamics.add_regime(regime)\n        self.current_regime = regime\n\n        self.process_nested_tags(node)\n\n        self.current_regime = old_regime", "code_tokens": ["def", "parse_regime", "(", "self", ",", "node", ")", ":", "if", "'name'", "in", "node", ".", "lattrib", ":", "name", "=", "node", ".", "lattrib", "[", "'name'", "]", "else", ":", "name", "=", "''", "if", "'initial'", "in", "node", ".", "lattrib", ":", "initial", "=", "(", "node", ".", "lattrib", "[", "'initial'", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", "==", "'true'", ")", "else", ":", "initial", "=", "False", "regime", "=", "Regime", "(", "name", ",", "self", ".", "current_dynamics", ",", "initial", ")", "old_regime", "=", "self", ".", "current_regime", "self", ".", "current_dynamics", ".", "add_regime", "(", "regime", ")", "self", ".", "current_regime", "=", "regime", "self", ".", "process_nested_tags", "(", "node", ")", "self", ".", "current_regime", "=", "old_regime"], "docstring": "Parses <Regime>\n\n        @param node: Node containing the <Behaviour> element\n        @type node: xml.etree.Element", "docstring_tokens": ["Parses", "<Regime", ">"], "sha": "4eeb719d2f23650fe16c38626663b69b5c83818b", "url": "https://github.com/LEMS/pylems/blob/4eeb719d2f23650fe16c38626663b69b5c83818b/lems/parser/LEMS.py#L1377-L1402", "partition": "train"}
{"repo": "Erotemic/utool", "path": "utool/util_sqlite.py", "func_name": "get_tablenames", "original_string": "def get_tablenames(cur):\n    \"\"\" Conveinience: \"\"\"\n    cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n    tablename_list_ = cur.fetchall()\n    tablename_list = [str(tablename[0]) for tablename in tablename_list_ ]\n    return tablename_list", "language": "python", "code": "def get_tablenames(cur):\n    \"\"\" Conveinience: \"\"\"\n    cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n    tablename_list_ = cur.fetchall()\n    tablename_list = [str(tablename[0]) for tablename in tablename_list_ ]\n    return tablename_list", "code_tokens": ["def", "get_tablenames", "(", "cur", ")", ":", "cur", ".", "execute", "(", "\"SELECT name FROM sqlite_master WHERE type='table'\"", ")", "tablename_list_", "=", "cur", ".", "fetchall", "(", ")", "tablename_list", "=", "[", "str", "(", "tablename", "[", "0", "]", ")", "for", "tablename", "in", "tablename_list_", "]", "return", "tablename_list"], "docstring": "Conveinience:", "docstring_tokens": ["Conveinience", ":"], "sha": "3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a", "url": "https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_sqlite.py#L9-L14", "partition": "train"}
{"repo": "Erotemic/utool", "path": "utool/util_sqlite.py", "func_name": "get_table_csv", "original_string": "def get_table_csv(cur, tablename, exclude_columns=[]):\n    \"\"\" Conveinience: Converts a tablename to csv format\n\n    Args:\n        tablename (str):\n        exclude_columns (list):\n\n    Returns:\n        str: csv_table\n\n    CommandLine:\n        python -m ibeis.control.SQLDatabaseControl --test-get_table_csv\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from ibeis.control.SQLDatabaseControl import *  # NOQA\n        >>> # build test data\n        >>> import ibeis\n        >>> ibs = ibeis.opendb('testdb1')\n        >>> db = ibs.db\n        >>> tablename = ibeis.const.NAME_TABLE\n        >>> exclude_columns = []\n        >>> # execute function\n        >>> csv_table = db.get_table_csv(tablename, exclude_columns)\n        >>> # verify results\n        >>> result = str(csv_table)\n        >>> print(result)\n    \"\"\"\n    import utool as ut\n    colnames_ = ut.get_table_columnname_list(cur, tablename)\n    colnames = tuple([colname for colname in colnames_ if colname not in exclude_columns])\n    row_list = ut.get_table_rows(cur, tablename, colnames, unpack=False)\n    column_list = zip(*row_list)\n    #=None, column_list=[], header='', column_type=None\n    #import utool as ut\n    #column_list, column_names = db.get_table_column_data(tablename, exclude_columns)\n    # remove column prefix for more compact csvs\n    column_lbls = [name.replace(tablename[:-1] + '_', '') for name in colnames]\n    #header = db.get_table_csv_header(tablename)\n    header = ''\n    csv_table = ut.make_csv_table(column_list, column_lbls, header)\n    return csv_table", "language": "python", "code": "def get_table_csv(cur, tablename, exclude_columns=[]):\n    \"\"\" Conveinience: Converts a tablename to csv format\n\n    Args:\n        tablename (str):\n        exclude_columns (list):\n\n    Returns:\n        str: csv_table\n\n    CommandLine:\n        python -m ibeis.control.SQLDatabaseControl --test-get_table_csv\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from ibeis.control.SQLDatabaseControl import *  # NOQA\n        >>> # build test data\n        >>> import ibeis\n        >>> ibs = ibeis.opendb('testdb1')\n        >>> db = ibs.db\n        >>> tablename = ibeis.const.NAME_TABLE\n        >>> exclude_columns = []\n        >>> # execute function\n        >>> csv_table = db.get_table_csv(tablename, exclude_columns)\n        >>> # verify results\n        >>> result = str(csv_table)\n        >>> print(result)\n    \"\"\"\n    import utool as ut\n    colnames_ = ut.get_table_columnname_list(cur, tablename)\n    colnames = tuple([colname for colname in colnames_ if colname not in exclude_columns])\n    row_list = ut.get_table_rows(cur, tablename, colnames, unpack=False)\n    column_list = zip(*row_list)\n    #=None, column_list=[], header='', column_type=None\n    #import utool as ut\n    #column_list, column_names = db.get_table_column_data(tablename, exclude_columns)\n    # remove column prefix for more compact csvs\n    column_lbls = [name.replace(tablename[:-1] + '_', '') for name in colnames]\n    #header = db.get_table_csv_header(tablename)\n    header = ''\n    csv_table = ut.make_csv_table(column_list, column_lbls, header)\n    return csv_table", "code_tokens": ["def", "get_table_csv", "(", "cur", ",", "tablename", ",", "exclude_columns", "=", "[", "]", ")", ":", "import", "utool", "as", "ut", "colnames_", "=", "ut", ".", "get_table_columnname_list", "(", "cur", ",", "tablename", ")", "colnames", "=", "tuple", "(", "[", "colname", "for", "colname", "in", "colnames_", "if", "colname", "not", "in", "exclude_columns", "]", ")", "row_list", "=", "ut", ".", "get_table_rows", "(", "cur", ",", "tablename", ",", "colnames", ",", "unpack", "=", "False", ")", "column_list", "=", "zip", "(", "*", "row_list", ")", "#=None, column_list=[], header='', column_type=None", "#import utool as ut", "#column_list, column_names = db.get_table_column_data(tablename, exclude_columns)", "# remove column prefix for more compact csvs", "column_lbls", "=", "[", "name", ".", "replace", "(", "tablename", "[", ":", "-", "1", "]", "+", "'_'", ",", "''", ")", "for", "name", "in", "colnames", "]", "#header = db.get_table_csv_header(tablename)", "header", "=", "''", "csv_table", "=", "ut", ".", "make_csv_table", "(", "column_list", ",", "column_lbls", ",", "header", ")", "return", "csv_table"], "docstring": "Conveinience: Converts a tablename to csv format\n\n    Args:\n        tablename (str):\n        exclude_columns (list):\n\n    Returns:\n        str: csv_table\n\n    CommandLine:\n        python -m ibeis.control.SQLDatabaseControl --test-get_table_csv\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from ibeis.control.SQLDatabaseControl import *  # NOQA\n        >>> # build test data\n        >>> import ibeis\n        >>> ibs = ibeis.opendb('testdb1')\n        >>> db = ibs.db\n        >>> tablename = ibeis.const.NAME_TABLE\n        >>> exclude_columns = []\n        >>> # execute function\n        >>> csv_table = db.get_table_csv(tablename, exclude_columns)\n        >>> # verify results\n        >>> result = str(csv_table)\n        >>> print(result)", "docstring_tokens": ["Conveinience", ":", "Converts", "a", "tablename", "to", "csv", "format"], "sha": "3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a", "url": "https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_sqlite.py#L28-L69", "partition": "train"}
{"repo": "Erotemic/utool", "path": "utool/util_sqlite.py", "func_name": "get_table_columninfo_list", "original_string": "def get_table_columninfo_list(cur, tablename):\n    \"\"\"\n    Args:\n        tablename (str): table name\n\n    Returns:\n        column_list : list of tuples with format:\n            (\n                [0] column_id  : id of the column\n                [1] name       : the name of the column\n                [2] type_      : the type of the column (TEXT, INT, etc...)\n                [3] notnull    : 0 or 1 if the column can contains null values\n                [4] dflt_value : the default value\n                [5] pk         : 0 or 1 if the column partecipate to the primary key\n            )\n\n    References:\n        http://stackoverflow.com/questions/17717829/how-to-get-column-names-from-a-table-in-sqlite-via-pragma-net-c\n\n    CommandLine:\n        python -m utool.util_sqlite --test-get_table_columninfo_list\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_sqlite import *  # NOQA\n    \"\"\"\n    cur.execute('PRAGMA TABLE_INFO(\"{tablename}\")'.format(tablename=tablename))\n    colinfo_list = cur.fetchall()\n    colrichinfo_list = [SQLColumnRichInfo(*colinfo) for colinfo in colinfo_list]\n    return colrichinfo_list", "language": "python", "code": "def get_table_columninfo_list(cur, tablename):\n    \"\"\"\n    Args:\n        tablename (str): table name\n\n    Returns:\n        column_list : list of tuples with format:\n            (\n                [0] column_id  : id of the column\n                [1] name       : the name of the column\n                [2] type_      : the type of the column (TEXT, INT, etc...)\n                [3] notnull    : 0 or 1 if the column can contains null values\n                [4] dflt_value : the default value\n                [5] pk         : 0 or 1 if the column partecipate to the primary key\n            )\n\n    References:\n        http://stackoverflow.com/questions/17717829/how-to-get-column-names-from-a-table-in-sqlite-via-pragma-net-c\n\n    CommandLine:\n        python -m utool.util_sqlite --test-get_table_columninfo_list\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_sqlite import *  # NOQA\n    \"\"\"\n    cur.execute('PRAGMA TABLE_INFO(\"{tablename}\")'.format(tablename=tablename))\n    colinfo_list = cur.fetchall()\n    colrichinfo_list = [SQLColumnRichInfo(*colinfo) for colinfo in colinfo_list]\n    return colrichinfo_list", "code_tokens": ["def", "get_table_columninfo_list", "(", "cur", ",", "tablename", ")", ":", "cur", ".", "execute", "(", "'PRAGMA TABLE_INFO(\"{tablename}\")'", ".", "format", "(", "tablename", "=", "tablename", ")", ")", "colinfo_list", "=", "cur", ".", "fetchall", "(", ")", "colrichinfo_list", "=", "[", "SQLColumnRichInfo", "(", "*", "colinfo", ")", "for", "colinfo", "in", "colinfo_list", "]", "return", "colrichinfo_list"], "docstring": "Args:\n        tablename (str): table name\n\n    Returns:\n        column_list : list of tuples with format:\n            (\n                [0] column_id  : id of the column\n                [1] name       : the name of the column\n                [2] type_      : the type of the column (TEXT, INT, etc...)\n                [3] notnull    : 0 or 1 if the column can contains null values\n                [4] dflt_value : the default value\n                [5] pk         : 0 or 1 if the column partecipate to the primary key\n            )\n\n    References:\n        http://stackoverflow.com/questions/17717829/how-to-get-column-names-from-a-table-in-sqlite-via-pragma-net-c\n\n    CommandLine:\n        python -m utool.util_sqlite --test-get_table_columninfo_list\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_sqlite import *  # NOQA", "docstring_tokens": ["Args", ":", "tablename", "(", "str", ")", ":", "table", "name"], "sha": "3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a", "url": "https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_sqlite.py#L77-L106", "partition": "train"}
{"repo": "timedata-org/loady", "path": "loady/code.py", "func_name": "load_location", "original_string": "def load_location(url, base_path=None, module=False):\n    \"\"\"\n    Read a single Python file in as code and extract members from it.\n\n    Args:\n        url -- a URL either absolute (contains ':') or relative\n        base_path -- if url is relative, base_path is prepended to it.\n\n    The resulting URL needs to look something like this:\n        https://github.com/foo/bar/blob/master/bibliopixel/myfile.MyClass\n    \"\"\"\n    if base_path and ':' not in url:\n        slashes = base_path.endswith('/') + url.startswith('/')\n        if slashes == 0:\n            url = base_path + '/' + url\n        elif slashes == 1:\n            url = base_path + url\n        else:\n            url = base_path[:-1] + url\n\n    slash = url.rfind('/')\n    url_root, filepath = url[:slash + 1], url[slash + 1:]\n    filename, *python_path = filepath.split('.')\n\n    whitelist.check_url(url_root)\n\n    file_url = url_root + filename + '.py'\n    source = data.load(file_url, False)\n    compiled = compile(source, file_url, mode='exec')\n    local = {}\n    exec(compiled, local)\n\n    try:\n        names = local['__all__']\n    except KeyError:\n        names = local\n\n    if python_path and python_path[0] == 'py':\n        python_path.pop(0)\n\n    if not python_path:\n        if module:\n            return local\n        python_path = [importer.guess_name(names, filename, url)]\n\n    first, *rest = python_path\n\n    try:\n        result = local[first]\n    except:\n        raise AttributeError(first)\n\n    for r in rest:\n        result = getattr(result, r)\n\n    return result", "language": "python", "code": "def load_location(url, base_path=None, module=False):\n    \"\"\"\n    Read a single Python file in as code and extract members from it.\n\n    Args:\n        url -- a URL either absolute (contains ':') or relative\n        base_path -- if url is relative, base_path is prepended to it.\n\n    The resulting URL needs to look something like this:\n        https://github.com/foo/bar/blob/master/bibliopixel/myfile.MyClass\n    \"\"\"\n    if base_path and ':' not in url:\n        slashes = base_path.endswith('/') + url.startswith('/')\n        if slashes == 0:\n            url = base_path + '/' + url\n        elif slashes == 1:\n            url = base_path + url\n        else:\n            url = base_path[:-1] + url\n\n    slash = url.rfind('/')\n    url_root, filepath = url[:slash + 1], url[slash + 1:]\n    filename, *python_path = filepath.split('.')\n\n    whitelist.check_url(url_root)\n\n    file_url = url_root + filename + '.py'\n    source = data.load(file_url, False)\n    compiled = compile(source, file_url, mode='exec')\n    local = {}\n    exec(compiled, local)\n\n    try:\n        names = local['__all__']\n    except KeyError:\n        names = local\n\n    if python_path and python_path[0] == 'py':\n        python_path.pop(0)\n\n    if not python_path:\n        if module:\n            return local\n        python_path = [importer.guess_name(names, filename, url)]\n\n    first, *rest = python_path\n\n    try:\n        result = local[first]\n    except:\n        raise AttributeError(first)\n\n    for r in rest:\n        result = getattr(result, r)\n\n    return result", "code_tokens": ["def", "load_location", "(", "url", ",", "base_path", "=", "None", ",", "module", "=", "False", ")", ":", "if", "base_path", "and", "':'", "not", "in", "url", ":", "slashes", "=", "base_path", ".", "endswith", "(", "'/'", ")", "+", "url", ".", "startswith", "(", "'/'", ")", "if", "slashes", "==", "0", ":", "url", "=", "base_path", "+", "'/'", "+", "url", "elif", "slashes", "==", "1", ":", "url", "=", "base_path", "+", "url", "else", ":", "url", "=", "base_path", "[", ":", "-", "1", "]", "+", "url", "slash", "=", "url", ".", "rfind", "(", "'/'", ")", "url_root", ",", "filepath", "=", "url", "[", ":", "slash", "+", "1", "]", ",", "url", "[", "slash", "+", "1", ":", "]", "filename", ",", "", "*", "python_path", "=", "filepath", ".", "split", "(", "'.'", ")", "whitelist", ".", "check_url", "(", "url_root", ")", "file_url", "=", "url_root", "+", "filename", "+", "'.py'", "source", "=", "data", ".", "load", "(", "file_url", ",", "False", ")", "compiled", "=", "compile", "(", "source", ",", "file_url", ",", "mode", "=", "'exec'", ")", "local", "=", "{", "}", "exec", "(", "compiled", ",", "local", ")", "try", ":", "names", "=", "local", "[", "'__all__'", "]", "except", "KeyError", ":", "names", "=", "local", "if", "python_path", "and", "python_path", "[", "0", "]", "==", "'py'", ":", "python_path", ".", "pop", "(", "0", ")", "if", "not", "python_path", ":", "if", "module", ":", "return", "local", "python_path", "=", "[", "importer", ".", "guess_name", "(", "names", ",", "filename", ",", "url", ")", "]", "first", ",", "", "*", "rest", "=", "python_path", "try", ":", "result", "=", "local", "[", "first", "]", "except", ":", "raise", "AttributeError", "(", "first", ")", "for", "r", "in", "rest", ":", "result", "=", "getattr", "(", "result", ",", "r", ")", "return", "result"], "docstring": "Read a single Python file in as code and extract members from it.\n\n    Args:\n        url -- a URL either absolute (contains ':') or relative\n        base_path -- if url is relative, base_path is prepended to it.\n\n    The resulting URL needs to look something like this:\n        https://github.com/foo/bar/blob/master/bibliopixel/myfile.MyClass", "docstring_tokens": ["Read", "a", "single", "Python", "file", "in", "as", "code", "and", "extract", "members", "from", "it", "."], "sha": "94ffcdb92f15a28f3c85f77bd293a9cb59de4cad", "url": "https://github.com/timedata-org/loady/blob/94ffcdb92f15a28f3c85f77bd293a9cb59de4cad/loady/code.py#L5-L60", "partition": "train"}
{"repo": "LEMS/pylems", "path": "lems/model/dynamics.py", "func_name": "Dynamics.add", "original_string": "def add(self, child):\n        \"\"\"\n        Adds a typed child object to the dynamics object.\n\n        @param child: Child object to be added.\n        \"\"\"\n\n        if isinstance(child, Regime):\n            self.add_regime(child)\n        else:\n            Behavioral.add(self, child)", "language": "python", "code": "def add(self, child):\n        \"\"\"\n        Adds a typed child object to the dynamics object.\n\n        @param child: Child object to be added.\n        \"\"\"\n\n        if isinstance(child, Regime):\n            self.add_regime(child)\n        else:\n            Behavioral.add(self, child)", "code_tokens": ["def", "add", "(", "self", ",", "child", ")", ":", "if", "isinstance", "(", "child", ",", "Regime", ")", ":", "self", ".", "add_regime", "(", "child", ")", "else", ":", "Behavioral", ".", "add", "(", "self", ",", "child", ")"], "docstring": "Adds a typed child object to the dynamics object.\n\n        @param child: Child object to be added.", "docstring_tokens": ["Adds", "a", "typed", "child", "object", "to", "the", "dynamics", "object", "."], "sha": "4eeb719d2f23650fe16c38626663b69b5c83818b", "url": "https://github.com/LEMS/pylems/blob/4eeb719d2f23650fe16c38626663b69b5c83818b/lems/model/dynamics.py#L876-L886", "partition": "train"}
{"repo": "glormph/msstitch", "path": "src/app/actions/mslookup/biosets.py", "func_name": "create_bioset_lookup", "original_string": "def create_bioset_lookup(lookupdb, spectrafns, set_names):\n    \"\"\"Fills lookup database with biological set names\"\"\"\n    unique_setnames = set(set_names)\n    lookupdb.store_biosets(((x,) for x in unique_setnames))\n    set_id_map = lookupdb.get_setnames()\n    mzmlfiles = ((os.path.basename(fn), set_id_map[setname])\n                 for fn, setname in zip(spectrafns, set_names))\n    lookupdb.store_mzmlfiles(mzmlfiles)\n    lookupdb.index_biosets()", "language": "python", "code": "def create_bioset_lookup(lookupdb, spectrafns, set_names):\n    \"\"\"Fills lookup database with biological set names\"\"\"\n    unique_setnames = set(set_names)\n    lookupdb.store_biosets(((x,) for x in unique_setnames))\n    set_id_map = lookupdb.get_setnames()\n    mzmlfiles = ((os.path.basename(fn), set_id_map[setname])\n                 for fn, setname in zip(spectrafns, set_names))\n    lookupdb.store_mzmlfiles(mzmlfiles)\n    lookupdb.index_biosets()", "code_tokens": ["def", "create_bioset_lookup", "(", "lookupdb", ",", "spectrafns", ",", "set_names", ")", ":", "unique_setnames", "=", "set", "(", "set_names", ")", "lookupdb", ".", "store_biosets", "(", "(", "(", "x", ",", ")", "for", "x", "in", "unique_setnames", ")", ")", "set_id_map", "=", "lookupdb", ".", "get_setnames", "(", ")", "mzmlfiles", "=", "(", "(", "os", ".", "path", ".", "basename", "(", "fn", ")", ",", "set_id_map", "[", "setname", "]", ")", "for", "fn", ",", "setname", "in", "zip", "(", "spectrafns", ",", "set_names", ")", ")", "lookupdb", ".", "store_mzmlfiles", "(", "mzmlfiles", ")", "lookupdb", ".", "index_biosets", "(", ")"], "docstring": "Fills lookup database with biological set names", "docstring_tokens": ["Fills", "lookup", "database", "with", "biological", "set", "names"], "sha": "ded7e5cbd813d7797dc9d42805778266e59ff042", "url": "https://github.com/glormph/msstitch/blob/ded7e5cbd813d7797dc9d42805778266e59ff042/src/app/actions/mslookup/biosets.py#L4-L12", "partition": "train"}
{"repo": "chriso/gauged", "path": "gauged/drivers/sqlite.py", "func_name": "SQLiteDriver.replace_blocks", "original_string": "def replace_blocks(self, blocks):\n        \"\"\"Replace multiple blocks. blocks must be a list of tuples where\n        each tuple consists of (namespace, offset, key, data, flags)\"\"\"\n        start = 0\n        bulk_insert = self.bulk_insert\n        blocks_len = len(blocks)\n        select = 'SELECT ?,?,?,?,?'\n        query = 'REPLACE INTO gauged_data (namespace, offset, `key`, ' \\\n            'data, flags) '\n        execute = self.cursor.execute\n        while start < blocks_len:\n            rows = blocks[start:start+bulk_insert]\n            params = [param for params in rows for param in params]\n            insert = (select + ' UNION ') * (len(rows) - 1) + select\n            execute(query + insert, params)\n            start += bulk_insert", "language": "python", "code": "def replace_blocks(self, blocks):\n        \"\"\"Replace multiple blocks. blocks must be a list of tuples where\n        each tuple consists of (namespace, offset, key, data, flags)\"\"\"\n        start = 0\n        bulk_insert = self.bulk_insert\n        blocks_len = len(blocks)\n        select = 'SELECT ?,?,?,?,?'\n        query = 'REPLACE INTO gauged_data (namespace, offset, `key`, ' \\\n            'data, flags) '\n        execute = self.cursor.execute\n        while start < blocks_len:\n            rows = blocks[start:start+bulk_insert]\n            params = [param for params in rows for param in params]\n            insert = (select + ' UNION ') * (len(rows) - 1) + select\n            execute(query + insert, params)\n            start += bulk_insert", "code_tokens": ["def", "replace_blocks", "(", "self", ",", "blocks", ")", ":", "start", "=", "0", "bulk_insert", "=", "self", ".", "bulk_insert", "blocks_len", "=", "len", "(", "blocks", ")", "select", "=", "'SELECT ?,?,?,?,?'", "query", "=", "'REPLACE INTO gauged_data (namespace, offset, `key`, '", "'data, flags) '", "execute", "=", "self", ".", "cursor", ".", "execute", "while", "start", "<", "blocks_len", ":", "rows", "=", "blocks", "[", "start", ":", "start", "+", "bulk_insert", "]", "params", "=", "[", "param", "for", "params", "in", "rows", "for", "param", "in", "params", "]", "insert", "=", "(", "select", "+", "' UNION '", ")", "*", "(", "len", "(", "rows", ")", "-", "1", ")", "+", "select", "execute", "(", "query", "+", "insert", ",", "params", ")", "start", "+=", "bulk_insert"], "docstring": "Replace multiple blocks. blocks must be a list of tuples where\n        each tuple consists of (namespace, offset, key, data, flags)", "docstring_tokens": ["Replace", "multiple", "blocks", ".", "blocks", "must", "be", "a", "list", "of", "tuples", "where", "each", "tuple", "consists", "of", "(", "namespace", "offset", "key", "data", "flags", ")"], "sha": "cda3bba2f3e92ce2fb4aa92132dcc0e689bf7976", "url": "https://github.com/chriso/gauged/blob/cda3bba2f3e92ce2fb4aa92132dcc0e689bf7976/gauged/drivers/sqlite.py#L90-L105", "partition": "train"}
{"repo": "chriso/gauged", "path": "gauged/drivers/sqlite.py", "func_name": "SQLiteDriver.insert_or_append_blocks", "original_string": "def insert_or_append_blocks(self, blocks):\n        \"\"\"Insert multiple blocks. If a block already exists, the data is\n        appended. blocks must be a list of tuples where each tuple consists\n        of (namespace, offset, key, data)\"\"\"\n        start = 0\n        bulk_insert = self.bulk_insert\n        blocks_len = len(blocks)\n        select = 'SELECT ?,?,?,\"\",0'\n        query = 'INSERT OR IGNORE INTO gauged_data (namespace, offset, ' \\\n            '`key`, data, flags) '\n        execute = self.cursor.execute\n        while start < blocks_len:\n            rows = blocks[start:start+bulk_insert]\n            params = []\n            for namespace, offset, key, _, _ in rows:\n                params.extend((namespace, offset, key))\n            insert = (select + ' UNION ') * (len(rows) - 1) + select\n            execute(query + insert, params)\n            start += bulk_insert\n        for namespace, offset, key, data, flags in blocks:\n            execute('UPDATE gauged_data SET data = CAST(data || ? AS BLOB),'\n                    'flags = ? WHERE namespace = ? AND offset = ? AND '\n                    '`key` = ?', (data, flags, namespace, offset, key))", "language": "python", "code": "def insert_or_append_blocks(self, blocks):\n        \"\"\"Insert multiple blocks. If a block already exists, the data is\n        appended. blocks must be a list of tuples where each tuple consists\n        of (namespace, offset, key, data)\"\"\"\n        start = 0\n        bulk_insert = self.bulk_insert\n        blocks_len = len(blocks)\n        select = 'SELECT ?,?,?,\"\",0'\n        query = 'INSERT OR IGNORE INTO gauged_data (namespace, offset, ' \\\n            '`key`, data, flags) '\n        execute = self.cursor.execute\n        while start < blocks_len:\n            rows = blocks[start:start+bulk_insert]\n            params = []\n            for namespace, offset, key, _, _ in rows:\n                params.extend((namespace, offset, key))\n            insert = (select + ' UNION ') * (len(rows) - 1) + select\n            execute(query + insert, params)\n            start += bulk_insert\n        for namespace, offset, key, data, flags in blocks:\n            execute('UPDATE gauged_data SET data = CAST(data || ? AS BLOB),'\n                    'flags = ? WHERE namespace = ? AND offset = ? AND '\n                    '`key` = ?', (data, flags, namespace, offset, key))", "code_tokens": ["def", "insert_or_append_blocks", "(", "self", ",", "blocks", ")", ":", "start", "=", "0", "bulk_insert", "=", "self", ".", "bulk_insert", "blocks_len", "=", "len", "(", "blocks", ")", "select", "=", "'SELECT ?,?,?,\"\",0'", "query", "=", "'INSERT OR IGNORE INTO gauged_data (namespace, offset, '", "'`key`, data, flags) '", "execute", "=", "self", ".", "cursor", ".", "execute", "while", "start", "<", "blocks_len", ":", "rows", "=", "blocks", "[", "start", ":", "start", "+", "bulk_insert", "]", "params", "=", "[", "]", "for", "namespace", ",", "offset", ",", "key", ",", "_", ",", "_", "in", "rows", ":", "params", ".", "extend", "(", "(", "namespace", ",", "offset", ",", "key", ")", ")", "insert", "=", "(", "select", "+", "' UNION '", ")", "*", "(", "len", "(", "rows", ")", "-", "1", ")", "+", "select", "execute", "(", "query", "+", "insert", ",", "params", ")", "start", "+=", "bulk_insert", "for", "namespace", ",", "offset", ",", "key", ",", "data", ",", "flags", "in", "blocks", ":", "execute", "(", "'UPDATE gauged_data SET data = CAST(data || ? AS BLOB),'", "'flags = ? WHERE namespace = ? AND offset = ? AND '", "'`key` = ?'", ",", "(", "data", ",", "flags", ",", "namespace", ",", "offset", ",", "key", ")", ")"], "docstring": "Insert multiple blocks. If a block already exists, the data is\n        appended. blocks must be a list of tuples where each tuple consists\n        of (namespace, offset, key, data)", "docstring_tokens": ["Insert", "multiple", "blocks", ".", "If", "a", "block", "already", "exists", "the", "data", "is", "appended", ".", "blocks", "must", "be", "a", "list", "of", "tuples", "where", "each", "tuple", "consists", "of", "(", "namespace", "offset", "key", "data", ")"], "sha": "cda3bba2f3e92ce2fb4aa92132dcc0e689bf7976", "url": "https://github.com/chriso/gauged/blob/cda3bba2f3e92ce2fb4aa92132dcc0e689bf7976/gauged/drivers/sqlite.py#L107-L129", "partition": "train"}
{"repo": "chriso/gauged", "path": "gauged/drivers/sqlite.py", "func_name": "SQLiteDriver.get_cache", "original_string": "def get_cache(self, namespace, query_hash, length, start, end):\n        \"\"\"Get a cached value for the specified date range and query\"\"\"\n        query = 'SELECT start, value FROM gauged_cache WHERE namespace = ? ' \\\n            'AND hash = ? AND length = ? AND start BETWEEN ? AND ?'\n        cursor = self.cursor\n        cursor.execute(query, (namespace, query_hash, length, start, end))\n        return tuple(cursor.fetchall())", "language": "python", "code": "def get_cache(self, namespace, query_hash, length, start, end):\n        \"\"\"Get a cached value for the specified date range and query\"\"\"\n        query = 'SELECT start, value FROM gauged_cache WHERE namespace = ? ' \\\n            'AND hash = ? AND length = ? AND start BETWEEN ? AND ?'\n        cursor = self.cursor\n        cursor.execute(query, (namespace, query_hash, length, start, end))\n        return tuple(cursor.fetchall())", "code_tokens": ["def", "get_cache", "(", "self", ",", "namespace", ",", "query_hash", ",", "length", ",", "start", ",", "end", ")", ":", "query", "=", "'SELECT start, value FROM gauged_cache WHERE namespace = ? '", "'AND hash = ? AND length = ? AND start BETWEEN ? AND ?'", "cursor", "=", "self", ".", "cursor", "cursor", ".", "execute", "(", "query", ",", "(", "namespace", ",", "query_hash", ",", "length", ",", "start", ",", "end", ")", ")", "return", "tuple", "(", "cursor", ".", "fetchall", "(", ")", ")"], "docstring": "Get a cached value for the specified date range and query", "docstring_tokens": ["Get", "a", "cached", "value", "for", "the", "specified", "date", "range", "and", "query"], "sha": "cda3bba2f3e92ce2fb4aa92132dcc0e689bf7976", "url": "https://github.com/chriso/gauged/blob/cda3bba2f3e92ce2fb4aa92132dcc0e689bf7976/gauged/drivers/sqlite.py#L237-L243", "partition": "train"}
{"repo": "ColinDuquesnoy/QCrash", "path": "qcrash/_dialogs/review.py", "func_name": "DlgReview.review", "original_string": "def review(cls, content, log, parent, window_icon):  # pragma: no cover\n        \"\"\"\n        Reviews the final bug report.\n\n        :param content: content of the final report, before review\n        :param parent: parent widget\n\n        :returns: the reviewed report content or None if the review was\n                  canceled.\n        \"\"\"\n        dlg = DlgReview(content, log, parent, window_icon)\n        if dlg.exec_():\n            return dlg.ui.edit_main.toPlainText(), \\\n                dlg.ui.edit_log.toPlainText()\n        return None, None", "language": "python", "code": "def review(cls, content, log, parent, window_icon):  # pragma: no cover\n        \"\"\"\n        Reviews the final bug report.\n\n        :param content: content of the final report, before review\n        :param parent: parent widget\n\n        :returns: the reviewed report content or None if the review was\n                  canceled.\n        \"\"\"\n        dlg = DlgReview(content, log, parent, window_icon)\n        if dlg.exec_():\n            return dlg.ui.edit_main.toPlainText(), \\\n                dlg.ui.edit_log.toPlainText()\n        return None, None", "code_tokens": ["def", "review", "(", "cls", ",", "content", ",", "log", ",", "parent", ",", "window_icon", ")", ":", "# pragma: no cover", "dlg", "=", "DlgReview", "(", "content", ",", "log", ",", "parent", ",", "window_icon", ")", "if", "dlg", ".", "exec_", "(", ")", ":", "return", "dlg", ".", "ui", ".", "edit_main", ".", "toPlainText", "(", ")", ",", "dlg", ".", "ui", ".", "edit_log", ".", "toPlainText", "(", ")", "return", "None", ",", "None"], "docstring": "Reviews the final bug report.\n\n        :param content: content of the final report, before review\n        :param parent: parent widget\n\n        :returns: the reviewed report content or None if the review was\n                  canceled.", "docstring_tokens": ["Reviews", "the", "final", "bug", "report", "."], "sha": "775e1b15764e2041a8f9a08bea938e4d6ce817c7", "url": "https://github.com/ColinDuquesnoy/QCrash/blob/775e1b15764e2041a8f9a08bea938e4d6ce817c7/qcrash/_dialogs/review.py#L44-L58", "partition": "train"}
{"repo": "rhazdon/django-sonic-screwdriver", "path": "django_sonic_screwdriver/version/version.py", "func_name": "Version.get_version", "original_string": "def get_version():\n        \"\"\"\n        Return version from setup.py\n        \"\"\"\n        version_desc = open(os.path.join(os.path.abspath(APISettings.VERSION_FILE)))\n        version_file = version_desc.read()\n\n        try:\n            version = re.search(r\"version=['\\\"]([^'\\\"]+)['\\\"]\", version_file).group(1)\n            return version\n        except FileNotFoundError:\n            Shell.fail('File not found!')\n            raise FileNotFoundError\n        except ValueError:\n            Shell.fail('Version not found in file ' + version_file + '!')\n            raise ValueError\n        finally:\n            version_desc.close()", "language": "python", "code": "def get_version():\n        \"\"\"\n        Return version from setup.py\n        \"\"\"\n        version_desc = open(os.path.join(os.path.abspath(APISettings.VERSION_FILE)))\n        version_file = version_desc.read()\n\n        try:\n            version = re.search(r\"version=['\\\"]([^'\\\"]+)['\\\"]\", version_file).group(1)\n            return version\n        except FileNotFoundError:\n            Shell.fail('File not found!')\n            raise FileNotFoundError\n        except ValueError:\n            Shell.fail('Version not found in file ' + version_file + '!')\n            raise ValueError\n        finally:\n            version_desc.close()", "code_tokens": ["def", "get_version", "(", ")", ":", "version_desc", "=", "open", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "abspath", "(", "APISettings", ".", "VERSION_FILE", ")", ")", ")", "version_file", "=", "version_desc", ".", "read", "(", ")", "try", ":", "version", "=", "re", ".", "search", "(", "r\"version=['\\\"]([^'\\\"]+)['\\\"]\"", ",", "version_file", ")", ".", "group", "(", "1", ")", "return", "version", "except", "FileNotFoundError", ":", "Shell", ".", "fail", "(", "'File not found!'", ")", "raise", "FileNotFoundError", "except", "ValueError", ":", "Shell", ".", "fail", "(", "'Version not found in file '", "+", "version_file", "+", "'!'", ")", "raise", "ValueError", "finally", ":", "version_desc", ".", "close", "(", ")"], "docstring": "Return version from setup.py", "docstring_tokens": ["Return", "version", "from", "setup", ".", "py"], "sha": "89e885e8c1322fc5c3e0f79b03a55acdc6e63972", "url": "https://github.com/rhazdon/django-sonic-screwdriver/blob/89e885e8c1322fc5c3e0f79b03a55acdc6e63972/django_sonic_screwdriver/version/version.py#L13-L30", "partition": "train"}
{"repo": "rhazdon/django-sonic-screwdriver", "path": "django_sonic_screwdriver/version/version.py", "func_name": "Version.set_version", "original_string": "def set_version(old_version, new_version):\n        \"\"\"\n        Write new version into VERSION_FILE\n        \"\"\"\n        try:\n            if APISettings.DEBUG:\n                Shell.debug('* ' + old_version + ' --> ' + new_version)\n                return True\n\n            for line in fileinput.input(os.path.abspath(APISettings.VERSION_FILE), inplace=True):\n                print(line.replace(old_version, new_version), end='')\n            Shell.success('* ' + old_version + ' --> ' + new_version)\n        except FileNotFoundError:\n            Shell.warn('File not found!')", "language": "python", "code": "def set_version(old_version, new_version):\n        \"\"\"\n        Write new version into VERSION_FILE\n        \"\"\"\n        try:\n            if APISettings.DEBUG:\n                Shell.debug('* ' + old_version + ' --> ' + new_version)\n                return True\n\n            for line in fileinput.input(os.path.abspath(APISettings.VERSION_FILE), inplace=True):\n                print(line.replace(old_version, new_version), end='')\n            Shell.success('* ' + old_version + ' --> ' + new_version)\n        except FileNotFoundError:\n            Shell.warn('File not found!')", "code_tokens": ["def", "set_version", "(", "old_version", ",", "new_version", ")", ":", "try", ":", "if", "APISettings", ".", "DEBUG", ":", "Shell", ".", "debug", "(", "'* '", "+", "old_version", "+", "' --> '", "+", "new_version", ")", "return", "True", "for", "line", "in", "fileinput", ".", "input", "(", "os", ".", "path", ".", "abspath", "(", "APISettings", ".", "VERSION_FILE", ")", ",", "inplace", "=", "True", ")", ":", "print", "(", "line", ".", "replace", "(", "old_version", ",", "new_version", ")", ",", "end", "=", "''", ")", "Shell", ".", "success", "(", "'* '", "+", "old_version", "+", "' --> '", "+", "new_version", ")", "except", "FileNotFoundError", ":", "Shell", ".", "warn", "(", "'File not found!'", ")"], "docstring": "Write new version into VERSION_FILE", "docstring_tokens": ["Write", "new", "version", "into", "VERSION_FILE"], "sha": "89e885e8c1322fc5c3e0f79b03a55acdc6e63972", "url": "https://github.com/rhazdon/django-sonic-screwdriver/blob/89e885e8c1322fc5c3e0f79b03a55acdc6e63972/django_sonic_screwdriver/version/version.py#L33-L46", "partition": "train"}
{"repo": "rhazdon/django-sonic-screwdriver", "path": "django_sonic_screwdriver/version/version.py", "func_name": "Version.set_major", "original_string": "def set_major(self):\n        \"\"\"\n        Increment the major number of project\n        \"\"\"\n        old_version = self.get_version()\n        new_version = str(int(old_version.split('.', 5)[0])+1) + '.0.0'\n        self.set_version(old_version, new_version)", "language": "python", "code": "def set_major(self):\n        \"\"\"\n        Increment the major number of project\n        \"\"\"\n        old_version = self.get_version()\n        new_version = str(int(old_version.split('.', 5)[0])+1) + '.0.0'\n        self.set_version(old_version, new_version)", "code_tokens": ["def", "set_major", "(", "self", ")", ":", "old_version", "=", "self", ".", "get_version", "(", ")", "new_version", "=", "str", "(", "int", "(", "old_version", ".", "split", "(", "'.'", ",", "5", ")", "[", "0", "]", ")", "+", "1", ")", "+", "'.0.0'", "self", ".", "set_version", "(", "old_version", ",", "new_version", ")"], "docstring": "Increment the major number of project", "docstring_tokens": ["Increment", "the", "major", "number", "of", "project"], "sha": "89e885e8c1322fc5c3e0f79b03a55acdc6e63972", "url": "https://github.com/rhazdon/django-sonic-screwdriver/blob/89e885e8c1322fc5c3e0f79b03a55acdc6e63972/django_sonic_screwdriver/version/version.py#L71-L77", "partition": "train"}
{"repo": "rhazdon/django-sonic-screwdriver", "path": "django_sonic_screwdriver/version/version.py", "func_name": "Version.set_minor", "original_string": "def set_minor(self):\n        \"\"\"\n        Increment the minor number of project\n        \"\"\"\n        old_version = self.get_version()\n        new_version = str(int(old_version.split('.', 5)[0])) + '.' + \\\n            str(int(old_version.split('.', 5)[1])+1) + '.0'\n        self.set_version(old_version, new_version)", "language": "python", "code": "def set_minor(self):\n        \"\"\"\n        Increment the minor number of project\n        \"\"\"\n        old_version = self.get_version()\n        new_version = str(int(old_version.split('.', 5)[0])) + '.' + \\\n            str(int(old_version.split('.', 5)[1])+1) + '.0'\n        self.set_version(old_version, new_version)", "code_tokens": ["def", "set_minor", "(", "self", ")", ":", "old_version", "=", "self", ".", "get_version", "(", ")", "new_version", "=", "str", "(", "int", "(", "old_version", ".", "split", "(", "'.'", ",", "5", ")", "[", "0", "]", ")", ")", "+", "'.'", "+", "str", "(", "int", "(", "old_version", ".", "split", "(", "'.'", ",", "5", ")", "[", "1", "]", ")", "+", "1", ")", "+", "'.0'", "self", ".", "set_version", "(", "old_version", ",", "new_version", ")"], "docstring": "Increment the minor number of project", "docstring_tokens": ["Increment", "the", "minor", "number", "of", "project"], "sha": "89e885e8c1322fc5c3e0f79b03a55acdc6e63972", "url": "https://github.com/rhazdon/django-sonic-screwdriver/blob/89e885e8c1322fc5c3e0f79b03a55acdc6e63972/django_sonic_screwdriver/version/version.py#L79-L86", "partition": "train"}
{"repo": "rhazdon/django-sonic-screwdriver", "path": "django_sonic_screwdriver/version/version.py", "func_name": "Version.set_patch", "original_string": "def set_patch(self, pre_release_tag=''):\n        \"\"\"\n        Increment the patch number of project\n\n        :var release_tag describes the tag ('a', 'b', 'rc', ...)\n        :var release_tag_version describes the number behind the 'a', 'b' or 'rc'\n        For e.g.:\n        \"\"\"\n\n        current_version = self.get_version()\n        current_patch = self.get_patch_version(current_version)\n        current_pre_release_tag = self.get_current_pre_release_tag(current_patch)\n        current_RELEASE_SEPARATOR = self.get_current_RELEASE_SEPARATOR(current_patch)\n        new_patch = ''\n\n        # The new patch should get a release tag\n        if pre_release_tag:\n\n            # Check, if the current patch already contains a pre_release_tag.\n            if current_pre_release_tag:\n                new_patch = str(current_patch.split(current_pre_release_tag, 2)[0]) + pre_release_tag\n\n                if pre_release_tag == current_pre_release_tag:\n                    new_patch += str(int(current_patch.split(current_pre_release_tag, 2)[1])+1)\n                else:\n                    new_patch += '0'\n\n            # The current patch does not contains a pre_release_tag.\n            else:\n                new_patch = str(int(current_patch)+1) + \\\n                            APISettings.RELEASE_SEPARATOR + \\\n                            pre_release_tag + \\\n                            '0'\n\n        # The new patch should not contain any tag. So just increase it.\n        else:\n            if current_RELEASE_SEPARATOR:\n                new_patch = str(int(current_patch.split(current_RELEASE_SEPARATOR, 2)[0])+1)\n            elif current_pre_release_tag:\n                new_patch = str(int(current_patch.split(current_pre_release_tag, 2)[0])+1)\n            else:\n                new_patch = str(int(current_patch)+1)\n\n        new_version = str(int(current_version.split('.', 5)[0])) + '.' + \\\n            str(int(current_version.split('.', 5)[1])) + '.' + \\\n            str(new_patch)\n        self.set_version(current_version, new_version)", "language": "python", "code": "def set_patch(self, pre_release_tag=''):\n        \"\"\"\n        Increment the patch number of project\n\n        :var release_tag describes the tag ('a', 'b', 'rc', ...)\n        :var release_tag_version describes the number behind the 'a', 'b' or 'rc'\n        For e.g.:\n        \"\"\"\n\n        current_version = self.get_version()\n        current_patch = self.get_patch_version(current_version)\n        current_pre_release_tag = self.get_current_pre_release_tag(current_patch)\n        current_RELEASE_SEPARATOR = self.get_current_RELEASE_SEPARATOR(current_patch)\n        new_patch = ''\n\n        # The new patch should get a release tag\n        if pre_release_tag:\n\n            # Check, if the current patch already contains a pre_release_tag.\n            if current_pre_release_tag:\n                new_patch = str(current_patch.split(current_pre_release_tag, 2)[0]) + pre_release_tag\n\n                if pre_release_tag == current_pre_release_tag:\n                    new_patch += str(int(current_patch.split(current_pre_release_tag, 2)[1])+1)\n                else:\n                    new_patch += '0'\n\n            # The current patch does not contains a pre_release_tag.\n            else:\n                new_patch = str(int(current_patch)+1) + \\\n                            APISettings.RELEASE_SEPARATOR + \\\n                            pre_release_tag + \\\n                            '0'\n\n        # The new patch should not contain any tag. So just increase it.\n        else:\n            if current_RELEASE_SEPARATOR:\n                new_patch = str(int(current_patch.split(current_RELEASE_SEPARATOR, 2)[0])+1)\n            elif current_pre_release_tag:\n                new_patch = str(int(current_patch.split(current_pre_release_tag, 2)[0])+1)\n            else:\n                new_patch = str(int(current_patch)+1)\n\n        new_version = str(int(current_version.split('.', 5)[0])) + '.' + \\\n            str(int(current_version.split('.', 5)[1])) + '.' + \\\n            str(new_patch)\n        self.set_version(current_version, new_version)", "code_tokens": ["def", "set_patch", "(", "self", ",", "pre_release_tag", "=", "''", ")", ":", "current_version", "=", "self", ".", "get_version", "(", ")", "current_patch", "=", "self", ".", "get_patch_version", "(", "current_version", ")", "current_pre_release_tag", "=", "self", ".", "get_current_pre_release_tag", "(", "current_patch", ")", "current_RELEASE_SEPARATOR", "=", "self", ".", "get_current_RELEASE_SEPARATOR", "(", "current_patch", ")", "new_patch", "=", "''", "# The new patch should get a release tag", "if", "pre_release_tag", ":", "# Check, if the current patch already contains a pre_release_tag.", "if", "current_pre_release_tag", ":", "new_patch", "=", "str", "(", "current_patch", ".", "split", "(", "current_pre_release_tag", ",", "2", ")", "[", "0", "]", ")", "+", "pre_release_tag", "if", "pre_release_tag", "==", "current_pre_release_tag", ":", "new_patch", "+=", "str", "(", "int", "(", "current_patch", ".", "split", "(", "current_pre_release_tag", ",", "2", ")", "[", "1", "]", ")", "+", "1", ")", "else", ":", "new_patch", "+=", "'0'", "# The current patch does not contains a pre_release_tag.", "else", ":", "new_patch", "=", "str", "(", "int", "(", "current_patch", ")", "+", "1", ")", "+", "APISettings", ".", "RELEASE_SEPARATOR", "+", "pre_release_tag", "+", "'0'", "# The new patch should not contain any tag. So just increase it.", "else", ":", "if", "current_RELEASE_SEPARATOR", ":", "new_patch", "=", "str", "(", "int", "(", "current_patch", ".", "split", "(", "current_RELEASE_SEPARATOR", ",", "2", ")", "[", "0", "]", ")", "+", "1", ")", "elif", "current_pre_release_tag", ":", "new_patch", "=", "str", "(", "int", "(", "current_patch", ".", "split", "(", "current_pre_release_tag", ",", "2", ")", "[", "0", "]", ")", "+", "1", ")", "else", ":", "new_patch", "=", "str", "(", "int", "(", "current_patch", ")", "+", "1", ")", "new_version", "=", "str", "(", "int", "(", "current_version", ".", "split", "(", "'.'", ",", "5", ")", "[", "0", "]", ")", ")", "+", "'.'", "+", "str", "(", "int", "(", "current_version", ".", "split", "(", "'.'", ",", "5", ")", "[", "1", "]", ")", ")", "+", "'.'", "+", "str", "(", "new_patch", ")", "self", ".", "set_version", "(", "current_version", ",", "new_version", ")"], "docstring": "Increment the patch number of project\n\n        :var release_tag describes the tag ('a', 'b', 'rc', ...)\n        :var release_tag_version describes the number behind the 'a', 'b' or 'rc'\n        For e.g.:", "docstring_tokens": ["Increment", "the", "patch", "number", "of", "project"], "sha": "89e885e8c1322fc5c3e0f79b03a55acdc6e63972", "url": "https://github.com/rhazdon/django-sonic-screwdriver/blob/89e885e8c1322fc5c3e0f79b03a55acdc6e63972/django_sonic_screwdriver/version/version.py#L88-L134", "partition": "train"}
{"repo": "dsoprea/NsqSpinner", "path": "nsq/connection.py", "func_name": "_Buffer.flush", "original_string": "def flush(self):\n        \"\"\"Return all buffered data, and clear the stack.\"\"\"\n\n        (slice_, self.__buffer) = (self.__buffer, '')\n        self.__size = 0\n\n        return slice_", "language": "python", "code": "def flush(self):\n        \"\"\"Return all buffered data, and clear the stack.\"\"\"\n\n        (slice_, self.__buffer) = (self.__buffer, '')\n        self.__size = 0\n\n        return slice_", "code_tokens": ["def", "flush", "(", "self", ")", ":", "(", "slice_", ",", "self", ".", "__buffer", ")", "=", "(", "self", ".", "__buffer", ",", "''", ")", "self", ".", "__size", "=", "0", "return", "slice_"], "docstring": "Return all buffered data, and clear the stack.", "docstring_tokens": ["Return", "all", "buffered", "data", "and", "clear", "the", "stack", "."], "sha": "972237b8ddce737983bfed001fde52e5236be695", "url": "https://github.com/dsoprea/NsqSpinner/blob/972237b8ddce737983bfed001fde52e5236be695/nsq/connection.py#L78-L84", "partition": "train"}
{"repo": "johnnoone/aioconsul", "path": "aioconsul/client/acl_endpoint.py", "func_name": "ACLEndpoint.items", "original_string": "async def items(self):\n        \"\"\"Lists all the active tokens\n\n        Returns:\n            ObjectMeta: where value is a list of tokens\n\n        It returns a body like this::\n\n            [\n              {\n                \"CreateIndex\": 3,\n                \"ModifyIndex\": 3,\n                \"ID\": \"8f246b77-f3e1-ff88-5b48-8ec93abf3e05\",\n                \"Name\": \"Client Token\",\n                \"Type\": \"client\",\n                \"Rules\": {\n                  \"key\": {\n                    \"\": { \"policy\": \"read\" },\n                    \"private/\": { \"policy\": \"deny\" }\n                  }\n                }\n              }\n            ]\n        \"\"\"\n        response = await self._api.get(\"/v1/acl/list\")\n        results = [decode_token(r) for r in response.body]\n        return consul(results, meta=extract_meta(response.headers))", "language": "python", "code": "async def items(self):\n        \"\"\"Lists all the active tokens\n\n        Returns:\n            ObjectMeta: where value is a list of tokens\n\n        It returns a body like this::\n\n            [\n              {\n                \"CreateIndex\": 3,\n                \"ModifyIndex\": 3,\n                \"ID\": \"8f246b77-f3e1-ff88-5b48-8ec93abf3e05\",\n                \"Name\": \"Client Token\",\n                \"Type\": \"client\",\n                \"Rules\": {\n                  \"key\": {\n                    \"\": { \"policy\": \"read\" },\n                    \"private/\": { \"policy\": \"deny\" }\n                  }\n                }\n              }\n            ]\n        \"\"\"\n        response = await self._api.get(\"/v1/acl/list\")\n        results = [decode_token(r) for r in response.body]\n        return consul(results, meta=extract_meta(response.headers))", "code_tokens": ["async", "def", "items", "(", "self", ")", ":", "response", "=", "await", "self", ".", "_api", ".", "get", "(", "\"/v1/acl/list\"", ")", "results", "=", "[", "decode_token", "(", "r", ")", "for", "r", "in", "response", ".", "body", "]", "return", "consul", "(", "results", ",", "meta", "=", "extract_meta", "(", "response", ".", "headers", ")", ")"], "docstring": "Lists all the active tokens\n\n        Returns:\n            ObjectMeta: where value is a list of tokens\n\n        It returns a body like this::\n\n            [\n              {\n                \"CreateIndex\": 3,\n                \"ModifyIndex\": 3,\n                \"ID\": \"8f246b77-f3e1-ff88-5b48-8ec93abf3e05\",\n                \"Name\": \"Client Token\",\n                \"Type\": \"client\",\n                \"Rules\": {\n                  \"key\": {\n                    \"\": { \"policy\": \"read\" },\n                    \"private/\": { \"policy\": \"deny\" }\n                  }\n                }\n              }\n            ]", "docstring_tokens": ["Lists", "all", "the", "active", "tokens"], "sha": "02f7a529d7dc2e49bed942111067aa5faf320e90", "url": "https://github.com/johnnoone/aioconsul/blob/02f7a529d7dc2e49bed942111067aa5faf320e90/aioconsul/client/acl_endpoint.py#L168-L194", "partition": "train"}
{"repo": "johnnoone/aioconsul", "path": "aioconsul/client/acl_endpoint.py", "func_name": "ACLEndpoint.replication", "original_string": "async def replication(self, *, dc=None):\n        \"\"\"Checks status of ACL replication\n\n        Parameters:\n            dc (str): Specify datacenter that will be used.\n                      Defaults to the agent's local datacenter.\n        Returns:\n            Object: Replication information\n\n        Returns the status of the ACL replication process in the datacenter.\n        This is intended to be used by operators, or by automation checking\n        the health of ACL replication.\n\n        By default, the datacenter of the agent is queried; however, the dc\n        can be provided using the \"dc\" parameter.\n\n        It returns a body like this::\n\n            {\n                \"Enabled\": True,\n                \"Running\": True,\n                \"SourceDatacenter\": \"dc1\",\n                \"ReplicatedIndex\": 1976,\n                \"LastSuccess\": datetime(2016, 8, 5, 6, 28, 58, tzinfo=tzutc()),\n                \"LastError\": datetime(2016, 8, 5, 6, 28, 58, tzinfo=tzutc())\n            }\n\n        **Enabled** reports whether ACL replication is enabled for the\n        datacenter.\n\n        **Running** reports whether the ACL replication process is running.\n        The process may take approximately 60 seconds to begin running after\n        a leader election occurs.\n\n        **SourceDatacenter** is the authoritative ACL datacenter that ACLs\n        are being replicated from, and will match the acl_datacenter\n        configuration.\n\n        **ReplicatedIndex** is the last index that was successfully replicated.\n        You can compare this to the Index meta returned by the items() endpoint\n        to determine if the replication process has gotten all available ACLs.\n        Note that replication runs as a background process approximately every\n        30 seconds, and that local updates are rate limited to 100\n        updates/second, so so it may take several minutes to perform the\n        initial sync of a large set of ACLs. After the initial sync, replica\n        lag should be on the order of about 30 seconds.\n\n        **LastSuccess** is the UTC time of the last successful sync operation.\n        Note that since ACL replication is done with a blocking query, this\n        may not update for up to 5 minutes if there have been no ACL changes\n        to replicate. A zero value of \"0001-01-01T00:00:00Z\" will be present\n        if no sync has been successful.\n\n        **LastError** is the UTC time of the last error encountered during a\n        sync operation. If this time is later than LastSuccess, you can assume\n        the replication process is not in a good state. A zero value of\n        \"0001-01-01T00:00:00Z\" will be present if no sync has resulted in an\n        error.\n        \"\"\"\n        params = {\"dc\": dc}\n        response = await self._api.get(\"/v1/acl/replication\", params=params)\n        return response.body", "language": "python", "code": "async def replication(self, *, dc=None):\n        \"\"\"Checks status of ACL replication\n\n        Parameters:\n            dc (str): Specify datacenter that will be used.\n                      Defaults to the agent's local datacenter.\n        Returns:\n            Object: Replication information\n\n        Returns the status of the ACL replication process in the datacenter.\n        This is intended to be used by operators, or by automation checking\n        the health of ACL replication.\n\n        By default, the datacenter of the agent is queried; however, the dc\n        can be provided using the \"dc\" parameter.\n\n        It returns a body like this::\n\n            {\n                \"Enabled\": True,\n                \"Running\": True,\n                \"SourceDatacenter\": \"dc1\",\n                \"ReplicatedIndex\": 1976,\n                \"LastSuccess\": datetime(2016, 8, 5, 6, 28, 58, tzinfo=tzutc()),\n                \"LastError\": datetime(2016, 8, 5, 6, 28, 58, tzinfo=tzutc())\n            }\n\n        **Enabled** reports whether ACL replication is enabled for the\n        datacenter.\n\n        **Running** reports whether the ACL replication process is running.\n        The process may take approximately 60 seconds to begin running after\n        a leader election occurs.\n\n        **SourceDatacenter** is the authoritative ACL datacenter that ACLs\n        are being replicated from, and will match the acl_datacenter\n        configuration.\n\n        **ReplicatedIndex** is the last index that was successfully replicated.\n        You can compare this to the Index meta returned by the items() endpoint\n        to determine if the replication process has gotten all available ACLs.\n        Note that replication runs as a background process approximately every\n        30 seconds, and that local updates are rate limited to 100\n        updates/second, so so it may take several minutes to perform the\n        initial sync of a large set of ACLs. After the initial sync, replica\n        lag should be on the order of about 30 seconds.\n\n        **LastSuccess** is the UTC time of the last successful sync operation.\n        Note that since ACL replication is done with a blocking query, this\n        may not update for up to 5 minutes if there have been no ACL changes\n        to replicate. A zero value of \"0001-01-01T00:00:00Z\" will be present\n        if no sync has been successful.\n\n        **LastError** is the UTC time of the last error encountered during a\n        sync operation. If this time is later than LastSuccess, you can assume\n        the replication process is not in a good state. A zero value of\n        \"0001-01-01T00:00:00Z\" will be present if no sync has resulted in an\n        error.\n        \"\"\"\n        params = {\"dc\": dc}\n        response = await self._api.get(\"/v1/acl/replication\", params=params)\n        return response.body", "code_tokens": ["async", "def", "replication", "(", "self", ",", "*", ",", "dc", "=", "None", ")", ":", "params", "=", "{", "\"dc\"", ":", "dc", "}", "response", "=", "await", "self", ".", "_api", ".", "get", "(", "\"/v1/acl/replication\"", ",", "params", "=", "params", ")", "return", "response", ".", "body"], "docstring": "Checks status of ACL replication\n\n        Parameters:\n            dc (str): Specify datacenter that will be used.\n                      Defaults to the agent's local datacenter.\n        Returns:\n            Object: Replication information\n\n        Returns the status of the ACL replication process in the datacenter.\n        This is intended to be used by operators, or by automation checking\n        the health of ACL replication.\n\n        By default, the datacenter of the agent is queried; however, the dc\n        can be provided using the \"dc\" parameter.\n\n        It returns a body like this::\n\n            {\n                \"Enabled\": True,\n                \"Running\": True,\n                \"SourceDatacenter\": \"dc1\",\n                \"ReplicatedIndex\": 1976,\n                \"LastSuccess\": datetime(2016, 8, 5, 6, 28, 58, tzinfo=tzutc()),\n                \"LastError\": datetime(2016, 8, 5, 6, 28, 58, tzinfo=tzutc())\n            }\n\n        **Enabled** reports whether ACL replication is enabled for the\n        datacenter.\n\n        **Running** reports whether the ACL replication process is running.\n        The process may take approximately 60 seconds to begin running after\n        a leader election occurs.\n\n        **SourceDatacenter** is the authoritative ACL datacenter that ACLs\n        are being replicated from, and will match the acl_datacenter\n        configuration.\n\n        **ReplicatedIndex** is the last index that was successfully replicated.\n        You can compare this to the Index meta returned by the items() endpoint\n        to determine if the replication process has gotten all available ACLs.\n        Note that replication runs as a background process approximately every\n        30 seconds, and that local updates are rate limited to 100\n        updates/second, so so it may take several minutes to perform the\n        initial sync of a large set of ACLs. After the initial sync, replica\n        lag should be on the order of about 30 seconds.\n\n        **LastSuccess** is the UTC time of the last successful sync operation.\n        Note that since ACL replication is done with a blocking query, this\n        may not update for up to 5 minutes if there have been no ACL changes\n        to replicate. A zero value of \"0001-01-01T00:00:00Z\" will be present\n        if no sync has been successful.\n\n        **LastError** is the UTC time of the last error encountered during a\n        sync operation. If this time is later than LastSuccess, you can assume\n        the replication process is not in a good state. A zero value of\n        \"0001-01-01T00:00:00Z\" will be present if no sync has resulted in an\n        error.", "docstring_tokens": ["Checks", "status", "of", "ACL", "replication"], "sha": "02f7a529d7dc2e49bed942111067aa5faf320e90", "url": "https://github.com/johnnoone/aioconsul/blob/02f7a529d7dc2e49bed942111067aa5faf320e90/aioconsul/client/acl_endpoint.py#L196-L257", "partition": "train"}
{"repo": "mikhaildubov/AST-text-analysis", "path": "east/asts/utils.py", "func_name": "match_strings", "original_string": "def match_strings(str1, str2):\n    \"\"\"\n    Returns the largest index i such that str1[:i] == str2[:i]\n    \n    \"\"\"\n    i = 0\n    min_len = len(str1) if len(str1) < len(str2) else len(str2)\n    while i < min_len and str1[i] == str2[i]: i += 1\n    return i", "language": "python", "code": "def match_strings(str1, str2):\n    \"\"\"\n    Returns the largest index i such that str1[:i] == str2[:i]\n    \n    \"\"\"\n    i = 0\n    min_len = len(str1) if len(str1) < len(str2) else len(str2)\n    while i < min_len and str1[i] == str2[i]: i += 1\n    return i", "code_tokens": ["def", "match_strings", "(", "str1", ",", "str2", ")", ":", "i", "=", "0", "min_len", "=", "len", "(", "str1", ")", "if", "len", "(", "str1", ")", "<", "len", "(", "str2", ")", "else", "len", "(", "str2", ")", "while", "i", "<", "min_len", "and", "str1", "[", "i", "]", "==", "str2", "[", "i", "]", ":", "i", "+=", "1", "return", "i"], "docstring": "Returns the largest index i such that str1[:i] == str2[:i]", "docstring_tokens": ["Returns", "the", "largest", "index", "i", "such", "that", "str1", "[", ":", "i", "]", "==", "str2", "[", ":", "i", "]"], "sha": "055ad8d2492c100bbbaa25309ec1074bdf1dfaa5", "url": "https://github.com/mikhaildubov/AST-text-analysis/blob/055ad8d2492c100bbbaa25309ec1074bdf1dfaa5/east/asts/utils.py#L14-L22", "partition": "train"}
{"repo": "mikhaildubov/AST-text-analysis", "path": "east/asts/utils.py", "func_name": "make_unique_endings", "original_string": "def make_unique_endings(strings_collection):\n    \"\"\"\n    Make each string in the collection end with a unique character.\n    Essential for correct builiding of a generalized annotated suffix tree.\n    Returns the updated strings collection, encoded in Unicode.\n\n    max strings_collection ~ 1.100.000\n    \n    \"\"\"\n    res = []\n    for i in range(len(strings_collection)):\n        # NOTE(msdubov): a trick to handle 'narrow' python installation issues.\n        hex_code = hex(consts.String.UNICODE_SPECIAL_SYMBOLS_START+i)\n        hex_code = r\"\\U\" + \"0\" * (8 - len(hex_code) + 2) + hex_code[2:]\n        res.append(strings_collection[i] + hex_code.decode(\"unicode-escape\"))\n    return res", "language": "python", "code": "def make_unique_endings(strings_collection):\n    \"\"\"\n    Make each string in the collection end with a unique character.\n    Essential for correct builiding of a generalized annotated suffix tree.\n    Returns the updated strings collection, encoded in Unicode.\n\n    max strings_collection ~ 1.100.000\n    \n    \"\"\"\n    res = []\n    for i in range(len(strings_collection)):\n        # NOTE(msdubov): a trick to handle 'narrow' python installation issues.\n        hex_code = hex(consts.String.UNICODE_SPECIAL_SYMBOLS_START+i)\n        hex_code = r\"\\U\" + \"0\" * (8 - len(hex_code) + 2) + hex_code[2:]\n        res.append(strings_collection[i] + hex_code.decode(\"unicode-escape\"))\n    return res", "code_tokens": ["def", "make_unique_endings", "(", "strings_collection", ")", ":", "res", "=", "[", "]", "for", "i", "in", "range", "(", "len", "(", "strings_collection", ")", ")", ":", "# NOTE(msdubov): a trick to handle 'narrow' python installation issues.", "hex_code", "=", "hex", "(", "consts", ".", "String", ".", "UNICODE_SPECIAL_SYMBOLS_START", "+", "i", ")", "hex_code", "=", "r\"\\U\"", "+", "\"0\"", "*", "(", "8", "-", "len", "(", "hex_code", ")", "+", "2", ")", "+", "hex_code", "[", "2", ":", "]", "res", ".", "append", "(", "strings_collection", "[", "i", "]", "+", "hex_code", ".", "decode", "(", "\"unicode-escape\"", ")", ")", "return", "res"], "docstring": "Make each string in the collection end with a unique character.\n    Essential for correct builiding of a generalized annotated suffix tree.\n    Returns the updated strings collection, encoded in Unicode.\n\n    max strings_collection ~ 1.100.000", "docstring_tokens": ["Make", "each", "string", "in", "the", "collection", "end", "with", "a", "unique", "character", ".", "Essential", "for", "correct", "builiding", "of", "a", "generalized", "annotated", "suffix", "tree", ".", "Returns", "the", "updated", "strings", "collection", "encoded", "in", "Unicode", "."], "sha": "055ad8d2492c100bbbaa25309ec1074bdf1dfaa5", "url": "https://github.com/mikhaildubov/AST-text-analysis/blob/055ad8d2492c100bbbaa25309ec1074bdf1dfaa5/east/asts/utils.py#L25-L40", "partition": "train"}
{"repo": "bitesofcode/projexui", "path": "projexui/widgets/xchart/axes/xdatetimeaxis.py", "func_name": "XDatetimeAxis.calculateValues", "original_string": "def calculateValues(self):\r\n        \"\"\"\r\n        Overloads the calculate values method to calculate the values for\r\n        this axis based on the minimum and maximum values, and the number\r\n        of steps desired.\r\n        \r\n        :return     [<variant>, ..]\r\n        \"\"\"\r\n        if self.maximum() <= self.minimum():\r\n            return []\r\n        \r\n        max_labels = self.maximumLabelCount()\r\n        seconds = (self.maximum() - self.minimum()).total_seconds()\r\n        step = datetime.timedelta(0, int(round(float(seconds) / max_labels)))\r\n        \r\n        values = []\r\n        value = self.minimum()\r\n        while value <= self.maximum():\r\n            values.append(value)\r\n            value += step\r\n        \r\n        return values", "language": "python", "code": "def calculateValues(self):\r\n        \"\"\"\r\n        Overloads the calculate values method to calculate the values for\r\n        this axis based on the minimum and maximum values, and the number\r\n        of steps desired.\r\n        \r\n        :return     [<variant>, ..]\r\n        \"\"\"\r\n        if self.maximum() <= self.minimum():\r\n            return []\r\n        \r\n        max_labels = self.maximumLabelCount()\r\n        seconds = (self.maximum() - self.minimum()).total_seconds()\r\n        step = datetime.timedelta(0, int(round(float(seconds) / max_labels)))\r\n        \r\n        values = []\r\n        value = self.minimum()\r\n        while value <= self.maximum():\r\n            values.append(value)\r\n            value += step\r\n        \r\n        return values", "code_tokens": ["def", "calculateValues", "(", "self", ")", ":", "if", "self", ".", "maximum", "(", ")", "<=", "self", ".", "minimum", "(", ")", ":", "return", "[", "]", "max_labels", "=", "self", ".", "maximumLabelCount", "(", ")", "seconds", "=", "(", "self", ".", "maximum", "(", ")", "-", "self", ".", "minimum", "(", ")", ")", ".", "total_seconds", "(", ")", "step", "=", "datetime", ".", "timedelta", "(", "0", ",", "int", "(", "round", "(", "float", "(", "seconds", ")", "/", "max_labels", ")", ")", ")", "values", "=", "[", "]", "value", "=", "self", ".", "minimum", "(", ")", "while", "value", "<=", "self", ".", "maximum", "(", ")", ":", "values", ".", "append", "(", "value", ")", "value", "+=", "step", "return", "values"], "docstring": "Overloads the calculate values method to calculate the values for\r\n        this axis based on the minimum and maximum values, and the number\r\n        of steps desired.\r\n        \r\n        :return     [<variant>, ..]", "docstring_tokens": ["Overloads", "the", "calculate", "values", "method", "to", "calculate", "the", "values", "for", "this", "axis", "based", "on", "the", "minimum", "and", "maximum", "values", "and", "the", "number", "of", "steps", "desired", ".", ":", "return", "[", "<variant", ">", "..", "]"], "sha": "f18a73bec84df90b034ca69b9deea118dbedfc4d", "url": "https://github.com/bitesofcode/projexui/blob/f18a73bec84df90b034ca69b9deea118dbedfc4d/projexui/widgets/xchart/axes/xdatetimeaxis.py#L36-L57", "partition": "train"}
{"repo": "bitesofcode/projexui", "path": "projexui/widgets/xchart/axes/xdatetimeaxis.py", "func_name": "XDatetimeAxis.percentAt", "original_string": "def percentAt(self, value):\r\n        \"\"\"\r\n        Returns the percentage the value represents between the minimum and\r\n        maximum for this axis.\r\n        \r\n        :param      value | <int> || <float>\r\n        \r\n        :return     <float>\r\n        \"\"\"\r\n        min_val = self.minimum()\r\n        max_val = self.maximum()\r\n        \r\n        # round the max value to sync with the values in the grid\r\n        total_seconds = (max_val - min_val).total_seconds()\r\n        value_seconds = (value - min_val).total_seconds()\r\n        \r\n        if value < min_val:\r\n            return 0.0\r\n        elif max_val < value:\r\n            return 1.0\r\n        \r\n        try:\r\n            perc = value_seconds / float(total_seconds)\r\n        except ZeroDivisionError:\r\n            perc = 0.0\r\n        \r\n        return perc", "language": "python", "code": "def percentAt(self, value):\r\n        \"\"\"\r\n        Returns the percentage the value represents between the minimum and\r\n        maximum for this axis.\r\n        \r\n        :param      value | <int> || <float>\r\n        \r\n        :return     <float>\r\n        \"\"\"\r\n        min_val = self.minimum()\r\n        max_val = self.maximum()\r\n        \r\n        # round the max value to sync with the values in the grid\r\n        total_seconds = (max_val - min_val).total_seconds()\r\n        value_seconds = (value - min_val).total_seconds()\r\n        \r\n        if value < min_val:\r\n            return 0.0\r\n        elif max_val < value:\r\n            return 1.0\r\n        \r\n        try:\r\n            perc = value_seconds / float(total_seconds)\r\n        except ZeroDivisionError:\r\n            perc = 0.0\r\n        \r\n        return perc", "code_tokens": ["def", "percentAt", "(", "self", ",", "value", ")", ":", "min_val", "=", "self", ".", "minimum", "(", ")", "max_val", "=", "self", ".", "maximum", "(", ")", "# round the max value to sync with the values in the grid\r", "total_seconds", "=", "(", "max_val", "-", "min_val", ")", ".", "total_seconds", "(", ")", "value_seconds", "=", "(", "value", "-", "min_val", ")", ".", "total_seconds", "(", ")", "if", "value", "<", "min_val", ":", "return", "0.0", "elif", "max_val", "<", "value", ":", "return", "1.0", "try", ":", "perc", "=", "value_seconds", "/", "float", "(", "total_seconds", ")", "except", "ZeroDivisionError", ":", "perc", "=", "0.0", "return", "perc"], "docstring": "Returns the percentage the value represents between the minimum and\r\n        maximum for this axis.\r\n        \r\n        :param      value | <int> || <float>\r\n        \r\n        :return     <float>", "docstring_tokens": ["Returns", "the", "percentage", "the", "value", "represents", "between", "the", "minimum", "and", "maximum", "for", "this", "axis", ".", ":", "param", "value", "|", "<int", ">", "||", "<float", ">", ":", "return", "<float", ">"], "sha": "f18a73bec84df90b034ca69b9deea118dbedfc4d", "url": "https://github.com/bitesofcode/projexui/blob/f18a73bec84df90b034ca69b9deea118dbedfc4d/projexui/widgets/xchart/axes/xdatetimeaxis.py#L59-L85", "partition": "train"}
{"repo": "mjirik/imcut", "path": "imcut/features.py", "func_name": "select_from_fv_by_seeds", "original_string": "def select_from_fv_by_seeds(fv, seeds, unique_cls):\n    \"\"\"\n    Tool to make simple feature functions take features from feature array by seeds.\n    :param fv: ndarray with lineariezed feature. It's shape is MxN, where M is number of image pixels and N is number\n    of features\n    :param seeds: ndarray with seeds. Does not to be linear.\n    :param unique_cls: number of used seeds clases. Like [1, 2]\n    :return: fv_selection, seeds_selection - selection from feature vector and selection from seeds\n    \"\"\"\n    logger.debug(\"seeds\" + str(seeds))\n    # fvlin = fv.reshape(-1, int(fv.size/seeds.size))\n    expected_shape = [seeds.size, int(fv.size/seeds.size)]\n    if fv.shape[0] != expected_shape[0] or fv.shape[1] != expected_shape[1]:\n        raise AssertionError(\"Wrong shape of input feature vector array fv\")\n    # sd = seeds.reshape(-1, 1)\n    selection = np.in1d(seeds, unique_cls)\n    fv_selection = fv[selection]\n    seeds_selection = seeds.flatten()[selection]\n    # sd = sd[]\n    return fv_selection, seeds_selection", "language": "python", "code": "def select_from_fv_by_seeds(fv, seeds, unique_cls):\n    \"\"\"\n    Tool to make simple feature functions take features from feature array by seeds.\n    :param fv: ndarray with lineariezed feature. It's shape is MxN, where M is number of image pixels and N is number\n    of features\n    :param seeds: ndarray with seeds. Does not to be linear.\n    :param unique_cls: number of used seeds clases. Like [1, 2]\n    :return: fv_selection, seeds_selection - selection from feature vector and selection from seeds\n    \"\"\"\n    logger.debug(\"seeds\" + str(seeds))\n    # fvlin = fv.reshape(-1, int(fv.size/seeds.size))\n    expected_shape = [seeds.size, int(fv.size/seeds.size)]\n    if fv.shape[0] != expected_shape[0] or fv.shape[1] != expected_shape[1]:\n        raise AssertionError(\"Wrong shape of input feature vector array fv\")\n    # sd = seeds.reshape(-1, 1)\n    selection = np.in1d(seeds, unique_cls)\n    fv_selection = fv[selection]\n    seeds_selection = seeds.flatten()[selection]\n    # sd = sd[]\n    return fv_selection, seeds_selection", "code_tokens": ["def", "select_from_fv_by_seeds", "(", "fv", ",", "seeds", ",", "unique_cls", ")", ":", "logger", ".", "debug", "(", "\"seeds\"", "+", "str", "(", "seeds", ")", ")", "# fvlin = fv.reshape(-1, int(fv.size/seeds.size))", "expected_shape", "=", "[", "seeds", ".", "size", ",", "int", "(", "fv", ".", "size", "/", "seeds", ".", "size", ")", "]", "if", "fv", ".", "shape", "[", "0", "]", "!=", "expected_shape", "[", "0", "]", "or", "fv", ".", "shape", "[", "1", "]", "!=", "expected_shape", "[", "1", "]", ":", "raise", "AssertionError", "(", "\"Wrong shape of input feature vector array fv\"", ")", "# sd = seeds.reshape(-1, 1)", "selection", "=", "np", ".", "in1d", "(", "seeds", ",", "unique_cls", ")", "fv_selection", "=", "fv", "[", "selection", "]", "seeds_selection", "=", "seeds", ".", "flatten", "(", ")", "[", "selection", "]", "# sd = sd[]", "return", "fv_selection", ",", "seeds_selection"], "docstring": "Tool to make simple feature functions take features from feature array by seeds.\n    :param fv: ndarray with lineariezed feature. It's shape is MxN, where M is number of image pixels and N is number\n    of features\n    :param seeds: ndarray with seeds. Does not to be linear.\n    :param unique_cls: number of used seeds clases. Like [1, 2]\n    :return: fv_selection, seeds_selection - selection from feature vector and selection from seeds", "docstring_tokens": ["Tool", "to", "make", "simple", "feature", "functions", "take", "features", "from", "feature", "array", "by", "seeds", ".", ":", "param", "fv", ":", "ndarray", "with", "lineariezed", "feature", ".", "It", "s", "shape", "is", "MxN", "where", "M", "is", "number", "of", "image", "pixels", "and", "N", "is", "number", "of", "features", ":", "param", "seeds", ":", "ndarray", "with", "seeds", ".", "Does", "not", "to", "be", "linear", ".", ":", "param", "unique_cls", ":", "number", "of", "used", "seeds", "clases", ".", "Like", "[", "1", "2", "]", ":", "return", ":", "fv_selection", "seeds_selection", "-", "selection", "from", "feature", "vector", "and", "selection", "from", "seeds"], "sha": "1b38e7cd18a7a38fe683c1cabe1222fe5fa03aa3", "url": "https://github.com/mjirik/imcut/blob/1b38e7cd18a7a38fe683c1cabe1222fe5fa03aa3/imcut/features.py#L39-L58", "partition": "train"}
{"repo": "mjirik/imcut", "path": "imcut/features.py", "func_name": "return_fv_by_seeds", "original_string": "def return_fv_by_seeds(fv, seeds=None, unique_cls=None):\n    \"\"\"\n    Return features selected by seeds and unique_cls or selection from features and corresponding seed classes.\n\n    :param fv: ndarray with lineariezed feature. It's shape is MxN, where M is number of image pixels and N is number\n    of features\n    :param seeds: ndarray with seeds. Does not to be linear.\n    :param unique_cls: number of used seeds clases. Like [1, 2]\n    :return: fv, sd - selection from feature vector and selection from seeds or just fv for whole image\n    \"\"\"\n    if seeds is not None:\n        if unique_cls is not None:\n            return select_from_fv_by_seeds(fv, seeds, unique_cls)\n        else:\n            raise AssertionError(\"Input unique_cls has to be not None if seeds is not None.\")\n    else:\n        return fv", "language": "python", "code": "def return_fv_by_seeds(fv, seeds=None, unique_cls=None):\n    \"\"\"\n    Return features selected by seeds and unique_cls or selection from features and corresponding seed classes.\n\n    :param fv: ndarray with lineariezed feature. It's shape is MxN, where M is number of image pixels and N is number\n    of features\n    :param seeds: ndarray with seeds. Does not to be linear.\n    :param unique_cls: number of used seeds clases. Like [1, 2]\n    :return: fv, sd - selection from feature vector and selection from seeds or just fv for whole image\n    \"\"\"\n    if seeds is not None:\n        if unique_cls is not None:\n            return select_from_fv_by_seeds(fv, seeds, unique_cls)\n        else:\n            raise AssertionError(\"Input unique_cls has to be not None if seeds is not None.\")\n    else:\n        return fv", "code_tokens": ["def", "return_fv_by_seeds", "(", "fv", ",", "seeds", "=", "None", ",", "unique_cls", "=", "None", ")", ":", "if", "seeds", "is", "not", "None", ":", "if", "unique_cls", "is", "not", "None", ":", "return", "select_from_fv_by_seeds", "(", "fv", ",", "seeds", ",", "unique_cls", ")", "else", ":", "raise", "AssertionError", "(", "\"Input unique_cls has to be not None if seeds is not None.\"", ")", "else", ":", "return", "fv"], "docstring": "Return features selected by seeds and unique_cls or selection from features and corresponding seed classes.\n\n    :param fv: ndarray with lineariezed feature. It's shape is MxN, where M is number of image pixels and N is number\n    of features\n    :param seeds: ndarray with seeds. Does not to be linear.\n    :param unique_cls: number of used seeds clases. Like [1, 2]\n    :return: fv, sd - selection from feature vector and selection from seeds or just fv for whole image", "docstring_tokens": ["Return", "features", "selected", "by", "seeds", "and", "unique_cls", "or", "selection", "from", "features", "and", "corresponding", "seed", "classes", "."], "sha": "1b38e7cd18a7a38fe683c1cabe1222fe5fa03aa3", "url": "https://github.com/mjirik/imcut/blob/1b38e7cd18a7a38fe683c1cabe1222fe5fa03aa3/imcut/features.py#L60-L76", "partition": "train"}
{"repo": "chitamoor/Rester", "path": "rester/manifest.py", "func_name": "Variables.expand", "original_string": "def expand(self, expression):\n        \"\"\"Expands logical constructions.\"\"\"\n        self.logger.debug(\"expand : expression %s\", str(expression))\n        if not is_string(expression):\n            return expression\n\n        result = self._pattern.sub(lambda var: str(self._variables[var.group(1)]), expression)\n\n        result = result.strip()\n        self.logger.debug('expand : %s - result : %s', expression, result)\n\n        if is_number(result):\n            if result.isdigit():\n                self.logger.debug('     expand is integer !!!')\n                return int(result)\n            else:\n                self.logger.debug('     expand is float !!!')\n                return float(result)\n        return result", "language": "python", "code": "def expand(self, expression):\n        \"\"\"Expands logical constructions.\"\"\"\n        self.logger.debug(\"expand : expression %s\", str(expression))\n        if not is_string(expression):\n            return expression\n\n        result = self._pattern.sub(lambda var: str(self._variables[var.group(1)]), expression)\n\n        result = result.strip()\n        self.logger.debug('expand : %s - result : %s', expression, result)\n\n        if is_number(result):\n            if result.isdigit():\n                self.logger.debug('     expand is integer !!!')\n                return int(result)\n            else:\n                self.logger.debug('     expand is float !!!')\n                return float(result)\n        return result", "code_tokens": ["def", "expand", "(", "self", ",", "expression", ")", ":", "self", ".", "logger", ".", "debug", "(", "\"expand : expression %s\"", ",", "str", "(", "expression", ")", ")", "if", "not", "is_string", "(", "expression", ")", ":", "return", "expression", "result", "=", "self", ".", "_pattern", ".", "sub", "(", "lambda", "var", ":", "str", "(", "self", ".", "_variables", "[", "var", ".", "group", "(", "1", ")", "]", ")", ",", "expression", ")", "result", "=", "result", ".", "strip", "(", ")", "self", ".", "logger", ".", "debug", "(", "'expand : %s - result : %s'", ",", "expression", ",", "result", ")", "if", "is_number", "(", "result", ")", ":", "if", "result", ".", "isdigit", "(", ")", ":", "self", ".", "logger", ".", "debug", "(", "'     expand is integer !!!'", ")", "return", "int", "(", "result", ")", "else", ":", "self", ".", "logger", ".", "debug", "(", "'     expand is float !!!'", ")", "return", "float", "(", "result", ")", "return", "result"], "docstring": "Expands logical constructions.", "docstring_tokens": ["Expands", "logical", "constructions", "."], "sha": "1865b17f70b7c597aeadde2d0907cb1b59f10c0f", "url": "https://github.com/chitamoor/Rester/blob/1865b17f70b7c597aeadde2d0907cb1b59f10c0f/rester/manifest.py#L34-L52", "partition": "train"}
{"repo": "disqus/gutter", "path": "gutter/client/__init__.py", "func_name": "get_gutter_client", "original_string": "def get_gutter_client(\n        alias='default',\n        cache=CLIENT_CACHE,\n        **kwargs\n):\n    \"\"\"\n    Creates gutter clients and memoizes them in a registry for future quick access.\n\n    Args:\n        alias (str or None): Name of the client. Used for caching.\n            If name is falsy then do not use the cache.\n        cache (dict): cache to store gutter managers in.\n        **kwargs: kwargs to be passed the Manger class.\n\n    Returns (Manager):\n        A gutter client.\n\n    \"\"\"\n    from gutter.client.models import Manager\n\n    if not alias:\n        return Manager(**kwargs)\n    elif alias not in cache:\n        cache[alias] = Manager(**kwargs)\n\n    return cache[alias]", "language": "python", "code": "def get_gutter_client(\n        alias='default',\n        cache=CLIENT_CACHE,\n        **kwargs\n):\n    \"\"\"\n    Creates gutter clients and memoizes them in a registry for future quick access.\n\n    Args:\n        alias (str or None): Name of the client. Used for caching.\n            If name is falsy then do not use the cache.\n        cache (dict): cache to store gutter managers in.\n        **kwargs: kwargs to be passed the Manger class.\n\n    Returns (Manager):\n        A gutter client.\n\n    \"\"\"\n    from gutter.client.models import Manager\n\n    if not alias:\n        return Manager(**kwargs)\n    elif alias not in cache:\n        cache[alias] = Manager(**kwargs)\n\n    return cache[alias]", "code_tokens": ["def", "get_gutter_client", "(", "alias", "=", "'default'", ",", "cache", "=", "CLIENT_CACHE", ",", "*", "*", "kwargs", ")", ":", "from", "gutter", ".", "client", ".", "models", "import", "Manager", "if", "not", "alias", ":", "return", "Manager", "(", "*", "*", "kwargs", ")", "elif", "alias", "not", "in", "cache", ":", "cache", "[", "alias", "]", "=", "Manager", "(", "*", "*", "kwargs", ")", "return", "cache", "[", "alias", "]"], "docstring": "Creates gutter clients and memoizes them in a registry for future quick access.\n\n    Args:\n        alias (str or None): Name of the client. Used for caching.\n            If name is falsy then do not use the cache.\n        cache (dict): cache to store gutter managers in.\n        **kwargs: kwargs to be passed the Manger class.\n\n    Returns (Manager):\n        A gutter client.", "docstring_tokens": ["Creates", "gutter", "clients", "and", "memoizes", "them", "in", "a", "registry", "for", "future", "quick", "access", "."], "sha": "d686fa3cd0551cacfc5630c8e7b5fa75e6dcfdf5", "url": "https://github.com/disqus/gutter/blob/d686fa3cd0551cacfc5630c8e7b5fa75e6dcfdf5/gutter/client/__init__.py#L17-L42", "partition": "train"}
{"repo": "disqus/gutter", "path": "gutter/client/operators/misc.py", "func_name": "PercentRange._modulo", "original_string": "def _modulo(self, decimal_argument):\n        \"\"\"\n        The mod operator is prone to floating point errors, so use decimal.\n\n        101.1 % 100\n        >>> 1.0999999999999943\n\n        decimal_context.divmod(Decimal('100.1'), 100)\n        >>> (Decimal('1'), Decimal('0.1'))\n        \"\"\"\n        _times, remainder = self._context.divmod(decimal_argument, 100)\n\n        # match the builtin % behavior by adding the N to the result if negative\n        return remainder if remainder >= 0 else remainder + 100", "language": "python", "code": "def _modulo(self, decimal_argument):\n        \"\"\"\n        The mod operator is prone to floating point errors, so use decimal.\n\n        101.1 % 100\n        >>> 1.0999999999999943\n\n        decimal_context.divmod(Decimal('100.1'), 100)\n        >>> (Decimal('1'), Decimal('0.1'))\n        \"\"\"\n        _times, remainder = self._context.divmod(decimal_argument, 100)\n\n        # match the builtin % behavior by adding the N to the result if negative\n        return remainder if remainder >= 0 else remainder + 100", "code_tokens": ["def", "_modulo", "(", "self", ",", "decimal_argument", ")", ":", "_times", ",", "remainder", "=", "self", ".", "_context", ".", "divmod", "(", "decimal_argument", ",", "100", ")", "# match the builtin % behavior by adding the N to the result if negative", "return", "remainder", "if", "remainder", ">=", "0", "else", "remainder", "+", "100"], "docstring": "The mod operator is prone to floating point errors, so use decimal.\n\n        101.1 % 100\n        >>> 1.0999999999999943\n\n        decimal_context.divmod(Decimal('100.1'), 100)\n        >>> (Decimal('1'), Decimal('0.1'))", "docstring_tokens": ["The", "mod", "operator", "is", "prone", "to", "floating", "point", "errors", "so", "use", "decimal", "."], "sha": "d686fa3cd0551cacfc5630c8e7b5fa75e6dcfdf5", "url": "https://github.com/disqus/gutter/blob/d686fa3cd0551cacfc5630c8e7b5fa75e6dcfdf5/gutter/client/operators/misc.py#L16-L29", "partition": "train"}
{"repo": "kaste/mockito-python", "path": "mockito/mocking.py", "func_name": "mock", "original_string": "def mock(config_or_spec=None, spec=None, strict=OMITTED):\n    \"\"\"Create 'empty' objects ('Mocks').\n\n    Will create an empty unconfigured object, that you can pass\n    around. All interactions (method calls) will be recorded and can be\n    verified using :func:`verify` et.al.\n\n    A plain `mock()` will be not `strict`, and thus all methods regardless\n    of the arguments will return ``None``.\n\n    .. note:: Technically all attributes will return an internal interface.\n        Because of that a simple ``if mock().foo:`` will surprisingly pass.\n\n    If you set strict to ``True``: ``mock(strict=True)`` all unexpected\n    interactions will raise an error instead.\n\n    You configure a mock using :func:`when`, :func:`when2` or :func:`expect`.\n    You can also very conveniently just pass in a dict here::\n\n        response = mock({'text': 'ok', 'raise_for_status': lambda: None})\n\n    You can also create an empty Mock which is specced against a given\n    `spec`: ``mock(requests.Response)``. These mock are by default strict,\n    thus they raise if you want to stub a method, the spec does not implement.\n    Mockito will also match the function signature.\n\n    You can pre-configure a specced mock as well::\n\n        response = mock({'json': lambda: {'status': 'Ok'}},\n                        spec=requests.Response)\n\n    Mocks are by default callable. Configure the callable behavior using\n    `when`::\n\n        dummy = mock()\n        when(dummy).__call_(1).thenReturn(2)\n\n    All other magic methods must be configured this way or they will raise an\n    AttributeError.\n\n\n    See :func:`verify` to verify your interactions after usage.\n\n    \"\"\"\n\n    if type(config_or_spec) is dict:\n        config = config_or_spec\n    else:\n        config = {}\n        spec = config_or_spec\n\n    if strict is OMITTED:\n        strict = False if spec is None else True\n\n\n    class Dummy(_Dummy):\n        if spec:\n            __class__ = spec  # make isinstance work\n\n        def __getattr__(self, method_name):\n            if strict:\n                raise AttributeError(\n                    \"'Dummy' has no attribute %r configured\" % method_name)\n            return functools.partial(\n                remembered_invocation_builder, theMock, method_name)\n\n        def __repr__(self):\n            name = 'Dummy'\n            if spec:\n                name += spec.__name__\n            return \"<%s id=%s>\" % (name, id(self))\n\n\n    # That's a tricky one: The object we will return is an *instance* of our\n    # Dummy class, but the mock we register will point and patch the class.\n    # T.i. so that magic methods (`__call__` etc.) can be configured.\n    obj = Dummy()\n    theMock = Mock(Dummy, strict=strict, spec=spec)\n\n    for n, v in config.items():\n        if inspect.isfunction(v):\n            invocation.StubbedInvocation(theMock, n)(Ellipsis).thenAnswer(v)\n        else:\n            setattr(obj, n, v)\n\n    mock_registry.register(obj, theMock)\n    return obj", "language": "python", "code": "def mock(config_or_spec=None, spec=None, strict=OMITTED):\n    \"\"\"Create 'empty' objects ('Mocks').\n\n    Will create an empty unconfigured object, that you can pass\n    around. All interactions (method calls) will be recorded and can be\n    verified using :func:`verify` et.al.\n\n    A plain `mock()` will be not `strict`, and thus all methods regardless\n    of the arguments will return ``None``.\n\n    .. note:: Technically all attributes will return an internal interface.\n        Because of that a simple ``if mock().foo:`` will surprisingly pass.\n\n    If you set strict to ``True``: ``mock(strict=True)`` all unexpected\n    interactions will raise an error instead.\n\n    You configure a mock using :func:`when`, :func:`when2` or :func:`expect`.\n    You can also very conveniently just pass in a dict here::\n\n        response = mock({'text': 'ok', 'raise_for_status': lambda: None})\n\n    You can also create an empty Mock which is specced against a given\n    `spec`: ``mock(requests.Response)``. These mock are by default strict,\n    thus they raise if you want to stub a method, the spec does not implement.\n    Mockito will also match the function signature.\n\n    You can pre-configure a specced mock as well::\n\n        response = mock({'json': lambda: {'status': 'Ok'}},\n                        spec=requests.Response)\n\n    Mocks are by default callable. Configure the callable behavior using\n    `when`::\n\n        dummy = mock()\n        when(dummy).__call_(1).thenReturn(2)\n\n    All other magic methods must be configured this way or they will raise an\n    AttributeError.\n\n\n    See :func:`verify` to verify your interactions after usage.\n\n    \"\"\"\n\n    if type(config_or_spec) is dict:\n        config = config_or_spec\n    else:\n        config = {}\n        spec = config_or_spec\n\n    if strict is OMITTED:\n        strict = False if spec is None else True\n\n\n    class Dummy(_Dummy):\n        if spec:\n            __class__ = spec  # make isinstance work\n\n        def __getattr__(self, method_name):\n            if strict:\n                raise AttributeError(\n                    \"'Dummy' has no attribute %r configured\" % method_name)\n            return functools.partial(\n                remembered_invocation_builder, theMock, method_name)\n\n        def __repr__(self):\n            name = 'Dummy'\n            if spec:\n                name += spec.__name__\n            return \"<%s id=%s>\" % (name, id(self))\n\n\n    # That's a tricky one: The object we will return is an *instance* of our\n    # Dummy class, but the mock we register will point and patch the class.\n    # T.i. so that magic methods (`__call__` etc.) can be configured.\n    obj = Dummy()\n    theMock = Mock(Dummy, strict=strict, spec=spec)\n\n    for n, v in config.items():\n        if inspect.isfunction(v):\n            invocation.StubbedInvocation(theMock, n)(Ellipsis).thenAnswer(v)\n        else:\n            setattr(obj, n, v)\n\n    mock_registry.register(obj, theMock)\n    return obj", "code_tokens": ["def", "mock", "(", "config_or_spec", "=", "None", ",", "spec", "=", "None", ",", "strict", "=", "OMITTED", ")", ":", "if", "type", "(", "config_or_spec", ")", "is", "dict", ":", "config", "=", "config_or_spec", "else", ":", "config", "=", "{", "}", "spec", "=", "config_or_spec", "if", "strict", "is", "OMITTED", ":", "strict", "=", "False", "if", "spec", "is", "None", "else", "True", "class", "Dummy", "(", "_Dummy", ")", ":", "if", "spec", ":", "__class__", "=", "spec", "# make isinstance work", "def", "__getattr__", "(", "self", ",", "method_name", ")", ":", "if", "strict", ":", "raise", "AttributeError", "(", "\"'Dummy' has no attribute %r configured\"", "%", "method_name", ")", "return", "functools", ".", "partial", "(", "remembered_invocation_builder", ",", "theMock", ",", "method_name", ")", "def", "__repr__", "(", "self", ")", ":", "name", "=", "'Dummy'", "if", "spec", ":", "name", "+=", "spec", ".", "__name__", "return", "\"<%s id=%s>\"", "%", "(", "name", ",", "id", "(", "self", ")", ")", "# That's a tricky one: The object we will return is an *instance* of our", "# Dummy class, but the mock we register will point and patch the class.", "# T.i. so that magic methods (`__call__` etc.) can be configured.", "obj", "=", "Dummy", "(", ")", "theMock", "=", "Mock", "(", "Dummy", ",", "strict", "=", "strict", ",", "spec", "=", "spec", ")", "for", "n", ",", "v", "in", "config", ".", "items", "(", ")", ":", "if", "inspect", ".", "isfunction", "(", "v", ")", ":", "invocation", ".", "StubbedInvocation", "(", "theMock", ",", "n", ")", "(", "Ellipsis", ")", ".", "thenAnswer", "(", "v", ")", "else", ":", "setattr", "(", "obj", ",", "n", ",", "v", ")", "mock_registry", ".", "register", "(", "obj", ",", "theMock", ")", "return", "obj"], "docstring": "Create 'empty' objects ('Mocks').\n\n    Will create an empty unconfigured object, that you can pass\n    around. All interactions (method calls) will be recorded and can be\n    verified using :func:`verify` et.al.\n\n    A plain `mock()` will be not `strict`, and thus all methods regardless\n    of the arguments will return ``None``.\n\n    .. note:: Technically all attributes will return an internal interface.\n        Because of that a simple ``if mock().foo:`` will surprisingly pass.\n\n    If you set strict to ``True``: ``mock(strict=True)`` all unexpected\n    interactions will raise an error instead.\n\n    You configure a mock using :func:`when`, :func:`when2` or :func:`expect`.\n    You can also very conveniently just pass in a dict here::\n\n        response = mock({'text': 'ok', 'raise_for_status': lambda: None})\n\n    You can also create an empty Mock which is specced against a given\n    `spec`: ``mock(requests.Response)``. These mock are by default strict,\n    thus they raise if you want to stub a method, the spec does not implement.\n    Mockito will also match the function signature.\n\n    You can pre-configure a specced mock as well::\n\n        response = mock({'json': lambda: {'status': 'Ok'}},\n                        spec=requests.Response)\n\n    Mocks are by default callable. Configure the callable behavior using\n    `when`::\n\n        dummy = mock()\n        when(dummy).__call_(1).thenReturn(2)\n\n    All other magic methods must be configured this way or they will raise an\n    AttributeError.\n\n\n    See :func:`verify` to verify your interactions after usage.", "docstring_tokens": ["Create", "empty", "objects", "(", "Mocks", ")", "."], "sha": "d6b22b003f56ee5b156dbd9d8ba209faf35b6713", "url": "https://github.com/kaste/mockito-python/blob/d6b22b003f56ee5b156dbd9d8ba209faf35b6713/mockito/mocking.py#L193-L279", "partition": "train"}
{"repo": "davidblaisonneau-orange/foreman", "path": "foreman/smartProxies.py", "func_name": "SmartProxies.importPuppetClasses", "original_string": "def importPuppetClasses(self, smartProxyId):\n        \"\"\" Function importPuppetClasses\n        Force the reload of puppet classes\n\n        @param smartProxyId: smartProxy Id\n        @return RETURN: the API result\n        \"\"\"\n        return self.api.create('{}/{}/import_puppetclasses'\n                               .format(self.objName, smartProxyId), '{}')", "language": "python", "code": "def importPuppetClasses(self, smartProxyId):\n        \"\"\" Function importPuppetClasses\n        Force the reload of puppet classes\n\n        @param smartProxyId: smartProxy Id\n        @return RETURN: the API result\n        \"\"\"\n        return self.api.create('{}/{}/import_puppetclasses'\n                               .format(self.objName, smartProxyId), '{}')", "code_tokens": ["def", "importPuppetClasses", "(", "self", ",", "smartProxyId", ")", ":", "return", "self", ".", "api", ".", "create", "(", "'{}/{}/import_puppetclasses'", ".", "format", "(", "self", ".", "objName", ",", "smartProxyId", ")", ",", "'{}'", ")"], "docstring": "Function importPuppetClasses\n        Force the reload of puppet classes\n\n        @param smartProxyId: smartProxy Id\n        @return RETURN: the API result", "docstring_tokens": ["Function", "importPuppetClasses", "Force", "the", "reload", "of", "puppet", "classes"], "sha": "acb8fd8d74657cfac3b25c82e9c6028b93eb6c92", "url": "https://github.com/davidblaisonneau-orange/foreman/blob/acb8fd8d74657cfac3b25c82e9c6028b93eb6c92/foreman/smartProxies.py#L29-L37", "partition": "train"}
{"repo": "developersociety/django-glitter", "path": "glitter/templates.py", "func_name": "get_templates", "original_string": "def get_templates(model):\n    \"\"\" Return a list of templates usable by a model. \"\"\"\n    for template_name, template in templates.items():\n        if issubclass(template.model, model):\n            yield (template_name, template.layout._meta.verbose_name)", "language": "python", "code": "def get_templates(model):\n    \"\"\" Return a list of templates usable by a model. \"\"\"\n    for template_name, template in templates.items():\n        if issubclass(template.model, model):\n            yield (template_name, template.layout._meta.verbose_name)", "code_tokens": ["def", "get_templates", "(", "model", ")", ":", "for", "template_name", ",", "template", "in", "templates", ".", "items", "(", ")", ":", "if", "issubclass", "(", "template", ".", "model", ",", "model", ")", ":", "yield", "(", "template_name", ",", "template", ".", "layout", ".", "_meta", ".", "verbose_name", ")"], "docstring": "Return a list of templates usable by a model.", "docstring_tokens": ["Return", "a", "list", "of", "templates", "usable", "by", "a", "model", "."], "sha": "2c0280ec83afee80deee94ee3934fc54239c2e87", "url": "https://github.com/developersociety/django-glitter/blob/2c0280ec83afee80deee94ee3934fc54239c2e87/glitter/templates.py#L26-L30", "partition": "train"}
{"repo": "developersociety/django-glitter", "path": "glitter/templates.py", "func_name": "attach", "original_string": "def attach(*layouts, **kwargs):\n    \"\"\"\n    Registers the given layout(s) classes\n    admin site:\n\n    @pages.register(Page)\n    class Default(PageLayout):\n        pass\n    \"\"\"\n\n    def _model_admin_wrapper(layout_class):\n        register(layout_class, layouts[0])\n        return layout_class\n    return _model_admin_wrapper", "language": "python", "code": "def attach(*layouts, **kwargs):\n    \"\"\"\n    Registers the given layout(s) classes\n    admin site:\n\n    @pages.register(Page)\n    class Default(PageLayout):\n        pass\n    \"\"\"\n\n    def _model_admin_wrapper(layout_class):\n        register(layout_class, layouts[0])\n        return layout_class\n    return _model_admin_wrapper", "code_tokens": ["def", "attach", "(", "*", "layouts", ",", "*", "*", "kwargs", ")", ":", "def", "_model_admin_wrapper", "(", "layout_class", ")", ":", "register", "(", "layout_class", ",", "layouts", "[", "0", "]", ")", "return", "layout_class", "return", "_model_admin_wrapper"], "docstring": "Registers the given layout(s) classes\n    admin site:\n\n    @pages.register(Page)\n    class Default(PageLayout):\n        pass", "docstring_tokens": ["Registers", "the", "given", "layout", "(", "s", ")", "classes", "admin", "site", ":"], "sha": "2c0280ec83afee80deee94ee3934fc54239c2e87", "url": "https://github.com/developersociety/django-glitter/blob/2c0280ec83afee80deee94ee3934fc54239c2e87/glitter/templates.py#L38-L51", "partition": "train"}
{"repo": "davidblaisonneau-orange/foreman", "path": "foreman/itemOperatingSystem.py", "func_name": "ItemOperatingSystem.enhance", "original_string": "def enhance(self):\n        \"\"\" Function enhance\n        Enhance the object with new item or enhanced items\n        \"\"\"\n        self.update({'os_default_templates':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemOsDefaultTemplate)})\n        self.update({'config_templates':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemConfigTemplate)})\n        self.update({'ptables':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemPTable)})\n        self.update({'media':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemMedia)})\n        self.update({'architectures':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemArchitecture)})", "language": "python", "code": "def enhance(self):\n        \"\"\" Function enhance\n        Enhance the object with new item or enhanced items\n        \"\"\"\n        self.update({'os_default_templates':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemOsDefaultTemplate)})\n        self.update({'config_templates':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemConfigTemplate)})\n        self.update({'ptables':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemPTable)})\n        self.update({'media':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemMedia)})\n        self.update({'architectures':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemArchitecture)})", "code_tokens": ["def", "enhance", "(", "self", ")", ":", "self", ".", "update", "(", "{", "'os_default_templates'", ":", "SubDict", "(", "self", ".", "api", ",", "self", ".", "objName", ",", "self", ".", "payloadObj", ",", "self", ".", "key", ",", "SubItemOsDefaultTemplate", ")", "}", ")", "self", ".", "update", "(", "{", "'config_templates'", ":", "SubDict", "(", "self", ".", "api", ",", "self", ".", "objName", ",", "self", ".", "payloadObj", ",", "self", ".", "key", ",", "SubItemConfigTemplate", ")", "}", ")", "self", ".", "update", "(", "{", "'ptables'", ":", "SubDict", "(", "self", ".", "api", ",", "self", ".", "objName", ",", "self", ".", "payloadObj", ",", "self", ".", "key", ",", "SubItemPTable", ")", "}", ")", "self", ".", "update", "(", "{", "'media'", ":", "SubDict", "(", "self", ".", "api", ",", "self", ".", "objName", ",", "self", ".", "payloadObj", ",", "self", ".", "key", ",", "SubItemMedia", ")", "}", ")", "self", ".", "update", "(", "{", "'architectures'", ":", "SubDict", "(", "self", ".", "api", ",", "self", ".", "objName", ",", "self", ".", "payloadObj", ",", "self", ".", "key", ",", "SubItemArchitecture", ")", "}", ")"], "docstring": "Function enhance\n        Enhance the object with new item or enhanced items", "docstring_tokens": ["Function", "enhance", "Enhance", "the", "object", "with", "new", "item", "or", "enhanced", "items"], "sha": "acb8fd8d74657cfac3b25c82e9c6028b93eb6c92", "url": "https://github.com/davidblaisonneau-orange/foreman/blob/acb8fd8d74657cfac3b25c82e9c6028b93eb6c92/foreman/itemOperatingSystem.py#L40-L63", "partition": "train"}
{"repo": "quizl/quizler", "path": "quizler/lib.py", "func_name": "get_api_envs", "original_string": "def get_api_envs():\n    \"\"\"Get required API keys from environment variables.\"\"\"\n    client_id = os.environ.get('CLIENT_ID')\n    user_id = os.environ.get('USER_ID')\n    if not client_id or not user_id:\n        raise ValueError('API keys are not found in the environment')\n    return client_id, user_id", "language": "python", "code": "def get_api_envs():\n    \"\"\"Get required API keys from environment variables.\"\"\"\n    client_id = os.environ.get('CLIENT_ID')\n    user_id = os.environ.get('USER_ID')\n    if not client_id or not user_id:\n        raise ValueError('API keys are not found in the environment')\n    return client_id, user_id", "code_tokens": ["def", "get_api_envs", "(", ")", ":", "client_id", "=", "os", ".", "environ", ".", "get", "(", "'CLIENT_ID'", ")", "user_id", "=", "os", ".", "environ", ".", "get", "(", "'USER_ID'", ")", "if", "not", "client_id", "or", "not", "user_id", ":", "raise", "ValueError", "(", "'API keys are not found in the environment'", ")", "return", "client_id", ",", "user_id"], "docstring": "Get required API keys from environment variables.", "docstring_tokens": ["Get", "required", "API", "keys", "from", "environment", "variables", "."], "sha": "44b3fd91f7074e7013ffde8147455f45ebdccc46", "url": "https://github.com/quizl/quizler/blob/44b3fd91f7074e7013ffde8147455f45ebdccc46/quizler/lib.py#L8-L14", "partition": "train"}
{"repo": "quizl/quizler", "path": "quizler/lib.py", "func_name": "api_call", "original_string": "def api_call(method, end_point, params=None, client_id=None, access_token=None):\n    \"\"\"Call given API end_point with API keys.\n    :param method: HTTP method (e.g. 'get', 'delete').\n    :param end_point: API endpoint (e.g. 'users/john/sets').\n    :param params: Dictionary to be sent in the query string (e.g. {'myparam': 'myval'})\n    :param client_id: Quizlet client ID as string.\n    :param access_token: Quizlet access token as string.\n    client_id and access_token are mutually exclusive but mandatory.\n    \"\"\"\n    if bool(client_id) == bool(access_token):\n        raise ValueError('Either client_id or access_token')\n\n    url = 'https://api.quizlet.com/2.0/{}'.format(end_point)\n\n    if not params:\n        params = {}\n    if client_id:\n        params['client_id'] = client_id\n\n    headers = {'Authorization': 'Bearer {}'.format(access_token)} if access_token else None\n\n    response = requests.request(method, url, params=params, headers=headers)\n\n    if int(response.status_code / 100) != 2:\n        error_title = ''\n        try:\n            error_title += ', ' + response.json()['error_title']\n        except ValueError:\n            pass\n        except KeyError:\n            pass\n        raise ValueError(\n            '{} returned {}{}'.format(url, response.status_code, error_title)\n        )\n\n    try:\n        return response.json()\n    except json.decoder.JSONDecodeError:\n        pass", "language": "python", "code": "def api_call(method, end_point, params=None, client_id=None, access_token=None):\n    \"\"\"Call given API end_point with API keys.\n    :param method: HTTP method (e.g. 'get', 'delete').\n    :param end_point: API endpoint (e.g. 'users/john/sets').\n    :param params: Dictionary to be sent in the query string (e.g. {'myparam': 'myval'})\n    :param client_id: Quizlet client ID as string.\n    :param access_token: Quizlet access token as string.\n    client_id and access_token are mutually exclusive but mandatory.\n    \"\"\"\n    if bool(client_id) == bool(access_token):\n        raise ValueError('Either client_id or access_token')\n\n    url = 'https://api.quizlet.com/2.0/{}'.format(end_point)\n\n    if not params:\n        params = {}\n    if client_id:\n        params['client_id'] = client_id\n\n    headers = {'Authorization': 'Bearer {}'.format(access_token)} if access_token else None\n\n    response = requests.request(method, url, params=params, headers=headers)\n\n    if int(response.status_code / 100) != 2:\n        error_title = ''\n        try:\n            error_title += ', ' + response.json()['error_title']\n        except ValueError:\n            pass\n        except KeyError:\n            pass\n        raise ValueError(\n            '{} returned {}{}'.format(url, response.status_code, error_title)\n        )\n\n    try:\n        return response.json()\n    except json.decoder.JSONDecodeError:\n        pass", "code_tokens": ["def", "api_call", "(", "method", ",", "end_point", ",", "params", "=", "None", ",", "client_id", "=", "None", ",", "access_token", "=", "None", ")", ":", "if", "bool", "(", "client_id", ")", "==", "bool", "(", "access_token", ")", ":", "raise", "ValueError", "(", "'Either client_id or access_token'", ")", "url", "=", "'https://api.quizlet.com/2.0/{}'", ".", "format", "(", "end_point", ")", "if", "not", "params", ":", "params", "=", "{", "}", "if", "client_id", ":", "params", "[", "'client_id'", "]", "=", "client_id", "headers", "=", "{", "'Authorization'", ":", "'Bearer {}'", ".", "format", "(", "access_token", ")", "}", "if", "access_token", "else", "None", "response", "=", "requests", ".", "request", "(", "method", ",", "url", ",", "params", "=", "params", ",", "headers", "=", "headers", ")", "if", "int", "(", "response", ".", "status_code", "/", "100", ")", "!=", "2", ":", "error_title", "=", "''", "try", ":", "error_title", "+=", "', '", "+", "response", ".", "json", "(", ")", "[", "'error_title'", "]", "except", "ValueError", ":", "pass", "except", "KeyError", ":", "pass", "raise", "ValueError", "(", "'{} returned {}{}'", ".", "format", "(", "url", ",", "response", ".", "status_code", ",", "error_title", ")", ")", "try", ":", "return", "response", ".", "json", "(", ")", "except", "json", ".", "decoder", ".", "JSONDecodeError", ":", "pass"], "docstring": "Call given API end_point with API keys.\n    :param method: HTTP method (e.g. 'get', 'delete').\n    :param end_point: API endpoint (e.g. 'users/john/sets').\n    :param params: Dictionary to be sent in the query string (e.g. {'myparam': 'myval'})\n    :param client_id: Quizlet client ID as string.\n    :param access_token: Quizlet access token as string.\n    client_id and access_token are mutually exclusive but mandatory.", "docstring_tokens": ["Call", "given", "API", "end_point", "with", "API", "keys", ".", ":", "param", "method", ":", "HTTP", "method", "(", "e", ".", "g", ".", "get", "delete", ")", ".", ":", "param", "end_point", ":", "API", "endpoint", "(", "e", ".", "g", ".", "users", "/", "john", "/", "sets", ")", ".", ":", "param", "params", ":", "Dictionary", "to", "be", "sent", "in", "the", "query", "string", "(", "e", ".", "g", ".", "{", "myparam", ":", "myval", "}", ")", ":", "param", "client_id", ":", "Quizlet", "client", "ID", "as", "string", ".", ":", "param", "access_token", ":", "Quizlet", "access", "token", "as", "string", ".", "client_id", "and", "access_token", "are", "mutually", "exclusive", "but", "mandatory", "."], "sha": "44b3fd91f7074e7013ffde8147455f45ebdccc46", "url": "https://github.com/quizl/quizler/blob/44b3fd91f7074e7013ffde8147455f45ebdccc46/quizler/lib.py#L17-L55", "partition": "train"}
{"repo": "qubell/contrib-python-qubell-client", "path": "qubell/api/private/service.py", "func_name": "ServiceMixin.request_upload_secret", "original_string": "def request_upload_secret(self, secret_id):\n        \"\"\"\n        :return: json with \"keyId\" as secret and \"url\" for posting key\n        \"\"\"\n        return self._router.post_request_upload_secret(org_id=self.organizationId,\n                                                       instance_id=self.instanceId,\n                                                       secret_id=secret_id).json()", "language": "python", "code": "def request_upload_secret(self, secret_id):\n        \"\"\"\n        :return: json with \"keyId\" as secret and \"url\" for posting key\n        \"\"\"\n        return self._router.post_request_upload_secret(org_id=self.organizationId,\n                                                       instance_id=self.instanceId,\n                                                       secret_id=secret_id).json()", "code_tokens": ["def", "request_upload_secret", "(", "self", ",", "secret_id", ")", ":", "return", "self", ".", "_router", ".", "post_request_upload_secret", "(", "org_id", "=", "self", ".", "organizationId", ",", "instance_id", "=", "self", ".", "instanceId", ",", "secret_id", "=", "secret_id", ")", ".", "json", "(", ")"], "docstring": ":return: json with \"keyId\" as secret and \"url\" for posting key", "docstring_tokens": [":", "return", ":", "json", "with", "keyId", "as", "secret", "and", "url", "for", "posting", "key"], "sha": "4586ea11d5103c2ff9607d3ed922b5a0991b8845", "url": "https://github.com/qubell/contrib-python-qubell-client/blob/4586ea11d5103c2ff9607d3ed922b5a0991b8845/qubell/api/private/service.py#L119-L125", "partition": "train"}
{"repo": "davidblaisonneau-orange/foreman", "path": "foreman/subnets.py", "func_name": "Subnets.checkAndCreate", "original_string": "def checkAndCreate(self, key, payload, domainId):\n        \"\"\" Function checkAndCreate\n        Check if a subnet exists and create it if not\n\n        @param key: The targeted subnet\n        @param payload: The targeted subnet description\n        @param domainId: The domainId to be attached wiuth the subnet\n        @return RETURN: The id of the subnet\n        \"\"\"\n        if key not in self:\n            self[key] = payload\n        oid = self[key]['id']\n        if not oid:\n            return False\n        #~ Ensure subnet contains the domain\n        subnetDomainIds = []\n        for domain in self[key]['domains']:\n            subnetDomainIds.append(domain['id'])\n        if domainId not in subnetDomainIds:\n            subnetDomainIds.append(domainId)\n            self[key][\"domain_ids\"] = subnetDomainIds\n            if len(self[key][\"domains\"]) is not len(subnetDomainIds):\n                return False\n        return oid", "language": "python", "code": "def checkAndCreate(self, key, payload, domainId):\n        \"\"\" Function checkAndCreate\n        Check if a subnet exists and create it if not\n\n        @param key: The targeted subnet\n        @param payload: The targeted subnet description\n        @param domainId: The domainId to be attached wiuth the subnet\n        @return RETURN: The id of the subnet\n        \"\"\"\n        if key not in self:\n            self[key] = payload\n        oid = self[key]['id']\n        if not oid:\n            return False\n        #~ Ensure subnet contains the domain\n        subnetDomainIds = []\n        for domain in self[key]['domains']:\n            subnetDomainIds.append(domain['id'])\n        if domainId not in subnetDomainIds:\n            subnetDomainIds.append(domainId)\n            self[key][\"domain_ids\"] = subnetDomainIds\n            if len(self[key][\"domains\"]) is not len(subnetDomainIds):\n                return False\n        return oid", "code_tokens": ["def", "checkAndCreate", "(", "self", ",", "key", ",", "payload", ",", "domainId", ")", ":", "if", "key", "not", "in", "self", ":", "self", "[", "key", "]", "=", "payload", "oid", "=", "self", "[", "key", "]", "[", "'id'", "]", "if", "not", "oid", ":", "return", "False", "#~ Ensure subnet contains the domain", "subnetDomainIds", "=", "[", "]", "for", "domain", "in", "self", "[", "key", "]", "[", "'domains'", "]", ":", "subnetDomainIds", ".", "append", "(", "domain", "[", "'id'", "]", ")", "if", "domainId", "not", "in", "subnetDomainIds", ":", "subnetDomainIds", ".", "append", "(", "domainId", ")", "self", "[", "key", "]", "[", "\"domain_ids\"", "]", "=", "subnetDomainIds", "if", "len", "(", "self", "[", "key", "]", "[", "\"domains\"", "]", ")", "is", "not", "len", "(", "subnetDomainIds", ")", ":", "return", "False", "return", "oid"], "docstring": "Function checkAndCreate\n        Check if a subnet exists and create it if not\n\n        @param key: The targeted subnet\n        @param payload: The targeted subnet description\n        @param domainId: The domainId to be attached wiuth the subnet\n        @return RETURN: The id of the subnet", "docstring_tokens": ["Function", "checkAndCreate", "Check", "if", "a", "subnet", "exists", "and", "create", "it", "if", "not"], "sha": "acb8fd8d74657cfac3b25c82e9c6028b93eb6c92", "url": "https://github.com/davidblaisonneau-orange/foreman/blob/acb8fd8d74657cfac3b25c82e9c6028b93eb6c92/foreman/subnets.py#L29-L52", "partition": "train"}
{"repo": "davidblaisonneau-orange/foreman", "path": "foreman/subnets.py", "func_name": "Subnets.removeDomain", "original_string": "def removeDomain(self, subnetId, domainId):\n        \"\"\" Function removeDomain\n        Delete a domain from a subnet\n\n        @param subnetId: The subnet Id\n        @param domainId: The domainId to be attached wiuth the subnet\n        @return RETURN: boolean\n        \"\"\"\n        subnetDomainIds = []\n        for domain in self[subnetId]['domains']:\n            subnetDomainIds.append(domain['id'])\n        subnetDomainIds.remove(domainId)\n        self[subnetId][\"domain_ids\"] = subnetDomainIds\n        return len(self[subnetId][\"domains\"]) is len(subnetDomainIds)", "language": "python", "code": "def removeDomain(self, subnetId, domainId):\n        \"\"\" Function removeDomain\n        Delete a domain from a subnet\n\n        @param subnetId: The subnet Id\n        @param domainId: The domainId to be attached wiuth the subnet\n        @return RETURN: boolean\n        \"\"\"\n        subnetDomainIds = []\n        for domain in self[subnetId]['domains']:\n            subnetDomainIds.append(domain['id'])\n        subnetDomainIds.remove(domainId)\n        self[subnetId][\"domain_ids\"] = subnetDomainIds\n        return len(self[subnetId][\"domains\"]) is len(subnetDomainIds)", "code_tokens": ["def", "removeDomain", "(", "self", ",", "subnetId", ",", "domainId", ")", ":", "subnetDomainIds", "=", "[", "]", "for", "domain", "in", "self", "[", "subnetId", "]", "[", "'domains'", "]", ":", "subnetDomainIds", ".", "append", "(", "domain", "[", "'id'", "]", ")", "subnetDomainIds", ".", "remove", "(", "domainId", ")", "self", "[", "subnetId", "]", "[", "\"domain_ids\"", "]", "=", "subnetDomainIds", "return", "len", "(", "self", "[", "subnetId", "]", "[", "\"domains\"", "]", ")", "is", "len", "(", "subnetDomainIds", ")"], "docstring": "Function removeDomain\n        Delete a domain from a subnet\n\n        @param subnetId: The subnet Id\n        @param domainId: The domainId to be attached wiuth the subnet\n        @return RETURN: boolean", "docstring_tokens": ["Function", "removeDomain", "Delete", "a", "domain", "from", "a", "subnet"], "sha": "acb8fd8d74657cfac3b25c82e9c6028b93eb6c92", "url": "https://github.com/davidblaisonneau-orange/foreman/blob/acb8fd8d74657cfac3b25c82e9c6028b93eb6c92/foreman/subnets.py#L54-L67", "partition": "train"}
{"repo": "MatterMiners/cobald", "path": "cobald/daemon/runners/guard.py", "func_name": "exclusive", "original_string": "def exclusive(via=threading.Lock):\n    \"\"\"\n    Mark a callable as exclusive\n\n    :param via: factory for a Lock to guard the callable\n\n    Guards the callable against being entered again before completion.\n    Explicitly raises a :py:exc:`RuntimeError` on violation.\n\n    :note: If applied to a method, it is exclusive across all instances.\n    \"\"\"\n    def make_exclusive(fnc):\n        fnc_guard = via()\n\n        @functools.wraps(fnc)\n        def exclusive_call(*args, **kwargs):\n            if fnc_guard.acquire(blocking=False):\n                try:\n                    return fnc(*args, **kwargs)\n                finally:\n                    fnc_guard.release()\n            else:\n                raise RuntimeError('exclusive call to %s violated')\n        return exclusive_call\n    return make_exclusive", "language": "python", "code": "def exclusive(via=threading.Lock):\n    \"\"\"\n    Mark a callable as exclusive\n\n    :param via: factory for a Lock to guard the callable\n\n    Guards the callable against being entered again before completion.\n    Explicitly raises a :py:exc:`RuntimeError` on violation.\n\n    :note: If applied to a method, it is exclusive across all instances.\n    \"\"\"\n    def make_exclusive(fnc):\n        fnc_guard = via()\n\n        @functools.wraps(fnc)\n        def exclusive_call(*args, **kwargs):\n            if fnc_guard.acquire(blocking=False):\n                try:\n                    return fnc(*args, **kwargs)\n                finally:\n                    fnc_guard.release()\n            else:\n                raise RuntimeError('exclusive call to %s violated')\n        return exclusive_call\n    return make_exclusive", "code_tokens": ["def", "exclusive", "(", "via", "=", "threading", ".", "Lock", ")", ":", "def", "make_exclusive", "(", "fnc", ")", ":", "fnc_guard", "=", "via", "(", ")", "@", "functools", ".", "wraps", "(", "fnc", ")", "def", "exclusive_call", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "if", "fnc_guard", ".", "acquire", "(", "blocking", "=", "False", ")", ":", "try", ":", "return", "fnc", "(", "*", "args", ",", "*", "*", "kwargs", ")", "finally", ":", "fnc_guard", ".", "release", "(", ")", "else", ":", "raise", "RuntimeError", "(", "'exclusive call to %s violated'", ")", "return", "exclusive_call", "return", "make_exclusive"], "docstring": "Mark a callable as exclusive\n\n    :param via: factory for a Lock to guard the callable\n\n    Guards the callable against being entered again before completion.\n    Explicitly raises a :py:exc:`RuntimeError` on violation.\n\n    :note: If applied to a method, it is exclusive across all instances.", "docstring_tokens": ["Mark", "a", "callable", "as", "exclusive"], "sha": "264138de4382d1c9b53fabcbc6660e10b33a914d", "url": "https://github.com/MatterMiners/cobald/blob/264138de4382d1c9b53fabcbc6660e10b33a914d/cobald/daemon/runners/guard.py#L5-L29", "partition": "train"}
{"repo": "MatterMiners/cobald", "path": "cobald/daemon/runners/service.py", "func_name": "service", "original_string": "def service(flavour):\n    r\"\"\"\n    Mark a class as implementing a Service\n\n    Each Service class must have a ``run`` method, which does not take any arguments.\n    This method is :py:meth:`~.ServiceRunner.adopt`\\ ed after the daemon starts, unless\n\n    * the Service has been garbage collected, or\n    * the ServiceUnit has been :py:meth:`~.ServiceUnit.cancel`\\ ed.\n\n    For each service instance, its :py:class:`~.ServiceUnit` is available at ``service_instance.__service_unit__``.\n    \"\"\"\n    def service_unit_decorator(raw_cls):\n        __new__ = raw_cls.__new__\n\n        def __new_service__(cls, *args, **kwargs):\n            if __new__ is object.__new__:\n                self = __new__(cls)\n            else:\n                self = __new__(cls, *args, **kwargs)\n            service_unit = ServiceUnit(self, flavour)\n            self.__service_unit__ = service_unit\n            return self\n\n        raw_cls.__new__ = __new_service__\n        if raw_cls.run.__doc__ is None:\n            raw_cls.run.__doc__ = \"Service entry point\"\n        return raw_cls\n    return service_unit_decorator", "language": "python", "code": "def service(flavour):\n    r\"\"\"\n    Mark a class as implementing a Service\n\n    Each Service class must have a ``run`` method, which does not take any arguments.\n    This method is :py:meth:`~.ServiceRunner.adopt`\\ ed after the daemon starts, unless\n\n    * the Service has been garbage collected, or\n    * the ServiceUnit has been :py:meth:`~.ServiceUnit.cancel`\\ ed.\n\n    For each service instance, its :py:class:`~.ServiceUnit` is available at ``service_instance.__service_unit__``.\n    \"\"\"\n    def service_unit_decorator(raw_cls):\n        __new__ = raw_cls.__new__\n\n        def __new_service__(cls, *args, **kwargs):\n            if __new__ is object.__new__:\n                self = __new__(cls)\n            else:\n                self = __new__(cls, *args, **kwargs)\n            service_unit = ServiceUnit(self, flavour)\n            self.__service_unit__ = service_unit\n            return self\n\n        raw_cls.__new__ = __new_service__\n        if raw_cls.run.__doc__ is None:\n            raw_cls.run.__doc__ = \"Service entry point\"\n        return raw_cls\n    return service_unit_decorator", "code_tokens": ["def", "service", "(", "flavour", ")", ":", "def", "service_unit_decorator", "(", "raw_cls", ")", ":", "__new__", "=", "raw_cls", ".", "__new__", "def", "__new_service__", "(", "cls", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "if", "__new__", "is", "object", ".", "__new__", ":", "self", "=", "__new__", "(", "cls", ")", "else", ":", "self", "=", "__new__", "(", "cls", ",", "*", "args", ",", "*", "*", "kwargs", ")", "service_unit", "=", "ServiceUnit", "(", "self", ",", "flavour", ")", "self", ".", "__service_unit__", "=", "service_unit", "return", "self", "raw_cls", ".", "__new__", "=", "__new_service__", "if", "raw_cls", ".", "run", ".", "__doc__", "is", "None", ":", "raw_cls", ".", "run", ".", "__doc__", "=", "\"Service entry point\"", "return", "raw_cls", "return", "service_unit_decorator"], "docstring": "r\"\"\"\n    Mark a class as implementing a Service\n\n    Each Service class must have a ``run`` method, which does not take any arguments.\n    This method is :py:meth:`~.ServiceRunner.adopt`\\ ed after the daemon starts, unless\n\n    * the Service has been garbage collected, or\n    * the ServiceUnit has been :py:meth:`~.ServiceUnit.cancel`\\ ed.\n\n    For each service instance, its :py:class:`~.ServiceUnit` is available at ``service_instance.__service_unit__``.", "docstring_tokens": ["r", "Mark", "a", "class", "as", "implementing", "a", "Service"], "sha": "264138de4382d1c9b53fabcbc6660e10b33a914d", "url": "https://github.com/MatterMiners/cobald/blob/264138de4382d1c9b53fabcbc6660e10b33a914d/cobald/daemon/runners/service.py#L54-L82", "partition": "train"}
{"repo": "MatterMiners/cobald", "path": "cobald/daemon/runners/service.py", "func_name": "ServiceRunner.execute", "original_string": "def execute(self, payload, *args, flavour: ModuleType, **kwargs):\n        \"\"\"\n        Synchronously run ``payload`` and provide its output\n\n        If ``*args*`` and/or ``**kwargs`` are provided, pass them to ``payload`` upon execution.\n        \"\"\"\n        if args or kwargs:\n            payload = functools.partial(payload, *args, **kwargs)\n        return self._meta_runner.run_payload(payload, flavour=flavour)", "language": "python", "code": "def execute(self, payload, *args, flavour: ModuleType, **kwargs):\n        \"\"\"\n        Synchronously run ``payload`` and provide its output\n\n        If ``*args*`` and/or ``**kwargs`` are provided, pass them to ``payload`` upon execution.\n        \"\"\"\n        if args or kwargs:\n            payload = functools.partial(payload, *args, **kwargs)\n        return self._meta_runner.run_payload(payload, flavour=flavour)", "code_tokens": ["def", "execute", "(", "self", ",", "payload", ",", "*", "args", ",", "flavour", ":", "ModuleType", ",", "*", "*", "kwargs", ")", ":", "if", "args", "or", "kwargs", ":", "payload", "=", "functools", ".", "partial", "(", "payload", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "self", ".", "_meta_runner", ".", "run_payload", "(", "payload", ",", "flavour", "=", "flavour", ")"], "docstring": "Synchronously run ``payload`` and provide its output\n\n        If ``*args*`` and/or ``**kwargs`` are provided, pass them to ``payload`` upon execution.", "docstring_tokens": ["Synchronously", "run", "payload", "and", "provide", "its", "output"], "sha": "264138de4382d1c9b53fabcbc6660e10b33a914d", "url": "https://github.com/MatterMiners/cobald/blob/264138de4382d1c9b53fabcbc6660e10b33a914d/cobald/daemon/runners/service.py#L97-L105", "partition": "train"}
{"repo": "astroduff/commah", "path": "examples.py", "func_name": "runcommand", "original_string": "def runcommand(cosmology='WMAP5'):\n    \"\"\" Example interface commands \"\"\"\n\n    # Return the WMAP5 cosmology concentration predicted for\n    # z=0 range of masses\n    Mi = [1e8, 1e9, 1e10]\n    zi = 0\n    print(\"Concentrations for haloes of mass %s at z=%s\" % (Mi, zi))\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi)\n\n    print(output['c'].flatten())\n\n    # Return the WMAP5 cosmology concentration predicted for\n    # z=0 range of masses AND cosmological parameters\n    Mi = [1e8, 1e9, 1e10]\n    zi = 0\n    print(\"Concentrations for haloes of mass %s at z=%s\" % (Mi, zi))\n    output, cosmo = commah.run(cosmology=cosmology, zi=zi, Mi=Mi,\n                               retcosmo=True)\n\n    print(output['c'].flatten())\n    print(cosmo)\n\n    # Return the WMAP5 cosmology concentration predicted for MW\n    # mass (2e12 Msol) across redshift\n    Mi = 2e12\n    z = [0, 0.5, 1, 1.5, 2, 2.5]\n    output = commah.run(cosmology=cosmology, zi=0, Mi=Mi, z=z)\n    for zval in z:\n        print(\"M(z=0)=%s has c(z=%s)=%s\"\n              % (Mi, zval, output[output['z'] == zval]['c'].flatten()))\n\n    # Return the WMAP5 cosmology concentration predicted for MW\n    # mass (2e12 Msol) across redshift\n    Mi = 2e12\n    zi = [0, 0.5, 1, 1.5, 2, 2.5]\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi)\n    for zval in zi:\n        print(\"M(z=%s)=%s has concentration %s\"\n              % (zval, Mi, output[(output['zi'] == zval) &\n                                  (output['z'] == zval)]['c'].flatten()))\n\n    # Return the WMAP5 cosmology concentration and\n    # rarity of high-z cluster\n    Mi = 2e14\n    zi = 6\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi)\n    print(\"Concentrations for haloes of mass %s at z=%s\" % (Mi, zi))\n    print(output['c'].flatten())\n    print(\"Mass variance sigma of haloes of mass %s at z=%s\" % (Mi, zi))\n    print(output['sig'].flatten())\n    print(\"Fluctuation for haloes of mass %s at z=%s\" % (Mi, zi))\n    print(output['nu'].flatten())\n\n    # Return the WMAP5 cosmology accretion rate prediction\n    # for haloes at range of redshift and mass\n    Mi = [1e8, 1e9, 1e10]\n    zi = [0]\n    z = [0, 0.5, 1, 1.5, 2, 2.5]\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi, z=z)\n    for Mval in Mi:\n        print(\"dM/dt for halo of mass %s at z=%s across redshift %s is: \"\n              % (Mval, zi, z))\n        print(output[output['Mi'] == Mval]['dMdt'].flatten())\n\n    # Return the WMAP5 cosmology Halo Mass History for haloes with M(z=0) = 1e8\n    M = [1e8]\n    z = [0, 0.5, 1, 1.5, 2, 2.5]\n    print(\"Halo Mass History for z=0 mass of %s across z=%s\" % (M, z))\n    output = commah.run(cosmology=cosmology, zi=0, Mi=M, z=z)\n    print(output['Mz'].flatten())\n\n    # Return the WMAP5 cosmology formation redshifts for haloes at\n    # range of redshift and mass\n    M = [1e8, 1e9, 1e10]\n    z = [0]\n    print(\"Formation Redshifts for haloes of mass %s at z=%s\" % (M, z))\n    output = commah.run(cosmology=cosmology, zi=0, Mi=M, z=z)\n    for Mval in M:\n        print(output[output['Mi'] == Mval]['zf'].flatten())\n\n    return(\"Done\")", "language": "python", "code": "def runcommand(cosmology='WMAP5'):\n    \"\"\" Example interface commands \"\"\"\n\n    # Return the WMAP5 cosmology concentration predicted for\n    # z=0 range of masses\n    Mi = [1e8, 1e9, 1e10]\n    zi = 0\n    print(\"Concentrations for haloes of mass %s at z=%s\" % (Mi, zi))\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi)\n\n    print(output['c'].flatten())\n\n    # Return the WMAP5 cosmology concentration predicted for\n    # z=0 range of masses AND cosmological parameters\n    Mi = [1e8, 1e9, 1e10]\n    zi = 0\n    print(\"Concentrations for haloes of mass %s at z=%s\" % (Mi, zi))\n    output, cosmo = commah.run(cosmology=cosmology, zi=zi, Mi=Mi,\n                               retcosmo=True)\n\n    print(output['c'].flatten())\n    print(cosmo)\n\n    # Return the WMAP5 cosmology concentration predicted for MW\n    # mass (2e12 Msol) across redshift\n    Mi = 2e12\n    z = [0, 0.5, 1, 1.5, 2, 2.5]\n    output = commah.run(cosmology=cosmology, zi=0, Mi=Mi, z=z)\n    for zval in z:\n        print(\"M(z=0)=%s has c(z=%s)=%s\"\n              % (Mi, zval, output[output['z'] == zval]['c'].flatten()))\n\n    # Return the WMAP5 cosmology concentration predicted for MW\n    # mass (2e12 Msol) across redshift\n    Mi = 2e12\n    zi = [0, 0.5, 1, 1.5, 2, 2.5]\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi)\n    for zval in zi:\n        print(\"M(z=%s)=%s has concentration %s\"\n              % (zval, Mi, output[(output['zi'] == zval) &\n                                  (output['z'] == zval)]['c'].flatten()))\n\n    # Return the WMAP5 cosmology concentration and\n    # rarity of high-z cluster\n    Mi = 2e14\n    zi = 6\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi)\n    print(\"Concentrations for haloes of mass %s at z=%s\" % (Mi, zi))\n    print(output['c'].flatten())\n    print(\"Mass variance sigma of haloes of mass %s at z=%s\" % (Mi, zi))\n    print(output['sig'].flatten())\n    print(\"Fluctuation for haloes of mass %s at z=%s\" % (Mi, zi))\n    print(output['nu'].flatten())\n\n    # Return the WMAP5 cosmology accretion rate prediction\n    # for haloes at range of redshift and mass\n    Mi = [1e8, 1e9, 1e10]\n    zi = [0]\n    z = [0, 0.5, 1, 1.5, 2, 2.5]\n    output = commah.run(cosmology=cosmology, zi=zi, Mi=Mi, z=z)\n    for Mval in Mi:\n        print(\"dM/dt for halo of mass %s at z=%s across redshift %s is: \"\n              % (Mval, zi, z))\n        print(output[output['Mi'] == Mval]['dMdt'].flatten())\n\n    # Return the WMAP5 cosmology Halo Mass History for haloes with M(z=0) = 1e8\n    M = [1e8]\n    z = [0, 0.5, 1, 1.5, 2, 2.5]\n    print(\"Halo Mass History for z=0 mass of %s across z=%s\" % (M, z))\n    output = commah.run(cosmology=cosmology, zi=0, Mi=M, z=z)\n    print(output['Mz'].flatten())\n\n    # Return the WMAP5 cosmology formation redshifts for haloes at\n    # range of redshift and mass\n    M = [1e8, 1e9, 1e10]\n    z = [0]\n    print(\"Formation Redshifts for haloes of mass %s at z=%s\" % (M, z))\n    output = commah.run(cosmology=cosmology, zi=0, Mi=M, z=z)\n    for Mval in M:\n        print(output[output['Mi'] == Mval]['zf'].flatten())\n\n    return(\"Done\")", "code_tokens": ["def", "runcommand", "(", "cosmology", "=", "'WMAP5'", ")", ":", "# Return the WMAP5 cosmology concentration predicted for", "# z=0 range of masses", "Mi", "=", "[", "1e8", ",", "1e9", ",", "1e10", "]", "zi", "=", "0", "print", "(", "\"Concentrations for haloes of mass %s at z=%s\"", "%", "(", "Mi", ",", "zi", ")", ")", "output", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "zi", ",", "Mi", "=", "Mi", ")", "print", "(", "output", "[", "'c'", "]", ".", "flatten", "(", ")", ")", "# Return the WMAP5 cosmology concentration predicted for", "# z=0 range of masses AND cosmological parameters", "Mi", "=", "[", "1e8", ",", "1e9", ",", "1e10", "]", "zi", "=", "0", "print", "(", "\"Concentrations for haloes of mass %s at z=%s\"", "%", "(", "Mi", ",", "zi", ")", ")", "output", ",", "cosmo", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "zi", ",", "Mi", "=", "Mi", ",", "retcosmo", "=", "True", ")", "print", "(", "output", "[", "'c'", "]", ".", "flatten", "(", ")", ")", "print", "(", "cosmo", ")", "# Return the WMAP5 cosmology concentration predicted for MW", "# mass (2e12 Msol) across redshift", "Mi", "=", "2e12", "z", "=", "[", "0", ",", "0.5", ",", "1", ",", "1.5", ",", "2", ",", "2.5", "]", "output", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "0", ",", "Mi", "=", "Mi", ",", "z", "=", "z", ")", "for", "zval", "in", "z", ":", "print", "(", "\"M(z=0)=%s has c(z=%s)=%s\"", "%", "(", "Mi", ",", "zval", ",", "output", "[", "output", "[", "'z'", "]", "==", "zval", "]", "[", "'c'", "]", ".", "flatten", "(", ")", ")", ")", "# Return the WMAP5 cosmology concentration predicted for MW", "# mass (2e12 Msol) across redshift", "Mi", "=", "2e12", "zi", "=", "[", "0", ",", "0.5", ",", "1", ",", "1.5", ",", "2", ",", "2.5", "]", "output", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "zi", ",", "Mi", "=", "Mi", ")", "for", "zval", "in", "zi", ":", "print", "(", "\"M(z=%s)=%s has concentration %s\"", "%", "(", "zval", ",", "Mi", ",", "output", "[", "(", "output", "[", "'zi'", "]", "==", "zval", ")", "&", "(", "output", "[", "'z'", "]", "==", "zval", ")", "]", "[", "'c'", "]", ".", "flatten", "(", ")", ")", ")", "# Return the WMAP5 cosmology concentration and", "# rarity of high-z cluster", "Mi", "=", "2e14", "zi", "=", "6", "output", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "zi", ",", "Mi", "=", "Mi", ")", "print", "(", "\"Concentrations for haloes of mass %s at z=%s\"", "%", "(", "Mi", ",", "zi", ")", ")", "print", "(", "output", "[", "'c'", "]", ".", "flatten", "(", ")", ")", "print", "(", "\"Mass variance sigma of haloes of mass %s at z=%s\"", "%", "(", "Mi", ",", "zi", ")", ")", "print", "(", "output", "[", "'sig'", "]", ".", "flatten", "(", ")", ")", "print", "(", "\"Fluctuation for haloes of mass %s at z=%s\"", "%", "(", "Mi", ",", "zi", ")", ")", "print", "(", "output", "[", "'nu'", "]", ".", "flatten", "(", ")", ")", "# Return the WMAP5 cosmology accretion rate prediction", "# for haloes at range of redshift and mass", "Mi", "=", "[", "1e8", ",", "1e9", ",", "1e10", "]", "zi", "=", "[", "0", "]", "z", "=", "[", "0", ",", "0.5", ",", "1", ",", "1.5", ",", "2", ",", "2.5", "]", "output", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "zi", ",", "Mi", "=", "Mi", ",", "z", "=", "z", ")", "for", "Mval", "in", "Mi", ":", "print", "(", "\"dM/dt for halo of mass %s at z=%s across redshift %s is: \"", "%", "(", "Mval", ",", "zi", ",", "z", ")", ")", "print", "(", "output", "[", "output", "[", "'Mi'", "]", "==", "Mval", "]", "[", "'dMdt'", "]", ".", "flatten", "(", ")", ")", "# Return the WMAP5 cosmology Halo Mass History for haloes with M(z=0) = 1e8", "M", "=", "[", "1e8", "]", "z", "=", "[", "0", ",", "0.5", ",", "1", ",", "1.5", ",", "2", ",", "2.5", "]", "print", "(", "\"Halo Mass History for z=0 mass of %s across z=%s\"", "%", "(", "M", ",", "z", ")", ")", "output", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "0", ",", "Mi", "=", "M", ",", "z", "=", "z", ")", "print", "(", "output", "[", "'Mz'", "]", ".", "flatten", "(", ")", ")", "# Return the WMAP5 cosmology formation redshifts for haloes at", "# range of redshift and mass", "M", "=", "[", "1e8", ",", "1e9", ",", "1e10", "]", "z", "=", "[", "0", "]", "print", "(", "\"Formation Redshifts for haloes of mass %s at z=%s\"", "%", "(", "M", ",", "z", ")", ")", "output", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "0", ",", "Mi", "=", "M", ",", "z", "=", "z", ")", "for", "Mval", "in", "M", ":", "print", "(", "output", "[", "output", "[", "'Mi'", "]", "==", "Mval", "]", "[", "'zf'", "]", ".", "flatten", "(", ")", ")", "return", "(", "\"Done\"", ")"], "docstring": "Example interface commands", "docstring_tokens": ["Example", "interface", "commands"], "sha": "3ec70338c5123a053c79ddcf2cb3beac26bc9137", "url": "https://github.com/astroduff/commah/blob/3ec70338c5123a053c79ddcf2cb3beac26bc9137/examples.py#L9-L90", "partition": "train"}
{"repo": "astroduff/commah", "path": "examples.py", "func_name": "plotcommand", "original_string": "def plotcommand(cosmology='WMAP5', plotname=None):\n    \"\"\" Example ways to interrogate the dataset and plot the commah output \"\"\"\n\n    # Plot the c-M relation as a functon of redshift\n    xarray = 10**(np.arange(1, 15, 0.2))\n    yval = 'c'\n\n    # Specify the redshift range\n    zarray = np.arange(0, 5, 0.5)\n\n    xtitle = r\"Halo Mass (M$_{sol}$)\"\n    ytitle = r\"Concentration\"\n    linelabel = \"z=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    plt.ylim([2, 30])\n\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=zval, Mi=xarray)\n\n        # Access the column yval from the data file\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray, label=linelabel+str(zval), color=colors[zind])\n        # Overplot the D08 predictions in black\n        ax.plot(xarray, commah.commah.cduffy(zval, xarray), color=\"black\")\n\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_CM_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_CM_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the c-z relation as a function of mass (so always Mz=M0)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'c'\n\n    # Specify the mass range\n    zarray = 10**np.arange(6, 14, 2)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"NFW Concentration\"\n    linelabel = r\"log$_{10}$ M$_{z}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=xarray, Mi=zval)\n\n        # Access the column yval from the data file\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colours\n        ax.plot(xarray, yarray,\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_Cz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_Cz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the zf-z relation for different masses (so always Mz=M0)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'zf'\n\n    # Specify the mass range\n    zarray = 10**np.arange(6, 14, 2)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"Formation Redshift\"\n    linelabel = r\"log$_{10}$ M$_{z}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=xarray, Mi=zval)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray,\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    leg = ax.legend(loc=2)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_zfz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_zfz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the dM/dt-z relation for different masses (so always Mz=M0)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'dMdt'\n\n    # Specify the mass range\n    zarray = 10**np.arange(10, 14, 0.5)\n\n    xtitle = r\"log$_{10}$ (1+z)\"\n    ytitle = r\"log$_{10}$ Accretion Rate M$_{sol}$ yr$^{-1}$\"\n    linelabel = r\"log$_{10}$ M$_z$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    cosmo = commah.getcosmo(cosmology)\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=xarray, Mi=zval,\n                            com=False, mah=True)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(np.log10(xarray+1.), np.log10(yarray),\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n        # Plot the semi-analytic approximate formula from Correa et al 2015b\n        semianalytic_approx = 71.6 * (zval / 1e12) * (cosmo['h'] / 0.7) *\\\n            (-0.24 + 0.75 * (xarray + 1)) * np.sqrt(\n            cosmo['omega_M_0'] * (xarray + 1)**3 + cosmo['omega_lambda_0'])\n\n        ax.plot(np.log10(xarray + 1), np.log10(semianalytic_approx),\n                color='black')\n\n    leg = ax.legend(loc=2)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_dMdtz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_dMdtz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the dMdt-M relation as a function of redshift\n    xarray = 10**(np.arange(10, 14, 0.5))\n    yval = 'dMdt'\n\n    # Specify the redshift range\n    zarray = np.arange(0, 5, 0.5)\n\n    xtitle = r\"Halo Mass M$_{sol}$\"\n    ytitle = r\"Accretion Rate M$_{sol}$ yr$^{-1}$\"\n    linelabel = \"z=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=zval, Mi=xarray,\n                            com=False, mah=True)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray, label=linelabel+str(zval),\n                color=colors[zind],)\n\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=2)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_MAH_M_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_MAH_M_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the (dM/M)dt-M relation as a function of redshift\n    xarray = 10**(np.arange(10, 14, 0.5))\n    yval = 'dMdt'\n\n    # Specify the redshift range\n    zarray = np.arange(0, 5, 0.5)\n\n    xtitle = r\"Halo Mass M$_{sol}$\"\n    ytitle = r\"Specific Accretion Rate yr$^{-1}$\"\n    linelabel = \"z=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=zval, Mi=xarray,\n                            mah=True, com=False)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray/xarray, label=linelabel+str(zval),\n                color=colors[zind],)\n\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_specificMAH_M_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_specificMAH_M_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the Mz-z relation as a function of mass\n    # (so mass is decreasing to zero as z-> inf)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'Mz'\n\n    # Specify the mass range\n    zarray = 10**np.arange(10, 14, 0.5)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"M(z) (M$_{sol}$)\"\n    linelabel = r\"log$_{10}$ M$_{0}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=0, Mi=zval, z=xarray)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray,\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_Mzz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_Mzz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the Mz/M0-z relation as a function of mass\n    xarray = 10**(np.arange(0, 1, 0.02)) - 1\n    yval = 'Mz'\n\n    # Specify the mass range\n    zarray = 10**np.arange(10, 14, 0.5)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"log$_{10}$ M(z)/M$_{0}$\"\n    linelabel = r\"log$_{10}$ M$_{0}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=0, Mi=zval, z=xarray)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, np.log10(yarray/zval),\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    leg = ax.legend(loc=3)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_MzM0z_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_MzM0z_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    return(\"Done\")", "language": "python", "code": "def plotcommand(cosmology='WMAP5', plotname=None):\n    \"\"\" Example ways to interrogate the dataset and plot the commah output \"\"\"\n\n    # Plot the c-M relation as a functon of redshift\n    xarray = 10**(np.arange(1, 15, 0.2))\n    yval = 'c'\n\n    # Specify the redshift range\n    zarray = np.arange(0, 5, 0.5)\n\n    xtitle = r\"Halo Mass (M$_{sol}$)\"\n    ytitle = r\"Concentration\"\n    linelabel = \"z=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    plt.ylim([2, 30])\n\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=zval, Mi=xarray)\n\n        # Access the column yval from the data file\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray, label=linelabel+str(zval), color=colors[zind])\n        # Overplot the D08 predictions in black\n        ax.plot(xarray, commah.commah.cduffy(zval, xarray), color=\"black\")\n\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_CM_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_CM_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the c-z relation as a function of mass (so always Mz=M0)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'c'\n\n    # Specify the mass range\n    zarray = 10**np.arange(6, 14, 2)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"NFW Concentration\"\n    linelabel = r\"log$_{10}$ M$_{z}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=xarray, Mi=zval)\n\n        # Access the column yval from the data file\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colours\n        ax.plot(xarray, yarray,\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_Cz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_Cz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the zf-z relation for different masses (so always Mz=M0)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'zf'\n\n    # Specify the mass range\n    zarray = 10**np.arange(6, 14, 2)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"Formation Redshift\"\n    linelabel = r\"log$_{10}$ M$_{z}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=xarray, Mi=zval)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray,\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    leg = ax.legend(loc=2)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_zfz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_zfz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the dM/dt-z relation for different masses (so always Mz=M0)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'dMdt'\n\n    # Specify the mass range\n    zarray = 10**np.arange(10, 14, 0.5)\n\n    xtitle = r\"log$_{10}$ (1+z)\"\n    ytitle = r\"log$_{10}$ Accretion Rate M$_{sol}$ yr$^{-1}$\"\n    linelabel = r\"log$_{10}$ M$_z$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    cosmo = commah.getcosmo(cosmology)\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=xarray, Mi=zval,\n                            com=False, mah=True)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(np.log10(xarray+1.), np.log10(yarray),\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n        # Plot the semi-analytic approximate formula from Correa et al 2015b\n        semianalytic_approx = 71.6 * (zval / 1e12) * (cosmo['h'] / 0.7) *\\\n            (-0.24 + 0.75 * (xarray + 1)) * np.sqrt(\n            cosmo['omega_M_0'] * (xarray + 1)**3 + cosmo['omega_lambda_0'])\n\n        ax.plot(np.log10(xarray + 1), np.log10(semianalytic_approx),\n                color='black')\n\n    leg = ax.legend(loc=2)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_dMdtz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_dMdtz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the dMdt-M relation as a function of redshift\n    xarray = 10**(np.arange(10, 14, 0.5))\n    yval = 'dMdt'\n\n    # Specify the redshift range\n    zarray = np.arange(0, 5, 0.5)\n\n    xtitle = r\"Halo Mass M$_{sol}$\"\n    ytitle = r\"Accretion Rate M$_{sol}$ yr$^{-1}$\"\n    linelabel = \"z=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=zval, Mi=xarray,\n                            com=False, mah=True)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray, label=linelabel+str(zval),\n                color=colors[zind],)\n\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=2)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_MAH_M_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_MAH_M_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the (dM/M)dt-M relation as a function of redshift\n    xarray = 10**(np.arange(10, 14, 0.5))\n    yval = 'dMdt'\n\n    # Specify the redshift range\n    zarray = np.arange(0, 5, 0.5)\n\n    xtitle = r\"Halo Mass M$_{sol}$\"\n    ytitle = r\"Specific Accretion Rate yr$^{-1}$\"\n    linelabel = \"z=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=zval, Mi=xarray,\n                            mah=True, com=False)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray/xarray, label=linelabel+str(zval),\n                color=colors[zind],)\n\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_specificMAH_M_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_specificMAH_M_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the Mz-z relation as a function of mass\n    # (so mass is decreasing to zero as z-> inf)\n    xarray = 10**(np.arange(0, 1, 0.05)) - 1\n    yval = 'Mz'\n\n    # Specify the mass range\n    zarray = 10**np.arange(10, 14, 0.5)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"M(z) (M$_{sol}$)\"\n    linelabel = r\"log$_{10}$ M$_{0}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=0, Mi=zval, z=xarray)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, yarray,\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    ax.set_yscale('log')\n\n    leg = ax.legend(loc=1)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_Mzz_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_Mzz_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    # Plot the Mz/M0-z relation as a function of mass\n    xarray = 10**(np.arange(0, 1, 0.02)) - 1\n    yval = 'Mz'\n\n    # Specify the mass range\n    zarray = 10**np.arange(10, 14, 0.5)\n\n    xtitle = r\"Redshift\"\n    ytitle = r\"log$_{10}$ M(z)/M$_{0}$\"\n    linelabel = r\"log$_{10}$ M$_{0}$(M$_{sol}$)=\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(xtitle)\n    ax.set_ylabel(ytitle)\n    colors = cm.rainbow(np.linspace(0, 1, len(zarray)))\n\n    for zind, zval in enumerate(zarray):\n        output = commah.run(cosmology=cosmology, zi=0, Mi=zval, z=xarray)\n\n        yarray = output[yval].flatten()\n\n        # Plot each line in turn with different colour\n        ax.plot(xarray, np.log10(yarray/zval),\n                label=linelabel+\"{0:.1f}\".format(np.log10(zval)),\n                color=colors[zind],)\n\n    leg = ax.legend(loc=3)\n    # Make box totally transparent\n    leg.get_frame().set_alpha(0)\n    leg.get_frame().set_edgecolor('white')\n    for label in leg.get_texts():\n        label.set_fontsize('small')  # the font size\n    for label in leg.get_lines():\n        label.set_linewidth(4)   # the legend line width\n\n    if plotname:\n        fig.tight_layout(pad=0.2)\n        print(\"Plotting to '%s_MzM0z_relation.png'\" % (plotname))\n        fig.savefig(plotname+\"_MzM0z_relation.png\", dpi=fig.dpi*5)\n    else:\n        plt.show()\n\n    return(\"Done\")", "code_tokens": ["def", "plotcommand", "(", "cosmology", "=", "'WMAP5'", ",", "plotname", "=", "None", ")", ":", "# Plot the c-M relation as a functon of redshift", "xarray", "=", "10", "**", "(", "np", ".", "arange", "(", "1", ",", "15", ",", "0.2", ")", ")", "yval", "=", "'c'", "# Specify the redshift range", "zarray", "=", "np", ".", "arange", "(", "0", ",", "5", ",", "0.5", ")", "xtitle", "=", "r\"Halo Mass (M$_{sol}$)\"", "ytitle", "=", "r\"Concentration\"", "linelabel", "=", "\"z=\"", "fig", "=", "plt", ".", "figure", "(", ")", "ax", "=", "fig", ".", "add_subplot", "(", "111", ")", "ax", ".", "set_xlabel", "(", "xtitle", ")", "ax", ".", "set_ylabel", "(", "ytitle", ")", "plt", ".", "ylim", "(", "[", "2", ",", "30", "]", ")", "colors", "=", "cm", ".", "rainbow", "(", "np", ".", "linspace", "(", "0", ",", "1", ",", "len", "(", "zarray", ")", ")", ")", "for", "zind", ",", "zval", "in", "enumerate", "(", "zarray", ")", ":", "output", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "zval", ",", "Mi", "=", "xarray", ")", "# Access the column yval from the data file", "yarray", "=", "output", "[", "yval", "]", ".", "flatten", "(", ")", "# Plot each line in turn with different colour", "ax", ".", "plot", "(", "xarray", ",", "yarray", ",", "label", "=", "linelabel", "+", "str", "(", "zval", ")", ",", "color", "=", "colors", "[", "zind", "]", ")", "# Overplot the D08 predictions in black", "ax", ".", "plot", "(", "xarray", ",", "commah", ".", "commah", ".", "cduffy", "(", "zval", ",", "xarray", ")", ",", "color", "=", "\"black\"", ")", "ax", ".", "set_xscale", "(", "'log'", ")", "ax", ".", "set_yscale", "(", "'log'", ")", "leg", "=", "ax", ".", "legend", "(", "loc", "=", "1", ")", "# Make box totally transparent", "leg", ".", "get_frame", "(", ")", ".", "set_alpha", "(", "0", ")", "leg", ".", "get_frame", "(", ")", ".", "set_edgecolor", "(", "'white'", ")", "for", "label", "in", "leg", ".", "get_texts", "(", ")", ":", "label", ".", "set_fontsize", "(", "'small'", ")", "# the font size", "for", "label", "in", "leg", ".", "get_lines", "(", ")", ":", "label", ".", "set_linewidth", "(", "4", ")", "# the legend line width", "if", "plotname", ":", "fig", ".", "tight_layout", "(", "pad", "=", "0.2", ")", "print", "(", "\"Plotting to '%s_CM_relation.png'\"", "%", "(", "plotname", ")", ")", "fig", ".", "savefig", "(", "plotname", "+", "\"_CM_relation.png\"", ",", "dpi", "=", "fig", ".", "dpi", "*", "5", ")", "else", ":", "plt", ".", "show", "(", ")", "# Plot the c-z relation as a function of mass (so always Mz=M0)", "xarray", "=", "10", "**", "(", "np", ".", "arange", "(", "0", ",", "1", ",", "0.05", ")", ")", "-", "1", "yval", "=", "'c'", "# Specify the mass range", "zarray", "=", "10", "**", "np", ".", "arange", "(", "6", ",", "14", ",", "2", ")", "xtitle", "=", "r\"Redshift\"", "ytitle", "=", "r\"NFW Concentration\"", "linelabel", "=", "r\"log$_{10}$ M$_{z}$(M$_{sol}$)=\"", "fig", "=", "plt", ".", "figure", "(", ")", "ax", "=", "fig", ".", "add_subplot", "(", "111", ")", "ax", ".", "set_xlabel", "(", "xtitle", ")", "ax", ".", "set_ylabel", "(", "ytitle", ")", "colors", "=", "cm", ".", "rainbow", "(", "np", ".", "linspace", "(", "0", ",", "1", ",", "len", "(", "zarray", ")", ")", ")", "for", "zind", ",", "zval", "in", "enumerate", "(", "zarray", ")", ":", "output", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "xarray", ",", "Mi", "=", "zval", ")", "# Access the column yval from the data file", "yarray", "=", "output", "[", "yval", "]", ".", "flatten", "(", ")", "# Plot each line in turn with different colours", "ax", ".", "plot", "(", "xarray", ",", "yarray", ",", "label", "=", "linelabel", "+", "\"{0:.1f}\"", ".", "format", "(", "np", ".", "log10", "(", "zval", ")", ")", ",", "color", "=", "colors", "[", "zind", "]", ",", ")", "leg", "=", "ax", ".", "legend", "(", "loc", "=", "1", ")", "# Make box totally transparent", "leg", ".", "get_frame", "(", ")", ".", "set_alpha", "(", "0", ")", "leg", ".", "get_frame", "(", ")", ".", "set_edgecolor", "(", "'white'", ")", "for", "label", "in", "leg", ".", "get_texts", "(", ")", ":", "label", ".", "set_fontsize", "(", "'small'", ")", "# the font size", "for", "label", "in", "leg", ".", "get_lines", "(", ")", ":", "label", ".", "set_linewidth", "(", "4", ")", "# the legend line width", "if", "plotname", ":", "fig", ".", "tight_layout", "(", "pad", "=", "0.2", ")", "print", "(", "\"Plotting to '%s_Cz_relation.png'\"", "%", "(", "plotname", ")", ")", "fig", ".", "savefig", "(", "plotname", "+", "\"_Cz_relation.png\"", ",", "dpi", "=", "fig", ".", "dpi", "*", "5", ")", "else", ":", "plt", ".", "show", "(", ")", "# Plot the zf-z relation for different masses (so always Mz=M0)", "xarray", "=", "10", "**", "(", "np", ".", "arange", "(", "0", ",", "1", ",", "0.05", ")", ")", "-", "1", "yval", "=", "'zf'", "# Specify the mass range", "zarray", "=", "10", "**", "np", ".", "arange", "(", "6", ",", "14", ",", "2", ")", "xtitle", "=", "r\"Redshift\"", "ytitle", "=", "r\"Formation Redshift\"", "linelabel", "=", "r\"log$_{10}$ M$_{z}$(M$_{sol}$)=\"", "fig", "=", "plt", ".", "figure", "(", ")", "ax", "=", "fig", ".", "add_subplot", "(", "111", ")", "ax", ".", "set_xlabel", "(", "xtitle", ")", "ax", ".", "set_ylabel", "(", "ytitle", ")", "colors", "=", "cm", ".", "rainbow", "(", "np", ".", "linspace", "(", "0", ",", "1", ",", "len", "(", "zarray", ")", ")", ")", "for", "zind", ",", "zval", "in", "enumerate", "(", "zarray", ")", ":", "output", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "xarray", ",", "Mi", "=", "zval", ")", "yarray", "=", "output", "[", "yval", "]", ".", "flatten", "(", ")", "# Plot each line in turn with different colour", "ax", ".", "plot", "(", "xarray", ",", "yarray", ",", "label", "=", "linelabel", "+", "\"{0:.1f}\"", ".", "format", "(", "np", ".", "log10", "(", "zval", ")", ")", ",", "color", "=", "colors", "[", "zind", "]", ",", ")", "leg", "=", "ax", ".", "legend", "(", "loc", "=", "2", ")", "# Make box totally transparent", "leg", ".", "get_frame", "(", ")", ".", "set_alpha", "(", "0", ")", "leg", ".", "get_frame", "(", ")", ".", "set_edgecolor", "(", "'white'", ")", "for", "label", "in", "leg", ".", "get_texts", "(", ")", ":", "label", ".", "set_fontsize", "(", "'small'", ")", "# the font size", "for", "label", "in", "leg", ".", "get_lines", "(", ")", ":", "label", ".", "set_linewidth", "(", "4", ")", "# the legend line width", "if", "plotname", ":", "fig", ".", "tight_layout", "(", "pad", "=", "0.2", ")", "print", "(", "\"Plotting to '%s_zfz_relation.png'\"", "%", "(", "plotname", ")", ")", "fig", ".", "savefig", "(", "plotname", "+", "\"_zfz_relation.png\"", ",", "dpi", "=", "fig", ".", "dpi", "*", "5", ")", "else", ":", "plt", ".", "show", "(", ")", "# Plot the dM/dt-z relation for different masses (so always Mz=M0)", "xarray", "=", "10", "**", "(", "np", ".", "arange", "(", "0", ",", "1", ",", "0.05", ")", ")", "-", "1", "yval", "=", "'dMdt'", "# Specify the mass range", "zarray", "=", "10", "**", "np", ".", "arange", "(", "10", ",", "14", ",", "0.5", ")", "xtitle", "=", "r\"log$_{10}$ (1+z)\"", "ytitle", "=", "r\"log$_{10}$ Accretion Rate M$_{sol}$ yr$^{-1}$\"", "linelabel", "=", "r\"log$_{10}$ M$_z$(M$_{sol}$)=\"", "fig", "=", "plt", ".", "figure", "(", ")", "ax", "=", "fig", ".", "add_subplot", "(", "111", ")", "ax", ".", "set_xlabel", "(", "xtitle", ")", "ax", ".", "set_ylabel", "(", "ytitle", ")", "colors", "=", "cm", ".", "rainbow", "(", "np", ".", "linspace", "(", "0", ",", "1", ",", "len", "(", "zarray", ")", ")", ")", "cosmo", "=", "commah", ".", "getcosmo", "(", "cosmology", ")", "for", "zind", ",", "zval", "in", "enumerate", "(", "zarray", ")", ":", "output", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "xarray", ",", "Mi", "=", "zval", ",", "com", "=", "False", ",", "mah", "=", "True", ")", "yarray", "=", "output", "[", "yval", "]", ".", "flatten", "(", ")", "# Plot each line in turn with different colour", "ax", ".", "plot", "(", "np", ".", "log10", "(", "xarray", "+", "1.", ")", ",", "np", ".", "log10", "(", "yarray", ")", ",", "label", "=", "linelabel", "+", "\"{0:.1f}\"", ".", "format", "(", "np", ".", "log10", "(", "zval", ")", ")", ",", "color", "=", "colors", "[", "zind", "]", ",", ")", "# Plot the semi-analytic approximate formula from Correa et al 2015b", "semianalytic_approx", "=", "71.6", "*", "(", "zval", "/", "1e12", ")", "*", "(", "cosmo", "[", "'h'", "]", "/", "0.7", ")", "*", "(", "-", "0.24", "+", "0.75", "*", "(", "xarray", "+", "1", ")", ")", "*", "np", ".", "sqrt", "(", "cosmo", "[", "'omega_M_0'", "]", "*", "(", "xarray", "+", "1", ")", "**", "3", "+", "cosmo", "[", "'omega_lambda_0'", "]", ")", "ax", ".", "plot", "(", "np", ".", "log10", "(", "xarray", "+", "1", ")", ",", "np", ".", "log10", "(", "semianalytic_approx", ")", ",", "color", "=", "'black'", ")", "leg", "=", "ax", ".", "legend", "(", "loc", "=", "2", ")", "# Make box totally transparent", "leg", ".", "get_frame", "(", ")", ".", "set_alpha", "(", "0", ")", "leg", ".", "get_frame", "(", ")", ".", "set_edgecolor", "(", "'white'", ")", "for", "label", "in", "leg", ".", "get_texts", "(", ")", ":", "label", ".", "set_fontsize", "(", "'small'", ")", "# the font size", "for", "label", "in", "leg", ".", "get_lines", "(", ")", ":", "label", ".", "set_linewidth", "(", "4", ")", "# the legend line width", "if", "plotname", ":", "fig", ".", "tight_layout", "(", "pad", "=", "0.2", ")", "print", "(", "\"Plotting to '%s_dMdtz_relation.png'\"", "%", "(", "plotname", ")", ")", "fig", ".", "savefig", "(", "plotname", "+", "\"_dMdtz_relation.png\"", ",", "dpi", "=", "fig", ".", "dpi", "*", "5", ")", "else", ":", "plt", ".", "show", "(", ")", "# Plot the dMdt-M relation as a function of redshift", "xarray", "=", "10", "**", "(", "np", ".", "arange", "(", "10", ",", "14", ",", "0.5", ")", ")", "yval", "=", "'dMdt'", "# Specify the redshift range", "zarray", "=", "np", ".", "arange", "(", "0", ",", "5", ",", "0.5", ")", "xtitle", "=", "r\"Halo Mass M$_{sol}$\"", "ytitle", "=", "r\"Accretion Rate M$_{sol}$ yr$^{-1}$\"", "linelabel", "=", "\"z=\"", "fig", "=", "plt", ".", "figure", "(", ")", "ax", "=", "fig", ".", "add_subplot", "(", "111", ")", "ax", ".", "set_xlabel", "(", "xtitle", ")", "ax", ".", "set_ylabel", "(", "ytitle", ")", "colors", "=", "cm", ".", "rainbow", "(", "np", ".", "linspace", "(", "0", ",", "1", ",", "len", "(", "zarray", ")", ")", ")", "for", "zind", ",", "zval", "in", "enumerate", "(", "zarray", ")", ":", "output", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "zval", ",", "Mi", "=", "xarray", ",", "com", "=", "False", ",", "mah", "=", "True", ")", "yarray", "=", "output", "[", "yval", "]", ".", "flatten", "(", ")", "# Plot each line in turn with different colour", "ax", ".", "plot", "(", "xarray", ",", "yarray", ",", "label", "=", "linelabel", "+", "str", "(", "zval", ")", ",", "color", "=", "colors", "[", "zind", "]", ",", ")", "ax", ".", "set_xscale", "(", "'log'", ")", "ax", ".", "set_yscale", "(", "'log'", ")", "leg", "=", "ax", ".", "legend", "(", "loc", "=", "2", ")", "# Make box totally transparent", "leg", ".", "get_frame", "(", ")", ".", "set_alpha", "(", "0", ")", "leg", ".", "get_frame", "(", ")", ".", "set_edgecolor", "(", "'white'", ")", "for", "label", "in", "leg", ".", "get_texts", "(", ")", ":", "label", ".", "set_fontsize", "(", "'small'", ")", "# the font size", "for", "label", "in", "leg", ".", "get_lines", "(", ")", ":", "label", ".", "set_linewidth", "(", "4", ")", "# the legend line width", "if", "plotname", ":", "fig", ".", "tight_layout", "(", "pad", "=", "0.2", ")", "print", "(", "\"Plotting to '%s_MAH_M_relation.png'\"", "%", "(", "plotname", ")", ")", "fig", ".", "savefig", "(", "plotname", "+", "\"_MAH_M_relation.png\"", ",", "dpi", "=", "fig", ".", "dpi", "*", "5", ")", "else", ":", "plt", ".", "show", "(", ")", "# Plot the (dM/M)dt-M relation as a function of redshift", "xarray", "=", "10", "**", "(", "np", ".", "arange", "(", "10", ",", "14", ",", "0.5", ")", ")", "yval", "=", "'dMdt'", "# Specify the redshift range", "zarray", "=", "np", ".", "arange", "(", "0", ",", "5", ",", "0.5", ")", "xtitle", "=", "r\"Halo Mass M$_{sol}$\"", "ytitle", "=", "r\"Specific Accretion Rate yr$^{-1}$\"", "linelabel", "=", "\"z=\"", "fig", "=", "plt", ".", "figure", "(", ")", "ax", "=", "fig", ".", "add_subplot", "(", "111", ")", "ax", ".", "set_xlabel", "(", "xtitle", ")", "ax", ".", "set_ylabel", "(", "ytitle", ")", "colors", "=", "cm", ".", "rainbow", "(", "np", ".", "linspace", "(", "0", ",", "1", ",", "len", "(", "zarray", ")", ")", ")", "for", "zind", ",", "zval", "in", "enumerate", "(", "zarray", ")", ":", "output", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "zval", ",", "Mi", "=", "xarray", ",", "mah", "=", "True", ",", "com", "=", "False", ")", "yarray", "=", "output", "[", "yval", "]", ".", "flatten", "(", ")", "# Plot each line in turn with different colour", "ax", ".", "plot", "(", "xarray", ",", "yarray", "/", "xarray", ",", "label", "=", "linelabel", "+", "str", "(", "zval", ")", ",", "color", "=", "colors", "[", "zind", "]", ",", ")", "ax", ".", "set_xscale", "(", "'log'", ")", "ax", ".", "set_yscale", "(", "'log'", ")", "leg", "=", "ax", ".", "legend", "(", "loc", "=", "1", ")", "# Make box totally transparent", "leg", ".", "get_frame", "(", ")", ".", "set_alpha", "(", "0", ")", "leg", ".", "get_frame", "(", ")", ".", "set_edgecolor", "(", "'white'", ")", "for", "label", "in", "leg", ".", "get_texts", "(", ")", ":", "label", ".", "set_fontsize", "(", "'small'", ")", "# the font size", "for", "label", "in", "leg", ".", "get_lines", "(", ")", ":", "label", ".", "set_linewidth", "(", "4", ")", "# the legend line width", "if", "plotname", ":", "fig", ".", "tight_layout", "(", "pad", "=", "0.2", ")", "print", "(", "\"Plotting to '%s_specificMAH_M_relation.png'\"", "%", "(", "plotname", ")", ")", "fig", ".", "savefig", "(", "plotname", "+", "\"_specificMAH_M_relation.png\"", ",", "dpi", "=", "fig", ".", "dpi", "*", "5", ")", "else", ":", "plt", ".", "show", "(", ")", "# Plot the Mz-z relation as a function of mass", "# (so mass is decreasing to zero as z-> inf)", "xarray", "=", "10", "**", "(", "np", ".", "arange", "(", "0", ",", "1", ",", "0.05", ")", ")", "-", "1", "yval", "=", "'Mz'", "# Specify the mass range", "zarray", "=", "10", "**", "np", ".", "arange", "(", "10", ",", "14", ",", "0.5", ")", "xtitle", "=", "r\"Redshift\"", "ytitle", "=", "r\"M(z) (M$_{sol}$)\"", "linelabel", "=", "r\"log$_{10}$ M$_{0}$(M$_{sol}$)=\"", "fig", "=", "plt", ".", "figure", "(", ")", "ax", "=", "fig", ".", "add_subplot", "(", "111", ")", "ax", ".", "set_xlabel", "(", "xtitle", ")", "ax", ".", "set_ylabel", "(", "ytitle", ")", "colors", "=", "cm", ".", "rainbow", "(", "np", ".", "linspace", "(", "0", ",", "1", ",", "len", "(", "zarray", ")", ")", ")", "for", "zind", ",", "zval", "in", "enumerate", "(", "zarray", ")", ":", "output", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "0", ",", "Mi", "=", "zval", ",", "z", "=", "xarray", ")", "yarray", "=", "output", "[", "yval", "]", ".", "flatten", "(", ")", "# Plot each line in turn with different colour", "ax", ".", "plot", "(", "xarray", ",", "yarray", ",", "label", "=", "linelabel", "+", "\"{0:.1f}\"", ".", "format", "(", "np", ".", "log10", "(", "zval", ")", ")", ",", "color", "=", "colors", "[", "zind", "]", ",", ")", "ax", ".", "set_yscale", "(", "'log'", ")", "leg", "=", "ax", ".", "legend", "(", "loc", "=", "1", ")", "# Make box totally transparent", "leg", ".", "get_frame", "(", ")", ".", "set_alpha", "(", "0", ")", "leg", ".", "get_frame", "(", ")", ".", "set_edgecolor", "(", "'white'", ")", "for", "label", "in", "leg", ".", "get_texts", "(", ")", ":", "label", ".", "set_fontsize", "(", "'small'", ")", "# the font size", "for", "label", "in", "leg", ".", "get_lines", "(", ")", ":", "label", ".", "set_linewidth", "(", "4", ")", "# the legend line width", "if", "plotname", ":", "fig", ".", "tight_layout", "(", "pad", "=", "0.2", ")", "print", "(", "\"Plotting to '%s_Mzz_relation.png'\"", "%", "(", "plotname", ")", ")", "fig", ".", "savefig", "(", "plotname", "+", "\"_Mzz_relation.png\"", ",", "dpi", "=", "fig", ".", "dpi", "*", "5", ")", "else", ":", "plt", ".", "show", "(", ")", "# Plot the Mz/M0-z relation as a function of mass", "xarray", "=", "10", "**", "(", "np", ".", "arange", "(", "0", ",", "1", ",", "0.02", ")", ")", "-", "1", "yval", "=", "'Mz'", "# Specify the mass range", "zarray", "=", "10", "**", "np", ".", "arange", "(", "10", ",", "14", ",", "0.5", ")", "xtitle", "=", "r\"Redshift\"", "ytitle", "=", "r\"log$_{10}$ M(z)/M$_{0}$\"", "linelabel", "=", "r\"log$_{10}$ M$_{0}$(M$_{sol}$)=\"", "fig", "=", "plt", ".", "figure", "(", ")", "ax", "=", "fig", ".", "add_subplot", "(", "111", ")", "ax", ".", "set_xlabel", "(", "xtitle", ")", "ax", ".", "set_ylabel", "(", "ytitle", ")", "colors", "=", "cm", ".", "rainbow", "(", "np", ".", "linspace", "(", "0", ",", "1", ",", "len", "(", "zarray", ")", ")", ")", "for", "zind", ",", "zval", "in", "enumerate", "(", "zarray", ")", ":", "output", "=", "commah", ".", "run", "(", "cosmology", "=", "cosmology", ",", "zi", "=", "0", ",", "Mi", "=", "zval", ",", "z", "=", "xarray", ")", "yarray", "=", "output", "[", "yval", "]", ".", "flatten", "(", ")", "# Plot each line in turn with different colour", "ax", ".", "plot", "(", "xarray", ",", "np", ".", "log10", "(", "yarray", "/", "zval", ")", ",", "label", "=", "linelabel", "+", "\"{0:.1f}\"", ".", "format", "(", "np", ".", "log10", "(", "zval", ")", ")", ",", "color", "=", "colors", "[", "zind", "]", ",", ")", "leg", "=", "ax", ".", "legend", "(", "loc", "=", "3", ")", "# Make box totally transparent", "leg", ".", "get_frame", "(", ")", ".", "set_alpha", "(", "0", ")", "leg", ".", "get_frame", "(", ")", ".", "set_edgecolor", "(", "'white'", ")", "for", "label", "in", "leg", ".", "get_texts", "(", ")", ":", "label", ".", "set_fontsize", "(", "'small'", ")", "# the font size", "for", "label", "in", "leg", ".", "get_lines", "(", ")", ":", "label", ".", "set_linewidth", "(", "4", ")", "# the legend line width", "if", "plotname", ":", "fig", ".", "tight_layout", "(", "pad", "=", "0.2", ")", "print", "(", "\"Plotting to '%s_MzM0z_relation.png'\"", "%", "(", "plotname", ")", ")", "fig", ".", "savefig", "(", "plotname", "+", "\"_MzM0z_relation.png\"", ",", "dpi", "=", "fig", ".", "dpi", "*", "5", ")", "else", ":", "plt", ".", "show", "(", ")", "return", "(", "\"Done\"", ")"], "docstring": "Example ways to interrogate the dataset and plot the commah output", "docstring_tokens": ["Example", "ways", "to", "interrogate", "the", "dataset", "and", "plot", "the", "commah", "output"], "sha": "3ec70338c5123a053c79ddcf2cb3beac26bc9137", "url": "https://github.com/astroduff/commah/blob/3ec70338c5123a053c79ddcf2cb3beac26bc9137/examples.py#L93-L466", "partition": "train"}
{"repo": "davidblaisonneau-orange/foreman", "path": "foreman/itemHostsGroup.py", "func_name": "ItemHostsGroup.enhance", "original_string": "def enhance(self):\n        \"\"\" Function enhance\n        Enhance the object with new item or enhanced items\n        \"\"\"\n        self.update({'puppetclasses':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemPuppetClasses)})\n        self.update({'parameters':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemParameter)})\n        self.update({'smart_class_parameters':\n                    SubDict(self.api, self.objName,\n                            self.payloadObj, self.key,\n                            ItemSmartClassParameter)})", "language": "python", "code": "def enhance(self):\n        \"\"\" Function enhance\n        Enhance the object with new item or enhanced items\n        \"\"\"\n        self.update({'puppetclasses':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemPuppetClasses)})\n        self.update({'parameters':\n                     SubDict(self.api, self.objName,\n                             self.payloadObj, self.key,\n                             SubItemParameter)})\n        self.update({'smart_class_parameters':\n                    SubDict(self.api, self.objName,\n                            self.payloadObj, self.key,\n                            ItemSmartClassParameter)})", "code_tokens": ["def", "enhance", "(", "self", ")", ":", "self", ".", "update", "(", "{", "'puppetclasses'", ":", "SubDict", "(", "self", ".", "api", ",", "self", ".", "objName", ",", "self", ".", "payloadObj", ",", "self", ".", "key", ",", "SubItemPuppetClasses", ")", "}", ")", "self", ".", "update", "(", "{", "'parameters'", ":", "SubDict", "(", "self", ".", "api", ",", "self", ".", "objName", ",", "self", ".", "payloadObj", ",", "self", ".", "key", ",", "SubItemParameter", ")", "}", ")", "self", ".", "update", "(", "{", "'smart_class_parameters'", ":", "SubDict", "(", "self", ".", "api", ",", "self", ".", "objName", ",", "self", ".", "payloadObj", ",", "self", ".", "key", ",", "ItemSmartClassParameter", ")", "}", ")"], "docstring": "Function enhance\n        Enhance the object with new item or enhanced items", "docstring_tokens": ["Function", "enhance", "Enhance", "the", "object", "with", "new", "item", "or", "enhanced", "items"], "sha": "acb8fd8d74657cfac3b25c82e9c6028b93eb6c92", "url": "https://github.com/davidblaisonneau-orange/foreman/blob/acb8fd8d74657cfac3b25c82e9c6028b93eb6c92/foreman/itemHostsGroup.py#L38-L53", "partition": "train"}
{"repo": "aroberge/experimental", "path": "experimental/core/transforms.py", "func_name": "add_transformers", "original_string": "def add_transformers(line):\n    '''Extract the transformers names from a line of code of the form\n       from __experimental__ import transformer1 [,...]\n       and adds them to the globally known dict\n    '''\n    assert FROM_EXPERIMENTAL.match(line)\n\n    line = FROM_EXPERIMENTAL.sub(' ', line)\n    # we now have: \" transformer1 [,...]\"\n    line = line.split(\"#\")[0]    # remove any end of line comments\n    # and insert each transformer as an item in a list\n    for trans in line.replace(' ', '').split(','):\n        import_transformer(trans)", "language": "python", "code": "def add_transformers(line):\n    '''Extract the transformers names from a line of code of the form\n       from __experimental__ import transformer1 [,...]\n       and adds them to the globally known dict\n    '''\n    assert FROM_EXPERIMENTAL.match(line)\n\n    line = FROM_EXPERIMENTAL.sub(' ', line)\n    # we now have: \" transformer1 [,...]\"\n    line = line.split(\"#\")[0]    # remove any end of line comments\n    # and insert each transformer as an item in a list\n    for trans in line.replace(' ', '').split(','):\n        import_transformer(trans)", "code_tokens": ["def", "add_transformers", "(", "line", ")", ":", "assert", "FROM_EXPERIMENTAL", ".", "match", "(", "line", ")", "line", "=", "FROM_EXPERIMENTAL", ".", "sub", "(", "' '", ",", "line", ")", "# we now have: \" transformer1 [,...]\"", "line", "=", "line", ".", "split", "(", "\"#\"", ")", "[", "0", "]", "# remove any end of line comments", "# and insert each transformer as an item in a list", "for", "trans", "in", "line", ".", "replace", "(", "' '", ",", "''", ")", ".", "split", "(", "','", ")", ":", "import_transformer", "(", "trans", ")"], "docstring": "Extract the transformers names from a line of code of the form\n       from __experimental__ import transformer1 [,...]\n       and adds them to the globally known dict", "docstring_tokens": ["Extract", "the", "transformers", "names", "from", "a", "line", "of", "code", "of", "the", "form", "from", "__experimental__", "import", "transformer1", "[", "...", "]", "and", "adds", "them", "to", "the", "globally", "known", "dict"], "sha": "031a9be10698b429998436da748b8fdb86f18b47", "url": "https://github.com/aroberge/experimental/blob/031a9be10698b429998436da748b8fdb86f18b47/experimental/core/transforms.py#L19-L31", "partition": "train"}
{"repo": "maxalbert/tohu", "path": "tohu/v6/item_list.py", "func_name": "ItemList.to_sql", "original_string": "def to_sql(self, url, table_name, *, schema=None, fields=None, fields_to_explode=None, if_exists=\"fail\", dtype=None):\n        \"\"\"\n        Export items as rows in a PostgreSQL table.\n\n        Parameters\n        ----------\n\n        url: string\n            Connection string to connect to the database.\n            Example: \"postgresql://postgres@127.0.0.1:5432/testdb\"\n\n        table_name: string\n            Name of the database table. Note that if this name contains a dot ('.')\n            and `schema` is not specified, the first part of the name before the dot\n            will be interpreted as the schema name.\n\n        schema : string, optional\n            Specify the schema (if database flavor supports this). If None,\n            use default schema or derive the schema name from `table_name`.\n\n        fields: list or dict\n            List of field names to export, or dictionary mapping output column names\n            to attribute names of the generators.\n\n            Examples:\n               fields=['field_name_1', 'field_name_2']\n               fields={'COL1': 'field_name_1', 'COL2': 'field_name_2'}\n\n        fields_to_explode: list or None\n            Optional list of field names where each entry (which must itself be a sequence)\n            is to be \"exploded\" into separate rows.\n\n        if_exists : {'fail', 'do_nothing', 'replace', 'append'}, default 'fail'\n            - fail: If table exists, raise an error.\n            - do_nothing: If table exists, do nothing and immediately return.\n            - replace: If table exists, drop it, recreate it, and insert data.\n            - append: If table exists, insert data. Create if does not exist.\n\n        dtype : dict, optional\n            Specifying the datatype for columns. The keys should be the column\n            names and the values should be the SQLAlchemy types or strings for\n            the sqlite3 legacy mode. This is passed through to pandas.DataFrame.to_sql().\n        \"\"\"\n        if schema is None:\n            schema, table_name = _extract_schema_if_given(table_name)\n\n        engine = create_engine(url)\n        ins = inspect(engine)\n\n        if schema is not None and schema not in ins.get_schema_names():\n            logger.debug(f\"Creating non-existing schema: '{schema}'\")\n            engine.execute(CreateSchema(schema))\n\n        if table_name in ins.get_table_names(schema=schema) and if_exists == 'do_nothing':\n            logger.debug(\"Table already exists (use if_exists='replace' or if_exists='append' to modify it).\")\n            return\n\n        if if_exists == 'do_nothing':\n            # we handled the 'do nothing' case above; change to an option that pandas will understand\n            if_exists = 'fail'\n\n        with engine.begin() as conn:\n            self.to_df(fields=fields, fields_to_explode=fields_to_explode).to_sql(\n                table_name, conn, schema=schema, index=False, if_exists=if_exists, dtype=dtype)", "language": "python", "code": "def to_sql(self, url, table_name, *, schema=None, fields=None, fields_to_explode=None, if_exists=\"fail\", dtype=None):\n        \"\"\"\n        Export items as rows in a PostgreSQL table.\n\n        Parameters\n        ----------\n\n        url: string\n            Connection string to connect to the database.\n            Example: \"postgresql://postgres@127.0.0.1:5432/testdb\"\n\n        table_name: string\n            Name of the database table. Note that if this name contains a dot ('.')\n            and `schema` is not specified, the first part of the name before the dot\n            will be interpreted as the schema name.\n\n        schema : string, optional\n            Specify the schema (if database flavor supports this). If None,\n            use default schema or derive the schema name from `table_name`.\n\n        fields: list or dict\n            List of field names to export, or dictionary mapping output column names\n            to attribute names of the generators.\n\n            Examples:\n               fields=['field_name_1', 'field_name_2']\n               fields={'COL1': 'field_name_1', 'COL2': 'field_name_2'}\n\n        fields_to_explode: list or None\n            Optional list of field names where each entry (which must itself be a sequence)\n            is to be \"exploded\" into separate rows.\n\n        if_exists : {'fail', 'do_nothing', 'replace', 'append'}, default 'fail'\n            - fail: If table exists, raise an error.\n            - do_nothing: If table exists, do nothing and immediately return.\n            - replace: If table exists, drop it, recreate it, and insert data.\n            - append: If table exists, insert data. Create if does not exist.\n\n        dtype : dict, optional\n            Specifying the datatype for columns. The keys should be the column\n            names and the values should be the SQLAlchemy types or strings for\n            the sqlite3 legacy mode. This is passed through to pandas.DataFrame.to_sql().\n        \"\"\"\n        if schema is None:\n            schema, table_name = _extract_schema_if_given(table_name)\n\n        engine = create_engine(url)\n        ins = inspect(engine)\n\n        if schema is not None and schema not in ins.get_schema_names():\n            logger.debug(f\"Creating non-existing schema: '{schema}'\")\n            engine.execute(CreateSchema(schema))\n\n        if table_name in ins.get_table_names(schema=schema) and if_exists == 'do_nothing':\n            logger.debug(\"Table already exists (use if_exists='replace' or if_exists='append' to modify it).\")\n            return\n\n        if if_exists == 'do_nothing':\n            # we handled the 'do nothing' case above; change to an option that pandas will understand\n            if_exists = 'fail'\n\n        with engine.begin() as conn:\n            self.to_df(fields=fields, fields_to_explode=fields_to_explode).to_sql(\n                table_name, conn, schema=schema, index=False, if_exists=if_exists, dtype=dtype)", "code_tokens": ["def", "to_sql", "(", "self", ",", "url", ",", "table_name", ",", "*", ",", "schema", "=", "None", ",", "fields", "=", "None", ",", "fields_to_explode", "=", "None", ",", "if_exists", "=", "\"fail\"", ",", "dtype", "=", "None", ")", ":", "if", "schema", "is", "None", ":", "schema", ",", "table_name", "=", "_extract_schema_if_given", "(", "table_name", ")", "engine", "=", "create_engine", "(", "url", ")", "ins", "=", "inspect", "(", "engine", ")", "if", "schema", "is", "not", "None", "and", "schema", "not", "in", "ins", ".", "get_schema_names", "(", ")", ":", "logger", ".", "debug", "(", "f\"Creating non-existing schema: '{schema}'\"", ")", "engine", ".", "execute", "(", "CreateSchema", "(", "schema", ")", ")", "if", "table_name", "in", "ins", ".", "get_table_names", "(", "schema", "=", "schema", ")", "and", "if_exists", "==", "'do_nothing'", ":", "logger", ".", "debug", "(", "\"Table already exists (use if_exists='replace' or if_exists='append' to modify it).\"", ")", "return", "if", "if_exists", "==", "'do_nothing'", ":", "# we handled the 'do nothing' case above; change to an option that pandas will understand", "if_exists", "=", "'fail'", "with", "engine", ".", "begin", "(", ")", "as", "conn", ":", "self", ".", "to_df", "(", "fields", "=", "fields", ",", "fields_to_explode", "=", "fields_to_explode", ")", ".", "to_sql", "(", "table_name", ",", "conn", ",", "schema", "=", "schema", ",", "index", "=", "False", ",", "if_exists", "=", "if_exists", ",", "dtype", "=", "dtype", ")"], "docstring": "Export items as rows in a PostgreSQL table.\n\n        Parameters\n        ----------\n\n        url: string\n            Connection string to connect to the database.\n            Example: \"postgresql://postgres@127.0.0.1:5432/testdb\"\n\n        table_name: string\n            Name of the database table. Note that if this name contains a dot ('.')\n            and `schema` is not specified, the first part of the name before the dot\n            will be interpreted as the schema name.\n\n        schema : string, optional\n            Specify the schema (if database flavor supports this). If None,\n            use default schema or derive the schema name from `table_name`.\n\n        fields: list or dict\n            List of field names to export, or dictionary mapping output column names\n            to attribute names of the generators.\n\n            Examples:\n               fields=['field_name_1', 'field_name_2']\n               fields={'COL1': 'field_name_1', 'COL2': 'field_name_2'}\n\n        fields_to_explode: list or None\n            Optional list of field names where each entry (which must itself be a sequence)\n            is to be \"exploded\" into separate rows.\n\n        if_exists : {'fail', 'do_nothing', 'replace', 'append'}, default 'fail'\n            - fail: If table exists, raise an error.\n            - do_nothing: If table exists, do nothing and immediately return.\n            - replace: If table exists, drop it, recreate it, and insert data.\n            - append: If table exists, insert data. Create if does not exist.\n\n        dtype : dict, optional\n            Specifying the datatype for columns. The keys should be the column\n            names and the values should be the SQLAlchemy types or strings for\n            the sqlite3 legacy mode. This is passed through to pandas.DataFrame.to_sql().", "docstring_tokens": ["Export", "items", "as", "rows", "in", "a", "PostgreSQL", "table", "."], "sha": "43380162fadec99cdd5c5c3152dd6b7d3a9d39a8", "url": "https://github.com/maxalbert/tohu/blob/43380162fadec99cdd5c5c3152dd6b7d3a9d39a8/tohu/v6/item_list.py#L282-L345", "partition": "train"}
{"repo": "maxalbert/tohu", "path": "tohu/v6/base.py", "func_name": "TohuBaseGenerator.reset", "original_string": "def reset(self, seed):\n        \"\"\"\n        Reset this generator's seed generator and any clones.\n        \"\"\"\n        logger.debug(f'Resetting {self} (seed={seed})')\n        self.seed_generator.reset(seed)\n\n        for c in self.clones:\n            c.reset(seed)", "language": "python", "code": "def reset(self, seed):\n        \"\"\"\n        Reset this generator's seed generator and any clones.\n        \"\"\"\n        logger.debug(f'Resetting {self} (seed={seed})')\n        self.seed_generator.reset(seed)\n\n        for c in self.clones:\n            c.reset(seed)", "code_tokens": ["def", "reset", "(", "self", ",", "seed", ")", ":", "logger", ".", "debug", "(", "f'Resetting {self} (seed={seed})'", ")", "self", ".", "seed_generator", ".", "reset", "(", "seed", ")", "for", "c", "in", "self", ".", "clones", ":", "c", ".", "reset", "(", "seed", ")"], "docstring": "Reset this generator's seed generator and any clones.", "docstring_tokens": ["Reset", "this", "generator", "s", "seed", "generator", "and", "any", "clones", "."], "sha": "43380162fadec99cdd5c5c3152dd6b7d3a9d39a8", "url": "https://github.com/maxalbert/tohu/blob/43380162fadec99cdd5c5c3152dd6b7d3a9d39a8/tohu/v6/base.py#L121-L129", "partition": "train"}
{"repo": "depop/python-flexisettings", "path": "flexisettings/__init__.py", "func_name": "_load_config", "original_string": "def _load_config(initial_namespace=None, defaults=None):\n    # type: (Optional[str], Optional[str]) -> ConfigLoader\n    \"\"\"\n    Kwargs:\n        initial_namespace:\n        defaults:\n    \"\"\"\n    # load defaults\n    if defaults:\n        config = ConfigLoader()\n        config.update_from_object(defaults)\n\n    namespace = getattr(config, 'CONFIG_NAMESPACE', initial_namespace)\n    app_config = getattr(config, 'APP_CONFIG', None)\n\n    # load customised config\n    if app_config:\n        if namespace is None:\n            config.update_from_object(app_config)\n        else:\n            _temp = ConfigLoader()\n            _temp.update_from_object(app_config, lambda key: key.startswith(namespace))\n            config.update(_temp.namespace(namespace))\n\n    return config", "language": "python", "code": "def _load_config(initial_namespace=None, defaults=None):\n    # type: (Optional[str], Optional[str]) -> ConfigLoader\n    \"\"\"\n    Kwargs:\n        initial_namespace:\n        defaults:\n    \"\"\"\n    # load defaults\n    if defaults:\n        config = ConfigLoader()\n        config.update_from_object(defaults)\n\n    namespace = getattr(config, 'CONFIG_NAMESPACE', initial_namespace)\n    app_config = getattr(config, 'APP_CONFIG', None)\n\n    # load customised config\n    if app_config:\n        if namespace is None:\n            config.update_from_object(app_config)\n        else:\n            _temp = ConfigLoader()\n            _temp.update_from_object(app_config, lambda key: key.startswith(namespace))\n            config.update(_temp.namespace(namespace))\n\n    return config", "code_tokens": ["def", "_load_config", "(", "initial_namespace", "=", "None", ",", "defaults", "=", "None", ")", ":", "# type: (Optional[str], Optional[str]) -> ConfigLoader", "# load defaults", "if", "defaults", ":", "config", "=", "ConfigLoader", "(", ")", "config", ".", "update_from_object", "(", "defaults", ")", "namespace", "=", "getattr", "(", "config", ",", "'CONFIG_NAMESPACE'", ",", "initial_namespace", ")", "app_config", "=", "getattr", "(", "config", ",", "'APP_CONFIG'", ",", "None", ")", "# load customised config", "if", "app_config", ":", "if", "namespace", "is", "None", ":", "config", ".", "update_from_object", "(", "app_config", ")", "else", ":", "_temp", "=", "ConfigLoader", "(", ")", "_temp", ".", "update_from_object", "(", "app_config", ",", "lambda", "key", ":", "key", ".", "startswith", "(", "namespace", ")", ")", "config", ".", "update", "(", "_temp", ".", "namespace", "(", "namespace", ")", ")", "return", "config"], "docstring": "Kwargs:\n        initial_namespace:\n        defaults:", "docstring_tokens": ["Kwargs", ":", "initial_namespace", ":", "defaults", ":"], "sha": "36d08280ab7c45568fdf206fcdb4cf771d240c6b", "url": "https://github.com/depop/python-flexisettings/blob/36d08280ab7c45568fdf206fcdb4cf771d240c6b/flexisettings/__init__.py#L90-L114", "partition": "train"}
{"repo": "depop/python-flexisettings", "path": "flexisettings/utils.py", "func_name": "override_environment", "original_string": "def override_environment(settings, **kwargs):\n    # type: (Settings, **str) -> Generator\n    \"\"\"\n    Override env vars and reload the Settings object\n\n    NOTE:\n    Obviously this context has to be in place before you import any\n    module which reads env values at import time.\n\n    NOTE:\n    The values in `kwargs` must be strings else you will get a cryptic:\n\n        TypeError: execve() arg 3 contains a non-string value\n    \"\"\"\n    old_env = os.environ.copy()\n    os.environ.update(kwargs)\n\n    settings._reload()\n\n    try:\n        yield\n    except Exception:\n        raise\n    finally:\n        for key in kwargs.keys():\n            del os.environ[key]\n        os.environ.update(old_env)\n\n        settings._reload()", "language": "python", "code": "def override_environment(settings, **kwargs):\n    # type: (Settings, **str) -> Generator\n    \"\"\"\n    Override env vars and reload the Settings object\n\n    NOTE:\n    Obviously this context has to be in place before you import any\n    module which reads env values at import time.\n\n    NOTE:\n    The values in `kwargs` must be strings else you will get a cryptic:\n\n        TypeError: execve() arg 3 contains a non-string value\n    \"\"\"\n    old_env = os.environ.copy()\n    os.environ.update(kwargs)\n\n    settings._reload()\n\n    try:\n        yield\n    except Exception:\n        raise\n    finally:\n        for key in kwargs.keys():\n            del os.environ[key]\n        os.environ.update(old_env)\n\n        settings._reload()", "code_tokens": ["def", "override_environment", "(", "settings", ",", "*", "*", "kwargs", ")", ":", "# type: (Settings, **str) -> Generator", "old_env", "=", "os", ".", "environ", ".", "copy", "(", ")", "os", ".", "environ", ".", "update", "(", "kwargs", ")", "settings", ".", "_reload", "(", ")", "try", ":", "yield", "except", "Exception", ":", "raise", "finally", ":", "for", "key", "in", "kwargs", ".", "keys", "(", ")", ":", "del", "os", ".", "environ", "[", "key", "]", "os", ".", "environ", ".", "update", "(", "old_env", ")", "settings", ".", "_reload", "(", ")"], "docstring": "Override env vars and reload the Settings object\n\n    NOTE:\n    Obviously this context has to be in place before you import any\n    module which reads env values at import time.\n\n    NOTE:\n    The values in `kwargs` must be strings else you will get a cryptic:\n\n        TypeError: execve() arg 3 contains a non-string value", "docstring_tokens": ["Override", "env", "vars", "and", "reload", "the", "Settings", "object"], "sha": "36d08280ab7c45568fdf206fcdb4cf771d240c6b", "url": "https://github.com/depop/python-flexisettings/blob/36d08280ab7c45568fdf206fcdb4cf771d240c6b/flexisettings/utils.py#L47-L75", "partition": "train"}
{"repo": "cloudify-cosmo/repex", "path": "repex.py", "func_name": "_import_yaml", "original_string": "def _import_yaml(config_file_path):\n    \"\"\"Return a configuration object\n    \"\"\"\n    try:\n        logger.info('Importing config %s...', config_file_path)\n        with open(config_file_path) as config_file:\n            return yaml.safe_load(config_file.read())\n    except IOError as ex:\n        raise RepexError('{0}: {1} ({2})'.format(\n            ERRORS['config_file_not_found'], config_file_path, ex))\n    except (yaml.parser.ParserError, yaml.scanner.ScannerError) as ex:\n        raise RepexError('{0} ({1})'.format(ERRORS['invalid_yaml'], ex))", "language": "python", "code": "def _import_yaml(config_file_path):\n    \"\"\"Return a configuration object\n    \"\"\"\n    try:\n        logger.info('Importing config %s...', config_file_path)\n        with open(config_file_path) as config_file:\n            return yaml.safe_load(config_file.read())\n    except IOError as ex:\n        raise RepexError('{0}: {1} ({2})'.format(\n            ERRORS['config_file_not_found'], config_file_path, ex))\n    except (yaml.parser.ParserError, yaml.scanner.ScannerError) as ex:\n        raise RepexError('{0} ({1})'.format(ERRORS['invalid_yaml'], ex))", "code_tokens": ["def", "_import_yaml", "(", "config_file_path", ")", ":", "try", ":", "logger", ".", "info", "(", "'Importing config %s...'", ",", "config_file_path", ")", "with", "open", "(", "config_file_path", ")", "as", "config_file", ":", "return", "yaml", ".", "safe_load", "(", "config_file", ".", "read", "(", ")", ")", "except", "IOError", "as", "ex", ":", "raise", "RepexError", "(", "'{0}: {1} ({2})'", ".", "format", "(", "ERRORS", "[", "'config_file_not_found'", "]", ",", "config_file_path", ",", "ex", ")", ")", "except", "(", "yaml", ".", "parser", ".", "ParserError", ",", "yaml", ".", "scanner", ".", "ScannerError", ")", "as", "ex", ":", "raise", "RepexError", "(", "'{0} ({1})'", ".", "format", "(", "ERRORS", "[", "'invalid_yaml'", "]", ",", "ex", ")", ")"], "docstring": "Return a configuration object", "docstring_tokens": ["Return", "a", "configuration", "object"], "sha": "589e442857fa4a99fa88670d7df1a72f983bbd28", "url": "https://github.com/cloudify-cosmo/repex/blob/589e442857fa4a99fa88670d7df1a72f983bbd28/repex.py#L72-L83", "partition": "train"}
{"repo": "cloudify-cosmo/repex", "path": "repex.py", "func_name": "_get_all_files", "original_string": "def _get_all_files(filename_regex,\n                   path,\n                   base_dir,\n                   excluded_paths=None,\n                   excluded_filename_regex=None):\n    \"\"\"Get all files for processing.\n\n    This starts iterating from `base_dir` and checks for all files\n    that look like `filename_regex` under `path` regex excluding\n    all paths under the `excluded_paths` list, whether they are files\n    or folders. `excluded_paths` are explicit paths, not regex.\n    `excluded_filename_regex` are files to be excluded as well.\n    \"\"\"\n    # For windows\n    def replace_backslashes(string):\n        return string.replace('\\\\', '/')\n\n    excluded_paths = _normalize_excluded_paths(base_dir, excluded_paths)\n    if excluded_paths:\n        logger.info('Excluding paths: %s', excluded_paths)\n\n    logger.info('Looking for %s under %s...',\n                filename_regex, os.path.join(base_dir, path))\n    if excluded_filename_regex:\n        logger.info('Excluding file names: %s', excluded_filename_regex)\n\n    path_expression = re.compile(replace_backslashes(path))\n\n    target_files = []\n\n    for root, _, files in os.walk(base_dir):\n        if not root.startswith(tuple(excluded_paths)) \\\n                and path_expression.search(replace_backslashes(root)):\n            for filename in files:\n                filepath = os.path.join(root, filename)\n                is_file, matched, excluded_filename, excluded_path = \\\n                    _set_match_parameters(\n                        filename,\n                        filepath,\n                        filename_regex,\n                        excluded_filename_regex,\n                        excluded_paths)\n                if is_file and matched and not excluded_filename \\\n                        and not excluded_path:\n                    logger.debug('%s is a match. Appending to list...',\n                                 filepath)\n                    target_files.append(filepath)\n    return target_files", "language": "python", "code": "def _get_all_files(filename_regex,\n                   path,\n                   base_dir,\n                   excluded_paths=None,\n                   excluded_filename_regex=None):\n    \"\"\"Get all files for processing.\n\n    This starts iterating from `base_dir` and checks for all files\n    that look like `filename_regex` under `path` regex excluding\n    all paths under the `excluded_paths` list, whether they are files\n    or folders. `excluded_paths` are explicit paths, not regex.\n    `excluded_filename_regex` are files to be excluded as well.\n    \"\"\"\n    # For windows\n    def replace_backslashes(string):\n        return string.replace('\\\\', '/')\n\n    excluded_paths = _normalize_excluded_paths(base_dir, excluded_paths)\n    if excluded_paths:\n        logger.info('Excluding paths: %s', excluded_paths)\n\n    logger.info('Looking for %s under %s...',\n                filename_regex, os.path.join(base_dir, path))\n    if excluded_filename_regex:\n        logger.info('Excluding file names: %s', excluded_filename_regex)\n\n    path_expression = re.compile(replace_backslashes(path))\n\n    target_files = []\n\n    for root, _, files in os.walk(base_dir):\n        if not root.startswith(tuple(excluded_paths)) \\\n                and path_expression.search(replace_backslashes(root)):\n            for filename in files:\n                filepath = os.path.join(root, filename)\n                is_file, matched, excluded_filename, excluded_path = \\\n                    _set_match_parameters(\n                        filename,\n                        filepath,\n                        filename_regex,\n                        excluded_filename_regex,\n                        excluded_paths)\n                if is_file and matched and not excluded_filename \\\n                        and not excluded_path:\n                    logger.debug('%s is a match. Appending to list...',\n                                 filepath)\n                    target_files.append(filepath)\n    return target_files", "code_tokens": ["def", "_get_all_files", "(", "filename_regex", ",", "path", ",", "base_dir", ",", "excluded_paths", "=", "None", ",", "excluded_filename_regex", "=", "None", ")", ":", "# For windows", "def", "replace_backslashes", "(", "string", ")", ":", "return", "string", ".", "replace", "(", "'\\\\'", ",", "'/'", ")", "excluded_paths", "=", "_normalize_excluded_paths", "(", "base_dir", ",", "excluded_paths", ")", "if", "excluded_paths", ":", "logger", ".", "info", "(", "'Excluding paths: %s'", ",", "excluded_paths", ")", "logger", ".", "info", "(", "'Looking for %s under %s...'", ",", "filename_regex", ",", "os", ".", "path", ".", "join", "(", "base_dir", ",", "path", ")", ")", "if", "excluded_filename_regex", ":", "logger", ".", "info", "(", "'Excluding file names: %s'", ",", "excluded_filename_regex", ")", "path_expression", "=", "re", ".", "compile", "(", "replace_backslashes", "(", "path", ")", ")", "target_files", "=", "[", "]", "for", "root", ",", "_", ",", "files", "in", "os", ".", "walk", "(", "base_dir", ")", ":", "if", "not", "root", ".", "startswith", "(", "tuple", "(", "excluded_paths", ")", ")", "and", "path_expression", ".", "search", "(", "replace_backslashes", "(", "root", ")", ")", ":", "for", "filename", "in", "files", ":", "filepath", "=", "os", ".", "path", ".", "join", "(", "root", ",", "filename", ")", "is_file", ",", "matched", ",", "excluded_filename", ",", "excluded_path", "=", "_set_match_parameters", "(", "filename", ",", "filepath", ",", "filename_regex", ",", "excluded_filename_regex", ",", "excluded_paths", ")", "if", "is_file", "and", "matched", "and", "not", "excluded_filename", "and", "not", "excluded_path", ":", "logger", ".", "debug", "(", "'%s is a match. Appending to list...'", ",", "filepath", ")", "target_files", ".", "append", "(", "filepath", ")", "return", "target_files"], "docstring": "Get all files for processing.\n\n    This starts iterating from `base_dir` and checks for all files\n    that look like `filename_regex` under `path` regex excluding\n    all paths under the `excluded_paths` list, whether they are files\n    or folders. `excluded_paths` are explicit paths, not regex.\n    `excluded_filename_regex` are files to be excluded as well.", "docstring_tokens": ["Get", "all", "files", "for", "processing", "."], "sha": "589e442857fa4a99fa88670d7df1a72f983bbd28", "url": "https://github.com/cloudify-cosmo/repex/blob/589e442857fa4a99fa88670d7df1a72f983bbd28/repex.py#L120-L167", "partition": "train"}
{"repo": "cloudify-cosmo/repex", "path": "repex.py", "func_name": "_match_tags", "original_string": "def _match_tags(repex_tags, path_tags):\n    \"\"\"Check for matching tags between what the user provided\n    and the tags set in the config.\n\n    If `any` is chosen, match.\n    If no tags are chosen and none are configured, match.\n    If the user provided tags match any of the configured tags, match.\n    \"\"\"\n    if 'any' in repex_tags or (not repex_tags and not path_tags):\n        return True\n    elif set(repex_tags) & set(path_tags):\n        return True\n    return False", "language": "python", "code": "def _match_tags(repex_tags, path_tags):\n    \"\"\"Check for matching tags between what the user provided\n    and the tags set in the config.\n\n    If `any` is chosen, match.\n    If no tags are chosen and none are configured, match.\n    If the user provided tags match any of the configured tags, match.\n    \"\"\"\n    if 'any' in repex_tags or (not repex_tags and not path_tags):\n        return True\n    elif set(repex_tags) & set(path_tags):\n        return True\n    return False", "code_tokens": ["def", "_match_tags", "(", "repex_tags", ",", "path_tags", ")", ":", "if", "'any'", "in", "repex_tags", "or", "(", "not", "repex_tags", "and", "not", "path_tags", ")", ":", "return", "True", "elif", "set", "(", "repex_tags", ")", "&", "set", "(", "path_tags", ")", ":", "return", "True", "return", "False"], "docstring": "Check for matching tags between what the user provided\n    and the tags set in the config.\n\n    If `any` is chosen, match.\n    If no tags are chosen and none are configured, match.\n    If the user provided tags match any of the configured tags, match.", "docstring_tokens": ["Check", "for", "matching", "tags", "between", "what", "the", "user", "provided", "and", "the", "tags", "set", "in", "the", "config", "."], "sha": "589e442857fa4a99fa88670d7df1a72f983bbd28", "url": "https://github.com/cloudify-cosmo/repex/blob/589e442857fa4a99fa88670d7df1a72f983bbd28/repex.py#L328-L340", "partition": "train"}
{"repo": "cloudify-cosmo/repex", "path": "repex.py", "func_name": "iterate", "original_string": "def iterate(config_file_path=None,\n            config=None,\n            variables=None,\n            tags=None,\n            validate=True,\n            validate_only=False,\n            with_diff=False):\n    \"\"\"Iterate over all paths in `config_file_path`\n\n    :param string config_file_path: a path to a repex config file\n    :param dict config: a dictionary representing a repex config\n    :param dict variables: a dict of variables (can be None)\n    :param list tags: a list of tags to check for\n    :param bool validate: whether to perform schema validation on the config\n    :param bool validate_only: only perform validation without running\n    :param bool with_diff: whether to write a diff of all changes to a file\n    \"\"\"\n    # TODO: Check if tags can be a tuple instead of a list\n    if not isinstance(variables or {}, dict):\n        raise TypeError(ERRORS['variables_not_dict'])\n    if not isinstance(tags or [], list):\n        raise TypeError(ERRORS['tags_not_list'])\n\n    config = _get_config(config_file_path, config)\n    if validate or validate_only:\n        _validate_config_schema(config)\n    if validate_only:\n        logger.info('Config file validation completed successfully!')\n        sys.exit(0)\n\n    repex_vars = _merge_variables(config['variables'], variables or {})\n    repex_tags = tags or []\n    logger.debug('Chosen tags: %s', repex_tags)\n\n    for path in config['paths']:\n        _process_path(path, repex_tags, repex_vars, with_diff)", "language": "python", "code": "def iterate(config_file_path=None,\n            config=None,\n            variables=None,\n            tags=None,\n            validate=True,\n            validate_only=False,\n            with_diff=False):\n    \"\"\"Iterate over all paths in `config_file_path`\n\n    :param string config_file_path: a path to a repex config file\n    :param dict config: a dictionary representing a repex config\n    :param dict variables: a dict of variables (can be None)\n    :param list tags: a list of tags to check for\n    :param bool validate: whether to perform schema validation on the config\n    :param bool validate_only: only perform validation without running\n    :param bool with_diff: whether to write a diff of all changes to a file\n    \"\"\"\n    # TODO: Check if tags can be a tuple instead of a list\n    if not isinstance(variables or {}, dict):\n        raise TypeError(ERRORS['variables_not_dict'])\n    if not isinstance(tags or [], list):\n        raise TypeError(ERRORS['tags_not_list'])\n\n    config = _get_config(config_file_path, config)\n    if validate or validate_only:\n        _validate_config_schema(config)\n    if validate_only:\n        logger.info('Config file validation completed successfully!')\n        sys.exit(0)\n\n    repex_vars = _merge_variables(config['variables'], variables or {})\n    repex_tags = tags or []\n    logger.debug('Chosen tags: %s', repex_tags)\n\n    for path in config['paths']:\n        _process_path(path, repex_tags, repex_vars, with_diff)", "code_tokens": ["def", "iterate", "(", "config_file_path", "=", "None", ",", "config", "=", "None", ",", "variables", "=", "None", ",", "tags", "=", "None", ",", "validate", "=", "True", ",", "validate_only", "=", "False", ",", "with_diff", "=", "False", ")", ":", "# TODO: Check if tags can be a tuple instead of a list", "if", "not", "isinstance", "(", "variables", "or", "{", "}", ",", "dict", ")", ":", "raise", "TypeError", "(", "ERRORS", "[", "'variables_not_dict'", "]", ")", "if", "not", "isinstance", "(", "tags", "or", "[", "]", ",", "list", ")", ":", "raise", "TypeError", "(", "ERRORS", "[", "'tags_not_list'", "]", ")", "config", "=", "_get_config", "(", "config_file_path", ",", "config", ")", "if", "validate", "or", "validate_only", ":", "_validate_config_schema", "(", "config", ")", "if", "validate_only", ":", "logger", ".", "info", "(", "'Config file validation completed successfully!'", ")", "sys", ".", "exit", "(", "0", ")", "repex_vars", "=", "_merge_variables", "(", "config", "[", "'variables'", "]", ",", "variables", "or", "{", "}", ")", "repex_tags", "=", "tags", "or", "[", "]", "logger", ".", "debug", "(", "'Chosen tags: %s'", ",", "repex_tags", ")", "for", "path", "in", "config", "[", "'paths'", "]", ":", "_process_path", "(", "path", ",", "repex_tags", ",", "repex_vars", ",", "with_diff", ")"], "docstring": "Iterate over all paths in `config_file_path`\n\n    :param string config_file_path: a path to a repex config file\n    :param dict config: a dictionary representing a repex config\n    :param dict variables: a dict of variables (can be None)\n    :param list tags: a list of tags to check for\n    :param bool validate: whether to perform schema validation on the config\n    :param bool validate_only: only perform validation without running\n    :param bool with_diff: whether to write a diff of all changes to a file", "docstring_tokens": ["Iterate", "over", "all", "paths", "in", "config_file_path"], "sha": "589e442857fa4a99fa88670d7df1a72f983bbd28", "url": "https://github.com/cloudify-cosmo/repex/blob/589e442857fa4a99fa88670d7df1a72f983bbd28/repex.py#L343-L378", "partition": "train"}
{"repo": "idlesign/uwsgiconf", "path": "uwsgiconf/options/monitoring.py", "func_name": "Monitoring.register_stats_pusher", "original_string": "def register_stats_pusher(self, pusher):\n        \"\"\"Registers a pusher to be used for pushing statistics to various remotes/locals.\n\n        :param Pusher|list[Pusher] pusher:\n\n        \"\"\"\n        for pusher in listify(pusher):\n            self._set('stats-push', pusher, multi=True)\n\n        return self._section", "language": "python", "code": "def register_stats_pusher(self, pusher):\n        \"\"\"Registers a pusher to be used for pushing statistics to various remotes/locals.\n\n        :param Pusher|list[Pusher] pusher:\n\n        \"\"\"\n        for pusher in listify(pusher):\n            self._set('stats-push', pusher, multi=True)\n\n        return self._section", "code_tokens": ["def", "register_stats_pusher", "(", "self", ",", "pusher", ")", ":", "for", "pusher", "in", "listify", "(", "pusher", ")", ":", "self", ".", "_set", "(", "'stats-push'", ",", "pusher", ",", "multi", "=", "True", ")", "return", "self", ".", "_section"], "docstring": "Registers a pusher to be used for pushing statistics to various remotes/locals.\n\n        :param Pusher|list[Pusher] pusher:", "docstring_tokens": ["Registers", "a", "pusher", "to", "be", "used", "for", "pushing", "statistics", "to", "various", "remotes", "/", "locals", "."], "sha": "475407acb44199edbf7e0a66261bfeb51de1afae", "url": "https://github.com/idlesign/uwsgiconf/blob/475407acb44199edbf7e0a66261bfeb51de1afae/uwsgiconf/options/monitoring.py#L200-L209", "partition": "train"}
{"repo": "idlesign/uwsgiconf", "path": "uwsgiconf/options/monitoring.py", "func_name": "Monitoring.enable_snmp", "original_string": "def enable_snmp(self, address, community_string):\n        \"\"\"Enables SNMP.\n\n        uWSGI server embeds a tiny SNMP server that you can use to integrate\n        your web apps with your monitoring infrastructure.\n\n        * http://uwsgi.readthedocs.io/en/latest/SNMP.html\n\n        .. note:: SNMP server is started in the master process after dropping the privileges.\n            If you want it to listen on a privileged port, you can either use Capabilities on Linux,\n            or use the ``as-root`` option to run the master process as root.\n\n        :param str|unicode address: UDP address to bind to.\n\n            Examples:\n\n                * 192.168.1.1:2222\n\n        :param str|unicode community_string: SNMP instance identifier to address it.\n\n        \"\"\"\n        self._set('snmp', address)\n        self._set('snmp-community', community_string)\n\n        return self._section", "language": "python", "code": "def enable_snmp(self, address, community_string):\n        \"\"\"Enables SNMP.\n\n        uWSGI server embeds a tiny SNMP server that you can use to integrate\n        your web apps with your monitoring infrastructure.\n\n        * http://uwsgi.readthedocs.io/en/latest/SNMP.html\n\n        .. note:: SNMP server is started in the master process after dropping the privileges.\n            If you want it to listen on a privileged port, you can either use Capabilities on Linux,\n            or use the ``as-root`` option to run the master process as root.\n\n        :param str|unicode address: UDP address to bind to.\n\n            Examples:\n\n                * 192.168.1.1:2222\n\n        :param str|unicode community_string: SNMP instance identifier to address it.\n\n        \"\"\"\n        self._set('snmp', address)\n        self._set('snmp-community', community_string)\n\n        return self._section", "code_tokens": ["def", "enable_snmp", "(", "self", ",", "address", ",", "community_string", ")", ":", "self", ".", "_set", "(", "'snmp'", ",", "address", ")", "self", ".", "_set", "(", "'snmp-community'", ",", "community_string", ")", "return", "self", ".", "_section"], "docstring": "Enables SNMP.\n\n        uWSGI server embeds a tiny SNMP server that you can use to integrate\n        your web apps with your monitoring infrastructure.\n\n        * http://uwsgi.readthedocs.io/en/latest/SNMP.html\n\n        .. note:: SNMP server is started in the master process after dropping the privileges.\n            If you want it to listen on a privileged port, you can either use Capabilities on Linux,\n            or use the ``as-root`` option to run the master process as root.\n\n        :param str|unicode address: UDP address to bind to.\n\n            Examples:\n\n                * 192.168.1.1:2222\n\n        :param str|unicode community_string: SNMP instance identifier to address it.", "docstring_tokens": ["Enables", "SNMP", "."], "sha": "475407acb44199edbf7e0a66261bfeb51de1afae", "url": "https://github.com/idlesign/uwsgiconf/blob/475407acb44199edbf7e0a66261bfeb51de1afae/uwsgiconf/options/monitoring.py#L211-L235", "partition": "train"}
{"repo": "idlesign/uwsgiconf", "path": "uwsgiconf/options/applications.py", "func_name": "Applications.mount", "original_string": "def mount(self, mountpoint, app, into_worker=False):\n        \"\"\"Load application under mountpoint.\n\n        Example:\n            * .mount('', 'app0.py') -- Root URL part\n            * .mount('/app1', 'app1.py') -- URL part\n            * .mount('/pinax/here', '/var/www/pinax/deploy/pinax.wsgi')\n            * .mount('the_app3', 'app3.py')  -- Variable value: application alias (can be set by ``UWSGI_APPID``)\n            * .mount('example.com', 'app2.py')  -- Variable value: Hostname (variable set in nginx)\n\n        * http://uwsgi-docs.readthedocs.io/en/latest/Nginx.html#hosting-multiple-apps-in-the-same-process-aka-managing-script-name-and-path-info\n\n        :param str|unicode mountpoint: URL part, or variable value.\n\n            .. note:: In case of URL part you may also want to set ``manage_script_name`` basic param to ``True``.\n\n            .. warning:: In case of URL part a trailing slash may case problems in some cases\n                (e.g. with Django based projects).\n\n        :param str|unicode app: App module/file.\n\n        :param bool into_worker: Load application under mountpoint\n            in the specified worker or after workers spawn.\n\n        \"\"\"\n        # todo check worker mount -- uwsgi_init_worker_mount_app() expects worker://\n        self._set('worker-mount' if into_worker else 'mount', '%s=%s' % (mountpoint, app), multi=True)\n\n        return self._section", "language": "python", "code": "def mount(self, mountpoint, app, into_worker=False):\n        \"\"\"Load application under mountpoint.\n\n        Example:\n            * .mount('', 'app0.py') -- Root URL part\n            * .mount('/app1', 'app1.py') -- URL part\n            * .mount('/pinax/here', '/var/www/pinax/deploy/pinax.wsgi')\n            * .mount('the_app3', 'app3.py')  -- Variable value: application alias (can be set by ``UWSGI_APPID``)\n            * .mount('example.com', 'app2.py')  -- Variable value: Hostname (variable set in nginx)\n\n        * http://uwsgi-docs.readthedocs.io/en/latest/Nginx.html#hosting-multiple-apps-in-the-same-process-aka-managing-script-name-and-path-info\n\n        :param str|unicode mountpoint: URL part, or variable value.\n\n            .. note:: In case of URL part you may also want to set ``manage_script_name`` basic param to ``True``.\n\n            .. warning:: In case of URL part a trailing slash may case problems in some cases\n                (e.g. with Django based projects).\n\n        :param str|unicode app: App module/file.\n\n        :param bool into_worker: Load application under mountpoint\n            in the specified worker or after workers spawn.\n\n        \"\"\"\n        # todo check worker mount -- uwsgi_init_worker_mount_app() expects worker://\n        self._set('worker-mount' if into_worker else 'mount', '%s=%s' % (mountpoint, app), multi=True)\n\n        return self._section", "code_tokens": ["def", "mount", "(", "self", ",", "mountpoint", ",", "app", ",", "into_worker", "=", "False", ")", ":", "# todo check worker mount -- uwsgi_init_worker_mount_app() expects worker://", "self", ".", "_set", "(", "'worker-mount'", "if", "into_worker", "else", "'mount'", ",", "'%s=%s'", "%", "(", "mountpoint", ",", "app", ")", ",", "multi", "=", "True", ")", "return", "self", ".", "_section"], "docstring": "Load application under mountpoint.\n\n        Example:\n            * .mount('', 'app0.py') -- Root URL part\n            * .mount('/app1', 'app1.py') -- URL part\n            * .mount('/pinax/here', '/var/www/pinax/deploy/pinax.wsgi')\n            * .mount('the_app3', 'app3.py')  -- Variable value: application alias (can be set by ``UWSGI_APPID``)\n            * .mount('example.com', 'app2.py')  -- Variable value: Hostname (variable set in nginx)\n\n        * http://uwsgi-docs.readthedocs.io/en/latest/Nginx.html#hosting-multiple-apps-in-the-same-process-aka-managing-script-name-and-path-info\n\n        :param str|unicode mountpoint: URL part, or variable value.\n\n            .. note:: In case of URL part you may also want to set ``manage_script_name`` basic param to ``True``.\n\n            .. warning:: In case of URL part a trailing slash may case problems in some cases\n                (e.g. with Django based projects).\n\n        :param str|unicode app: App module/file.\n\n        :param bool into_worker: Load application under mountpoint\n            in the specified worker or after workers spawn.", "docstring_tokens": ["Load", "application", "under", "mountpoint", "."], "sha": "475407acb44199edbf7e0a66261bfeb51de1afae", "url": "https://github.com/idlesign/uwsgiconf/blob/475407acb44199edbf7e0a66261bfeb51de1afae/uwsgiconf/options/applications.py#L40-L68", "partition": "train"}
{"repo": "idlesign/uwsgiconf", "path": "uwsgiconf/options/applications.py", "func_name": "Applications.switch_into_lazy_mode", "original_string": "def switch_into_lazy_mode(self, affect_master=None):\n        \"\"\"Load apps in workers instead of master.\n\n        This option may have memory usage implications\n        as Copy-on-Write semantics can not be used.\n\n        .. note:: Consider using ``touch_chain_reload`` option in ``workers`` basic params\n            for lazy apps reloading.\n\n        :param bool affect_master: If **True** only workers will be\n          reloaded by uWSGI's reload signals; the master will remain alive.\n\n          .. warning:: uWSGI configuration changes are not picked up on reload by the master.\n\n\n        \"\"\"\n        self._set('lazy' if affect_master else 'lazy-apps', True, cast=bool)\n\n        return self._section", "language": "python", "code": "def switch_into_lazy_mode(self, affect_master=None):\n        \"\"\"Load apps in workers instead of master.\n\n        This option may have memory usage implications\n        as Copy-on-Write semantics can not be used.\n\n        .. note:: Consider using ``touch_chain_reload`` option in ``workers`` basic params\n            for lazy apps reloading.\n\n        :param bool affect_master: If **True** only workers will be\n          reloaded by uWSGI's reload signals; the master will remain alive.\n\n          .. warning:: uWSGI configuration changes are not picked up on reload by the master.\n\n\n        \"\"\"\n        self._set('lazy' if affect_master else 'lazy-apps', True, cast=bool)\n\n        return self._section", "code_tokens": ["def", "switch_into_lazy_mode", "(", "self", ",", "affect_master", "=", "None", ")", ":", "self", ".", "_set", "(", "'lazy'", "if", "affect_master", "else", "'lazy-apps'", ",", "True", ",", "cast", "=", "bool", ")", "return", "self", ".", "_section"], "docstring": "Load apps in workers instead of master.\n\n        This option may have memory usage implications\n        as Copy-on-Write semantics can not be used.\n\n        .. note:: Consider using ``touch_chain_reload`` option in ``workers`` basic params\n            for lazy apps reloading.\n\n        :param bool affect_master: If **True** only workers will be\n          reloaded by uWSGI's reload signals; the master will remain alive.\n\n          .. warning:: uWSGI configuration changes are not picked up on reload by the master.", "docstring_tokens": ["Load", "apps", "in", "workers", "instead", "of", "master", "."], "sha": "475407acb44199edbf7e0a66261bfeb51de1afae", "url": "https://github.com/idlesign/uwsgiconf/blob/475407acb44199edbf7e0a66261bfeb51de1afae/uwsgiconf/options/applications.py#L70-L88", "partition": "train"}
{"repo": "idlesign/uwsgiconf", "path": "uwsgiconf/options/routing_routers.py", "func_name": "RouterHttp.set_basic_params", "original_string": "def set_basic_params(\n            self, workers=None, zerg_server=None, fallback_node=None, concurrent_events=None,\n            cheap_mode=None, stats_server=None, quiet=None, buffer_size=None,\n            keepalive=None, resubscribe_addresses=None):\n        \"\"\"\n        :param int workers: Number of worker processes to spawn.\n\n        :param str|unicode zerg_server: Attach the router to a zerg server.\n\n        :param str|unicode fallback_node: Fallback to the specified node in case of error.\n\n        :param int concurrent_events: Set the maximum number of concurrent events router can manage.\n\n            Default: system dependent.\n\n        :param bool cheap_mode: Enables cheap mode. When the router is in cheap mode,\n            it will not respond to requests until a node is available.\n            This means that when there are no nodes subscribed, only your local app (if any) will respond.\n            When all of the nodes go down, the router will return in cheap mode.\n\n        :param str|unicode stats_server: Router stats server address to run at.\n\n        :param bool quiet: Do not report failed connections to instances.\n\n        :param int buffer_size: Set internal buffer size in bytes. Default: page size.\n\n        :param int keepalive: Allows holding the connection open even if the request has a body.\n\n            * http://uwsgi.readthedocs.io/en/latest/HTTP.html#http-keep-alive\n\n            .. note:: See http11 socket type for an alternative.\n\n        :param str|unicode|list[str|unicode] resubscribe_addresses: Forward subscriptions\n            to the specified subscription server.\n\n\n        \"\"\"\n        super(RouterHttp, self).set_basic_params(**filter_locals(locals(), drop=[\n            'keepalive',\n            'resubscribe_addresses',\n        ]))\n\n        self._set_aliased('keepalive', keepalive)\n        self._set_aliased('resubscribe', resubscribe_addresses, multi=True)\n\n        return self", "language": "python", "code": "def set_basic_params(\n            self, workers=None, zerg_server=None, fallback_node=None, concurrent_events=None,\n            cheap_mode=None, stats_server=None, quiet=None, buffer_size=None,\n            keepalive=None, resubscribe_addresses=None):\n        \"\"\"\n        :param int workers: Number of worker processes to spawn.\n\n        :param str|unicode zerg_server: Attach the router to a zerg server.\n\n        :param str|unicode fallback_node: Fallback to the specified node in case of error.\n\n        :param int concurrent_events: Set the maximum number of concurrent events router can manage.\n\n            Default: system dependent.\n\n        :param bool cheap_mode: Enables cheap mode. When the router is in cheap mode,\n            it will not respond to requests until a node is available.\n            This means that when there are no nodes subscribed, only your local app (if any) will respond.\n            When all of the nodes go down, the router will return in cheap mode.\n\n        :param str|unicode stats_server: Router stats server address to run at.\n\n        :param bool quiet: Do not report failed connections to instances.\n\n        :param int buffer_size: Set internal buffer size in bytes. Default: page size.\n\n        :param int keepalive: Allows holding the connection open even if the request has a body.\n\n            * http://uwsgi.readthedocs.io/en/latest/HTTP.html#http-keep-alive\n\n            .. note:: See http11 socket type for an alternative.\n\n        :param str|unicode|list[str|unicode] resubscribe_addresses: Forward subscriptions\n            to the specified subscription server.\n\n\n        \"\"\"\n        super(RouterHttp, self).set_basic_params(**filter_locals(locals(), drop=[\n            'keepalive',\n            'resubscribe_addresses',\n        ]))\n\n        self._set_aliased('keepalive', keepalive)\n        self._set_aliased('resubscribe', resubscribe_addresses, multi=True)\n\n        return self", "code_tokens": ["def", "set_basic_params", "(", "self", ",", "workers", "=", "None", ",", "zerg_server", "=", "None", ",", "fallback_node", "=", "None", ",", "concurrent_events", "=", "None", ",", "cheap_mode", "=", "None", ",", "stats_server", "=", "None", ",", "quiet", "=", "None", ",", "buffer_size", "=", "None", ",", "keepalive", "=", "None", ",", "resubscribe_addresses", "=", "None", ")", ":", "super", "(", "RouterHttp", ",", "self", ")", ".", "set_basic_params", "(", "*", "*", "filter_locals", "(", "locals", "(", ")", ",", "drop", "=", "[", "'keepalive'", ",", "'resubscribe_addresses'", ",", "]", ")", ")", "self", ".", "_set_aliased", "(", "'keepalive'", ",", "keepalive", ")", "self", ".", "_set_aliased", "(", "'resubscribe'", ",", "resubscribe_addresses", ",", "multi", "=", "True", ")", "return", "self"], "docstring": ":param int workers: Number of worker processes to spawn.\n\n        :param str|unicode zerg_server: Attach the router to a zerg server.\n\n        :param str|unicode fallback_node: Fallback to the specified node in case of error.\n\n        :param int concurrent_events: Set the maximum number of concurrent events router can manage.\n\n            Default: system dependent.\n\n        :param bool cheap_mode: Enables cheap mode. When the router is in cheap mode,\n            it will not respond to requests until a node is available.\n            This means that when there are no nodes subscribed, only your local app (if any) will respond.\n            When all of the nodes go down, the router will return in cheap mode.\n\n        :param str|unicode stats_server: Router stats server address to run at.\n\n        :param bool quiet: Do not report failed connections to instances.\n\n        :param int buffer_size: Set internal buffer size in bytes. Default: page size.\n\n        :param int keepalive: Allows holding the connection open even if the request has a body.\n\n            * http://uwsgi.readthedocs.io/en/latest/HTTP.html#http-keep-alive\n\n            .. note:: See http11 socket type for an alternative.\n\n        :param str|unicode|list[str|unicode] resubscribe_addresses: Forward subscriptions\n            to the specified subscription server.", "docstring_tokens": [":", "param", "int", "workers", ":", "Number", "of", "worker", "processes", "to", "spawn", "."], "sha": "475407acb44199edbf7e0a66261bfeb51de1afae", "url": "https://github.com/idlesign/uwsgiconf/blob/475407acb44199edbf7e0a66261bfeb51de1afae/uwsgiconf/options/routing_routers.py#L319-L364", "partition": "train"}
{"repo": "idlesign/uwsgiconf", "path": "uwsgiconf/options/routing_routers.py", "func_name": "RouterHttp.set_connections_params", "original_string": "def set_connections_params(\n            self, harakiri=None, timeout_socket=None, retry_delay=None, timeout_headers=None, timeout_backend=None):\n        \"\"\"Sets connection-related parameters.\n\n        :param int harakiri: Set gateway harakiri timeout (seconds).\n\n        :param int timeout_socket: Node socket timeout (seconds).\n            Used to set the SPDY timeout. This is the maximum amount of inactivity after\n            the SPDY connection is closed.\n\n            Default: 60.\n\n        :param int retry_delay: Retry connections to dead static nodes after the specified\n            amount of seconds. Default: 30.\n\n        :param int timeout_headers: Defines the timeout (seconds) while waiting for http headers.\n\n            Default: `socket_timeout`.\n\n        :param int timeout_backend: Defines the timeout (seconds) when connecting to backend instances.\n\n            Default: `socket_timeout`.\n\n        \"\"\"\n\n        super(RouterHttp, self).set_connections_params(\n            **filter_locals(locals(), ['timeout_headers', 'timeout_backend']))\n\n        self._set_aliased('headers-timeout', timeout_headers)\n        self._set_aliased('connect-timeout', timeout_backend)\n\n        return self", "language": "python", "code": "def set_connections_params(\n            self, harakiri=None, timeout_socket=None, retry_delay=None, timeout_headers=None, timeout_backend=None):\n        \"\"\"Sets connection-related parameters.\n\n        :param int harakiri: Set gateway harakiri timeout (seconds).\n\n        :param int timeout_socket: Node socket timeout (seconds).\n            Used to set the SPDY timeout. This is the maximum amount of inactivity after\n            the SPDY connection is closed.\n\n            Default: 60.\n\n        :param int retry_delay: Retry connections to dead static nodes after the specified\n            amount of seconds. Default: 30.\n\n        :param int timeout_headers: Defines the timeout (seconds) while waiting for http headers.\n\n            Default: `socket_timeout`.\n\n        :param int timeout_backend: Defines the timeout (seconds) when connecting to backend instances.\n\n            Default: `socket_timeout`.\n\n        \"\"\"\n\n        super(RouterHttp, self).set_connections_params(\n            **filter_locals(locals(), ['timeout_headers', 'timeout_backend']))\n\n        self._set_aliased('headers-timeout', timeout_headers)\n        self._set_aliased('connect-timeout', timeout_backend)\n\n        return self", "code_tokens": ["def", "set_connections_params", "(", "self", ",", "harakiri", "=", "None", ",", "timeout_socket", "=", "None", ",", "retry_delay", "=", "None", ",", "timeout_headers", "=", "None", ",", "timeout_backend", "=", "None", ")", ":", "super", "(", "RouterHttp", ",", "self", ")", ".", "set_connections_params", "(", "*", "*", "filter_locals", "(", "locals", "(", ")", ",", "[", "'timeout_headers'", ",", "'timeout_backend'", "]", ")", ")", "self", ".", "_set_aliased", "(", "'headers-timeout'", ",", "timeout_headers", ")", "self", ".", "_set_aliased", "(", "'connect-timeout'", ",", "timeout_backend", ")", "return", "self"], "docstring": "Sets connection-related parameters.\n\n        :param int harakiri: Set gateway harakiri timeout (seconds).\n\n        :param int timeout_socket: Node socket timeout (seconds).\n            Used to set the SPDY timeout. This is the maximum amount of inactivity after\n            the SPDY connection is closed.\n\n            Default: 60.\n\n        :param int retry_delay: Retry connections to dead static nodes after the specified\n            amount of seconds. Default: 30.\n\n        :param int timeout_headers: Defines the timeout (seconds) while waiting for http headers.\n\n            Default: `socket_timeout`.\n\n        :param int timeout_backend: Defines the timeout (seconds) when connecting to backend instances.\n\n            Default: `socket_timeout`.", "docstring_tokens": ["Sets", "connection", "-", "related", "parameters", "."], "sha": "475407acb44199edbf7e0a66261bfeb51de1afae", "url": "https://github.com/idlesign/uwsgiconf/blob/475407acb44199edbf7e0a66261bfeb51de1afae/uwsgiconf/options/routing_routers.py#L366-L397", "partition": "train"}
{"repo": "eaton-lab/toytree", "path": "toytree/Coords.py", "func_name": "Coords.assign_vertices", "original_string": "def assign_vertices(self):\n        \"\"\"\n        Sets .edges, .verts for node positions. \n        X and Y positions here refer to base assumption that tree is right\n        facing, reorient_coordinates() will handle re-translating this.        \n        \"\"\"\n        # shortname \n        uselen = bool(self.ttree.style.use_edge_lengths)\n\n        # postorder: children then parents (nidxs from 0 up)\n        # store edge array for connecting child nodes to parent nodes\n        nidx = 0\n        for node in self.ttree.treenode.traverse(\"postorder\"):            \n            if not node.is_root():\n                self.edges[nidx, :] = [node.up.idx, node.idx]\n                nidx += 1\n\n        # store verts array with x,y positions of nodes (lengths of branches)\n        # we want tips to align at the right face (larger axis number)\n        _root = self.ttree.treenode.get_tree_root()\n        _treeheight = _root.get_distance(_root.get_farthest_leaf()[0])\n\n        # set node x, y\n        tidx = len(self.ttree) - 1\n        for node in self.ttree.treenode.traverse(\"postorder\"):\n\n            # Just leaves: x positions are evenly spread and ordered on axis\n            if node.is_leaf() and (not node.is_root()):\n                \n                # set y-positions (heights). Distance from root or zero\n                node.y = _treeheight - _root.get_distance(node)\n                if not uselen:\n                    node.y = 0.0\n                \n                # set x-positions (order of samples)\n                if self.ttree._fixed_order:\n                    node.x = self.ttree._fixed_order.index(node.name)# - tidx\n                else:\n                    node.x = tidx\n                    tidx -= 1\n                \n                # store the x,y vertex positions\n                self.verts[node.idx] = [node.x, node.y]\n\n            # All internal node positions are not evenly spread or ordered\n            else:\n                # height is either distance or nnodes from root\n                node.y = _treeheight - _root.get_distance(node)\n                if not uselen:\n                    node.y = max([i.y for i in node.children]) + 1\n\n                # x position is halfway between childrens x-positions\n                if node.children:\n                    nch = node.children\n                    node.x = sum(i.x for i in nch) / float(len(nch))\n                else:\n                    node.x = tidx\n\n                # store the x,y vertex positions                    \n                self.verts[node.idx] = [node.x, node.y]", "language": "python", "code": "def assign_vertices(self):\n        \"\"\"\n        Sets .edges, .verts for node positions. \n        X and Y positions here refer to base assumption that tree is right\n        facing, reorient_coordinates() will handle re-translating this.        \n        \"\"\"\n        # shortname \n        uselen = bool(self.ttree.style.use_edge_lengths)\n\n        # postorder: children then parents (nidxs from 0 up)\n        # store edge array for connecting child nodes to parent nodes\n        nidx = 0\n        for node in self.ttree.treenode.traverse(\"postorder\"):            \n            if not node.is_root():\n                self.edges[nidx, :] = [node.up.idx, node.idx]\n                nidx += 1\n\n        # store verts array with x,y positions of nodes (lengths of branches)\n        # we want tips to align at the right face (larger axis number)\n        _root = self.ttree.treenode.get_tree_root()\n        _treeheight = _root.get_distance(_root.get_farthest_leaf()[0])\n\n        # set node x, y\n        tidx = len(self.ttree) - 1\n        for node in self.ttree.treenode.traverse(\"postorder\"):\n\n            # Just leaves: x positions are evenly spread and ordered on axis\n            if node.is_leaf() and (not node.is_root()):\n                \n                # set y-positions (heights). Distance from root or zero\n                node.y = _treeheight - _root.get_distance(node)\n                if not uselen:\n                    node.y = 0.0\n                \n                # set x-positions (order of samples)\n                if self.ttree._fixed_order:\n                    node.x = self.ttree._fixed_order.index(node.name)# - tidx\n                else:\n                    node.x = tidx\n                    tidx -= 1\n                \n                # store the x,y vertex positions\n                self.verts[node.idx] = [node.x, node.y]\n\n            # All internal node positions are not evenly spread or ordered\n            else:\n                # height is either distance or nnodes from root\n                node.y = _treeheight - _root.get_distance(node)\n                if not uselen:\n                    node.y = max([i.y for i in node.children]) + 1\n\n                # x position is halfway between childrens x-positions\n                if node.children:\n                    nch = node.children\n                    node.x = sum(i.x for i in nch) / float(len(nch))\n                else:\n                    node.x = tidx\n\n                # store the x,y vertex positions                    \n                self.verts[node.idx] = [node.x, node.y]", "code_tokens": ["def", "assign_vertices", "(", "self", ")", ":", "# shortname ", "uselen", "=", "bool", "(", "self", ".", "ttree", ".", "style", ".", "use_edge_lengths", ")", "# postorder: children then parents (nidxs from 0 up)", "# store edge array for connecting child nodes to parent nodes", "nidx", "=", "0", "for", "node", "in", "self", ".", "ttree", ".", "treenode", ".", "traverse", "(", "\"postorder\"", ")", ":", "if", "not", "node", ".", "is_root", "(", ")", ":", "self", ".", "edges", "[", "nidx", ",", ":", "]", "=", "[", "node", ".", "up", ".", "idx", ",", "node", ".", "idx", "]", "nidx", "+=", "1", "# store verts array with x,y positions of nodes (lengths of branches)", "# we want tips to align at the right face (larger axis number)", "_root", "=", "self", ".", "ttree", ".", "treenode", ".", "get_tree_root", "(", ")", "_treeheight", "=", "_root", ".", "get_distance", "(", "_root", ".", "get_farthest_leaf", "(", ")", "[", "0", "]", ")", "# set node x, y", "tidx", "=", "len", "(", "self", ".", "ttree", ")", "-", "1", "for", "node", "in", "self", ".", "ttree", ".", "treenode", ".", "traverse", "(", "\"postorder\"", ")", ":", "# Just leaves: x positions are evenly spread and ordered on axis", "if", "node", ".", "is_leaf", "(", ")", "and", "(", "not", "node", ".", "is_root", "(", ")", ")", ":", "# set y-positions (heights). Distance from root or zero", "node", ".", "y", "=", "_treeheight", "-", "_root", ".", "get_distance", "(", "node", ")", "if", "not", "uselen", ":", "node", ".", "y", "=", "0.0", "# set x-positions (order of samples)", "if", "self", ".", "ttree", ".", "_fixed_order", ":", "node", ".", "x", "=", "self", ".", "ttree", ".", "_fixed_order", ".", "index", "(", "node", ".", "name", ")", "# - tidx", "else", ":", "node", ".", "x", "=", "tidx", "tidx", "-=", "1", "# store the x,y vertex positions", "self", ".", "verts", "[", "node", ".", "idx", "]", "=", "[", "node", ".", "x", ",", "node", ".", "y", "]", "# All internal node positions are not evenly spread or ordered", "else", ":", "# height is either distance or nnodes from root", "node", ".", "y", "=", "_treeheight", "-", "_root", ".", "get_distance", "(", "node", ")", "if", "not", "uselen", ":", "node", ".", "y", "=", "max", "(", "[", "i", ".", "y", "for", "i", "in", "node", ".", "children", "]", ")", "+", "1", "# x position is halfway between childrens x-positions", "if", "node", ".", "children", ":", "nch", "=", "node", ".", "children", "node", ".", "x", "=", "sum", "(", "i", ".", "x", "for", "i", "in", "nch", ")", "/", "float", "(", "len", "(", "nch", ")", ")", "else", ":", "node", ".", "x", "=", "tidx", "# store the x,y vertex positions                    ", "self", ".", "verts", "[", "node", ".", "idx", "]", "=", "[", "node", ".", "x", ",", "node", ".", "y", "]"], "docstring": "Sets .edges, .verts for node positions. \n        X and Y positions here refer to base assumption that tree is right\n        facing, reorient_coordinates() will handle re-translating this.", "docstring_tokens": ["Sets", ".", "edges", ".", "verts", "for", "node", "positions", ".", "X", "and", "Y", "positions", "here", "refer", "to", "base", "assumption", "that", "tree", "is", "right", "facing", "reorient_coordinates", "()", "will", "handle", "re", "-", "translating", "this", "."], "sha": "0347ed2098acc5f707fadf52a0ecd411a6d1859c", "url": "https://github.com/eaton-lab/toytree/blob/0347ed2098acc5f707fadf52a0ecd411a6d1859c/toytree/Coords.py#L91-L150", "partition": "train"}
{"repo": "eaton-lab/toytree", "path": "toytree/Coords.py", "func_name": "Coords.reorient_coordinates", "original_string": "def reorient_coordinates(self):\n        \"\"\"\n        Returns a modified .verts array with new coordinates for nodes. \n        This does not need to modify .edges. The order of nodes, and therefore\n        of verts rows is still the same because it is still based on the tree\n        branching order (ladderized usually). \n        \"\"\"\n        # if tree is empty then bail out\n        if len(self.ttree) < 2:\n            return\n\n        # down is the default orientation\n        # down-facing tips align at y=0, first ladderized tip at x=0\n        if self.ttree.style.orient in ('down', 0):\n            pass\n\n        # right-facing tips align at x=0, last ladderized tip at y=0\n        elif self.ttree.style.orient in ('right', 3):\n\n            # verts swap x and ys and make xs 0 to negative\n            tmp = np.zeros(self.verts.shape)\n            tmp[:, 1] = self.verts[:, 0]\n            tmp[:, 0] = self.verts[:, 1] * -1\n            self.verts = tmp\n\n            # coords...\n            tmp = np.zeros(self.coords.shape)\n            tmp[:, 1] = self.coords[:, 0]\n            tmp[:, 0] = self.coords[:, 1] * -1\n            self.coords = tmp\n\n        elif self.ttree.style.orient in ('left', 1):\n            raise NotImplementedError(\"todo: left facing\")\n\n        else:\n            raise NotImplementedError(\"todo: up facing\")", "language": "python", "code": "def reorient_coordinates(self):\n        \"\"\"\n        Returns a modified .verts array with new coordinates for nodes. \n        This does not need to modify .edges. The order of nodes, and therefore\n        of verts rows is still the same because it is still based on the tree\n        branching order (ladderized usually). \n        \"\"\"\n        # if tree is empty then bail out\n        if len(self.ttree) < 2:\n            return\n\n        # down is the default orientation\n        # down-facing tips align at y=0, first ladderized tip at x=0\n        if self.ttree.style.orient in ('down', 0):\n            pass\n\n        # right-facing tips align at x=0, last ladderized tip at y=0\n        elif self.ttree.style.orient in ('right', 3):\n\n            # verts swap x and ys and make xs 0 to negative\n            tmp = np.zeros(self.verts.shape)\n            tmp[:, 1] = self.verts[:, 0]\n            tmp[:, 0] = self.verts[:, 1] * -1\n            self.verts = tmp\n\n            # coords...\n            tmp = np.zeros(self.coords.shape)\n            tmp[:, 1] = self.coords[:, 0]\n            tmp[:, 0] = self.coords[:, 1] * -1\n            self.coords = tmp\n\n        elif self.ttree.style.orient in ('left', 1):\n            raise NotImplementedError(\"todo: left facing\")\n\n        else:\n            raise NotImplementedError(\"todo: up facing\")", "code_tokens": ["def", "reorient_coordinates", "(", "self", ")", ":", "# if tree is empty then bail out", "if", "len", "(", "self", ".", "ttree", ")", "<", "2", ":", "return", "# down is the default orientation", "# down-facing tips align at y=0, first ladderized tip at x=0", "if", "self", ".", "ttree", ".", "style", ".", "orient", "in", "(", "'down'", ",", "0", ")", ":", "pass", "# right-facing tips align at x=0, last ladderized tip at y=0", "elif", "self", ".", "ttree", ".", "style", ".", "orient", "in", "(", "'right'", ",", "3", ")", ":", "# verts swap x and ys and make xs 0 to negative", "tmp", "=", "np", ".", "zeros", "(", "self", ".", "verts", ".", "shape", ")", "tmp", "[", ":", ",", "1", "]", "=", "self", ".", "verts", "[", ":", ",", "0", "]", "tmp", "[", ":", ",", "0", "]", "=", "self", ".", "verts", "[", ":", ",", "1", "]", "*", "-", "1", "self", ".", "verts", "=", "tmp", "# coords...", "tmp", "=", "np", ".", "zeros", "(", "self", ".", "coords", ".", "shape", ")", "tmp", "[", ":", ",", "1", "]", "=", "self", ".", "coords", "[", ":", ",", "0", "]", "tmp", "[", ":", ",", "0", "]", "=", "self", ".", "coords", "[", ":", ",", "1", "]", "*", "-", "1", "self", ".", "coords", "=", "tmp", "elif", "self", ".", "ttree", ".", "style", ".", "orient", "in", "(", "'left'", ",", "1", ")", ":", "raise", "NotImplementedError", "(", "\"todo: left facing\"", ")", "else", ":", "raise", "NotImplementedError", "(", "\"todo: up facing\"", ")"], "docstring": "Returns a modified .verts array with new coordinates for nodes. \n        This does not need to modify .edges. The order of nodes, and therefore\n        of verts rows is still the same because it is still based on the tree\n        branching order (ladderized usually).", "docstring_tokens": ["Returns", "a", "modified", ".", "verts", "array", "with", "new", "coordinates", "for", "nodes", ".", "This", "does", "not", "need", "to", "modify", ".", "edges", ".", "The", "order", "of", "nodes", "and", "therefore", "of", "verts", "rows", "is", "still", "the", "same", "because", "it", "is", "still", "based", "on", "the", "tree", "branching", "order", "(", "ladderized", "usually", ")", "."], "sha": "0347ed2098acc5f707fadf52a0ecd411a6d1859c", "url": "https://github.com/eaton-lab/toytree/blob/0347ed2098acc5f707fadf52a0ecd411a6d1859c/toytree/Coords.py#L252-L287", "partition": "train"}
{"repo": "quantmind/dynts", "path": "dynts/formatters/base.py", "func_name": "tsiterator", "original_string": "def tsiterator(ts, dateconverter=None, desc=None,\n               clean=False, start_value=None, **kwargs):\n    '''An iterator of timeseries as tuples.'''\n    dateconverter = dateconverter or default_converter\n    yield ['Date'] + ts.names()\n    if clean == 'full':\n        for dt, value in full_clean(ts, dateconverter, desc, start_value):\n             yield (dt,) + tuple(value)\n    else:\n        if clean:\n            ts = ts.clean()\n        for dt, value in ts.items(desc=desc, start_value=start_value):\n            dt = dateconverter(dt)\n            yield (dt,) + tuple(value)", "language": "python", "code": "def tsiterator(ts, dateconverter=None, desc=None,\n               clean=False, start_value=None, **kwargs):\n    '''An iterator of timeseries as tuples.'''\n    dateconverter = dateconverter or default_converter\n    yield ['Date'] + ts.names()\n    if clean == 'full':\n        for dt, value in full_clean(ts, dateconverter, desc, start_value):\n             yield (dt,) + tuple(value)\n    else:\n        if clean:\n            ts = ts.clean()\n        for dt, value in ts.items(desc=desc, start_value=start_value):\n            dt = dateconverter(dt)\n            yield (dt,) + tuple(value)", "code_tokens": ["def", "tsiterator", "(", "ts", ",", "dateconverter", "=", "None", ",", "desc", "=", "None", ",", "clean", "=", "False", ",", "start_value", "=", "None", ",", "*", "*", "kwargs", ")", ":", "dateconverter", "=", "dateconverter", "or", "default_converter", "yield", "[", "'Date'", "]", "+", "ts", ".", "names", "(", ")", "if", "clean", "==", "'full'", ":", "for", "dt", ",", "value", "in", "full_clean", "(", "ts", ",", "dateconverter", ",", "desc", ",", "start_value", ")", ":", "yield", "(", "dt", ",", ")", "+", "tuple", "(", "value", ")", "else", ":", "if", "clean", ":", "ts", "=", "ts", ".", "clean", "(", ")", "for", "dt", ",", "value", "in", "ts", ".", "items", "(", "desc", "=", "desc", ",", "start_value", "=", "start_value", ")", ":", "dt", "=", "dateconverter", "(", "dt", ")", "yield", "(", "dt", ",", ")", "+", "tuple", "(", "value", ")"], "docstring": "An iterator of timeseries as tuples.", "docstring_tokens": ["An", "iterator", "of", "timeseries", "as", "tuples", "."], "sha": "21ac57c648bfec402fa6b1fe569496cf098fb5e8", "url": "https://github.com/quantmind/dynts/blob/21ac57c648bfec402fa6b1fe569496cf098fb5e8/dynts/formatters/base.py#L23-L36", "partition": "train"}
{"repo": "eaton-lab/toytree", "path": "toytree/Drawing.py", "func_name": "Drawing.set_baselines", "original_string": "def set_baselines(self):\n        \"\"\"\n        Modify coords to shift tree position for x,y baseline arguments. This\n        is useful for arrangeing trees onto a Canvas with other plots, but \n        still sharing a common cartesian axes coordinates. \n        \"\"\"\n        if self.style.xbaseline:\n            if self.style.orient in (\"up\", \"down\"):\n                self.coords.coords[:, 0] += self.style.xbaseline\n                self.coords.verts[:, 0] += self.style.xbaseline                \n            else:\n                self.coords.coords[:, 1] += self.style.xbaseline\n                self.coords.verts[:, 1] += self.style.xbaseline", "language": "python", "code": "def set_baselines(self):\n        \"\"\"\n        Modify coords to shift tree position for x,y baseline arguments. This\n        is useful for arrangeing trees onto a Canvas with other plots, but \n        still sharing a common cartesian axes coordinates. \n        \"\"\"\n        if self.style.xbaseline:\n            if self.style.orient in (\"up\", \"down\"):\n                self.coords.coords[:, 0] += self.style.xbaseline\n                self.coords.verts[:, 0] += self.style.xbaseline                \n            else:\n                self.coords.coords[:, 1] += self.style.xbaseline\n                self.coords.verts[:, 1] += self.style.xbaseline", "code_tokens": ["def", "set_baselines", "(", "self", ")", ":", "if", "self", ".", "style", ".", "xbaseline", ":", "if", "self", ".", "style", ".", "orient", "in", "(", "\"up\"", ",", "\"down\"", ")", ":", "self", ".", "coords", ".", "coords", "[", ":", ",", "0", "]", "+=", "self", ".", "style", ".", "xbaseline", "self", ".", "coords", ".", "verts", "[", ":", ",", "0", "]", "+=", "self", ".", "style", ".", "xbaseline", "else", ":", "self", ".", "coords", ".", "coords", "[", ":", ",", "1", "]", "+=", "self", ".", "style", ".", "xbaseline", "self", ".", "coords", ".", "verts", "[", ":", ",", "1", "]", "+=", "self", ".", "style", ".", "xbaseline"], "docstring": "Modify coords to shift tree position for x,y baseline arguments. This\n        is useful for arrangeing trees onto a Canvas with other plots, but \n        still sharing a common cartesian axes coordinates.", "docstring_tokens": ["Modify", "coords", "to", "shift", "tree", "position", "for", "x", "y", "baseline", "arguments", ".", "This", "is", "useful", "for", "arrangeing", "trees", "onto", "a", "Canvas", "with", "other", "plots", "but", "still", "sharing", "a", "common", "cartesian", "axes", "coordinates", "."], "sha": "0347ed2098acc5f707fadf52a0ecd411a6d1859c", "url": "https://github.com/eaton-lab/toytree/blob/0347ed2098acc5f707fadf52a0ecd411a6d1859c/toytree/Drawing.py#L73-L85", "partition": "train"}
{"repo": "eaton-lab/toytree", "path": "toytree/Drawing.py", "func_name": "Drawing.add_tip_labels_to_axes", "original_string": "def add_tip_labels_to_axes(self):\n        \"\"\"\n        Add text offset from tips of tree with correction for orientation, \n        and fixed_order which is usually used in multitree plotting.\n        \"\"\"\n        # get tip-coords and replace if using fixed_order\n        xpos = self.ttree.get_tip_coordinates('x')\n        ypos = self.ttree.get_tip_coordinates('y')\n\n        if self.style.orient in (\"up\", \"down\"):\n            if self.ttree._fixed_order:\n                xpos = list(range(self.ttree.ntips))\n                ypos = ypos[self.ttree._fixed_idx]\n            if self.style.tip_labels_align:\n                ypos = np.zeros(self.ttree.ntips)\n\n        if self.style.orient in (\"right\", \"left\"):\n            if self.ttree._fixed_order:\n                xpos = xpos[self.ttree._fixed_idx]\n                ypos = list(range(self.ttree.ntips))\n            if self.style.tip_labels_align:\n                xpos = np.zeros(self.ttree.ntips)\n\n        # pop fill from color dict if using color\n        tstyle = deepcopy(self.style.tip_labels_style)\n        if self.style.tip_labels_colors:\n            tstyle.pop(\"fill\")\n\n        # add tip names to coordinates calculated above\n        self.axes.text(\n            xpos, \n            ypos,\n            self.tip_labels,\n            angle=(0 if self.style.orient in (\"right\", \"left\") else -90),\n            style=tstyle,\n            color=self.style.tip_labels_colors,\n        )\n        \n        # get stroke-width for aligned tip-label lines (optional)\n        # copy stroke-width from the edge_style unless user set it\n        if not self.style.edge_align_style.get(\"stroke-width\"):\n            self.style.edge_align_style[\"stroke-width\"] = (\n                self.style.edge_style[\"stroke-width\"])", "language": "python", "code": "def add_tip_labels_to_axes(self):\n        \"\"\"\n        Add text offset from tips of tree with correction for orientation, \n        and fixed_order which is usually used in multitree plotting.\n        \"\"\"\n        # get tip-coords and replace if using fixed_order\n        xpos = self.ttree.get_tip_coordinates('x')\n        ypos = self.ttree.get_tip_coordinates('y')\n\n        if self.style.orient in (\"up\", \"down\"):\n            if self.ttree._fixed_order:\n                xpos = list(range(self.ttree.ntips))\n                ypos = ypos[self.ttree._fixed_idx]\n            if self.style.tip_labels_align:\n                ypos = np.zeros(self.ttree.ntips)\n\n        if self.style.orient in (\"right\", \"left\"):\n            if self.ttree._fixed_order:\n                xpos = xpos[self.ttree._fixed_idx]\n                ypos = list(range(self.ttree.ntips))\n            if self.style.tip_labels_align:\n                xpos = np.zeros(self.ttree.ntips)\n\n        # pop fill from color dict if using color\n        tstyle = deepcopy(self.style.tip_labels_style)\n        if self.style.tip_labels_colors:\n            tstyle.pop(\"fill\")\n\n        # add tip names to coordinates calculated above\n        self.axes.text(\n            xpos, \n            ypos,\n            self.tip_labels,\n            angle=(0 if self.style.orient in (\"right\", \"left\") else -90),\n            style=tstyle,\n            color=self.style.tip_labels_colors,\n        )\n        \n        # get stroke-width for aligned tip-label lines (optional)\n        # copy stroke-width from the edge_style unless user set it\n        if not self.style.edge_align_style.get(\"stroke-width\"):\n            self.style.edge_align_style[\"stroke-width\"] = (\n                self.style.edge_style[\"stroke-width\"])", "code_tokens": ["def", "add_tip_labels_to_axes", "(", "self", ")", ":", "# get tip-coords and replace if using fixed_order", "xpos", "=", "self", ".", "ttree", ".", "get_tip_coordinates", "(", "'x'", ")", "ypos", "=", "self", ".", "ttree", ".", "get_tip_coordinates", "(", "'y'", ")", "if", "self", ".", "style", ".", "orient", "in", "(", "\"up\"", ",", "\"down\"", ")", ":", "if", "self", ".", "ttree", ".", "_fixed_order", ":", "xpos", "=", "list", "(", "range", "(", "self", ".", "ttree", ".", "ntips", ")", ")", "ypos", "=", "ypos", "[", "self", ".", "ttree", ".", "_fixed_idx", "]", "if", "self", ".", "style", ".", "tip_labels_align", ":", "ypos", "=", "np", ".", "zeros", "(", "self", ".", "ttree", ".", "ntips", ")", "if", "self", ".", "style", ".", "orient", "in", "(", "\"right\"", ",", "\"left\"", ")", ":", "if", "self", ".", "ttree", ".", "_fixed_order", ":", "xpos", "=", "xpos", "[", "self", ".", "ttree", ".", "_fixed_idx", "]", "ypos", "=", "list", "(", "range", "(", "self", ".", "ttree", ".", "ntips", ")", ")", "if", "self", ".", "style", ".", "tip_labels_align", ":", "xpos", "=", "np", ".", "zeros", "(", "self", ".", "ttree", ".", "ntips", ")", "# pop fill from color dict if using color", "tstyle", "=", "deepcopy", "(", "self", ".", "style", ".", "tip_labels_style", ")", "if", "self", ".", "style", ".", "tip_labels_colors", ":", "tstyle", ".", "pop", "(", "\"fill\"", ")", "# add tip names to coordinates calculated above", "self", ".", "axes", ".", "text", "(", "xpos", ",", "ypos", ",", "self", ".", "tip_labels", ",", "angle", "=", "(", "0", "if", "self", ".", "style", ".", "orient", "in", "(", "\"right\"", ",", "\"left\"", ")", "else", "-", "90", ")", ",", "style", "=", "tstyle", ",", "color", "=", "self", ".", "style", ".", "tip_labels_colors", ",", ")", "# get stroke-width for aligned tip-label lines (optional)", "# copy stroke-width from the edge_style unless user set it", "if", "not", "self", ".", "style", ".", "edge_align_style", ".", "get", "(", "\"stroke-width\"", ")", ":", "self", ".", "style", ".", "edge_align_style", "[", "\"stroke-width\"", "]", "=", "(", "self", ".", "style", ".", "edge_style", "[", "\"stroke-width\"", "]", ")"], "docstring": "Add text offset from tips of tree with correction for orientation, \n        and fixed_order which is usually used in multitree plotting.", "docstring_tokens": ["Add", "text", "offset", "from", "tips", "of", "tree", "with", "correction", "for", "orientation", "and", "fixed_order", "which", "is", "usually", "used", "in", "multitree", "plotting", "."], "sha": "0347ed2098acc5f707fadf52a0ecd411a6d1859c", "url": "https://github.com/eaton-lab/toytree/blob/0347ed2098acc5f707fadf52a0ecd411a6d1859c/toytree/Drawing.py#L90-L132", "partition": "train"}
{"repo": "eaton-lab/toytree", "path": "toytree/Drawing.py", "func_name": "Drawing.add_tip_lines_to_axes", "original_string": "def add_tip_lines_to_axes(self):\n        \"add lines to connect tips to zero axis for tip_labels_align=True\"\n\n        # get tip-coords and align-coords from verts\n        xpos, ypos, aedges, averts = self.get_tip_label_coords() \n        if self.style.tip_labels_align:\n            self.axes.graph(\n                aedges,\n                vcoordinates=averts,\n                estyle=self.style.edge_align_style, \n                vlshow=False,\n                vsize=0,\n            )", "language": "python", "code": "def add_tip_lines_to_axes(self):\n        \"add lines to connect tips to zero axis for tip_labels_align=True\"\n\n        # get tip-coords and align-coords from verts\n        xpos, ypos, aedges, averts = self.get_tip_label_coords() \n        if self.style.tip_labels_align:\n            self.axes.graph(\n                aedges,\n                vcoordinates=averts,\n                estyle=self.style.edge_align_style, \n                vlshow=False,\n                vsize=0,\n            )", "code_tokens": ["def", "add_tip_lines_to_axes", "(", "self", ")", ":", "# get tip-coords and align-coords from verts", "xpos", ",", "ypos", ",", "aedges", ",", "averts", "=", "self", ".", "get_tip_label_coords", "(", ")", "if", "self", ".", "style", ".", "tip_labels_align", ":", "self", ".", "axes", ".", "graph", "(", "aedges", ",", "vcoordinates", "=", "averts", ",", "estyle", "=", "self", ".", "style", ".", "edge_align_style", ",", "vlshow", "=", "False", ",", "vsize", "=", "0", ",", ")"], "docstring": "add lines to connect tips to zero axis for tip_labels_align=True", "docstring_tokens": ["add", "lines", "to", "connect", "tips", "to", "zero", "axis", "for", "tip_labels_align", "=", "True"], "sha": "0347ed2098acc5f707fadf52a0ecd411a6d1859c", "url": "https://github.com/eaton-lab/toytree/blob/0347ed2098acc5f707fadf52a0ecd411a6d1859c/toytree/Drawing.py#L135-L147", "partition": "train"}
{"repo": "dbcli/athenacli", "path": "athenacli/packages/parseutils.py", "func_name": "query_starts_with", "original_string": "def query_starts_with(query, prefixes):\n    \"\"\"Check if the query starts with any item from *prefixes*.\"\"\"\n    prefixes = [prefix.lower() for prefix in prefixes]\n    formatted_sql = sqlparse.format(query.lower(), strip_comments=True)\n    return bool(formatted_sql) and formatted_sql.split()[0] in prefixes", "language": "python", "code": "def query_starts_with(query, prefixes):\n    \"\"\"Check if the query starts with any item from *prefixes*.\"\"\"\n    prefixes = [prefix.lower() for prefix in prefixes]\n    formatted_sql = sqlparse.format(query.lower(), strip_comments=True)\n    return bool(formatted_sql) and formatted_sql.split()[0] in prefixes", "code_tokens": ["def", "query_starts_with", "(", "query", ",", "prefixes", ")", ":", "prefixes", "=", "[", "prefix", ".", "lower", "(", ")", "for", "prefix", "in", "prefixes", "]", "formatted_sql", "=", "sqlparse", ".", "format", "(", "query", ".", "lower", "(", ")", ",", "strip_comments", "=", "True", ")", "return", "bool", "(", "formatted_sql", ")", "and", "formatted_sql", ".", "split", "(", ")", "[", "0", "]", "in", "prefixes"], "docstring": "Check if the query starts with any item from *prefixes*.", "docstring_tokens": ["Check", "if", "the", "query", "starts", "with", "any", "item", "from", "*", "prefixes", "*", "."], "sha": "bcab59e4953145866430083e902ed4d042d4ebba", "url": "https://github.com/dbcli/athenacli/blob/bcab59e4953145866430083e902ed4d042d4ebba/athenacli/packages/parseutils.py#L187-L191", "partition": "train"}
{"repo": "dbcli/athenacli", "path": "athenacli/packages/parseutils.py", "func_name": "queries_start_with", "original_string": "def queries_start_with(queries, prefixes):\n    \"\"\"Check if any queries start with any item from *prefixes*.\"\"\"\n    for query in sqlparse.split(queries):\n        if query and query_starts_with(query, prefixes) is True:\n            return True\n    return False", "language": "python", "code": "def queries_start_with(queries, prefixes):\n    \"\"\"Check if any queries start with any item from *prefixes*.\"\"\"\n    for query in sqlparse.split(queries):\n        if query and query_starts_with(query, prefixes) is True:\n            return True\n    return False", "code_tokens": ["def", "queries_start_with", "(", "queries", ",", "prefixes", ")", ":", "for", "query", "in", "sqlparse", ".", "split", "(", "queries", ")", ":", "if", "query", "and", "query_starts_with", "(", "query", ",", "prefixes", ")", "is", "True", ":", "return", "True", "return", "False"], "docstring": "Check if any queries start with any item from *prefixes*.", "docstring_tokens": ["Check", "if", "any", "queries", "start", "with", "any", "item", "from", "*", "prefixes", "*", "."], "sha": "bcab59e4953145866430083e902ed4d042d4ebba", "url": "https://github.com/dbcli/athenacli/blob/bcab59e4953145866430083e902ed4d042d4ebba/athenacli/packages/parseutils.py#L194-L199", "partition": "train"}
{"repo": "divio/cmsplugin-filer", "path": "cmsplugin_filer_teaser/cms_plugins.py", "func_name": "FilerTeaserPlugin._get_thumbnail_options", "original_string": "def _get_thumbnail_options(self, context, instance):\n        \"\"\"\n        Return the size and options of the thumbnail that should be inserted\n        \"\"\"\n        width, height = None, None\n        subject_location = False\n        placeholder_width = context.get('width', None)\n        placeholder_height = context.get('height', None)\n        if instance.use_autoscale and placeholder_width:\n            # use the placeholder width as a hint for sizing\n            width = int(placeholder_width)\n        if instance.use_autoscale and placeholder_height:\n            height = int(placeholder_height)\n        elif instance.width:\n            width = instance.width\n        if instance.height:\n            height = instance.height\n        if instance.image:\n            if instance.image.subject_location:\n                subject_location = instance.image.subject_location\n            if not height and width:\n                # height was not externally defined: use ratio to scale it by the width\n                height = int(float(width) * float(instance.image.height) / float(instance.image.width))\n            if not width and height:\n                # width was not externally defined: use ratio to scale it by the height\n                width = int(float(height) * float(instance.image.width) / float(instance.image.height))\n            if not width:\n                # width is still not defined. fallback the actual image width\n                width = instance.image.width\n            if not height:\n                # height is still not defined. fallback the actual image height\n                height = instance.image.height\n        return {'size': (width, height),\n                'subject_location': subject_location}", "language": "python", "code": "def _get_thumbnail_options(self, context, instance):\n        \"\"\"\n        Return the size and options of the thumbnail that should be inserted\n        \"\"\"\n        width, height = None, None\n        subject_location = False\n        placeholder_width = context.get('width', None)\n        placeholder_height = context.get('height', None)\n        if instance.use_autoscale and placeholder_width:\n            # use the placeholder width as a hint for sizing\n            width = int(placeholder_width)\n        if instance.use_autoscale and placeholder_height:\n            height = int(placeholder_height)\n        elif instance.width:\n            width = instance.width\n        if instance.height:\n            height = instance.height\n        if instance.image:\n            if instance.image.subject_location:\n                subject_location = instance.image.subject_location\n            if not height and width:\n                # height was not externally defined: use ratio to scale it by the width\n                height = int(float(width) * float(instance.image.height) / float(instance.image.width))\n            if not width and height:\n                # width was not externally defined: use ratio to scale it by the height\n                width = int(float(height) * float(instance.image.width) / float(instance.image.height))\n            if not width:\n                # width is still not defined. fallback the actual image width\n                width = instance.image.width\n            if not height:\n                # height is still not defined. fallback the actual image height\n                height = instance.image.height\n        return {'size': (width, height),\n                'subject_location': subject_location}", "code_tokens": ["def", "_get_thumbnail_options", "(", "self", ",", "context", ",", "instance", ")", ":", "width", ",", "height", "=", "None", ",", "None", "subject_location", "=", "False", "placeholder_width", "=", "context", ".", "get", "(", "'width'", ",", "None", ")", "placeholder_height", "=", "context", ".", "get", "(", "'height'", ",", "None", ")", "if", "instance", ".", "use_autoscale", "and", "placeholder_width", ":", "# use the placeholder width as a hint for sizing", "width", "=", "int", "(", "placeholder_width", ")", "if", "instance", ".", "use_autoscale", "and", "placeholder_height", ":", "height", "=", "int", "(", "placeholder_height", ")", "elif", "instance", ".", "width", ":", "width", "=", "instance", ".", "width", "if", "instance", ".", "height", ":", "height", "=", "instance", ".", "height", "if", "instance", ".", "image", ":", "if", "instance", ".", "image", ".", "subject_location", ":", "subject_location", "=", "instance", ".", "image", ".", "subject_location", "if", "not", "height", "and", "width", ":", "# height was not externally defined: use ratio to scale it by the width", "height", "=", "int", "(", "float", "(", "width", ")", "*", "float", "(", "instance", ".", "image", ".", "height", ")", "/", "float", "(", "instance", ".", "image", ".", "width", ")", ")", "if", "not", "width", "and", "height", ":", "# width was not externally defined: use ratio to scale it by the height", "width", "=", "int", "(", "float", "(", "height", ")", "*", "float", "(", "instance", ".", "image", ".", "width", ")", "/", "float", "(", "instance", ".", "image", ".", "height", ")", ")", "if", "not", "width", ":", "# width is still not defined. fallback the actual image width", "width", "=", "instance", ".", "image", ".", "width", "if", "not", "height", ":", "# height is still not defined. fallback the actual image height", "height", "=", "instance", ".", "image", ".", "height", "return", "{", "'size'", ":", "(", "width", ",", "height", ")", ",", "'subject_location'", ":", "subject_location", "}"], "docstring": "Return the size and options of the thumbnail that should be inserted", "docstring_tokens": ["Return", "the", "size", "and", "options", "of", "the", "thumbnail", "that", "should", "be", "inserted"], "sha": "4f9b0307dd768852ead64e651b743a165b3efccb", "url": "https://github.com/divio/cmsplugin-filer/blob/4f9b0307dd768852ead64e651b743a165b3efccb/cmsplugin_filer_teaser/cms_plugins.py#L42-L75", "partition": "train"}
{"repo": "divio/cmsplugin-filer", "path": "cmsplugin_filer_image/integrations/ckeditor.py", "func_name": "create_image_plugin", "original_string": "def create_image_plugin(filename, image, parent_plugin, **kwargs):\n    \"\"\"\n    Used for drag-n-drop image insertion with djangocms-text-ckeditor.\n    Set TEXT_SAVE_IMAGE_FUNCTION='cmsplugin_filer_image.integrations.ckeditor.create_image_plugin' to enable.\n    \"\"\"\n    from cmsplugin_filer_image.models import FilerImage\n    from filer.models import Image\n    image_plugin = FilerImage()\n    image_plugin.placeholder = parent_plugin.placeholder\n    image_plugin.parent = CMSPlugin.objects.get(pk=parent_plugin.id)\n    image_plugin.position = CMSPlugin.objects.filter(parent=parent_plugin).count()\n    image_plugin.language = parent_plugin.language\n    image_plugin.plugin_type = 'FilerImagePlugin'\n    image.seek(0)\n    image_model = Image.objects.create(file=SimpleUploadedFile(name=filename, content=image.read()))\n    image_plugin.image = image_model\n    image_plugin.save()\n    return image_plugin", "language": "python", "code": "def create_image_plugin(filename, image, parent_plugin, **kwargs):\n    \"\"\"\n    Used for drag-n-drop image insertion with djangocms-text-ckeditor.\n    Set TEXT_SAVE_IMAGE_FUNCTION='cmsplugin_filer_image.integrations.ckeditor.create_image_plugin' to enable.\n    \"\"\"\n    from cmsplugin_filer_image.models import FilerImage\n    from filer.models import Image\n    image_plugin = FilerImage()\n    image_plugin.placeholder = parent_plugin.placeholder\n    image_plugin.parent = CMSPlugin.objects.get(pk=parent_plugin.id)\n    image_plugin.position = CMSPlugin.objects.filter(parent=parent_plugin).count()\n    image_plugin.language = parent_plugin.language\n    image_plugin.plugin_type = 'FilerImagePlugin'\n    image.seek(0)\n    image_model = Image.objects.create(file=SimpleUploadedFile(name=filename, content=image.read()))\n    image_plugin.image = image_model\n    image_plugin.save()\n    return image_plugin", "code_tokens": ["def", "create_image_plugin", "(", "filename", ",", "image", ",", "parent_plugin", ",", "*", "*", "kwargs", ")", ":", "from", "cmsplugin_filer_image", ".", "models", "import", "FilerImage", "from", "filer", ".", "models", "import", "Image", "image_plugin", "=", "FilerImage", "(", ")", "image_plugin", ".", "placeholder", "=", "parent_plugin", ".", "placeholder", "image_plugin", ".", "parent", "=", "CMSPlugin", ".", "objects", ".", "get", "(", "pk", "=", "parent_plugin", ".", "id", ")", "image_plugin", ".", "position", "=", "CMSPlugin", ".", "objects", ".", "filter", "(", "parent", "=", "parent_plugin", ")", ".", "count", "(", ")", "image_plugin", ".", "language", "=", "parent_plugin", ".", "language", "image_plugin", ".", "plugin_type", "=", "'FilerImagePlugin'", "image", ".", "seek", "(", "0", ")", "image_model", "=", "Image", ".", "objects", ".", "create", "(", "file", "=", "SimpleUploadedFile", "(", "name", "=", "filename", ",", "content", "=", "image", ".", "read", "(", ")", ")", ")", "image_plugin", ".", "image", "=", "image_model", "image_plugin", ".", "save", "(", ")", "return", "image_plugin"], "docstring": "Used for drag-n-drop image insertion with djangocms-text-ckeditor.\n    Set TEXT_SAVE_IMAGE_FUNCTION='cmsplugin_filer_image.integrations.ckeditor.create_image_plugin' to enable.", "docstring_tokens": ["Used", "for", "drag", "-", "n", "-", "drop", "image", "insertion", "with", "djangocms", "-", "text", "-", "ckeditor", ".", "Set", "TEXT_SAVE_IMAGE_FUNCTION", "=", "cmsplugin_filer_image", ".", "integrations", ".", "ckeditor", ".", "create_image_plugin", "to", "enable", "."], "sha": "4f9b0307dd768852ead64e651b743a165b3efccb", "url": "https://github.com/divio/cmsplugin-filer/blob/4f9b0307dd768852ead64e651b743a165b3efccb/cmsplugin_filer_image/integrations/ckeditor.py#L6-L23", "partition": "train"}
{"repo": "divio/cmsplugin-filer", "path": "cmsplugin_filer_utils/migration.py", "func_name": "rename_tables", "original_string": "def rename_tables(db, table_mapping, reverse=False):\n    \"\"\"\n    renames tables from source to destination name, if the source exists and the destination does\n    not exist yet.\n    \"\"\"\n    from django.db import connection\n    if reverse:\n        table_mapping = [(dst, src) for src, dst in table_mapping]\n    table_names = connection.introspection.table_names()\n    for source, destination in table_mapping:\n        if source in table_names and destination in table_names:\n            print(u\"    WARNING: not renaming {0} to {1}, because both tables already exist.\".format(source, destination))\n        elif source in table_names and destination not in table_names:\n            print(u\"     - renaming {0} to {1}\".format(source, destination))\n            db.rename_table(source, destination)", "language": "python", "code": "def rename_tables(db, table_mapping, reverse=False):\n    \"\"\"\n    renames tables from source to destination name, if the source exists and the destination does\n    not exist yet.\n    \"\"\"\n    from django.db import connection\n    if reverse:\n        table_mapping = [(dst, src) for src, dst in table_mapping]\n    table_names = connection.introspection.table_names()\n    for source, destination in table_mapping:\n        if source in table_names and destination in table_names:\n            print(u\"    WARNING: not renaming {0} to {1}, because both tables already exist.\".format(source, destination))\n        elif source in table_names and destination not in table_names:\n            print(u\"     - renaming {0} to {1}\".format(source, destination))\n            db.rename_table(source, destination)", "code_tokens": ["def", "rename_tables", "(", "db", ",", "table_mapping", ",", "reverse", "=", "False", ")", ":", "from", "django", ".", "db", "import", "connection", "if", "reverse", ":", "table_mapping", "=", "[", "(", "dst", ",", "src", ")", "for", "src", ",", "dst", "in", "table_mapping", "]", "table_names", "=", "connection", ".", "introspection", ".", "table_names", "(", ")", "for", "source", ",", "destination", "in", "table_mapping", ":", "if", "source", "in", "table_names", "and", "destination", "in", "table_names", ":", "print", "(", "u\"    WARNING: not renaming {0} to {1}, because both tables already exist.\"", ".", "format", "(", "source", ",", "destination", ")", ")", "elif", "source", "in", "table_names", "and", "destination", "not", "in", "table_names", ":", "print", "(", "u\"     - renaming {0} to {1}\"", ".", "format", "(", "source", ",", "destination", ")", ")", "db", ".", "rename_table", "(", "source", ",", "destination", ")"], "docstring": "renames tables from source to destination name, if the source exists and the destination does\n    not exist yet.", "docstring_tokens": ["renames", "tables", "from", "source", "to", "destination", "name", "if", "the", "source", "exists", "and", "the", "destination", "does", "not", "exist", "yet", "."], "sha": "4f9b0307dd768852ead64e651b743a165b3efccb", "url": "https://github.com/divio/cmsplugin-filer/blob/4f9b0307dd768852ead64e651b743a165b3efccb/cmsplugin_filer_utils/migration.py#L4-L18", "partition": "train"}
{"repo": "sorgerlab/indra", "path": "indra/util/statement_presentation.py", "func_name": "group_and_sort_statements", "original_string": "def group_and_sort_statements(stmt_list, ev_totals=None):\n    \"\"\"Group statements by type and arguments, and sort by prevalence.\n\n    Parameters\n    ----------\n    stmt_list : list[Statement]\n        A list of INDRA statements.\n    ev_totals : dict{int: int}\n        A dictionary, keyed by statement hash (shallow) with counts of total\n        evidence as the values. Including this will allow statements to be\n        better sorted.\n\n    Returns\n    -------\n    sorted_groups : list[tuple]\n        A list of tuples containing a sort key, the statement type, and a list\n        of statements, also sorted by evidence count, for that key and type.\n        The sort key contains a count of statements with those argument, the\n        arguments (normalized strings), the count of statements with those\n        arguements and type, and then the statement type.\n    \"\"\"\n    def _count(stmt):\n        if ev_totals is None:\n            return len(stmt.evidence)\n        else:\n            return ev_totals[stmt.get_hash()]\n\n    stmt_rows = defaultdict(list)\n    stmt_counts = defaultdict(lambda: 0)\n    arg_counts = defaultdict(lambda: 0)\n    for key, s in _get_keyed_stmts(stmt_list):\n        # Update the counts, and add key if needed.\n        stmt_rows[key].append(s)\n\n        # Keep track of the total evidence counts for this statement and the\n        # arguments.\n        stmt_counts[key] += _count(s)\n\n        # Add up the counts for the arguments, pairwise for Complexes and\n        # Conversions. This allows, for example, a complex between MEK, ERK,\n        # and something else to lend weight to the interactions between MEK\n        # and ERK.\n        if key[0] == 'Conversion':\n            subj = key[1]\n            for obj in key[2] + key[3]:\n                arg_counts[(subj, obj)] += _count(s)\n        else:\n            arg_counts[key[1:]] += _count(s)\n\n    # Sort the rows by count and agent names.\n    def process_rows(stmt_rows):\n        for key, stmts in stmt_rows.items():\n            verb = key[0]\n            inps = key[1:]\n            sub_count = stmt_counts[key]\n            arg_count = arg_counts[inps]\n            if verb == 'Complex' and sub_count == arg_count and len(inps) <= 2:\n                if all([len(set(ag.name for ag in s.agent_list())) > 2\n                        for s in stmts]):\n                    continue\n            new_key = (arg_count, inps, sub_count, verb)\n            stmts = sorted(stmts,\n                           key=lambda s: _count(s) + 1/(1+len(s.agent_list())),\n                           reverse=True)\n            yield new_key, verb, stmts\n\n    sorted_groups = sorted(process_rows(stmt_rows),\n                           key=lambda tpl: tpl[0], reverse=True)\n\n    return sorted_groups", "language": "python", "code": "def group_and_sort_statements(stmt_list, ev_totals=None):\n    \"\"\"Group statements by type and arguments, and sort by prevalence.\n\n    Parameters\n    ----------\n    stmt_list : list[Statement]\n        A list of INDRA statements.\n    ev_totals : dict{int: int}\n        A dictionary, keyed by statement hash (shallow) with counts of total\n        evidence as the values. Including this will allow statements to be\n        better sorted.\n\n    Returns\n    -------\n    sorted_groups : list[tuple]\n        A list of tuples containing a sort key, the statement type, and a list\n        of statements, also sorted by evidence count, for that key and type.\n        The sort key contains a count of statements with those argument, the\n        arguments (normalized strings), the count of statements with those\n        arguements and type, and then the statement type.\n    \"\"\"\n    def _count(stmt):\n        if ev_totals is None:\n            return len(stmt.evidence)\n        else:\n            return ev_totals[stmt.get_hash()]\n\n    stmt_rows = defaultdict(list)\n    stmt_counts = defaultdict(lambda: 0)\n    arg_counts = defaultdict(lambda: 0)\n    for key, s in _get_keyed_stmts(stmt_list):\n        # Update the counts, and add key if needed.\n        stmt_rows[key].append(s)\n\n        # Keep track of the total evidence counts for this statement and the\n        # arguments.\n        stmt_counts[key] += _count(s)\n\n        # Add up the counts for the arguments, pairwise for Complexes and\n        # Conversions. This allows, for example, a complex between MEK, ERK,\n        # and something else to lend weight to the interactions between MEK\n        # and ERK.\n        if key[0] == 'Conversion':\n            subj = key[1]\n            for obj in key[2] + key[3]:\n                arg_counts[(subj, obj)] += _count(s)\n        else:\n            arg_counts[key[1:]] += _count(s)\n\n    # Sort the rows by count and agent names.\n    def process_rows(stmt_rows):\n        for key, stmts in stmt_rows.items():\n            verb = key[0]\n            inps = key[1:]\n            sub_count = stmt_counts[key]\n            arg_count = arg_counts[inps]\n            if verb == 'Complex' and sub_count == arg_count and len(inps) <= 2:\n                if all([len(set(ag.name for ag in s.agent_list())) > 2\n                        for s in stmts]):\n                    continue\n            new_key = (arg_count, inps, sub_count, verb)\n            stmts = sorted(stmts,\n                           key=lambda s: _count(s) + 1/(1+len(s.agent_list())),\n                           reverse=True)\n            yield new_key, verb, stmts\n\n    sorted_groups = sorted(process_rows(stmt_rows),\n                           key=lambda tpl: tpl[0], reverse=True)\n\n    return sorted_groups", "code_tokens": ["def", "group_and_sort_statements", "(", "stmt_list", ",", "ev_totals", "=", "None", ")", ":", "def", "_count", "(", "stmt", ")", ":", "if", "ev_totals", "is", "None", ":", "return", "len", "(", "stmt", ".", "evidence", ")", "else", ":", "return", "ev_totals", "[", "stmt", ".", "get_hash", "(", ")", "]", "stmt_rows", "=", "defaultdict", "(", "list", ")", "stmt_counts", "=", "defaultdict", "(", "lambda", ":", "0", ")", "arg_counts", "=", "defaultdict", "(", "lambda", ":", "0", ")", "for", "key", ",", "s", "in", "_get_keyed_stmts", "(", "stmt_list", ")", ":", "# Update the counts, and add key if needed.", "stmt_rows", "[", "key", "]", ".", "append", "(", "s", ")", "# Keep track of the total evidence counts for this statement and the", "# arguments.", "stmt_counts", "[", "key", "]", "+=", "_count", "(", "s", ")", "# Add up the counts for the arguments, pairwise for Complexes and", "# Conversions. This allows, for example, a complex between MEK, ERK,", "# and something else to lend weight to the interactions between MEK", "# and ERK.", "if", "key", "[", "0", "]", "==", "'Conversion'", ":", "subj", "=", "key", "[", "1", "]", "for", "obj", "in", "key", "[", "2", "]", "+", "key", "[", "3", "]", ":", "arg_counts", "[", "(", "subj", ",", "obj", ")", "]", "+=", "_count", "(", "s", ")", "else", ":", "arg_counts", "[", "key", "[", "1", ":", "]", "]", "+=", "_count", "(", "s", ")", "# Sort the rows by count and agent names.", "def", "process_rows", "(", "stmt_rows", ")", ":", "for", "key", ",", "stmts", "in", "stmt_rows", ".", "items", "(", ")", ":", "verb", "=", "key", "[", "0", "]", "inps", "=", "key", "[", "1", ":", "]", "sub_count", "=", "stmt_counts", "[", "key", "]", "arg_count", "=", "arg_counts", "[", "inps", "]", "if", "verb", "==", "'Complex'", "and", "sub_count", "==", "arg_count", "and", "len", "(", "inps", ")", "<=", "2", ":", "if", "all", "(", "[", "len", "(", "set", "(", "ag", ".", "name", "for", "ag", "in", "s", ".", "agent_list", "(", ")", ")", ")", ">", "2", "for", "s", "in", "stmts", "]", ")", ":", "continue", "new_key", "=", "(", "arg_count", ",", "inps", ",", "sub_count", ",", "verb", ")", "stmts", "=", "sorted", "(", "stmts", ",", "key", "=", "lambda", "s", ":", "_count", "(", "s", ")", "+", "1", "/", "(", "1", "+", "len", "(", "s", ".", "agent_list", "(", ")", ")", ")", ",", "reverse", "=", "True", ")", "yield", "new_key", ",", "verb", ",", "stmts", "sorted_groups", "=", "sorted", "(", "process_rows", "(", "stmt_rows", ")", ",", "key", "=", "lambda", "tpl", ":", "tpl", "[", "0", "]", ",", "reverse", "=", "True", ")", "return", "sorted_groups"], "docstring": "Group statements by type and arguments, and sort by prevalence.\n\n    Parameters\n    ----------\n    stmt_list : list[Statement]\n        A list of INDRA statements.\n    ev_totals : dict{int: int}\n        A dictionary, keyed by statement hash (shallow) with counts of total\n        evidence as the values. Including this will allow statements to be\n        better sorted.\n\n    Returns\n    -------\n    sorted_groups : list[tuple]\n        A list of tuples containing a sort key, the statement type, and a list\n        of statements, also sorted by evidence count, for that key and type.\n        The sort key contains a count of statements with those argument, the\n        arguments (normalized strings), the count of statements with those\n        arguements and type, and then the statement type.", "docstring_tokens": ["Group", "statements", "by", "type", "and", "arguments", "and", "sort", "by", "prevalence", "."], "sha": "79a70415832c5702d7a820c7c9ccc8e25010124b", "url": "https://github.com/sorgerlab/indra/blob/79a70415832c5702d7a820c7c9ccc8e25010124b/indra/util/statement_presentation.py#L40-L109", "partition": "train"}
{"repo": "sorgerlab/indra", "path": "indra/util/statement_presentation.py", "func_name": "make_stmt_from_sort_key", "original_string": "def make_stmt_from_sort_key(key, verb):\n    \"\"\"Make a Statement from the sort key.\n\n    Specifically, the sort key used by `group_and_sort_statements`.\n    \"\"\"\n    def make_agent(name):\n        if name == 'None' or name is None:\n            return None\n        return Agent(name)\n\n    StmtClass = get_statement_by_name(verb)\n    inps = list(key[1])\n    if verb == 'Complex':\n        stmt = StmtClass([make_agent(name) for name in inps])\n    elif verb == 'Conversion':\n        stmt = StmtClass(make_agent(inps[0]),\n                         [make_agent(name) for name in inps[1]],\n                         [make_agent(name) for name in inps[2]])\n    elif verb == 'ActiveForm' or verb == 'HasActivity':\n        stmt = StmtClass(make_agent(inps[0]), inps[1], inps[2])\n    else:\n        stmt = StmtClass(*[make_agent(name) for name in inps])\n    return stmt", "language": "python", "code": "def make_stmt_from_sort_key(key, verb):\n    \"\"\"Make a Statement from the sort key.\n\n    Specifically, the sort key used by `group_and_sort_statements`.\n    \"\"\"\n    def make_agent(name):\n        if name == 'None' or name is None:\n            return None\n        return Agent(name)\n\n    StmtClass = get_statement_by_name(verb)\n    inps = list(key[1])\n    if verb == 'Complex':\n        stmt = StmtClass([make_agent(name) for name in inps])\n    elif verb == 'Conversion':\n        stmt = StmtClass(make_agent(inps[0]),\n                         [make_agent(name) for name in inps[1]],\n                         [make_agent(name) for name in inps[2]])\n    elif verb == 'ActiveForm' or verb == 'HasActivity':\n        stmt = StmtClass(make_agent(inps[0]), inps[1], inps[2])\n    else:\n        stmt = StmtClass(*[make_agent(name) for name in inps])\n    return stmt", "code_tokens": ["def", "make_stmt_from_sort_key", "(", "key", ",", "verb", ")", ":", "def", "make_agent", "(", "name", ")", ":", "if", "name", "==", "'None'", "or", "name", "is", "None", ":", "return", "None", "return", "Agent", "(", "name", ")", "StmtClass", "=", "get_statement_by_name", "(", "verb", ")", "inps", "=", "list", "(", "key", "[", "1", "]", ")", "if", "verb", "==", "'Complex'", ":", "stmt", "=", "StmtClass", "(", "[", "make_agent", "(", "name", ")", "for", "name", "in", "inps", "]", ")", "elif", "verb", "==", "'Conversion'", ":", "stmt", "=", "StmtClass", "(", "make_agent", "(", "inps", "[", "0", "]", ")", ",", "[", "make_agent", "(", "name", ")", "for", "name", "in", "inps", "[", "1", "]", "]", ",", "[", "make_agent", "(", "name", ")", "for", "name", "in", "inps", "[", "2", "]", "]", ")", "elif", "verb", "==", "'ActiveForm'", "or", "verb", "==", "'HasActivity'", ":", "stmt", "=", "StmtClass", "(", "make_agent", "(", "inps", "[", "0", "]", ")", ",", "inps", "[", "1", "]", ",", "inps", "[", "2", "]", ")", "else", ":", "stmt", "=", "StmtClass", "(", "*", "[", "make_agent", "(", "name", ")", "for", "name", "in", "inps", "]", ")", "return", "stmt"], "docstring": "Make a Statement from the sort key.\n\n    Specifically, the sort key used by `group_and_sort_statements`.", "docstring_tokens": ["Make", "a", "Statement", "from", "the", "sort", "key", "."], "sha": "79a70415832c5702d7a820c7c9ccc8e25010124b", "url": "https://github.com/sorgerlab/indra/blob/79a70415832c5702d7a820c7c9ccc8e25010124b/indra/util/statement_presentation.py#L112-L134", "partition": "train"}
{"repo": "sorgerlab/indra", "path": "indra/tools/reading/submit_reading_pipeline.py", "func_name": "wait_for_complete", "original_string": "def wait_for_complete(queue_name, job_list=None, job_name_prefix=None,\n                      poll_interval=10, idle_log_timeout=None,\n                      kill_on_log_timeout=False, stash_log_method=None,\n                      tag_instances=False, result_record=None):\n    \"\"\"Return when all jobs in the given list finished.\n\n    If not job list is given, return when all jobs in queue finished.\n\n    Parameters\n    ----------\n    queue_name : str\n        The name of the queue to wait for completion.\n    job_list : Optional[list(dict)]\n        A list of jobID-s in a dict, as returned by the submit function.\n        Example: [{'jobId': 'e6b00f24-a466-4a72-b735-d205e29117b4'}, ...]\n        If not given, this function will return if all jobs completed.\n    job_name_prefix : Optional[str]\n        A prefix for the name of the jobs to wait for. This is useful if the\n        explicit job list is not available but filtering is needed.\n    poll_interval : Optional[int]\n        The time delay between API calls to check the job statuses.\n    idle_log_timeout : Optional[int] or None\n        If not None, then track the logs of the active jobs, and if new output\n        is not produced after `idle_log_timeout` seconds, a warning is printed.\n        If `kill_on_log_timeout` is set to True, the job will also be\n        terminated.\n    kill_on_log_timeout : Optional[bool]\n        If True, and if `idle_log_timeout` is set, jobs will be terminated\n        after timeout. This has no effect if `idle_log_timeout` is None.\n        Default is False.\n    stash_log_method : Optional[str]\n        Select a method to store the job logs, either 's3' or 'local'. If no\n        method is specified, the logs will not be loaded off of AWS. If 's3' is\n        specified, then `job_name_prefix` must also be given, as this will\n        indicate where on s3 to store the logs.\n    tag_instances : bool\n        Default is False. If True, apply tags to the instances. This is toady\n        typically done by each job, so in most cases this should not be needed.\n    result_record : dict\n        A dict which will be modified in place to record the results of the job.\n    \"\"\"\n    if stash_log_method == 's3' and job_name_prefix is None:\n        raise Exception('A job_name_prefix is required to post logs on s3.')\n\n    start_time = datetime.now()\n    if job_list is None:\n        job_id_list = []\n    else:\n        job_id_list = [job['jobId'] for job in job_list]\n\n    if result_record is None:\n        result_record = {}\n\n    def get_jobs_by_status(status, job_id_filter=None, job_name_prefix=None):\n        res = batch_client.list_jobs(jobQueue=queue_name,\n                                     jobStatus=status, maxResults=10000)\n        jobs = res['jobSummaryList']\n        if job_name_prefix:\n            jobs = [job for job in jobs if\n                    job['jobName'].startswith(job_name_prefix)]\n        if job_id_filter:\n            jobs = [job_def for job_def in jobs\n                    if job_def['jobId'] in job_id_filter]\n        return jobs\n\n    job_log_dict = {}\n\n    def check_logs(job_defs):\n        \"\"\"Updates teh job_log_dict.\"\"\"\n        stalled_jobs = set()\n\n        # Check the status of all the jobs we're tracking.\n        for job_def in job_defs:\n            try:\n                # Get the logs for this job.\n                log_lines = get_job_log(job_def, write_file=False)\n\n                # Get the job id.\n                jid = job_def['jobId']\n                now = datetime.now()\n                if jid not in job_log_dict.keys():\n                    # If the job is new...\n                    logger.info(\"Adding job %s to the log tracker at %s.\"\n                                % (jid, now))\n                    job_log_dict[jid] = {'log': log_lines,\n                                         'last change time': now}\n                elif len(job_log_dict[jid]['log']) == len(log_lines):\n                    # If the job log hasn't changed, announce as such, and check\n                    # to see if it has been the same for longer than stall time.\n                    check_dt = now - job_log_dict[jid]['last change time']\n                    logger.warning(('Job \\'%s\\' has not produced output for '\n                                    '%d seconds.')\n                                   % (job_def['jobName'], check_dt.seconds))\n                    if check_dt.seconds > idle_log_timeout:\n                        logger.warning(\"Job \\'%s\\' has stalled.\"\n                                       % job_def['jobName'])\n                        stalled_jobs.add(jid)\n                else:\n                    # If the job is known, and the logs have changed, update the\n                    # \"last change time\".\n                    old_log = job_log_dict[jid]['log']\n                    old_log += log_lines[len(old_log):]\n                    job_log_dict[jid]['last change time'] = now\n            except Exception as e:\n                # Sometimes due to sync et al. issues, a part of this will fail.\n                # Such things are usually transitory issues so we keep trying.\n                logger.error(\"Failed to check log for: %s\" % str(job_def))\n                logger.exception(e)\n\n        # Pass up the set of job id's for stalled jobs.\n        return stalled_jobs\n\n    # Don't start watching jobs added after this command was initialized.\n    observed_job_def_dict = {}\n    def get_dict_of_job_tuples(job_defs):\n        return {jdef['jobId']: [(k, jdef[k]) for k in ['jobName', 'jobId']]\n                for jdef in job_defs}\n\n    batch_client = boto3.client('batch')\n    if tag_instances:\n        ecs_cluster_name = get_ecs_cluster_for_queue(queue_name, batch_client)\n\n    terminate_msg = 'Job log has stalled for at least %f minutes.'\n    terminated_jobs = set()\n    stashed_id_set = set()\n    while True:\n        pre_run = []\n        for status in ('SUBMITTED', 'PENDING', 'RUNNABLE', 'STARTING'):\n            pre_run += get_jobs_by_status(status, job_id_list, job_name_prefix)\n        running = get_jobs_by_status('RUNNING', job_id_list, job_name_prefix)\n        failed = get_jobs_by_status('FAILED', job_id_list, job_name_prefix)\n        done = get_jobs_by_status('SUCCEEDED', job_id_list, job_name_prefix)\n\n        observed_job_def_dict.update(get_dict_of_job_tuples(pre_run + running))\n\n        logger.info('(%d s)=(pre: %d, running: %d, failed: %d, done: %d)' %\n                    ((datetime.now() - start_time).seconds, len(pre_run),\n                     len(running), len(failed), len(done)))\n\n        # Check the logs for new output, and possibly terminate some jobs.\n        stalled_jobs = check_logs(running)\n        if idle_log_timeout is not None:\n            if kill_on_log_timeout:\n                # Keep track of terminated jobs so we don't send a terminate\n                # message twice.\n                for jid in stalled_jobs - terminated_jobs:\n                    batch_client.terminate_job(\n                        jobId=jid,\n                        reason=terminate_msg % (idle_log_timeout/60.0)\n                        )\n                    logger.info('Terminating %s.' % jid)\n                    terminated_jobs.add(jid)\n\n        if job_id_list:\n            if (len(failed) + len(done)) == len(job_id_list):\n                ret = 0\n                break\n        else:\n            if (len(failed) + len(done) > 0) and \\\n               (len(pre_run) + len(running) == 0):\n                ret = 0\n                break\n\n        if tag_instances:\n            tag_instances_on_cluster(ecs_cluster_name)\n\n        # Stash the logs of things that have finished so far. Note that jobs\n        # terminated in this round will not be picked up until the next round.\n        if stash_log_method:\n            stash_logs(observed_job_def_dict, done, failed, queue_name,\n                       stash_log_method, job_name_prefix,\n                       start_time.strftime('%Y%m%d_%H%M%S'),\n                       ids_stashed=stashed_id_set)\n        sleep(poll_interval)\n\n    # Pick up any stragglers\n    if stash_log_method:\n        stash_logs(observed_job_def_dict, done, failed, queue_name,\n                   stash_log_method, job_name_prefix,\n                   start_time.strftime('%Y%m%d_%H%M%S'),\n                   ids_stashed=stashed_id_set)\n\n    result_record['terminated'] = terminated_jobs\n    result_record['failed'] = failed\n    result_record['succeeded'] = done\n\n    return ret", "language": "python", "code": "def wait_for_complete(queue_name, job_list=None, job_name_prefix=None,\n                      poll_interval=10, idle_log_timeout=None,\n                      kill_on_log_timeout=False, stash_log_method=None,\n                      tag_instances=False, result_record=None):\n    \"\"\"Return when all jobs in the given list finished.\n\n    If not job list is given, return when all jobs in queue finished.\n\n    Parameters\n    ----------\n    queue_name : str\n        The name of the queue to wait for completion.\n    job_list : Optional[list(dict)]\n        A list of jobID-s in a dict, as returned by the submit function.\n        Example: [{'jobId': 'e6b00f24-a466-4a72-b735-d205e29117b4'}, ...]\n        If not given, this function will return if all jobs completed.\n    job_name_prefix : Optional[str]\n        A prefix for the name of the jobs to wait for. This is useful if the\n        explicit job list is not available but filtering is needed.\n    poll_interval : Optional[int]\n        The time delay between API calls to check the job statuses.\n    idle_log_timeout : Optional[int] or None\n        If not None, then track the logs of the active jobs, and if new output\n        is not produced after `idle_log_timeout` seconds, a warning is printed.\n        If `kill_on_log_timeout` is set to True, the job will also be\n        terminated.\n    kill_on_log_timeout : Optional[bool]\n        If True, and if `idle_log_timeout` is set, jobs will be terminated\n        after timeout. This has no effect if `idle_log_timeout` is None.\n        Default is False.\n    stash_log_method : Optional[str]\n        Select a method to store the job logs, either 's3' or 'local'. If no\n        method is specified, the logs will not be loaded off of AWS. If 's3' is\n        specified, then `job_name_prefix` must also be given, as this will\n        indicate where on s3 to store the logs.\n    tag_instances : bool\n        Default is False. If True, apply tags to the instances. This is toady\n        typically done by each job, so in most cases this should not be needed.\n    result_record : dict\n        A dict which will be modified in place to record the results of the job.\n    \"\"\"\n    if stash_log_method == 's3' and job_name_prefix is None:\n        raise Exception('A job_name_prefix is required to post logs on s3.')\n\n    start_time = datetime.now()\n    if job_list is None:\n        job_id_list = []\n    else:\n        job_id_list = [job['jobId'] for job in job_list]\n\n    if result_record is None:\n        result_record = {}\n\n    def get_jobs_by_status(status, job_id_filter=None, job_name_prefix=None):\n        res = batch_client.list_jobs(jobQueue=queue_name,\n                                     jobStatus=status, maxResults=10000)\n        jobs = res['jobSummaryList']\n        if job_name_prefix:\n            jobs = [job for job in jobs if\n                    job['jobName'].startswith(job_name_prefix)]\n        if job_id_filter:\n            jobs = [job_def for job_def in jobs\n                    if job_def['jobId'] in job_id_filter]\n        return jobs\n\n    job_log_dict = {}\n\n    def check_logs(job_defs):\n        \"\"\"Updates teh job_log_dict.\"\"\"\n        stalled_jobs = set()\n\n        # Check the status of all the jobs we're tracking.\n        for job_def in job_defs:\n            try:\n                # Get the logs for this job.\n                log_lines = get_job_log(job_def, write_file=False)\n\n                # Get the job id.\n                jid = job_def['jobId']\n                now = datetime.now()\n                if jid not in job_log_dict.keys():\n                    # If the job is new...\n                    logger.info(\"Adding job %s to the log tracker at %s.\"\n                                % (jid, now))\n                    job_log_dict[jid] = {'log': log_lines,\n                                         'last change time': now}\n                elif len(job_log_dict[jid]['log']) == len(log_lines):\n                    # If the job log hasn't changed, announce as such, and check\n                    # to see if it has been the same for longer than stall time.\n                    check_dt = now - job_log_dict[jid]['last change time']\n                    logger.warning(('Job \\'%s\\' has not produced output for '\n                                    '%d seconds.')\n                                   % (job_def['jobName'], check_dt.seconds))\n                    if check_dt.seconds > idle_log_timeout:\n                        logger.warning(\"Job \\'%s\\' has stalled.\"\n                                       % job_def['jobName'])\n                        stalled_jobs.add(jid)\n                else:\n                    # If the job is known, and the logs have changed, update the\n                    # \"last change time\".\n                    old_log = job_log_dict[jid]['log']\n                    old_log += log_lines[len(old_log):]\n                    job_log_dict[jid]['last change time'] = now\n            except Exception as e:\n                # Sometimes due to sync et al. issues, a part of this will fail.\n                # Such things are usually transitory issues so we keep trying.\n                logger.error(\"Failed to check log for: %s\" % str(job_def))\n                logger.exception(e)\n\n        # Pass up the set of job id's for stalled jobs.\n        return stalled_jobs\n\n    # Don't start watching jobs added after this command was initialized.\n    observed_job_def_dict = {}\n    def get_dict_of_job_tuples(job_defs):\n        return {jdef['jobId']: [(k, jdef[k]) for k in ['jobName', 'jobId']]\n                for jdef in job_defs}\n\n    batch_client = boto3.client('batch')\n    if tag_instances:\n        ecs_cluster_name = get_ecs_cluster_for_queue(queue_name, batch_client)\n\n    terminate_msg = 'Job log has stalled for at least %f minutes.'\n    terminated_jobs = set()\n    stashed_id_set = set()\n    while True:\n        pre_run = []\n        for status in ('SUBMITTED', 'PENDING', 'RUNNABLE', 'STARTING'):\n            pre_run += get_jobs_by_status(status, job_id_list, job_name_prefix)\n        running = get_jobs_by_status('RUNNING', job_id_list, job_name_prefix)\n        failed = get_jobs_by_status('FAILED', job_id_list, job_name_prefix)\n        done = get_jobs_by_status('SUCCEEDED', job_id_list, job_name_prefix)\n\n        observed_job_def_dict.update(get_dict_of_job_tuples(pre_run + running))\n\n        logger.info('(%d s)=(pre: %d, running: %d, failed: %d, done: %d)' %\n                    ((datetime.now() - start_time).seconds, len(pre_run),\n                     len(running), len(failed), len(done)))\n\n        # Check the logs for new output, and possibly terminate some jobs.\n        stalled_jobs = check_logs(running)\n        if idle_log_timeout is not None:\n            if kill_on_log_timeout:\n                # Keep track of terminated jobs so we don't send a terminate\n                # message twice.\n                for jid in stalled_jobs - terminated_jobs:\n                    batch_client.terminate_job(\n                        jobId=jid,\n                        reason=terminate_msg % (idle_log_timeout/60.0)\n                        )\n                    logger.info('Terminating %s.' % jid)\n                    terminated_jobs.add(jid)\n\n        if job_id_list:\n            if (len(failed) + len(done)) == len(job_id_list):\n                ret = 0\n                break\n        else:\n            if (len(failed) + len(done) > 0) and \\\n               (len(pre_run) + len(running) == 0):\n                ret = 0\n                break\n\n        if tag_instances:\n            tag_instances_on_cluster(ecs_cluster_name)\n\n        # Stash the logs of things that have finished so far. Note that jobs\n        # terminated in this round will not be picked up until the next round.\n        if stash_log_method:\n            stash_logs(observed_job_def_dict, done, failed, queue_name,\n                       stash_log_method, job_name_prefix,\n                       start_time.strftime('%Y%m%d_%H%M%S'),\n                       ids_stashed=stashed_id_set)\n        sleep(poll_interval)\n\n    # Pick up any stragglers\n    if stash_log_method:\n        stash_logs(observed_job_def_dict, done, failed, queue_name,\n                   stash_log_method, job_name_prefix,\n                   start_time.strftime('%Y%m%d_%H%M%S'),\n                   ids_stashed=stashed_id_set)\n\n    result_record['terminated'] = terminated_jobs\n    result_record['failed'] = failed\n    result_record['succeeded'] = done\n\n    return ret", "code_tokens": ["def", "wait_for_complete", "(", "queue_name", ",", "job_list", "=", "None", ",", "job_name_prefix", "=", "None", ",", "poll_interval", "=", "10", ",", "idle_log_timeout", "=", "None", ",", "kill_on_log_timeout", "=", "False", ",", "stash_log_method", "=", "None", ",", "tag_instances", "=", "False", ",", "result_record", "=", "None", ")", ":", "if", "stash_log_method", "==", "'s3'", "and", "job_name_prefix", "is", "None", ":", "raise", "Exception", "(", "'A job_name_prefix is required to post logs on s3.'", ")", "start_time", "=", "datetime", ".", "now", "(", ")", "if", "job_list", "is", "None", ":", "job_id_list", "=", "[", "]", "else", ":", "job_id_list", "=", "[", "job", "[", "'jobId'", "]", "for", "job", "in", "job_list", "]", "if", "result_record", "is", "None", ":", "result_record", "=", "{", "}", "def", "get_jobs_by_status", "(", "status", ",", "job_id_filter", "=", "None", ",", "job_name_prefix", "=", "None", ")", ":", "res", "=", "batch_client", ".", "list_jobs", "(", "jobQueue", "=", "queue_name", ",", "jobStatus", "=", "status", ",", "maxResults", "=", "10000", ")", "jobs", "=", "res", "[", "'jobSummaryList'", "]", "if", "job_name_prefix", ":", "jobs", "=", "[", "job", "for", "job", "in", "jobs", "if", "job", "[", "'jobName'", "]", ".", "startswith", "(", "job_name_prefix", ")", "]", "if", "job_id_filter", ":", "jobs", "=", "[", "job_def", "for", "job_def", "in", "jobs", "if", "job_def", "[", "'jobId'", "]", "in", "job_id_filter", "]", "return", "jobs", "job_log_dict", "=", "{", "}", "def", "check_logs", "(", "job_defs", ")", ":", "\"\"\"Updates teh job_log_dict.\"\"\"", "stalled_jobs", "=", "set", "(", ")", "# Check the status of all the jobs we're tracking.", "for", "job_def", "in", "job_defs", ":", "try", ":", "# Get the logs for this job.", "log_lines", "=", "get_job_log", "(", "job_def", ",", "write_file", "=", "False", ")", "# Get the job id.", "jid", "=", "job_def", "[", "'jobId'", "]", "now", "=", "datetime", ".", "now", "(", ")", "if", "jid", "not", "in", "job_log_dict", ".", "keys", "(", ")", ":", "# If the job is new...", "logger", ".", "info", "(", "\"Adding job %s to the log tracker at %s.\"", "%", "(", "jid", ",", "now", ")", ")", "job_log_dict", "[", "jid", "]", "=", "{", "'log'", ":", "log_lines", ",", "'last change time'", ":", "now", "}", "elif", "len", "(", "job_log_dict", "[", "jid", "]", "[", "'log'", "]", ")", "==", "len", "(", "log_lines", ")", ":", "# If the job log hasn't changed, announce as such, and check", "# to see if it has been the same for longer than stall time.", "check_dt", "=", "now", "-", "job_log_dict", "[", "jid", "]", "[", "'last change time'", "]", "logger", ".", "warning", "(", "(", "'Job \\'%s\\' has not produced output for '", "'%d seconds.'", ")", "%", "(", "job_def", "[", "'jobName'", "]", ",", "check_dt", ".", "seconds", ")", ")", "if", "check_dt", ".", "seconds", ">", "idle_log_timeout", ":", "logger", ".", "warning", "(", "\"Job \\'%s\\' has stalled.\"", "%", "job_def", "[", "'jobName'", "]", ")", "stalled_jobs", ".", "add", "(", "jid", ")", "else", ":", "# If the job is known, and the logs have changed, update the", "# \"last change time\".", "old_log", "=", "job_log_dict", "[", "jid", "]", "[", "'log'", "]", "old_log", "+=", "log_lines", "[", "len", "(", "old_log", ")", ":", "]", "job_log_dict", "[", "jid", "]", "[", "'last change time'", "]", "=", "now", "except", "Exception", "as", "e", ":", "# Sometimes due to sync et al. issues, a part of this will fail.", "# Such things are usually transitory issues so we keep trying.", "logger", ".", "error", "(", "\"Failed to check log for: %s\"", "%", "str", "(", "job_def", ")", ")", "logger", ".", "exception", "(", "e", ")", "# Pass up the set of job id's for stalled jobs.", "return", "stalled_jobs", "# Don't start watching jobs added after this command was initialized.", "observed_job_def_dict", "=", "{", "}", "def", "get_dict_of_job_tuples", "(", "job_defs", ")", ":", "return", "{", "jdef", "[", "'jobId'", "]", ":", "[", "(", "k", ",", "jdef", "[", "k", "]", ")", "for", "k", "in", "[", "'jobName'", ",", "'jobId'", "]", "]", "for", "jdef", "in", "job_defs", "}", "batch_client", "=", "boto3", ".", "client", "(", "'batch'", ")", "if", "tag_instances", ":", "ecs_cluster_name", "=", "get_ecs_cluster_for_queue", "(", "queue_name", ",", "batch_client", ")", "terminate_msg", "=", "'Job log has stalled for at least %f minutes.'", "terminated_jobs", "=", "set", "(", ")", "stashed_id_set", "=", "set", "(", ")", "while", "True", ":", "pre_run", "=", "[", "]", "for", "status", "in", "(", "'SUBMITTED'", ",", "'PENDING'", ",", "'RUNNABLE'", ",", "'STARTING'", ")", ":", "pre_run", "+=", "get_jobs_by_status", "(", "status", ",", "job_id_list", ",", "job_name_prefix", ")", "running", "=", "get_jobs_by_status", "(", "'RUNNING'", ",", "job_id_list", ",", "job_name_prefix", ")", "failed", "=", "get_jobs_by_status", "(", "'FAILED'", ",", "job_id_list", ",", "job_name_prefix", ")", "done", "=", "get_jobs_by_status", "(", "'SUCCEEDED'", ",", "job_id_list", ",", "job_name_prefix", ")", "observed_job_def_dict", ".", "update", "(", "get_dict_of_job_tuples", "(", "pre_run", "+", "running", ")", ")", "logger", ".", "info", "(", "'(%d s)=(pre: %d, running: %d, failed: %d, done: %d)'", "%", "(", "(", "datetime", ".", "now", "(", ")", "-", "start_time", ")", ".", "seconds", ",", "len", "(", "pre_run", ")", ",", "len", "(", "running", ")", ",", "len", "(", "failed", ")", ",", "len", "(", "done", ")", ")", ")", "# Check the logs for new output, and possibly terminate some jobs.", "stalled_jobs", "=", "check_logs", "(", "running", ")", "if", "idle_log_timeout", "is", "not", "None", ":", "if", "kill_on_log_timeout", ":", "# Keep track of terminated jobs so we don't send a terminate", "# message twice.", "for", "jid", "in", "stalled_jobs", "-", "terminated_jobs", ":", "batch_client", ".", "terminate_job", "(", "jobId", "=", "jid", ",", "reason", "=", "terminate_msg", "%", "(", "idle_log_timeout", "/", "60.0", ")", ")", "logger", ".", "info", "(", "'Terminating %s.'", "%", "jid", ")", "terminated_jobs", ".", "add", "(", "jid", ")", "if", "job_id_list", ":", "if", "(", "len", "(", "failed", ")", "+", "len", "(", "done", ")", ")", "==", "len", "(", "job_id_list", ")", ":", "ret", "=", "0", "break", "else", ":", "if", "(", "len", "(", "failed", ")", "+", "len", "(", "done", ")", ">", "0", ")", "and", "(", "len", "(", "pre_run", ")", "+", "len", "(", "running", ")", "==", "0", ")", ":", "ret", "=", "0", "break", "if", "tag_instances", ":", "tag_instances_on_cluster", "(", "ecs_cluster_name", ")", "# Stash the logs of things that have finished so far. Note that jobs", "# terminated in this round will not be picked up until the next round.", "if", "stash_log_method", ":", "stash_logs", "(", "observed_job_def_dict", ",", "done", ",", "failed", ",", "queue_name", ",", "stash_log_method", ",", "job_name_prefix", ",", "start_time", ".", "strftime", "(", "'%Y%m%d_%H%M%S'", ")", ",", "ids_stashed", "=", "stashed_id_set", ")", "sleep", "(", "poll_interval", ")", "# Pick up any stragglers", "if", "stash_log_method", ":", "stash_logs", "(", "observed_job_def_dict", ",", "done", ",", "failed", ",", "queue_name", ",", "stash_log_method", ",", "job_name_prefix", ",", "start_time", ".", "strftime", "(", "'%Y%m%d_%H%M%S'", ")", ",", "ids_stashed", "=", "stashed_id_set", ")", "result_record", "[", "'terminated'", "]", "=", "terminated_jobs", "result_record", "[", "'failed'", "]", "=", "failed", "result_record", "[", "'succeeded'", "]", "=", "done", "return", "ret"], "docstring": "Return when all jobs in the given list finished.\n\n    If not job list is given, return when all jobs in queue finished.\n\n    Parameters\n    ----------\n    queue_name : str\n        The name of the queue to wait for completion.\n    job_list : Optional[list(dict)]\n        A list of jobID-s in a dict, as returned by the submit function.\n        Example: [{'jobId': 'e6b00f24-a466-4a72-b735-d205e29117b4'}, ...]\n        If not given, this function will return if all jobs completed.\n    job_name_prefix : Optional[str]\n        A prefix for the name of the jobs to wait for. This is useful if the\n        explicit job list is not available but filtering is needed.\n    poll_interval : Optional[int]\n        The time delay between API calls to check the job statuses.\n    idle_log_timeout : Optional[int] or None\n        If not None, then track the logs of the active jobs, and if new output\n        is not produced after `idle_log_timeout` seconds, a warning is printed.\n        If `kill_on_log_timeout` is set to True, the job will also be\n        terminated.\n    kill_on_log_timeout : Optional[bool]\n        If True, and if `idle_log_timeout` is set, jobs will be terminated\n        after timeout. This has no effect if `idle_log_timeout` is None.\n        Default is False.\n    stash_log_method : Optional[str]\n        Select a method to store the job logs, either 's3' or 'local'. If no\n        method is specified, the logs will not be loaded off of AWS. If 's3' is\n        specified, then `job_name_prefix` must also be given, as this will\n        indicate where on s3 to store the logs.\n    tag_instances : bool\n        Default is False. If True, apply tags to the instances. This is toady\n        typically done by each job, so in most cases this should not be needed.\n    result_record : dict\n        A dict which will be modified in place to record the results of the job.", "docstring_tokens": ["Return", "when", "all", "jobs", "in", "the", "given", "list", "finished", "."], "sha": "79a70415832c5702d7a820c7c9ccc8e25010124b", "url": "https://github.com/sorgerlab/indra/blob/79a70415832c5702d7a820c7c9ccc8e25010124b/indra/tools/reading/submit_reading_pipeline.py#L22-L208", "partition": "train"}
{"repo": "sorgerlab/indra", "path": "indra/tools/reading/submit_reading_pipeline.py", "func_name": "get_ecs_cluster_for_queue", "original_string": "def get_ecs_cluster_for_queue(queue_name, batch_client=None):\n    \"\"\"Get the name of the ecs cluster using the batch client.\"\"\"\n    if batch_client is None:\n        batch_client = boto3.client('batch')\n\n    queue_resp = batch_client.describe_job_queues(jobQueues=[queue_name])\n    if len(queue_resp['jobQueues']) == 1:\n        queue = queue_resp['jobQueues'][0]\n    else:\n        raise BatchReadingError('Error finding queue with name %s.'\n                                % queue_name)\n\n    compute_env_names = queue['computeEnvironmentOrder']\n    if len(compute_env_names) == 1:\n        compute_env_name = compute_env_names[0]['computeEnvironment']\n    else:\n        raise BatchReadingError('Error finding the compute environment name '\n                                'for %s.' % queue_name)\n\n    compute_envs = batch_client.describe_compute_environments(\n        computeEnvironments=[compute_env_name]\n        )['computeEnvironments']\n    if len(compute_envs) == 1:\n        compute_env = compute_envs[0]\n    else:\n        raise BatchReadingError(\"Error getting compute environment %s for %s. \"\n                                \"Got %d environments instead of 1.\"\n                                % (compute_env_name, queue_name,\n                                   len(compute_envs)))\n\n    ecs_cluster_name = os.path.basename(compute_env['ecsClusterArn'])\n    return ecs_cluster_name", "language": "python", "code": "def get_ecs_cluster_for_queue(queue_name, batch_client=None):\n    \"\"\"Get the name of the ecs cluster using the batch client.\"\"\"\n    if batch_client is None:\n        batch_client = boto3.client('batch')\n\n    queue_resp = batch_client.describe_job_queues(jobQueues=[queue_name])\n    if len(queue_resp['jobQueues']) == 1:\n        queue = queue_resp['jobQueues'][0]\n    else:\n        raise BatchReadingError('Error finding queue with name %s.'\n                                % queue_name)\n\n    compute_env_names = queue['computeEnvironmentOrder']\n    if len(compute_env_names) == 1:\n        compute_env_name = compute_env_names[0]['computeEnvironment']\n    else:\n        raise BatchReadingError('Error finding the compute environment name '\n                                'for %s.' % queue_name)\n\n    compute_envs = batch_client.describe_compute_environments(\n        computeEnvironments=[compute_env_name]\n        )['computeEnvironments']\n    if len(compute_envs) == 1:\n        compute_env = compute_envs[0]\n    else:\n        raise BatchReadingError(\"Error getting compute environment %s for %s. \"\n                                \"Got %d environments instead of 1.\"\n                                % (compute_env_name, queue_name,\n                                   len(compute_envs)))\n\n    ecs_cluster_name = os.path.basename(compute_env['ecsClusterArn'])\n    return ecs_cluster_name", "code_tokens": ["def", "get_ecs_cluster_for_queue", "(", "queue_name", ",", "batch_client", "=", "None", ")", ":", "if", "batch_client", "is", "None", ":", "batch_client", "=", "boto3", ".", "client", "(", "'batch'", ")", "queue_resp", "=", "batch_client", ".", "describe_job_queues", "(", "jobQueues", "=", "[", "queue_name", "]", ")", "if", "len", "(", "queue_resp", "[", "'jobQueues'", "]", ")", "==", "1", ":", "queue", "=", "queue_resp", "[", "'jobQueues'", "]", "[", "0", "]", "else", ":", "raise", "BatchReadingError", "(", "'Error finding queue with name %s.'", "%", "queue_name", ")", "compute_env_names", "=", "queue", "[", "'computeEnvironmentOrder'", "]", "if", "len", "(", "compute_env_names", ")", "==", "1", ":", "compute_env_name", "=", "compute_env_names", "[", "0", "]", "[", "'computeEnvironment'", "]", "else", ":", "raise", "BatchReadingError", "(", "'Error finding the compute environment name '", "'for %s.'", "%", "queue_name", ")", "compute_envs", "=", "batch_client", ".", "describe_compute_environments", "(", "computeEnvironments", "=", "[", "compute_env_name", "]", ")", "[", "'computeEnvironments'", "]", "if", "len", "(", "compute_envs", ")", "==", "1", ":", "compute_env", "=", "compute_envs", "[", "0", "]", "else", ":", "raise", "BatchReadingError", "(", "\"Error getting compute environment %s for %s. \"", "\"Got %d environments instead of 1.\"", "%", "(", "compute_env_name", ",", "queue_name", ",", "len", "(", "compute_envs", ")", ")", ")", "ecs_cluster_name", "=", "os", ".", "path", ".", "basename", "(", "compute_env", "[", "'ecsClusterArn'", "]", ")", "return", "ecs_cluster_name"], "docstring": "Get the name of the ecs cluster using the batch client.", "docstring_tokens": ["Get", "the", "name", "of", "the", "ecs", "cluster", "using", "the", "batch", "client", "."], "sha": "79a70415832c5702d7a820c7c9ccc8e25010124b", "url": "https://github.com/sorgerlab/indra/blob/79a70415832c5702d7a820c7c9ccc8e25010124b/indra/tools/reading/submit_reading_pipeline.py#L275-L306", "partition": "train"}
{"repo": "sorgerlab/indra", "path": "indra/literature/elsevier_client.py", "func_name": "get_dois", "original_string": "def get_dois(query_str, count=100):\n    \"\"\"Search ScienceDirect through the API for articles.\n\n    See http://api.elsevier.com/content/search/fields/scidir for constructing a\n    query string to pass here.  Example: 'abstract(BRAF) AND all(\"colorectal\n    cancer\")'\n    \"\"\"\n    url = '%s/%s' % (elsevier_search_url, query_str)\n    params = {'query': query_str,\n              'count': count,\n              'httpAccept': 'application/xml',\n              'sort': '-coverdate',\n              'field': 'doi'}\n    res = requests.get(url, params)\n    if not res.status_code == 200:\n        return None\n    tree = ET.XML(res.content, parser=UTB())\n    doi_tags = tree.findall('atom:entry/prism:doi', elsevier_ns)\n    dois = [dt.text for dt in doi_tags]\n    return dois", "language": "python", "code": "def get_dois(query_str, count=100):\n    \"\"\"Search ScienceDirect through the API for articles.\n\n    See http://api.elsevier.com/content/search/fields/scidir for constructing a\n    query string to pass here.  Example: 'abstract(BRAF) AND all(\"colorectal\n    cancer\")'\n    \"\"\"\n    url = '%s/%s' % (elsevier_search_url, query_str)\n    params = {'query': query_str,\n              'count': count,\n              'httpAccept': 'application/xml',\n              'sort': '-coverdate',\n              'field': 'doi'}\n    res = requests.get(url, params)\n    if not res.status_code == 200:\n        return None\n    tree = ET.XML(res.content, parser=UTB())\n    doi_tags = tree.findall('atom:entry/prism:doi', elsevier_ns)\n    dois = [dt.text for dt in doi_tags]\n    return dois", "code_tokens": ["def", "get_dois", "(", "query_str", ",", "count", "=", "100", ")", ":", "url", "=", "'%s/%s'", "%", "(", "elsevier_search_url", ",", "query_str", ")", "params", "=", "{", "'query'", ":", "query_str", ",", "'count'", ":", "count", ",", "'httpAccept'", ":", "'application/xml'", ",", "'sort'", ":", "'-coverdate'", ",", "'field'", ":", "'doi'", "}", "res", "=", "requests", ".", "get", "(", "url", ",", "params", ")", "if", "not", "res", ".", "status_code", "==", "200", ":", "return", "None", "tree", "=", "ET", ".", "XML", "(", "res", ".", "content", ",", "parser", "=", "UTB", "(", ")", ")", "doi_tags", "=", "tree", ".", "findall", "(", "'atom:entry/prism:doi'", ",", "elsevier_ns", ")", "dois", "=", "[", "dt", ".", "text", "for", "dt", "in", "doi_tags", "]", "return", "dois"], "docstring": "Search ScienceDirect through the API for articles.\n\n    See http://api.elsevier.com/content/search/fields/scidir for constructing a\n    query string to pass here.  Example: 'abstract(BRAF) AND all(\"colorectal\n    cancer\")'", "docstring_tokens": ["Search", "ScienceDirect", "through", "the", "API", "for", "articles", "."], "sha": "79a70415832c5702d7a820c7c9ccc8e25010124b", "url": "https://github.com/sorgerlab/indra/blob/79a70415832c5702d7a820c7c9ccc8e25010124b/indra/literature/elsevier_client.py#L264-L283", "partition": "train"}
{"repo": "sorgerlab/indra", "path": "indra/literature/elsevier_client.py", "func_name": "get_piis", "original_string": "def get_piis(query_str):\n    \"\"\"Search ScienceDirect through the API for articles and return PIIs.\n\n    Note that ScienceDirect has a limitation in which a maximum of 6,000\n    PIIs can be retrieved for a given search and therefore this call is\n    internally broken up into multiple queries by a range of years and the\n    results are combined.\n\n    Parameters\n    ----------\n    query_str : str\n        The query string to search with\n\n    Returns\n    -------\n    piis : list[str]\n        The list of PIIs identifying the papers returned by the search\n    \"\"\"\n    dates = range(1960, datetime.datetime.now().year)\n    all_piis = flatten([get_piis_for_date(query_str, date) for date in dates])\n    return all_piis", "language": "python", "code": "def get_piis(query_str):\n    \"\"\"Search ScienceDirect through the API for articles and return PIIs.\n\n    Note that ScienceDirect has a limitation in which a maximum of 6,000\n    PIIs can be retrieved for a given search and therefore this call is\n    internally broken up into multiple queries by a range of years and the\n    results are combined.\n\n    Parameters\n    ----------\n    query_str : str\n        The query string to search with\n\n    Returns\n    -------\n    piis : list[str]\n        The list of PIIs identifying the papers returned by the search\n    \"\"\"\n    dates = range(1960, datetime.datetime.now().year)\n    all_piis = flatten([get_piis_for_date(query_str, date) for date in dates])\n    return all_piis", "code_tokens": ["def", "get_piis", "(", "query_str", ")", ":", "dates", "=", "range", "(", "1960", ",", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "year", ")", "all_piis", "=", "flatten", "(", "[", "get_piis_for_date", "(", "query_str", ",", "date", ")", "for", "date", "in", "dates", "]", ")", "return", "all_piis"], "docstring": "Search ScienceDirect through the API for articles and return PIIs.\n\n    Note that ScienceDirect has a limitation in which a maximum of 6,000\n    PIIs can be retrieved for a given search and therefore this call is\n    internally broken up into multiple queries by a range of years and the\n    results are combined.\n\n    Parameters\n    ----------\n    query_str : str\n        The query string to search with\n\n    Returns\n    -------\n    piis : list[str]\n        The list of PIIs identifying the papers returned by the search", "docstring_tokens": ["Search", "ScienceDirect", "through", "the", "API", "for", "articles", "and", "return", "PIIs", "."], "sha": "79a70415832c5702d7a820c7c9ccc8e25010124b", "url": "https://github.com/sorgerlab/indra/blob/79a70415832c5702d7a820c7c9ccc8e25010124b/indra/literature/elsevier_client.py#L286-L306", "partition": "train"}
{"repo": "sorgerlab/indra", "path": "indra/literature/elsevier_client.py", "func_name": "get_piis_for_date", "original_string": "def get_piis_for_date(query_str, date):\n    \"\"\"Search ScienceDirect with a query string constrained to a given year.\n\n    Parameters\n    ----------\n    query_str : str\n        The query string to search with\n    date : str\n        The year to constrain the search to\n\n    Returns\n    -------\n    piis : list[str]\n        The list of PIIs identifying the papers returned by the search\n    \"\"\"\n    count = 200\n    params = {'query': query_str,\n              'count': count,\n              'start': 0,\n              'sort': '-coverdate',\n              'date': date,\n              'field': 'pii'}\n    all_piis = []\n    while True:\n        res = requests.get(elsevier_search_url, params, headers=ELSEVIER_KEYS)\n        if not res.status_code == 200:\n            logger.info('Got status code: %d' % res.status_code)\n            break\n        res_json = res.json()\n        entries = res_json['search-results']['entry']\n        logger.info(res_json['search-results']['opensearch:totalResults'])\n        if entries == [{'@_fa': 'true', 'error': 'Result set was empty'}]:\n            logger.info('Search result was empty')\n            return []\n        piis = [entry['pii'] for entry in entries]\n        all_piis += piis\n        # Get next batch\n        links = res_json['search-results'].get('link', [])\n        cont = False\n        for link in links:\n            if link.get('@ref') == 'next':\n                logger.info('Found link to next batch of results.')\n                params['start'] += count\n                cont = True\n                break\n        if not cont:\n            break\n    return all_piis", "language": "python", "code": "def get_piis_for_date(query_str, date):\n    \"\"\"Search ScienceDirect with a query string constrained to a given year.\n\n    Parameters\n    ----------\n    query_str : str\n        The query string to search with\n    date : str\n        The year to constrain the search to\n\n    Returns\n    -------\n    piis : list[str]\n        The list of PIIs identifying the papers returned by the search\n    \"\"\"\n    count = 200\n    params = {'query': query_str,\n              'count': count,\n              'start': 0,\n              'sort': '-coverdate',\n              'date': date,\n              'field': 'pii'}\n    all_piis = []\n    while True:\n        res = requests.get(elsevier_search_url, params, headers=ELSEVIER_KEYS)\n        if not res.status_code == 200:\n            logger.info('Got status code: %d' % res.status_code)\n            break\n        res_json = res.json()\n        entries = res_json['search-results']['entry']\n        logger.info(res_json['search-results']['opensearch:totalResults'])\n        if entries == [{'@_fa': 'true', 'error': 'Result set was empty'}]:\n            logger.info('Search result was empty')\n            return []\n        piis = [entry['pii'] for entry in entries]\n        all_piis += piis\n        # Get next batch\n        links = res_json['search-results'].get('link', [])\n        cont = False\n        for link in links:\n            if link.get('@ref') == 'next':\n                logger.info('Found link to next batch of results.')\n                params['start'] += count\n                cont = True\n                break\n        if not cont:\n            break\n    return all_piis", "code_tokens": ["def", "get_piis_for_date", "(", "query_str", ",", "date", ")", ":", "count", "=", "200", "params", "=", "{", "'query'", ":", "query_str", ",", "'count'", ":", "count", ",", "'start'", ":", "0", ",", "'sort'", ":", "'-coverdate'", ",", "'date'", ":", "date", ",", "'field'", ":", "'pii'", "}", "all_piis", "=", "[", "]", "while", "True", ":", "res", "=", "requests", ".", "get", "(", "elsevier_search_url", ",", "params", ",", "headers", "=", "ELSEVIER_KEYS", ")", "if", "not", "res", ".", "status_code", "==", "200", ":", "logger", ".", "info", "(", "'Got status code: %d'", "%", "res", ".", "status_code", ")", "break", "res_json", "=", "res", ".", "json", "(", ")", "entries", "=", "res_json", "[", "'search-results'", "]", "[", "'entry'", "]", "logger", ".", "info", "(", "res_json", "[", "'search-results'", "]", "[", "'opensearch:totalResults'", "]", ")", "if", "entries", "==", "[", "{", "'@_fa'", ":", "'true'", ",", "'error'", ":", "'Result set was empty'", "}", "]", ":", "logger", ".", "info", "(", "'Search result was empty'", ")", "return", "[", "]", "piis", "=", "[", "entry", "[", "'pii'", "]", "for", "entry", "in", "entries", "]", "all_piis", "+=", "piis", "# Get next batch", "links", "=", "res_json", "[", "'search-results'", "]", ".", "get", "(", "'link'", ",", "[", "]", ")", "cont", "=", "False", "for", "link", "in", "links", ":", "if", "link", ".", "get", "(", "'@ref'", ")", "==", "'next'", ":", "logger", ".", "info", "(", "'Found link to next batch of results.'", ")", "params", "[", "'start'", "]", "+=", "count", "cont", "=", "True", "break", "if", "not", "cont", ":", "break", "return", "all_piis"], "docstring": "Search ScienceDirect with a query string constrained to a given year.\n\n    Parameters\n    ----------\n    query_str : str\n        The query string to search with\n    date : str\n        The year to constrain the search to\n\n    Returns\n    -------\n    piis : list[str]\n        The list of PIIs identifying the papers returned by the search", "docstring_tokens": ["Search", "ScienceDirect", "with", "a", "query", "string", "constrained", "to", "a", "given", "year", "."], "sha": "79a70415832c5702d7a820c7c9ccc8e25010124b", "url": "https://github.com/sorgerlab/indra/blob/79a70415832c5702d7a820c7c9ccc8e25010124b/indra/literature/elsevier_client.py#L311-L358", "partition": "train"}
{"repo": "sorgerlab/indra", "path": "indra/literature/elsevier_client.py", "func_name": "download_from_search", "original_string": "def download_from_search(query_str, folder, do_extract_text=True,\n                         max_results=None):\n    \"\"\"Save raw text files based on a search for papers on ScienceDirect.\n\n    This performs a search to get PIIs, downloads the XML corresponding to\n    the PII, extracts the raw text and then saves the text into a file\n    in the designated folder.\n\n    Parameters\n    ----------\n    query_str : str\n        The query string to search with\n    folder : str\n        The local path to an existing folder in which the text files\n        will be dumped\n    do_extract_text : bool\n        Choose whether to extract text from the xml, or simply save the raw xml\n        files. Default is True, so text is extracted.\n    max_results : int or None\n        Default is None. If specified, limit the number of results to the given\n        maximum.\n    \"\"\"\n    piis = get_piis(query_str)\n    for pii in piis[:max_results]:\n        if os.path.exists(os.path.join(folder, '%s.txt' % pii)):\n            continue\n        logger.info('Downloading %s' % pii)\n        xml = download_article(pii, 'pii')\n        sleep(1)\n        if do_extract_text:\n            txt = extract_text(xml)\n            if not txt:\n                continue\n\n            with open(os.path.join(folder, '%s.txt' % pii), 'wb') as fh:\n                fh.write(txt.encode('utf-8'))\n        else:\n            with open(os.path.join(folder, '%s.xml' % pii), 'wb') as fh:\n                fh.write(xml.encode('utf-8'))\n    return", "language": "python", "code": "def download_from_search(query_str, folder, do_extract_text=True,\n                         max_results=None):\n    \"\"\"Save raw text files based on a search for papers on ScienceDirect.\n\n    This performs a search to get PIIs, downloads the XML corresponding to\n    the PII, extracts the raw text and then saves the text into a file\n    in the designated folder.\n\n    Parameters\n    ----------\n    query_str : str\n        The query string to search with\n    folder : str\n        The local path to an existing folder in which the text files\n        will be dumped\n    do_extract_text : bool\n        Choose whether to extract text from the xml, or simply save the raw xml\n        files. Default is True, so text is extracted.\n    max_results : int or None\n        Default is None. If specified, limit the number of results to the given\n        maximum.\n    \"\"\"\n    piis = get_piis(query_str)\n    for pii in piis[:max_results]:\n        if os.path.exists(os.path.join(folder, '%s.txt' % pii)):\n            continue\n        logger.info('Downloading %s' % pii)\n        xml = download_article(pii, 'pii')\n        sleep(1)\n        if do_extract_text:\n            txt = extract_text(xml)\n            if not txt:\n                continue\n\n            with open(os.path.join(folder, '%s.txt' % pii), 'wb') as fh:\n                fh.write(txt.encode('utf-8'))\n        else:\n            with open(os.path.join(folder, '%s.xml' % pii), 'wb') as fh:\n                fh.write(xml.encode('utf-8'))\n    return", "code_tokens": ["def", "download_from_search", "(", "query_str", ",", "folder", ",", "do_extract_text", "=", "True", ",", "max_results", "=", "None", ")", ":", "piis", "=", "get_piis", "(", "query_str", ")", "for", "pii", "in", "piis", "[", ":", "max_results", "]", ":", "if", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "folder", ",", "'%s.txt'", "%", "pii", ")", ")", ":", "continue", "logger", ".", "info", "(", "'Downloading %s'", "%", "pii", ")", "xml", "=", "download_article", "(", "pii", ",", "'pii'", ")", "sleep", "(", "1", ")", "if", "do_extract_text", ":", "txt", "=", "extract_text", "(", "xml", ")", "if", "not", "txt", ":", "continue", "with", "open", "(", "os", ".", "path", ".", "join", "(", "folder", ",", "'%s.txt'", "%", "pii", ")", ",", "'wb'", ")", "as", "fh", ":", "fh", ".", "write", "(", "txt", ".", "encode", "(", "'utf-8'", ")", ")", "else", ":", "with", "open", "(", "os", ".", "path", ".", "join", "(", "folder", ",", "'%s.xml'", "%", "pii", ")", ",", "'wb'", ")", "as", "fh", ":", "fh", ".", "write", "(", "xml", ".", "encode", "(", "'utf-8'", ")", ")", "return"], "docstring": "Save raw text files based on a search for papers on ScienceDirect.\n\n    This performs a search to get PIIs, downloads the XML corresponding to\n    the PII, extracts the raw text and then saves the text into a file\n    in the designated folder.\n\n    Parameters\n    ----------\n    query_str : str\n        The query string to search with\n    folder : str\n        The local path to an existing folder in which the text files\n        will be dumped\n    do_extract_text : bool\n        Choose whether to extract text from the xml, or simply save the raw xml\n        files. Default is True, so text is extracted.\n    max_results : int or None\n        Default is None. If specified, limit the number of results to the given\n        maximum.", "docstring_tokens": ["Save", "raw", "text", "files", "based", "on", "a", "search", "for", "papers", "on", "ScienceDirect", "."], "sha": "79a70415832c5702d7a820c7c9ccc8e25010124b", "url": "https://github.com/sorgerlab/indra/blob/79a70415832c5702d7a820c7c9ccc8e25010124b/indra/literature/elsevier_client.py#L361-L400", "partition": "train"}
{"repo": "sorgerlab/indra", "path": "indra/sources/cwms/rdf_processor.py", "func_name": "CWMSRDFProcessor.extract_statement_from_query_result", "original_string": "def extract_statement_from_query_result(self, res):\n        \"\"\"Adds a statement based on one element of a rdflib SPARQL query.\n\n        Parameters\n        ----------\n        res: rdflib.query.ResultRow\n            Element of rdflib SPARQL query result\n        \"\"\"\n        agent_start, agent_end, affected_start, affected_end = res\n\n        # Convert from rdflib literals to python integers so we can use\n        # them to index strings\n        agent_start = int(agent_start)\n        agent_end = int(agent_end)\n        affected_start = int(affected_start)\n        affected_end = int(affected_end)\n\n        # Find the text corresponding to these indices\n        agent = self.text[agent_start:agent_end]\n        affected = self.text[affected_start:affected_end]\n\n        # Strip off surrounding whitespace\n        agent = agent.lstrip().rstrip()\n        affected = affected.lstrip().rstrip()\n\n        # Make an Agent object for both the subject and the object\n        subj = Agent(agent, db_refs={'TEXT': agent})\n        obj = Agent(affected, db_refs={'TEXT': affected})\n\n        statement = Influence(subj=subj, obj=obj)\n\n        # Add the statement to the list of statements\n        self.statements.append(statement)", "language": "python", "code": "def extract_statement_from_query_result(self, res):\n        \"\"\"Adds a statement based on one element of a rdflib SPARQL query.\n\n        Parameters\n        ----------\n        res: rdflib.query.ResultRow\n            Element of rdflib SPARQL query result\n        \"\"\"\n        agent_start, agent_end, affected_start, affected_end = res\n\n        # Convert from rdflib literals to python integers so we can use\n        # them to index strings\n        agent_start = int(agent_start)\n        agent_end = int(agent_end)\n        affected_start = int(affected_start)\n        affected_end = int(affected_end)\n\n        # Find the text corresponding to these indices\n        agent = self.text[agent_start:agent_end]\n        affected = self.text[affected_start:affected_end]\n\n        # Strip off surrounding whitespace\n        agent = agent.lstrip().rstrip()\n        affected = affected.lstrip().rstrip()\n\n        # Make an Agent object for both the subject and the object\n        subj = Agent(agent, db_refs={'TEXT': agent})\n        obj = Agent(affected, db_refs={'TEXT': affected})\n\n        statement = Influence(subj=subj, obj=obj)\n\n        # Add the statement to the list of statements\n        self.statements.append(statement)", "code_tokens": ["def", "extract_statement_from_query_result", "(", "self", ",", "res", ")", ":", "agent_start", ",", "agent_end", ",", "affected_start", ",", "affected_end", "=", "res", "# Convert from rdflib literals to python integers so we can use", "# them to index strings", "agent_start", "=", "int", "(", "agent_start", ")", "agent_end", "=", "int", "(", "agent_end", ")", "affected_start", "=", "int", "(", "affected_start", ")", "affected_end", "=", "int", "(", "affected_end", ")", "# Find the text corresponding to these indices", "agent", "=", "self", ".", "text", "[", "agent_start", ":", "agent_end", "]", "affected", "=", "self", ".", "text", "[", "affected_start", ":", "affected_end", "]", "# Strip off surrounding whitespace", "agent", "=", "agent", ".", "lstrip", "(", ")", ".", "rstrip", "(", ")", "affected", "=", "affected", ".", "lstrip", "(", ")", ".", "rstrip", "(", ")", "# Make an Agent object for both the subject and the object", "subj", "=", "Agent", "(", "agent", ",", "db_refs", "=", "{", "'TEXT'", ":", "agent", "}", ")", "obj", "=", "Agent", "(", "affected", ",", "db_refs", "=", "{", "'TEXT'", ":", "affected", "}", ")", "statement", "=", "Influence", "(", "subj", "=", "subj", ",", "obj", "=", "obj", ")", "# Add the statement to the list of statements", "self", ".", "statements", ".", "append", "(", "statement", ")"], "docstring": "Adds a statement based on one element of a rdflib SPARQL query.\n\n        Parameters\n        ----------\n        res: rdflib.query.ResultRow\n            Element of rdflib SPARQL query result", "docstring_tokens": ["Adds", "a", "statement", "based", "on", "one", "element", "of", "a", "rdflib", "SPARQL", "query", "."], "sha": "79a70415832c5702d7a820c7c9ccc8e25010124b", "url": "https://github.com/sorgerlab/indra/blob/79a70415832c5702d7a820c7c9ccc8e25010124b/indra/sources/cwms/rdf_processor.py#L45-L77", "partition": "train"}
{"repo": "sorgerlab/indra", "path": "indra/sources/cwms/rdf_processor.py", "func_name": "CWMSRDFProcessor.extract_statements", "original_string": "def extract_statements(self):\n        \"\"\"Extracts INDRA statements from the RDF graph via SPARQL queries.\n        \"\"\"\n\n        # Look for events that have an AGENT and an AFFECTED, and get the\n        # start and ending text indices for each.\n        query = prefixes + \"\"\"\n        SELECT\n            ?agent_start\n            ?agent_end\n            ?affected_start\n            ?affected_end\n        WHERE {\n            ?rel role:AGENT ?agent .\n            ?rel role:AFFECTED ?affected .\n            ?agent lf:start ?agent_start .\n            ?agent lf:end ?agent_end .\n            ?affected lf:start ?affected_start .\n            ?affected lf:end ?affected_end .\n        }\n        \"\"\"\n        results = self.graph.query(query)\n        for res in results:\n            # Make a statement for each query match\n            self.extract_statement_from_query_result(res)\n\n        # Look for events that have an AGENT and a RESULT, and get the start\n        # and ending text indices for each.\n        query = query.replace('role:AFFECTED', 'role:RESULT')\n        results = self.graph.query(query)\n        for res in results:\n            # Make a statement for each query match\n            self.extract_statement_from_query_result(res)", "language": "python", "code": "def extract_statements(self):\n        \"\"\"Extracts INDRA statements from the RDF graph via SPARQL queries.\n        \"\"\"\n\n        # Look for events that have an AGENT and an AFFECTED, and get the\n        # start and ending text indices for each.\n        query = prefixes + \"\"\"\n        SELECT\n            ?agent_start\n            ?agent_end\n            ?affected_start\n            ?affected_end\n        WHERE {\n            ?rel role:AGENT ?agent .\n            ?rel role:AFFECTED ?affected .\n            ?agent lf:start ?agent_start .\n            ?agent lf:end ?agent_end .\n            ?affected lf:start ?affected_start .\n            ?affected lf:end ?affected_end .\n        }\n        \"\"\"\n        results = self.graph.query(query)\n        for res in results:\n            # Make a statement for each query match\n            self.extract_statement_from_query_result(res)\n\n        # Look for events that have an AGENT and a RESULT, and get the start\n        # and ending text indices for each.\n        query = query.replace('role:AFFECTED', 'role:RESULT')\n        results = self.graph.query(query)\n        for res in results:\n            # Make a statement for each query match\n            self.extract_statement_from_query_result(res)", "code_tokens": ["def", "extract_statements", "(", "self", ")", ":", "# Look for events that have an AGENT and an AFFECTED, and get the", "# start and ending text indices for each.", "query", "=", "prefixes", "+", "\"\"\"\n        SELECT\n            ?agent_start\n            ?agent_end\n            ?affected_start\n            ?affected_end\n        WHERE {\n            ?rel role:AGENT ?agent .\n            ?rel role:AFFECTED ?affected .\n            ?agent lf:start ?agent_start .\n            ?agent lf:end ?agent_end .\n            ?affected lf:start ?affected_start .\n            ?affected lf:end ?affected_end .\n        }\n        \"\"\"", "results", "=", "self", ".", "graph", ".", "query", "(", "query", ")", "for", "res", "in", "results", ":", "# Make a statement for each query match", "self", ".", "extract_statement_from_query_result", "(", "res", ")", "# Look for events that have an AGENT and a RESULT, and get the start", "# and ending text indices for each.", "query", "=", "query", ".", "replace", "(", "'role:AFFECTED'", ",", "'role:RESULT'", ")", "results", "=", "self", ".", "graph", ".", "query", "(", "query", ")", "for", "res", "in", "results", ":", "# Make a statement for each query match", "self", ".", "extract_statement_from_query_result", "(", "res", ")"], "docstring": "Extracts INDRA statements from the RDF graph via SPARQL queries.", "docstring_tokens": ["Extracts", "INDRA", "statements", "from", "the", "RDF", "graph", "via", "SPARQL", "queries", "."], "sha": "79a70415832c5702d7a820c7c9ccc8e25010124b", "url": "https://github.com/sorgerlab/indra/blob/79a70415832c5702d7a820c7c9ccc8e25010124b/indra/sources/cwms/rdf_processor.py#L79-L111", "partition": "train"}
{"repo": "sorgerlab/indra", "path": "indra/sources/signor/processor.py", "func_name": "SignorProcessor._recursively_lookup_complex", "original_string": "def _recursively_lookup_complex(self, complex_id):\n        \"\"\"Looks up the constitutents of a complex. If any constituent is\n        itself a complex, recursively expands until all constituents are\n        not complexes.\"\"\"\n        assert complex_id in self.complex_map\n\n        expanded_agent_strings = []\n        expand_these_next = [complex_id]\n        while len(expand_these_next) > 0:\n            # Pop next element\n            c = expand_these_next[0]\n            expand_these_next = expand_these_next[1:]\n\n            # If a complex, add expanding it to the end of the queue\n            # If an agent string, add it to the agent string list immediately\n            assert c in self.complex_map\n            for s in self.complex_map[c]:\n                if s in self.complex_map:\n                    expand_these_next.append(s)\n                else:\n                    expanded_agent_strings.append(s)\n        return expanded_agent_strings", "language": "python", "code": "def _recursively_lookup_complex(self, complex_id):\n        \"\"\"Looks up the constitutents of a complex. If any constituent is\n        itself a complex, recursively expands until all constituents are\n        not complexes.\"\"\"\n        assert complex_id in self.complex_map\n\n        expanded_agent_strings = []\n        expand_these_next = [complex_id]\n        while len(expand_these_next) > 0:\n            # Pop next element\n            c = expand_these_next[0]\n            expand_these_next = expand_these_next[1:]\n\n            # If a complex, add expanding it to the end of the queue\n            # If an agent string, add it to the agent string list immediately\n            assert c in self.complex_map\n            for s in self.complex_map[c]:\n                if s in self.complex_map:\n                    expand_these_next.append(s)\n                else:\n                    expanded_agent_strings.append(s)\n        return expanded_agent_strings", "code_tokens": ["def", "_recursively_lookup_complex", "(", "self", ",", "complex_id", ")", ":", "assert", "complex_id", "in", "self", ".", "complex_map", "expanded_agent_strings", "=", "[", "]", "expand_these_next", "=", "[", "complex_id", "]", "while", "len", "(", "expand_these_next", ")", ">", "0", ":", "# Pop next element", "c", "=", "expand_these_next", "[", "0", "]", "expand_these_next", "=", "expand_these_next", "[", "1", ":", "]", "# If a complex, add expanding it to the end of the queue", "# If an agent string, add it to the agent string list immediately", "assert", "c", "in", "self", ".", "complex_map", "for", "s", "in", "self", ".", "complex_map", "[", "c", "]", ":", "if", "s", "in", "self", ".", "complex_map", ":", "expand_these_next", ".", "append", "(", "s", ")", "else", ":", "expanded_agent_strings", ".", "append", "(", "s", ")", "return", "expanded_agent_strings"], "docstring": "Looks up the constitutents of a complex. If any constituent is\n        itself a complex, recursively expands until all constituents are\n        not complexes.", "docstring_tokens": ["Looks", "up", "the", "constitutents", "of", "a", "complex", ".", "If", "any", "constituent", "is", "itself", "a", "complex", "recursively", "expands", "until", "all", "constituents", "are", "not", "complexes", "."], "sha": "79a70415832c5702d7a820c7c9ccc8e25010124b", "url": "https://github.com/sorgerlab/indra/blob/79a70415832c5702d7a820c7c9ccc8e25010124b/indra/sources/signor/processor.py#L223-L244", "partition": "train"}
{"repo": "sorgerlab/indra", "path": "indra/sources/signor/processor.py", "func_name": "SignorProcessor._get_complex_agents", "original_string": "def _get_complex_agents(self, complex_id):\n        \"\"\"Returns a list of agents corresponding to each of the constituents\n        in a SIGNOR complex.\"\"\"\n        agents = []\n        components = self._recursively_lookup_complex(complex_id)\n\n        for c in components:\n            db_refs = {}\n            name = uniprot_client.get_gene_name(c)\n            if name is None:\n                db_refs['SIGNOR'] = c\n            else:\n                db_refs['UP'] = c\n                hgnc_id = hgnc_client.get_hgnc_id(name)\n                if hgnc_id:\n                    db_refs['HGNC'] = hgnc_id\n\n            famplex_key = ('SIGNOR', c)\n            if famplex_key in famplex_map:\n                db_refs['FPLX'] = famplex_map[famplex_key]\n                if not name:\n                    name = db_refs['FPLX']  # Set agent name to Famplex name if\n                                            # the Uniprot name is not available\n            elif not name:\n                # We neither have a Uniprot nor Famplex grounding\n                logger.info('Have neither a Uniprot nor Famplex grounding ' + \\\n                            'for ' + c)\n                if not name:\n                    name = db_refs['SIGNOR']  # Set the agent name to the\n                                              # Signor name if neither the\n                                              # Uniprot nor Famplex names are\n                                              # available\n            assert(name is not None)\n            agents.append(Agent(name, db_refs=db_refs))\n        return agents", "language": "python", "code": "def _get_complex_agents(self, complex_id):\n        \"\"\"Returns a list of agents corresponding to each of the constituents\n        in a SIGNOR complex.\"\"\"\n        agents = []\n        components = self._recursively_lookup_complex(complex_id)\n\n        for c in components:\n            db_refs = {}\n            name = uniprot_client.get_gene_name(c)\n            if name is None:\n                db_refs['SIGNOR'] = c\n            else:\n                db_refs['UP'] = c\n                hgnc_id = hgnc_client.get_hgnc_id(name)\n                if hgnc_id:\n                    db_refs['HGNC'] = hgnc_id\n\n            famplex_key = ('SIGNOR', c)\n            if famplex_key in famplex_map:\n                db_refs['FPLX'] = famplex_map[famplex_key]\n                if not name:\n                    name = db_refs['FPLX']  # Set agent name to Famplex name if\n                                            # the Uniprot name is not available\n            elif not name:\n                # We neither have a Uniprot nor Famplex grounding\n                logger.info('Have neither a Uniprot nor Famplex grounding ' + \\\n                            'for ' + c)\n                if not name:\n                    name = db_refs['SIGNOR']  # Set the agent name to the\n                                              # Signor name if neither the\n                                              # Uniprot nor Famplex names are\n                                              # available\n            assert(name is not None)\n            agents.append(Agent(name, db_refs=db_refs))\n        return agents", "code_tokens": ["def", "_get_complex_agents", "(", "self", ",", "complex_id", ")", ":", "agents", "=", "[", "]", "components", "=", "self", ".", "_recursively_lookup_complex", "(", "complex_id", ")", "for", "c", "in", "components", ":", "db_refs", "=", "{", "}", "name", "=", "uniprot_client", ".", "get_gene_name", "(", "c", ")", "if", "name", "is", "None", ":", "db_refs", "[", "'SIGNOR'", "]", "=", "c", "else", ":", "db_refs", "[", "'UP'", "]", "=", "c", "hgnc_id", "=", "hgnc_client", ".", "get_hgnc_id", "(", "name", ")", "if", "hgnc_id", ":", "db_refs", "[", "'HGNC'", "]", "=", "hgnc_id", "famplex_key", "=", "(", "'SIGNOR'", ",", "c", ")", "if", "famplex_key", "in", "famplex_map", ":", "db_refs", "[", "'FPLX'", "]", "=", "famplex_map", "[", "famplex_key", "]", "if", "not", "name", ":", "name", "=", "db_refs", "[", "'FPLX'", "]", "# Set agent name to Famplex name if", "# the Uniprot name is not available", "elif", "not", "name", ":", "# We neither have a Uniprot nor Famplex grounding", "logger", ".", "info", "(", "'Have neither a Uniprot nor Famplex grounding '", "+", "'for '", "+", "c", ")", "if", "not", "name", ":", "name", "=", "db_refs", "[", "'SIGNOR'", "]", "# Set the agent name to the", "# Signor name if neither the", "# Uniprot nor Famplex names are", "# available", "assert", "(", "name", "is", "not", "None", ")", "agents", ".", "append", "(", "Agent", "(", "name", ",", "db_refs", "=", "db_refs", ")", ")", "return", "agents"], "docstring": "Returns a list of agents corresponding to each of the constituents\n        in a SIGNOR complex.", "docstring_tokens": ["Returns", "a", "list", "of", "agents", "corresponding", "to", "each", "of", "the", "constituents", "in", "a", "SIGNOR", "complex", "."], "sha": "79a70415832c5702d7a820c7c9ccc8e25010124b", "url": "https://github.com/sorgerlab/indra/blob/79a70415832c5702d7a820c7c9ccc8e25010124b/indra/sources/signor/processor.py#L246-L280", "partition": "train"}
{"repo": "sorgerlab/indra", "path": "indra/statements/io.py", "func_name": "stmts_from_json", "original_string": "def stmts_from_json(json_in, on_missing_support='handle'):\n    \"\"\"Get a list of Statements from Statement jsons.\n\n    In the case of pre-assembled Statements which have `supports` and\n    `supported_by` lists, the uuids will be replaced with references to\n    Statement objects from the json, where possible. The method of handling\n    missing support is controled by the `on_missing_support` key-word argument.\n\n    Parameters\n    ----------\n    json_in : iterable[dict]\n        A json list containing json dict representations of INDRA Statements,\n        as produced by the `to_json` methods of subclasses of Statement, or\n        equivalently by `stmts_to_json`.\n    on_missing_support : Optional[str]\n        Handles the behavior when a uuid reference in `supports` or\n        `supported_by` attribute cannot be resolved. This happens because uuids\n        can only be linked to Statements contained in the `json_in` list, and\n        some may be missing if only some of all the Statements from pre-\n        assembly are contained in the list.\n\n        Options:\n\n        - *'handle'* : (default) convert unresolved uuids into `Unresolved`\n          Statement objects.\n        - *'ignore'* : Simply omit any uuids that cannot be linked to any\n          Statements in the list.\n        - *'error'* : Raise an error upon hitting an un-linkable uuid.\n\n    Returns\n    -------\n    stmts : list[:py:class:`Statement`]\n        A list of INDRA Statements.\n    \"\"\"\n\n    stmts = []\n    uuid_dict = {}\n    for json_stmt in json_in:\n        try:\n            st = Statement._from_json(json_stmt)\n        except Exception as e:\n            logger.warning(\"Error creating statement: %s\" % e)\n            continue\n        stmts.append(st)\n        uuid_dict[st.uuid] = st\n    for st in stmts:\n        _promote_support(st.supports, uuid_dict, on_missing_support)\n        _promote_support(st.supported_by, uuid_dict, on_missing_support)\n    return stmts", "language": "python", "code": "def stmts_from_json(json_in, on_missing_support='handle'):\n    \"\"\"Get a list of Statements from Statement jsons.\n\n    In the case of pre-assembled Statements which have `supports` and\n    `supported_by` lists, the uuids will be replaced with references to\n    Statement objects from the json, where possible. The method of handling\n    missing support is controled by the `on_missing_support` key-word argument.\n\n    Parameters\n    ----------\n    json_in : iterable[dict]\n        A json list containing json dict representations of INDRA Statements,\n        as produced by the `to_json` methods of subclasses of Statement, or\n        equivalently by `stmts_to_json`.\n    on_missing_support : Optional[str]\n        Handles the behavior when a uuid reference in `supports` or\n        `supported_by` attribute cannot be resolved. This happens because uuids\n        can only be linked to Statements contained in the `json_in` list, and\n        some may be missing if only some of all the Statements from pre-\n        assembly are contained in the list.\n\n        Options:\n\n        - *'handle'* : (default) convert unresolved uuids into `Unresolved`\n          Statement objects.\n        - *'ignore'* : Simply omit any uuids that cannot be linked to any\n          Statements in the list.\n        - *'error'* : Raise an error upon hitting an un-linkable uuid.\n\n    Returns\n    -------\n    stmts : list[:py:class:`Statement`]\n        A list of INDRA Statements.\n    \"\"\"\n\n    stmts = []\n    uuid_dict = {}\n    for json_stmt in json_in:\n        try:\n            st = Statement._from_json(json_stmt)\n        except Exception as e:\n            logger.warning(\"Error creating statement: %s\" % e)\n            continue\n        stmts.append(st)\n        uuid_dict[st.uuid] = st\n    for st in stmts:\n        _promote_support(st.supports, uuid_dict, on_missing_support)\n        _promote_support(st.supported_by, uuid_dict, on_missing_support)\n    return stmts", "code_tokens": ["def", "stmts_from_json", "(", "json_in", ",", "on_missing_support", "=", "'handle'", ")", ":", "stmts", "=", "[", "]", "uuid_dict", "=", "{", "}", "for", "json_stmt", "in", "json_in", ":", "try", ":", "st", "=", "Statement", ".", "_from_json", "(", "json_stmt", ")", "except", "Exception", "as", "e", ":", "logger", ".", "warning", "(", "\"Error creating statement: %s\"", "%", "e", ")", "continue", "stmts", ".", "append", "(", "st", ")", "uuid_dict", "[", "st", ".", "uuid", "]", "=", "st", "for", "st", "in", "stmts", ":", "_promote_support", "(", "st", ".", "supports", ",", "uuid_dict", ",", "on_missing_support", ")", "_promote_support", "(", "st", ".", "supported_by", ",", "uuid_dict", ",", "on_missing_support", ")", "return", "stmts"], "docstring": "Get a list of Statements from Statement jsons.\n\n    In the case of pre-assembled Statements which have `supports` and\n    `supported_by` lists, the uuids will be replaced with references to\n    Statement objects from the json, where possible. The method of handling\n    missing support is controled by the `on_missing_support` key-word argument.\n\n    Parameters\n    ----------\n    json_in : iterable[dict]\n        A json list containing json dict representations of INDRA Statements,\n        as produced by the `to_json` methods of subclasses of Statement, or\n        equivalently by `stmts_to_json`.\n    on_missing_support : Optional[str]\n        Handles the behavior when a uuid reference in `supports` or\n        `supported_by` attribute cannot be resolved. This happens because uuids\n        can only be linked to Statements contained in the `json_in` list, and\n        some may be missing if only some of all the Statements from pre-\n        assembly are contained in the list.\n\n        Options:\n\n        - *'handle'* : (default) convert unresolved uuids into `Unresolved`\n          Statement objects.\n        - *'ignore'* : Simply omit any uuids that cannot be linked to any\n          Statements in the list.\n        - *'error'* : Raise an error upon hitting an un-linkable uuid.\n\n    Returns\n    -------\n    stmts : list[:py:class:`Statement`]\n        A list of INDRA Statements.", "docstring_tokens": ["Get", "a", "list", "of", "Statements", "from", "Statement", "jsons", "."], "sha": "79a70415832c5702d7a820c7c9ccc8e25010124b", "url": "https://github.com/sorgerlab/indra/blob/79a70415832c5702d7a820c7c9ccc8e25010124b/indra/statements/io.py#L16-L64", "partition": "train"}
{"repo": "sorgerlab/indra", "path": "indra/statements/io.py", "func_name": "stmts_to_json_file", "original_string": "def stmts_to_json_file(stmts, fname):\n    \"\"\"Serialize a list of INDRA Statements into a JSON file.\n\n    Parameters\n    ----------\n    stmts : list[indra.statement.Statements]\n        The list of INDRA Statements to serialize into the JSON file.\n    fname : str\n        Path to the JSON file to serialize Statements into.\n    \"\"\"\n    with open(fname, 'w') as fh:\n        json.dump(stmts_to_json(stmts), fh, indent=1)", "language": "python", "code": "def stmts_to_json_file(stmts, fname):\n    \"\"\"Serialize a list of INDRA Statements into a JSON file.\n\n    Parameters\n    ----------\n    stmts : list[indra.statement.Statements]\n        The list of INDRA Statements to serialize into the JSON file.\n    fname : str\n        Path to the JSON file to serialize Statements into.\n    \"\"\"\n    with open(fname, 'w') as fh:\n        json.dump(stmts_to_json(stmts), fh, indent=1)", "code_tokens": ["def", "stmts_to_json_file", "(", "stmts", ",", "fname", ")", ":", "with", "open", "(", "fname", ",", "'w'", ")", "as", "fh", ":", "json", ".", "dump", "(", "stmts_to_json", "(", "stmts", ")", ",", "fh", ",", "indent", "=", "1", ")"], "docstring": "Serialize a list of INDRA Statements into a JSON file.\n\n    Parameters\n    ----------\n    stmts : list[indra.statement.Statements]\n        The list of INDRA Statements to serialize into the JSON file.\n    fname : str\n        Path to the JSON file to serialize Statements into.", "docstring_tokens": ["Serialize", "a", "list", "of", "INDRA", "Statements", "into", "a", "JSON", "file", "."], "sha": "79a70415832c5702d7a820c7c9ccc8e25010124b", "url": "https://github.com/sorgerlab/indra/blob/79a70415832c5702d7a820c7c9ccc8e25010124b/indra/statements/io.py#L84-L95", "partition": "train"}
{"repo": "sorgerlab/indra", "path": "indra/statements/io.py", "func_name": "stmts_to_json", "original_string": "def stmts_to_json(stmts_in, use_sbo=False):\n    \"\"\"Return the JSON-serialized form of one or more INDRA Statements.\n\n    Parameters\n    ----------\n    stmts_in : Statement or list[Statement]\n        A Statement or list of Statement objects to serialize into JSON.\n    use_sbo : Optional[bool]\n        If True, SBO annotations are added to each applicable element of the\n        JSON. Default: False\n\n    Returns\n    -------\n    json_dict : dict\n        JSON-serialized INDRA Statements.\n    \"\"\"\n    if not isinstance(stmts_in, list):\n        json_dict = stmts_in.to_json(use_sbo=use_sbo)\n        return json_dict\n    else:\n        json_dict = [st.to_json(use_sbo=use_sbo) for st in stmts_in]\n    return json_dict", "language": "python", "code": "def stmts_to_json(stmts_in, use_sbo=False):\n    \"\"\"Return the JSON-serialized form of one or more INDRA Statements.\n\n    Parameters\n    ----------\n    stmts_in : Statement or list[Statement]\n        A Statement or list of Statement objects to serialize into JSON.\n    use_sbo : Optional[bool]\n        If True, SBO annotations are added to each applicable element of the\n        JSON. Default: False\n\n    Returns\n    -------\n    json_dict : dict\n        JSON-serialized INDRA Statements.\n    \"\"\"\n    if not isinstance(stmts_in, list):\n        json_dict = stmts_in.to_json(use_sbo=use_sbo)\n        return json_dict\n    else:\n        json_dict = [st.to_json(use_sbo=use_sbo) for st in stmts_in]\n    return json_dict", "code_tokens": ["def", "stmts_to_json", "(", "stmts_in", ",", "use_sbo", "=", "False", ")", ":", "if", "not", "isinstance", "(", "stmts_in", ",", "list", ")", ":", "json_dict", "=", "stmts_in", ".", "to_json", "(", "use_sbo", "=", "use_sbo", ")", "return", "json_dict", "else", ":", "json_dict", "=", "[", "st", ".", "to_json", "(", "use_sbo", "=", "use_sbo", ")", "for", "st", "in", "stmts_in", "]", "return", "json_dict"], "docstring": "Return the JSON-serialized form of one or more INDRA Statements.\n\n    Parameters\n    ----------\n    stmts_in : Statement or list[Statement]\n        A Statement or list of Statement objects to serialize into JSON.\n    use_sbo : Optional[bool]\n        If True, SBO annotations are added to each applicable element of the\n        JSON. Default: False\n\n    Returns\n    -------\n    json_dict : dict\n        JSON-serialized INDRA Statements.", "docstring_tokens": ["Return", "the", "JSON", "-", "serialized", "form", "of", "one", "or", "more", "INDRA", "Statements", "."], "sha": "79a70415832c5702d7a820c7c9ccc8e25010124b", "url": "https://github.com/sorgerlab/indra/blob/79a70415832c5702d7a820c7c9ccc8e25010124b/indra/statements/io.py#L98-L119", "partition": "train"}
{"repo": "martinpitt/python-dbusmock", "path": "dbusmock/mockobject.py", "func_name": "DBusMockObject.format_args", "original_string": "def format_args(self, args):\n        '''Format a D-Bus argument tuple into an appropriate logging string.'''\n\n        def format_arg(a):\n            if isinstance(a, dbus.Boolean):\n                return str(bool(a))\n            if isinstance(a, dbus.Byte):\n                return str(int(a))\n            if isinstance(a, int) or isinstance(a, long):\n                return str(a)\n            if isinstance(a, str):\n                return '\"' + str(a) + '\"'\n            if isinstance(a, unicode):  # Python 2 only\n                return '\"' + repr(a.encode('UTF-8'))[1:-1] + '\"'\n            if isinstance(a, list):\n                return '[' + ', '.join([format_arg(x) for x in a]) + ']'\n            if isinstance(a, dict):\n                fmta = '{'\n                first = True\n                for k, v in a.items():\n                    if first:\n                        first = False\n                    else:\n                        fmta += ', '\n                    fmta += format_arg(k) + ': ' + format_arg(v)\n                return fmta + '}'\n\n            # fallback\n            return repr(a)\n\n        s = ''\n        for a in args:\n            if s:\n                s += ' '\n            s += format_arg(a)\n        if s:\n            s = ' ' + s\n        return s", "language": "python", "code": "def format_args(self, args):\n        '''Format a D-Bus argument tuple into an appropriate logging string.'''\n\n        def format_arg(a):\n            if isinstance(a, dbus.Boolean):\n                return str(bool(a))\n            if isinstance(a, dbus.Byte):\n                return str(int(a))\n            if isinstance(a, int) or isinstance(a, long):\n                return str(a)\n            if isinstance(a, str):\n                return '\"' + str(a) + '\"'\n            if isinstance(a, unicode):  # Python 2 only\n                return '\"' + repr(a.encode('UTF-8'))[1:-1] + '\"'\n            if isinstance(a, list):\n                return '[' + ', '.join([format_arg(x) for x in a]) + ']'\n            if isinstance(a, dict):\n                fmta = '{'\n                first = True\n                for k, v in a.items():\n                    if first:\n                        first = False\n                    else:\n                        fmta += ', '\n                    fmta += format_arg(k) + ': ' + format_arg(v)\n                return fmta + '}'\n\n            # fallback\n            return repr(a)\n\n        s = ''\n        for a in args:\n            if s:\n                s += ' '\n            s += format_arg(a)\n        if s:\n            s = ' ' + s\n        return s", "code_tokens": ["def", "format_args", "(", "self", ",", "args", ")", ":", "def", "format_arg", "(", "a", ")", ":", "if", "isinstance", "(", "a", ",", "dbus", ".", "Boolean", ")", ":", "return", "str", "(", "bool", "(", "a", ")", ")", "if", "isinstance", "(", "a", ",", "dbus", ".", "Byte", ")", ":", "return", "str", "(", "int", "(", "a", ")", ")", "if", "isinstance", "(", "a", ",", "int", ")", "or", "isinstance", "(", "a", ",", "long", ")", ":", "return", "str", "(", "a", ")", "if", "isinstance", "(", "a", ",", "str", ")", ":", "return", "'\"'", "+", "str", "(", "a", ")", "+", "'\"'", "if", "isinstance", "(", "a", ",", "unicode", ")", ":", "# Python 2 only", "return", "'\"'", "+", "repr", "(", "a", ".", "encode", "(", "'UTF-8'", ")", ")", "[", "1", ":", "-", "1", "]", "+", "'\"'", "if", "isinstance", "(", "a", ",", "list", ")", ":", "return", "'['", "+", "', '", ".", "join", "(", "[", "format_arg", "(", "x", ")", "for", "x", "in", "a", "]", ")", "+", "']'", "if", "isinstance", "(", "a", ",", "dict", ")", ":", "fmta", "=", "'{'", "first", "=", "True", "for", "k", ",", "v", "in", "a", ".", "items", "(", ")", ":", "if", "first", ":", "first", "=", "False", "else", ":", "fmta", "+=", "', '", "fmta", "+=", "format_arg", "(", "k", ")", "+", "': '", "+", "format_arg", "(", "v", ")", "return", "fmta", "+", "'}'", "# fallback", "return", "repr", "(", "a", ")", "s", "=", "''", "for", "a", "in", "args", ":", "if", "s", ":", "s", "+=", "' '", "s", "+=", "format_arg", "(", "a", ")", "if", "s", ":", "s", "=", "' '", "+", "s", "return", "s"], "docstring": "Format a D-Bus argument tuple into an appropriate logging string.", "docstring_tokens": ["Format", "a", "D", "-", "Bus", "argument", "tuple", "into", "an", "appropriate", "logging", "string", "."], "sha": "26f65f78bc0ed347233f699a8d6ee0e6880e7eb0", "url": "https://github.com/martinpitt/python-dbusmock/blob/26f65f78bc0ed347233f699a8d6ee0e6880e7eb0/dbusmock/mockobject.py#L581-L618", "partition": "train"}
{"repo": "martinpitt/python-dbusmock", "path": "dbusmock/mockobject.py", "func_name": "DBusMockObject.log", "original_string": "def log(self, msg):\n        '''Log a message, prefixed with a timestamp.\n\n        If a log file was specified in the constructor, it is written there,\n        otherwise it goes to stdout.\n        '''\n        if self.logfile:\n            fd = self.logfile.fileno()\n        else:\n            fd = sys.stdout.fileno()\n\n        os.write(fd, ('%.3f %s\\n' % (time.time(), msg)).encode('UTF-8'))", "language": "python", "code": "def log(self, msg):\n        '''Log a message, prefixed with a timestamp.\n\n        If a log file was specified in the constructor, it is written there,\n        otherwise it goes to stdout.\n        '''\n        if self.logfile:\n            fd = self.logfile.fileno()\n        else:\n            fd = sys.stdout.fileno()\n\n        os.write(fd, ('%.3f %s\\n' % (time.time(), msg)).encode('UTF-8'))", "code_tokens": ["def", "log", "(", "self", ",", "msg", ")", ":", "if", "self", ".", "logfile", ":", "fd", "=", "self", ".", "logfile", ".", "fileno", "(", ")", "else", ":", "fd", "=", "sys", ".", "stdout", ".", "fileno", "(", ")", "os", ".", "write", "(", "fd", ",", "(", "'%.3f %s\\n'", "%", "(", "time", ".", "time", "(", ")", ",", "msg", ")", ")", ".", "encode", "(", "'UTF-8'", ")", ")"], "docstring": "Log a message, prefixed with a timestamp.\n\n        If a log file was specified in the constructor, it is written there,\n        otherwise it goes to stdout.", "docstring_tokens": ["Log", "a", "message", "prefixed", "with", "a", "timestamp", "."], "sha": "26f65f78bc0ed347233f699a8d6ee0e6880e7eb0", "url": "https://github.com/martinpitt/python-dbusmock/blob/26f65f78bc0ed347233f699a8d6ee0e6880e7eb0/dbusmock/mockobject.py#L620-L631", "partition": "train"}
{"repo": "martinpitt/python-dbusmock", "path": "dbusmock/mockobject.py", "func_name": "DBusMockObject.Introspect", "original_string": "def Introspect(self, object_path, connection):\n        '''Return XML description of this object's interfaces, methods and signals.\n\n        This wraps dbus-python's Introspect() method to include the dynamic\n        methods and properties.\n        '''\n        # temporarily add our dynamic methods\n        cls = self.__class__.__module__ + '.' + self.__class__.__name__\n        orig_interfaces = self._dbus_class_table[cls]\n\n        mock_interfaces = orig_interfaces.copy()\n        for interface, methods in self.methods.items():\n            for method in methods:\n                mock_interfaces.setdefault(interface, {})[method] = self.methods[interface][method][3]\n        self._dbus_class_table[cls] = mock_interfaces\n\n        xml = dbus.service.Object.Introspect(self, object_path, connection)\n\n        tree = ElementTree.fromstring(xml)\n\n        for name in self.props:\n            # We might have properties for new interfaces we don't know about\n            # yet. Try to find an existing <interface> node named after our\n            # interface to append to, and create one if we can't.\n            interface = tree.find(\".//interface[@name='%s']\" % name)\n            if interface is None:\n                interface = ElementTree.Element(\"interface\", {\"name\": name})\n                tree.append(interface)\n\n            for prop, val in self.props[name].items():\n                if val is None:\n                    # can't guess type from None, skip\n                    continue\n                elem = ElementTree.Element(\"property\", {\n                    \"name\": prop,\n                    # We don't store the signature anywhere, so guess it.\n                    \"type\": dbus.lowlevel.Message.guess_signature(val),\n                    \"access\": \"readwrite\"})\n\n                interface.append(elem)\n\n        xml = ElementTree.tostring(tree, encoding='utf8', method='xml').decode('utf8')\n\n        # restore original class table\n        self._dbus_class_table[cls] = orig_interfaces\n\n        return xml", "language": "python", "code": "def Introspect(self, object_path, connection):\n        '''Return XML description of this object's interfaces, methods and signals.\n\n        This wraps dbus-python's Introspect() method to include the dynamic\n        methods and properties.\n        '''\n        # temporarily add our dynamic methods\n        cls = self.__class__.__module__ + '.' + self.__class__.__name__\n        orig_interfaces = self._dbus_class_table[cls]\n\n        mock_interfaces = orig_interfaces.copy()\n        for interface, methods in self.methods.items():\n            for method in methods:\n                mock_interfaces.setdefault(interface, {})[method] = self.methods[interface][method][3]\n        self._dbus_class_table[cls] = mock_interfaces\n\n        xml = dbus.service.Object.Introspect(self, object_path, connection)\n\n        tree = ElementTree.fromstring(xml)\n\n        for name in self.props:\n            # We might have properties for new interfaces we don't know about\n            # yet. Try to find an existing <interface> node named after our\n            # interface to append to, and create one if we can't.\n            interface = tree.find(\".//interface[@name='%s']\" % name)\n            if interface is None:\n                interface = ElementTree.Element(\"interface\", {\"name\": name})\n                tree.append(interface)\n\n            for prop, val in self.props[name].items():\n                if val is None:\n                    # can't guess type from None, skip\n                    continue\n                elem = ElementTree.Element(\"property\", {\n                    \"name\": prop,\n                    # We don't store the signature anywhere, so guess it.\n                    \"type\": dbus.lowlevel.Message.guess_signature(val),\n                    \"access\": \"readwrite\"})\n\n                interface.append(elem)\n\n        xml = ElementTree.tostring(tree, encoding='utf8', method='xml').decode('utf8')\n\n        # restore original class table\n        self._dbus_class_table[cls] = orig_interfaces\n\n        return xml", "code_tokens": ["def", "Introspect", "(", "self", ",", "object_path", ",", "connection", ")", ":", "# temporarily add our dynamic methods", "cls", "=", "self", ".", "__class__", ".", "__module__", "+", "'.'", "+", "self", ".", "__class__", ".", "__name__", "orig_interfaces", "=", "self", ".", "_dbus_class_table", "[", "cls", "]", "mock_interfaces", "=", "orig_interfaces", ".", "copy", "(", ")", "for", "interface", ",", "methods", "in", "self", ".", "methods", ".", "items", "(", ")", ":", "for", "method", "in", "methods", ":", "mock_interfaces", ".", "setdefault", "(", "interface", ",", "{", "}", ")", "[", "method", "]", "=", "self", ".", "methods", "[", "interface", "]", "[", "method", "]", "[", "3", "]", "self", ".", "_dbus_class_table", "[", "cls", "]", "=", "mock_interfaces", "xml", "=", "dbus", ".", "service", ".", "Object", ".", "Introspect", "(", "self", ",", "object_path", ",", "connection", ")", "tree", "=", "ElementTree", ".", "fromstring", "(", "xml", ")", "for", "name", "in", "self", ".", "props", ":", "# We might have properties for new interfaces we don't know about", "# yet. Try to find an existing <interface> node named after our", "# interface to append to, and create one if we can't.", "interface", "=", "tree", ".", "find", "(", "\".//interface[@name='%s']\"", "%", "name", ")", "if", "interface", "is", "None", ":", "interface", "=", "ElementTree", ".", "Element", "(", "\"interface\"", ",", "{", "\"name\"", ":", "name", "}", ")", "tree", ".", "append", "(", "interface", ")", "for", "prop", ",", "val", "in", "self", ".", "props", "[", "name", "]", ".", "items", "(", ")", ":", "if", "val", "is", "None", ":", "# can't guess type from None, skip", "continue", "elem", "=", "ElementTree", ".", "Element", "(", "\"property\"", ",", "{", "\"name\"", ":", "prop", ",", "# We don't store the signature anywhere, so guess it.", "\"type\"", ":", "dbus", ".", "lowlevel", ".", "Message", ".", "guess_signature", "(", "val", ")", ",", "\"access\"", ":", "\"readwrite\"", "}", ")", "interface", ".", "append", "(", "elem", ")", "xml", "=", "ElementTree", ".", "tostring", "(", "tree", ",", "encoding", "=", "'utf8'", ",", "method", "=", "'xml'", ")", ".", "decode", "(", "'utf8'", ")", "# restore original class table", "self", ".", "_dbus_class_table", "[", "cls", "]", "=", "orig_interfaces", "return", "xml"], "docstring": "Return XML description of this object's interfaces, methods and signals.\n\n        This wraps dbus-python's Introspect() method to include the dynamic\n        methods and properties.", "docstring_tokens": ["Return", "XML", "description", "of", "this", "object", "s", "interfaces", "methods", "and", "signals", "."], "sha": "26f65f78bc0ed347233f699a8d6ee0e6880e7eb0", "url": "https://github.com/martinpitt/python-dbusmock/blob/26f65f78bc0ed347233f699a8d6ee0e6880e7eb0/dbusmock/mockobject.py#L638-L684", "partition": "train"}
{"repo": "martinpitt/python-dbusmock", "path": "dbusmock/templates/bluez5-obex.py", "func_name": "CreateSession", "original_string": "def CreateSession(self, destination, args):\n    '''OBEX method to create a new transfer session.\n\n    The destination must be the address of the destination Bluetooth device.\n    The given arguments must be a map from well-known keys to values,\n    containing at least the \u2018Target\u2019 key, whose value must be \u2018PBAP\u2019 (other\n    keys and values are accepted by the real daemon, but not by this mock\n    daemon at the moment). If the target is missing or incorrect, an\n    Unsupported error is returned on the bus.\n\n    Returns the path of a new Session object.\n    '''\n\n    if 'Target' not in args or args['Target'].upper() != 'PBAP':\n        raise dbus.exceptions.DBusException(\n            'Non-PBAP targets are not currently supported by this python-dbusmock template.',\n            name=OBEX_MOCK_IFACE + '.Unsupported')\n\n    # Find the first unused session ID.\n    client_path = '/org/bluez/obex/client'\n    session_id = 0\n    while client_path + '/session' + str(session_id) in mockobject.objects:\n        session_id += 1\n\n    path = client_path + '/session' + str(session_id)\n    properties = {\n        'Source': dbus.String('FIXME', variant_level=1),\n        'Destination': dbus.String(destination, variant_level=1),\n        'Channel': dbus.Byte(0, variant_level=1),\n        'Target': dbus.String('FIXME', variant_level=1),\n        'Root': dbus.String('FIXME', variant_level=1),\n    }\n\n    self.AddObject(path,\n                   SESSION_IFACE,\n                   # Properties\n                   properties,\n                   # Methods\n                   [\n                       ('GetCapabilities', '', 's', ''),  # Currently a no-op\n                   ])\n\n    session = mockobject.objects[path]\n    session.AddMethods(PHONEBOOK_ACCESS_IFACE, [\n        ('Select', 'ss', '', ''),  # Currently a no-op\n        # Currently a no-op\n        ('List', 'a{sv}', 'a(ss)', 'ret = dbus.Array(signature=\"(ss)\")'),\n        # Currently a no-op\n        ('ListFilterFields', '', 'as', 'ret = dbus.Array(signature=\"(s)\")'),\n        ('PullAll', 'sa{sv}', 'sa{sv}', PullAll),\n        ('GetSize', '', 'q', 'ret = 0'),  # TODO\n    ])\n\n    manager = mockobject.objects['/']\n    manager.EmitSignal(OBJECT_MANAGER_IFACE, 'InterfacesAdded',\n                       'oa{sa{sv}}', [\n                           dbus.ObjectPath(path),\n                           {SESSION_IFACE: properties},\n                       ])\n\n    return path", "language": "python", "code": "def CreateSession(self, destination, args):\n    '''OBEX method to create a new transfer session.\n\n    The destination must be the address of the destination Bluetooth device.\n    The given arguments must be a map from well-known keys to values,\n    containing at least the \u2018Target\u2019 key, whose value must be \u2018PBAP\u2019 (other\n    keys and values are accepted by the real daemon, but not by this mock\n    daemon at the moment). If the target is missing or incorrect, an\n    Unsupported error is returned on the bus.\n\n    Returns the path of a new Session object.\n    '''\n\n    if 'Target' not in args or args['Target'].upper() != 'PBAP':\n        raise dbus.exceptions.DBusException(\n            'Non-PBAP targets are not currently supported by this python-dbusmock template.',\n            name=OBEX_MOCK_IFACE + '.Unsupported')\n\n    # Find the first unused session ID.\n    client_path = '/org/bluez/obex/client'\n    session_id = 0\n    while client_path + '/session' + str(session_id) in mockobject.objects:\n        session_id += 1\n\n    path = client_path + '/session' + str(session_id)\n    properties = {\n        'Source': dbus.String('FIXME', variant_level=1),\n        'Destination': dbus.String(destination, variant_level=1),\n        'Channel': dbus.Byte(0, variant_level=1),\n        'Target': dbus.String('FIXME', variant_level=1),\n        'Root': dbus.String('FIXME', variant_level=1),\n    }\n\n    self.AddObject(path,\n                   SESSION_IFACE,\n                   # Properties\n                   properties,\n                   # Methods\n                   [\n                       ('GetCapabilities', '', 's', ''),  # Currently a no-op\n                   ])\n\n    session = mockobject.objects[path]\n    session.AddMethods(PHONEBOOK_ACCESS_IFACE, [\n        ('Select', 'ss', '', ''),  # Currently a no-op\n        # Currently a no-op\n        ('List', 'a{sv}', 'a(ss)', 'ret = dbus.Array(signature=\"(ss)\")'),\n        # Currently a no-op\n        ('ListFilterFields', '', 'as', 'ret = dbus.Array(signature=\"(s)\")'),\n        ('PullAll', 'sa{sv}', 'sa{sv}', PullAll),\n        ('GetSize', '', 'q', 'ret = 0'),  # TODO\n    ])\n\n    manager = mockobject.objects['/']\n    manager.EmitSignal(OBJECT_MANAGER_IFACE, 'InterfacesAdded',\n                       'oa{sa{sv}}', [\n                           dbus.ObjectPath(path),\n                           {SESSION_IFACE: properties},\n                       ])\n\n    return path", "code_tokens": ["def", "CreateSession", "(", "self", ",", "destination", ",", "args", ")", ":", "if", "'Target'", "not", "in", "args", "or", "args", "[", "'Target'", "]", ".", "upper", "(", ")", "!=", "'PBAP'", ":", "raise", "dbus", ".", "exceptions", ".", "DBusException", "(", "'Non-PBAP targets are not currently supported by this python-dbusmock template.'", ",", "name", "=", "OBEX_MOCK_IFACE", "+", "'.Unsupported'", ")", "# Find the first unused session ID.", "client_path", "=", "'/org/bluez/obex/client'", "session_id", "=", "0", "while", "client_path", "+", "'/session'", "+", "str", "(", "session_id", ")", "in", "mockobject", ".", "objects", ":", "session_id", "+=", "1", "path", "=", "client_path", "+", "'/session'", "+", "str", "(", "session_id", ")", "properties", "=", "{", "'Source'", ":", "dbus", ".", "String", "(", "'FIXME'", ",", "variant_level", "=", "1", ")", ",", "'Destination'", ":", "dbus", ".", "String", "(", "destination", ",", "variant_level", "=", "1", ")", ",", "'Channel'", ":", "dbus", ".", "Byte", "(", "0", ",", "variant_level", "=", "1", ")", ",", "'Target'", ":", "dbus", ".", "String", "(", "'FIXME'", ",", "variant_level", "=", "1", ")", ",", "'Root'", ":", "dbus", ".", "String", "(", "'FIXME'", ",", "variant_level", "=", "1", ")", ",", "}", "self", ".", "AddObject", "(", "path", ",", "SESSION_IFACE", ",", "# Properties", "properties", ",", "# Methods", "[", "(", "'GetCapabilities'", ",", "''", ",", "'s'", ",", "''", ")", ",", "# Currently a no-op", "]", ")", "session", "=", "mockobject", ".", "objects", "[", "path", "]", "session", ".", "AddMethods", "(", "PHONEBOOK_ACCESS_IFACE", ",", "[", "(", "'Select'", ",", "'ss'", ",", "''", ",", "''", ")", ",", "# Currently a no-op", "# Currently a no-op", "(", "'List'", ",", "'a{sv}'", ",", "'a(ss)'", ",", "'ret = dbus.Array(signature=\"(ss)\")'", ")", ",", "# Currently a no-op", "(", "'ListFilterFields'", ",", "''", ",", "'as'", ",", "'ret = dbus.Array(signature=\"(s)\")'", ")", ",", "(", "'PullAll'", ",", "'sa{sv}'", ",", "'sa{sv}'", ",", "PullAll", ")", ",", "(", "'GetSize'", ",", "''", ",", "'q'", ",", "'ret = 0'", ")", ",", "# TODO", "]", ")", "manager", "=", "mockobject", ".", "objects", "[", "'/'", "]", "manager", ".", "EmitSignal", "(", "OBJECT_MANAGER_IFACE", ",", "'InterfacesAdded'", ",", "'oa{sa{sv}}'", ",", "[", "dbus", ".", "ObjectPath", "(", "path", ")", ",", "{", "SESSION_IFACE", ":", "properties", "}", ",", "]", ")", "return", "path"], "docstring": "OBEX method to create a new transfer session.\n\n    The destination must be the address of the destination Bluetooth device.\n    The given arguments must be a map from well-known keys to values,\n    containing at least the \u2018Target\u2019 key, whose value must be \u2018PBAP\u2019 (other\n    keys and values are accepted by the real daemon, but not by this mock\n    daemon at the moment). If the target is missing or incorrect, an\n    Unsupported error is returned on the bus.\n\n    Returns the path of a new Session object.", "docstring_tokens": ["OBEX", "method", "to", "create", "a", "new", "transfer", "session", "."], "sha": "26f65f78bc0ed347233f699a8d6ee0e6880e7eb0", "url": "https://github.com/martinpitt/python-dbusmock/blob/26f65f78bc0ed347233f699a8d6ee0e6880e7eb0/dbusmock/templates/bluez5-obex.py#L58-L118", "partition": "train"}
{"repo": "briancappello/flask-unchained", "path": "flask_unchained/bundles/security/views/security_controller.py", "func_name": "SecurityController.change_password", "original_string": "def change_password(self):\n        \"\"\"\n        View function for a user to change their password.\n        Supports html and json requests.\n        \"\"\"\n        form = self._get_form('SECURITY_CHANGE_PASSWORD_FORM')\n        if form.validate_on_submit():\n            self.security_service.change_password(\n                current_user._get_current_object(),\n                form.new_password.data)\n            self.after_this_request(self._commit)\n            self.flash(_('flask_unchained.bundles.security:flash.password_change'),\n                       category='success')\n            if request.is_json:\n                return self.jsonify({'token': current_user.get_auth_token()})\n            return self.redirect('SECURITY_POST_CHANGE_REDIRECT_ENDPOINT',\n                                 'SECURITY_POST_LOGIN_REDIRECT_ENDPOINT')\n\n        elif form.errors and request.is_json:\n            return self.errors(form.errors)\n\n        return self.render('change_password',\n                           change_password_form=form,\n                           **self.security.run_ctx_processor('change_password'))", "language": "python", "code": "def change_password(self):\n        \"\"\"\n        View function for a user to change their password.\n        Supports html and json requests.\n        \"\"\"\n        form = self._get_form('SECURITY_CHANGE_PASSWORD_FORM')\n        if form.validate_on_submit():\n            self.security_service.change_password(\n                current_user._get_current_object(),\n                form.new_password.data)\n            self.after_this_request(self._commit)\n            self.flash(_('flask_unchained.bundles.security:flash.password_change'),\n                       category='success')\n            if request.is_json:\n                return self.jsonify({'token': current_user.get_auth_token()})\n            return self.redirect('SECURITY_POST_CHANGE_REDIRECT_ENDPOINT',\n                                 'SECURITY_POST_LOGIN_REDIRECT_ENDPOINT')\n\n        elif form.errors and request.is_json:\n            return self.errors(form.errors)\n\n        return self.render('change_password',\n                           change_password_form=form,\n                           **self.security.run_ctx_processor('change_password'))", "code_tokens": ["def", "change_password", "(", "self", ")", ":", "form", "=", "self", ".", "_get_form", "(", "'SECURITY_CHANGE_PASSWORD_FORM'", ")", "if", "form", ".", "validate_on_submit", "(", ")", ":", "self", ".", "security_service", ".", "change_password", "(", "current_user", ".", "_get_current_object", "(", ")", ",", "form", ".", "new_password", ".", "data", ")", "self", ".", "after_this_request", "(", "self", ".", "_commit", ")", "self", ".", "flash", "(", "_", "(", "'flask_unchained.bundles.security:flash.password_change'", ")", ",", "category", "=", "'success'", ")", "if", "request", ".", "is_json", ":", "return", "self", ".", "jsonify", "(", "{", "'token'", ":", "current_user", ".", "get_auth_token", "(", ")", "}", ")", "return", "self", ".", "redirect", "(", "'SECURITY_POST_CHANGE_REDIRECT_ENDPOINT'", ",", "'SECURITY_POST_LOGIN_REDIRECT_ENDPOINT'", ")", "elif", "form", ".", "errors", "and", "request", ".", "is_json", ":", "return", "self", ".", "errors", "(", "form", ".", "errors", ")", "return", "self", ".", "render", "(", "'change_password'", ",", "change_password_form", "=", "form", ",", "*", "*", "self", ".", "security", ".", "run_ctx_processor", "(", "'change_password'", ")", ")"], "docstring": "View function for a user to change their password.\n        Supports html and json requests.", "docstring_tokens": ["View", "function", "for", "a", "user", "to", "change", "their", "password", ".", "Supports", "html", "and", "json", "requests", "."], "sha": "4d536cb90e2cc4829c1c05f2c74d3e22901a1399", "url": "https://github.com/briancappello/flask-unchained/blob/4d536cb90e2cc4829c1c05f2c74d3e22901a1399/flask_unchained/bundles/security/views/security_controller.py#L247-L270", "partition": "train"}
{"repo": "briancappello/flask-unchained", "path": "flask_unchained/bundles/security/extensions/security.py", "func_name": "Security._get_hashing_context", "original_string": "def _get_hashing_context(self, app: FlaskUnchained) -> CryptContext:\n        \"\"\"\n        Get the token hashing (and verifying) context.\n        \"\"\"\n        return CryptContext(schemes=app.config.SECURITY_HASHING_SCHEMES,\n                            deprecated=app.config.SECURITY_DEPRECATED_HASHING_SCHEMES)", "language": "python", "code": "def _get_hashing_context(self, app: FlaskUnchained) -> CryptContext:\n        \"\"\"\n        Get the token hashing (and verifying) context.\n        \"\"\"\n        return CryptContext(schemes=app.config.SECURITY_HASHING_SCHEMES,\n                            deprecated=app.config.SECURITY_DEPRECATED_HASHING_SCHEMES)", "code_tokens": ["def", "_get_hashing_context", "(", "self", ",", "app", ":", "FlaskUnchained", ")", "->", "CryptContext", ":", "return", "CryptContext", "(", "schemes", "=", "app", ".", "config", ".", "SECURITY_HASHING_SCHEMES", ",", "deprecated", "=", "app", ".", "config", ".", "SECURITY_DEPRECATED_HASHING_SCHEMES", ")"], "docstring": "Get the token hashing (and verifying) context.", "docstring_tokens": ["Get", "the", "token", "hashing", "(", "and", "verifying", ")", "context", "."], "sha": "4d536cb90e2cc4829c1c05f2c74d3e22901a1399", "url": "https://github.com/briancappello/flask-unchained/blob/4d536cb90e2cc4829c1c05f2c74d3e22901a1399/flask_unchained/bundles/security/extensions/security.py#L175-L180", "partition": "train"}
{"repo": "briancappello/flask-unchained", "path": "flask_unchained/bundles/security/extensions/security.py", "func_name": "Security._get_login_manager", "original_string": "def _get_login_manager(self,\n                           app: FlaskUnchained,\n                           anonymous_user: AnonymousUser,\n                           ) -> LoginManager:\n        \"\"\"\n        Get an initialized instance of Flask Login's\n        :class:`~flask_login.LoginManager`.\n        \"\"\"\n        login_manager = LoginManager()\n        login_manager.anonymous_user = anonymous_user or AnonymousUser\n        login_manager.localize_callback = _\n        login_manager.request_loader(self._request_loader)\n        login_manager.user_loader(\n            lambda *a, **kw: self.security_utils_service.user_loader(*a, **kw))\n        login_manager.login_view = 'security_controller.login'\n        login_manager.login_message = _(\n            'flask_unchained.bundles.security:error.login_required')\n        login_manager.login_message_category = 'info'\n        login_manager.needs_refresh_message = _(\n            'flask_unchained.bundles.security:error.fresh_login_required')\n        login_manager.needs_refresh_message_category = 'info'\n        login_manager.init_app(app)\n        return login_manager", "language": "python", "code": "def _get_login_manager(self,\n                           app: FlaskUnchained,\n                           anonymous_user: AnonymousUser,\n                           ) -> LoginManager:\n        \"\"\"\n        Get an initialized instance of Flask Login's\n        :class:`~flask_login.LoginManager`.\n        \"\"\"\n        login_manager = LoginManager()\n        login_manager.anonymous_user = anonymous_user or AnonymousUser\n        login_manager.localize_callback = _\n        login_manager.request_loader(self._request_loader)\n        login_manager.user_loader(\n            lambda *a, **kw: self.security_utils_service.user_loader(*a, **kw))\n        login_manager.login_view = 'security_controller.login'\n        login_manager.login_message = _(\n            'flask_unchained.bundles.security:error.login_required')\n        login_manager.login_message_category = 'info'\n        login_manager.needs_refresh_message = _(\n            'flask_unchained.bundles.security:error.fresh_login_required')\n        login_manager.needs_refresh_message_category = 'info'\n        login_manager.init_app(app)\n        return login_manager", "code_tokens": ["def", "_get_login_manager", "(", "self", ",", "app", ":", "FlaskUnchained", ",", "anonymous_user", ":", "AnonymousUser", ",", ")", "->", "LoginManager", ":", "login_manager", "=", "LoginManager", "(", ")", "login_manager", ".", "anonymous_user", "=", "anonymous_user", "or", "AnonymousUser", "login_manager", ".", "localize_callback", "=", "_", "login_manager", ".", "request_loader", "(", "self", ".", "_request_loader", ")", "login_manager", ".", "user_loader", "(", "lambda", "*", "a", ",", "*", "*", "kw", ":", "self", ".", "security_utils_service", ".", "user_loader", "(", "*", "a", ",", "*", "*", "kw", ")", ")", "login_manager", ".", "login_view", "=", "'security_controller.login'", "login_manager", ".", "login_message", "=", "_", "(", "'flask_unchained.bundles.security:error.login_required'", ")", "login_manager", ".", "login_message_category", "=", "'info'", "login_manager", ".", "needs_refresh_message", "=", "_", "(", "'flask_unchained.bundles.security:error.fresh_login_required'", ")", "login_manager", ".", "needs_refresh_message_category", "=", "'info'", "login_manager", ".", "init_app", "(", "app", ")", "return", "login_manager"], "docstring": "Get an initialized instance of Flask Login's\n        :class:`~flask_login.LoginManager`.", "docstring_tokens": ["Get", "an", "initialized", "instance", "of", "Flask", "Login", "s", ":", "class", ":", "~flask_login", ".", "LoginManager", "."], "sha": "4d536cb90e2cc4829c1c05f2c74d3e22901a1399", "url": "https://github.com/briancappello/flask-unchained/blob/4d536cb90e2cc4829c1c05f2c74d3e22901a1399/flask_unchained/bundles/security/extensions/security.py#L182-L204", "partition": "train"}
{"repo": "grantmcconnaughey/Lintly", "path": "lintly/builds.py", "func_name": "LintlyBuild._post_status", "original_string": "def _post_status(self, state, description):\n        \"\"\"\n        Creates a GitHub status for this build's commit if enabled for the project.\n        :param state: The state of the status (pending, error, failure, success)\n        :param description: The description for the status\n        \"\"\"\n        if self.config.post_status:\n            logger.info('Commit statuses enabled')\n            if self.config.commit_sha:\n                logger.info('Posting {} status to commit SHA {}'.format(state, self.config.commit_sha))\n                self.git_client.post_status(\n                    state,\n                    description,\n                    sha=self.config.commit_sha\n                )\n            else:\n                logger.warning('Cannot post commit status because no commit SHA has been '\n                               'specified. Either use the --commit-sha CLI argument or '\n                               'set the LINTLY_COMMIT_SHA environment variable.')\n        else:\n            logger.info('Commit statuses disabled')", "language": "python", "code": "def _post_status(self, state, description):\n        \"\"\"\n        Creates a GitHub status for this build's commit if enabled for the project.\n        :param state: The state of the status (pending, error, failure, success)\n        :param description: The description for the status\n        \"\"\"\n        if self.config.post_status:\n            logger.info('Commit statuses enabled')\n            if self.config.commit_sha:\n                logger.info('Posting {} status to commit SHA {}'.format(state, self.config.commit_sha))\n                self.git_client.post_status(\n                    state,\n                    description,\n                    sha=self.config.commit_sha\n                )\n            else:\n                logger.warning('Cannot post commit status because no commit SHA has been '\n                               'specified. Either use the --commit-sha CLI argument or '\n                               'set the LINTLY_COMMIT_SHA environment variable.')\n        else:\n            logger.info('Commit statuses disabled')", "code_tokens": ["def", "_post_status", "(", "self", ",", "state", ",", "description", ")", ":", "if", "self", ".", "config", ".", "post_status", ":", "logger", ".", "info", "(", "'Commit statuses enabled'", ")", "if", "self", ".", "config", ".", "commit_sha", ":", "logger", ".", "info", "(", "'Posting {} status to commit SHA {}'", ".", "format", "(", "state", ",", "self", ".", "config", ".", "commit_sha", ")", ")", "self", ".", "git_client", ".", "post_status", "(", "state", ",", "description", ",", "sha", "=", "self", ".", "config", ".", "commit_sha", ")", "else", ":", "logger", ".", "warning", "(", "'Cannot post commit status because no commit SHA has been '", "'specified. Either use the --commit-sha CLI argument or '", "'set the LINTLY_COMMIT_SHA environment variable.'", ")", "else", ":", "logger", ".", "info", "(", "'Commit statuses disabled'", ")"], "docstring": "Creates a GitHub status for this build's commit if enabled for the project.\n        :param state: The state of the status (pending, error, failure, success)\n        :param description: The description for the status", "docstring_tokens": ["Creates", "a", "GitHub", "status", "for", "this", "build", "s", "commit", "if", "enabled", "for", "the", "project", ".", ":", "param", "state", ":", "The", "state", "of", "the", "status", "(", "pending", "error", "failure", "success", ")", ":", "param", "description", ":", "The", "description", "for", "the", "status"], "sha": "73c1ee36740ac5bb2a32d3f24fca2a27f4d4e466", "url": "https://github.com/grantmcconnaughey/Lintly/blob/73c1ee36740ac5bb2a32d3f24fca2a27f4d4e466/lintly/builds.py#L145-L165", "partition": "train"}
{"repo": "alexprengere/FormalSystems", "path": "formalsystems/leplparsing.py", "func_name": "reg_to_lex", "original_string": "def reg_to_lex(conditions, wildcards):\n    \"\"\"Transform a regular expression into a LEPL object.\n\n    Replace the wildcards in the conditions by LEPL elements,\n    like xM will be replaced by Any() & 'M'.\n    In case of multiple same wildcards (like xMx), aliases\n    are created to allow the regexp to compile, like\n    Any() > 'x_0' & 'M' & Any() > 'x_1', and we chech that the matched values\n    for all aliases like x_0, x_1 are the same.\n    \"\"\"\n    aliases = defaultdict(set)\n    n_conds = []\n\n    # All conditions\n    for i, _ in enumerate(conditions):\n        n_cond = []\n\n        for char in conditions[i]:\n            if char in wildcards:\n                alias = '%s_%s' % (char, len(aliases[char]))\n                aliases[char].add(alias)\n                n_cond.append(make_token(alias, reg=wildcards[char]))\n            else:\n                n_cond.append(~Literal(char))\n\n        n_cond.append(Eos())\n        n_conds.append(reduce(operator.and_, n_cond) > make_dict)\n\n    return tuple(n_conds), aliases", "language": "python", "code": "def reg_to_lex(conditions, wildcards):\n    \"\"\"Transform a regular expression into a LEPL object.\n\n    Replace the wildcards in the conditions by LEPL elements,\n    like xM will be replaced by Any() & 'M'.\n    In case of multiple same wildcards (like xMx), aliases\n    are created to allow the regexp to compile, like\n    Any() > 'x_0' & 'M' & Any() > 'x_1', and we chech that the matched values\n    for all aliases like x_0, x_1 are the same.\n    \"\"\"\n    aliases = defaultdict(set)\n    n_conds = []\n\n    # All conditions\n    for i, _ in enumerate(conditions):\n        n_cond = []\n\n        for char in conditions[i]:\n            if char in wildcards:\n                alias = '%s_%s' % (char, len(aliases[char]))\n                aliases[char].add(alias)\n                n_cond.append(make_token(alias, reg=wildcards[char]))\n            else:\n                n_cond.append(~Literal(char))\n\n        n_cond.append(Eos())\n        n_conds.append(reduce(operator.and_, n_cond) > make_dict)\n\n    return tuple(n_conds), aliases", "code_tokens": ["def", "reg_to_lex", "(", "conditions", ",", "wildcards", ")", ":", "aliases", "=", "defaultdict", "(", "set", ")", "n_conds", "=", "[", "]", "# All conditions", "for", "i", ",", "_", "in", "enumerate", "(", "conditions", ")", ":", "n_cond", "=", "[", "]", "for", "char", "in", "conditions", "[", "i", "]", ":", "if", "char", "in", "wildcards", ":", "alias", "=", "'%s_%s'", "%", "(", "char", ",", "len", "(", "aliases", "[", "char", "]", ")", ")", "aliases", "[", "char", "]", ".", "add", "(", "alias", ")", "n_cond", ".", "append", "(", "make_token", "(", "alias", ",", "reg", "=", "wildcards", "[", "char", "]", ")", ")", "else", ":", "n_cond", ".", "append", "(", "~", "Literal", "(", "char", ")", ")", "n_cond", ".", "append", "(", "Eos", "(", ")", ")", "n_conds", ".", "append", "(", "reduce", "(", "operator", ".", "and_", ",", "n_cond", ")", ">", "make_dict", ")", "return", "tuple", "(", "n_conds", ")", ",", "aliases"], "docstring": "Transform a regular expression into a LEPL object.\n\n    Replace the wildcards in the conditions by LEPL elements,\n    like xM will be replaced by Any() & 'M'.\n    In case of multiple same wildcards (like xMx), aliases\n    are created to allow the regexp to compile, like\n    Any() > 'x_0' & 'M' & Any() > 'x_1', and we chech that the matched values\n    for all aliases like x_0, x_1 are the same.", "docstring_tokens": ["Transform", "a", "regular", "expression", "into", "a", "LEPL", "object", "."], "sha": "e46d9cc6f8dc076e9dc86f6f8511fc6f3aa95f6e", "url": "https://github.com/alexprengere/FormalSystems/blob/e46d9cc6f8dc076e9dc86f6f8511fc6f3aa95f6e/formalsystems/leplparsing.py#L11-L39", "partition": "train"}
{"repo": "grantmcconnaughey/Lintly", "path": "lintly/cli.py", "func_name": "main", "original_string": "def main(**options):\n    \"\"\"Slurp up linter output and send it to a GitHub PR review.\"\"\"\n    configure_logging(log_all=options.get('log'))\n\n    stdin_stream = click.get_text_stream('stdin')\n    stdin_text = stdin_stream.read()\n\n    click.echo(stdin_text)\n\n    ci = find_ci_provider()\n    config = Config(options, ci=ci)\n\n    build = LintlyBuild(config, stdin_text)\n    try:\n        build.execute()\n    except NotPullRequestException:\n        logger.info('Not a PR. Lintly is exiting.')\n        sys.exit(0)\n\n    # Exit with the number of files that have violations\n    sys.exit(len(build.violations))", "language": "python", "code": "def main(**options):\n    \"\"\"Slurp up linter output and send it to a GitHub PR review.\"\"\"\n    configure_logging(log_all=options.get('log'))\n\n    stdin_stream = click.get_text_stream('stdin')\n    stdin_text = stdin_stream.read()\n\n    click.echo(stdin_text)\n\n    ci = find_ci_provider()\n    config = Config(options, ci=ci)\n\n    build = LintlyBuild(config, stdin_text)\n    try:\n        build.execute()\n    except NotPullRequestException:\n        logger.info('Not a PR. Lintly is exiting.')\n        sys.exit(0)\n\n    # Exit with the number of files that have violations\n    sys.exit(len(build.violations))", "code_tokens": ["def", "main", "(", "*", "*", "options", ")", ":", "configure_logging", "(", "log_all", "=", "options", ".", "get", "(", "'log'", ")", ")", "stdin_stream", "=", "click", ".", "get_text_stream", "(", "'stdin'", ")", "stdin_text", "=", "stdin_stream", ".", "read", "(", ")", "click", ".", "echo", "(", "stdin_text", ")", "ci", "=", "find_ci_provider", "(", ")", "config", "=", "Config", "(", "options", ",", "ci", "=", "ci", ")", "build", "=", "LintlyBuild", "(", "config", ",", "stdin_text", ")", "try", ":", "build", ".", "execute", "(", ")", "except", "NotPullRequestException", ":", "logger", ".", "info", "(", "'Not a PR. Lintly is exiting.'", ")", "sys", ".", "exit", "(", "0", ")", "# Exit with the number of files that have violations", "sys", ".", "exit", "(", "len", "(", "build", ".", "violations", ")", ")"], "docstring": "Slurp up linter output and send it to a GitHub PR review.", "docstring_tokens": ["Slurp", "up", "linter", "output", "and", "send", "it", "to", "a", "GitHub", "PR", "review", "."], "sha": "73c1ee36740ac5bb2a32d3f24fca2a27f4d4e466", "url": "https://github.com/grantmcconnaughey/Lintly/blob/73c1ee36740ac5bb2a32d3f24fca2a27f4d4e466/lintly/cli.py#L53-L73", "partition": "train"}
{"repo": "grantmcconnaughey/Lintly", "path": "lintly/backends/gitlab.py", "func_name": "translate_gitlab_exception", "original_string": "def translate_gitlab_exception(func):\n    \"\"\"\n    Decorator to catch GitLab-specific exceptions and raise them as GitClientError exceptions.\n    \"\"\"\n\n    @functools.wraps(func)\n    def _wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except gitlab.GitlabError as e:\n            status_to_exception = {\n                401: UnauthorizedError,\n                404: NotFoundError,\n            }\n\n            exc_class = status_to_exception.get(e.response_code, GitClientError)\n            raise exc_class(str(e), status_code=e.response_code)\n\n    return _wrapper", "language": "python", "code": "def translate_gitlab_exception(func):\n    \"\"\"\n    Decorator to catch GitLab-specific exceptions and raise them as GitClientError exceptions.\n    \"\"\"\n\n    @functools.wraps(func)\n    def _wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except gitlab.GitlabError as e:\n            status_to_exception = {\n                401: UnauthorizedError,\n                404: NotFoundError,\n            }\n\n            exc_class = status_to_exception.get(e.response_code, GitClientError)\n            raise exc_class(str(e), status_code=e.response_code)\n\n    return _wrapper", "code_tokens": ["def", "translate_gitlab_exception", "(", "func", ")", ":", "@", "functools", ".", "wraps", "(", "func", ")", "def", "_wrapper", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "try", ":", "return", "func", "(", "*", "args", ",", "*", "*", "kwargs", ")", "except", "gitlab", ".", "GitlabError", "as", "e", ":", "status_to_exception", "=", "{", "401", ":", "UnauthorizedError", ",", "404", ":", "NotFoundError", ",", "}", "exc_class", "=", "status_to_exception", ".", "get", "(", "e", ".", "response_code", ",", "GitClientError", ")", "raise", "exc_class", "(", "str", "(", "e", ")", ",", "status_code", "=", "e", ".", "response_code", ")", "return", "_wrapper"], "docstring": "Decorator to catch GitLab-specific exceptions and raise them as GitClientError exceptions.", "docstring_tokens": ["Decorator", "to", "catch", "GitLab", "-", "specific", "exceptions", "and", "raise", "them", "as", "GitClientError", "exceptions", "."], "sha": "73c1ee36740ac5bb2a32d3f24fca2a27f4d4e466", "url": "https://github.com/grantmcconnaughey/Lintly/blob/73c1ee36740ac5bb2a32d3f24fca2a27f4d4e466/lintly/backends/gitlab.py#L29-L47", "partition": "train"}
{"repo": "ARMmbed/icetea", "path": "icetea_lib/Plugin/plugins/LocalAllocator/LocalAllocator.py", "func_name": "init_generic_serial_dut", "original_string": "def init_generic_serial_dut(contextlist, conf, index, args):\n    \"\"\"\n    Initializes a local hardware dut\n    \"\"\"\n    port = conf['serial_port']\n    baudrate = (args.baudrate if args.baudrate else conf.get(\n        \"application\", {}).get(\"baudrate\", 115200))\n    serial_config = {}\n    if args.serial_rtscts:\n        serial_config[\"serial_rtscts\"] = args.serial_rtscts\n    elif args.serial_xonxoff:\n        serial_config[\"serial_xonxoff\"] = args.serial_xonxoff\n\n    if args.serial_timeout:\n        serial_config[\"serial_timeout\"] = args.serial_timeout\n\n    ch_mode_config = {}\n    if args.serial_ch_size > 0:\n        ch_mode_config[\"ch_mode\"] = True\n        ch_mode_config[\"ch_mode_chunk_size\"] = args.serial_ch_size\n    elif args.serial_ch_size is 0:\n        ch_mode_config[\"ch_mode\"] = False\n\n    if args.ch_mode_ch_delay:\n        ch_mode_config[\"ch_mode_ch_delay\"] = args.ch_mode_ch_delay\n    dut = DutSerial(name=\"D%d\" % index, port=port, baudrate=baudrate,\n                    config=conf, ch_mode_config=ch_mode_config,\n                    serial_config=serial_config, params=args)\n    dut.index = index\n\n    dut.platform = conf.get(\"platform_name\", \"serial\")\n\n    msg = 'Use device in serial port {} as D{}'\n    contextlist.logger.info(msg.format(port,\n                                       index))\n\n    contextlist.duts.append(dut)\n    contextlist.dutinformations.append(dut.get_info())", "language": "python", "code": "def init_generic_serial_dut(contextlist, conf, index, args):\n    \"\"\"\n    Initializes a local hardware dut\n    \"\"\"\n    port = conf['serial_port']\n    baudrate = (args.baudrate if args.baudrate else conf.get(\n        \"application\", {}).get(\"baudrate\", 115200))\n    serial_config = {}\n    if args.serial_rtscts:\n        serial_config[\"serial_rtscts\"] = args.serial_rtscts\n    elif args.serial_xonxoff:\n        serial_config[\"serial_xonxoff\"] = args.serial_xonxoff\n\n    if args.serial_timeout:\n        serial_config[\"serial_timeout\"] = args.serial_timeout\n\n    ch_mode_config = {}\n    if args.serial_ch_size > 0:\n        ch_mode_config[\"ch_mode\"] = True\n        ch_mode_config[\"ch_mode_chunk_size\"] = args.serial_ch_size\n    elif args.serial_ch_size is 0:\n        ch_mode_config[\"ch_mode\"] = False\n\n    if args.ch_mode_ch_delay:\n        ch_mode_config[\"ch_mode_ch_delay\"] = args.ch_mode_ch_delay\n    dut = DutSerial(name=\"D%d\" % index, port=port, baudrate=baudrate,\n                    config=conf, ch_mode_config=ch_mode_config,\n                    serial_config=serial_config, params=args)\n    dut.index = index\n\n    dut.platform = conf.get(\"platform_name\", \"serial\")\n\n    msg = 'Use device in serial port {} as D{}'\n    contextlist.logger.info(msg.format(port,\n                                       index))\n\n    contextlist.duts.append(dut)\n    contextlist.dutinformations.append(dut.get_info())", "code_tokens": ["def", "init_generic_serial_dut", "(", "contextlist", ",", "conf", ",", "index", ",", "args", ")", ":", "port", "=", "conf", "[", "'serial_port'", "]", "baudrate", "=", "(", "args", ".", "baudrate", "if", "args", ".", "baudrate", "else", "conf", ".", "get", "(", "\"application\"", ",", "{", "}", ")", ".", "get", "(", "\"baudrate\"", ",", "115200", ")", ")", "serial_config", "=", "{", "}", "if", "args", ".", "serial_rtscts", ":", "serial_config", "[", "\"serial_rtscts\"", "]", "=", "args", ".", "serial_rtscts", "elif", "args", ".", "serial_xonxoff", ":", "serial_config", "[", "\"serial_xonxoff\"", "]", "=", "args", ".", "serial_xonxoff", "if", "args", ".", "serial_timeout", ":", "serial_config", "[", "\"serial_timeout\"", "]", "=", "args", ".", "serial_timeout", "ch_mode_config", "=", "{", "}", "if", "args", ".", "serial_ch_size", ">", "0", ":", "ch_mode_config", "[", "\"ch_mode\"", "]", "=", "True", "ch_mode_config", "[", "\"ch_mode_chunk_size\"", "]", "=", "args", ".", "serial_ch_size", "elif", "args", ".", "serial_ch_size", "is", "0", ":", "ch_mode_config", "[", "\"ch_mode\"", "]", "=", "False", "if", "args", ".", "ch_mode_ch_delay", ":", "ch_mode_config", "[", "\"ch_mode_ch_delay\"", "]", "=", "args", ".", "ch_mode_ch_delay", "dut", "=", "DutSerial", "(", "name", "=", "\"D%d\"", "%", "index", ",", "port", "=", "port", ",", "baudrate", "=", "baudrate", ",", "config", "=", "conf", ",", "ch_mode_config", "=", "ch_mode_config", ",", "serial_config", "=", "serial_config", ",", "params", "=", "args", ")", "dut", ".", "index", "=", "index", "dut", ".", "platform", "=", "conf", ".", "get", "(", "\"platform_name\"", ",", "\"serial\"", ")", "msg", "=", "'Use device in serial port {} as D{}'", "contextlist", ".", "logger", ".", "info", "(", "msg", ".", "format", "(", "port", ",", "index", ")", ")", "contextlist", ".", "duts", ".", "append", "(", "dut", ")", "contextlist", ".", "dutinformations", ".", "append", "(", "dut", ".", "get_info", "(", ")", ")"], "docstring": "Initializes a local hardware dut", "docstring_tokens": ["Initializes", "a", "local", "hardware", "dut"], "sha": "b2b97ac607429830cf7d62dae2e3903692c7c778", "url": "https://github.com/ARMmbed/icetea/blob/b2b97ac607429830cf7d62dae2e3903692c7c778/icetea_lib/Plugin/plugins/LocalAllocator/LocalAllocator.py#L35-L72", "partition": "train"}
{"repo": "ARMmbed/icetea", "path": "icetea_lib/Plugin/plugins/LocalAllocator/LocalAllocator.py", "func_name": "init_mbed_dut", "original_string": "def init_mbed_dut(contextlist, conf, index, args):\n    \"\"\"\n    Initializes a local hardware dut\n    \"\"\"\n    binary = None\n    try:\n        binary = conf[\"application\"]['bin']\n    except KeyError:\n        pass\n\n    dev = conf.get('allocated', None)\n    if not dev:\n        raise ResourceInitError(\"Allocated device not found.\")\n\n    port = dev['serial_port']\n    baudrate = (args.baudrate if args.baudrate else\n                conf.get(\"application\", {}).get(\"baudrate\", 115200))\n    serial_config = {}\n    if args.serial_rtscts:\n        serial_config[\"serial_rtscts\"] = args.serial_rtscts\n    elif args.serial_xonxoff:\n        serial_config[\"serial_xonxoff\"] = args.serial_xonxoff\n\n    if args.serial_timeout:\n        serial_config[\"serial_timeout\"] = args.serial_timeout\n\n    ch_mode_config = {}\n    if args.serial_ch_size > 0:\n        ch_mode_config[\"ch_mode\"] = True\n        ch_mode_config[\"ch_mode_chunk_size\"] = args.serial_ch_size\n    elif args.serial_ch_size is 0:\n        ch_mode_config[\"ch_mode\"] = False\n\n    if args.ch_mode_ch_delay:\n        ch_mode_config[\"ch_mode_ch_delay\"] = args.ch_mode_ch_delay\n\n    dut = DutMbed(name=\"D%d\" % index, port=port, baudrate=baudrate,\n                  config=conf, ch_mode_config=ch_mode_config,\n                  serial_config=serial_config, params=args)\n    dut.index = index\n\n    # If something to flash, get the allocated device and flash it\n    if binary and not args.skip_flash:\n        if contextlist.check_flashing_need('hardware',\n                                           binary,\n                                           args.forceflash):\n            if dut.flash(binary_location=binary, forceflash=args.forceflash):\n                contextlist.logger.info('flash ready')\n            else:\n                dut.close_dut(False)\n                dut.close_connection()\n                dut = None\n                raise ResourceInitError(\"Dut flashing failed!\")\n\n    dut.platform = dev[\"platform_name\"]\n\n    msg = 'Use board {} as \"{}\" (id: {})'\n    contextlist.logger.info(msg.format(dev['platform_name'],\n                                       dut.get_dut_name(),\n                                       dev['target_id']))\n\n    contextlist.duts.append(dut)\n    contextlist.dutinformations.append(dut.get_info())", "language": "python", "code": "def init_mbed_dut(contextlist, conf, index, args):\n    \"\"\"\n    Initializes a local hardware dut\n    \"\"\"\n    binary = None\n    try:\n        binary = conf[\"application\"]['bin']\n    except KeyError:\n        pass\n\n    dev = conf.get('allocated', None)\n    if not dev:\n        raise ResourceInitError(\"Allocated device not found.\")\n\n    port = dev['serial_port']\n    baudrate = (args.baudrate if args.baudrate else\n                conf.get(\"application\", {}).get(\"baudrate\", 115200))\n    serial_config = {}\n    if args.serial_rtscts:\n        serial_config[\"serial_rtscts\"] = args.serial_rtscts\n    elif args.serial_xonxoff:\n        serial_config[\"serial_xonxoff\"] = args.serial_xonxoff\n\n    if args.serial_timeout:\n        serial_config[\"serial_timeout\"] = args.serial_timeout\n\n    ch_mode_config = {}\n    if args.serial_ch_size > 0:\n        ch_mode_config[\"ch_mode\"] = True\n        ch_mode_config[\"ch_mode_chunk_size\"] = args.serial_ch_size\n    elif args.serial_ch_size is 0:\n        ch_mode_config[\"ch_mode\"] = False\n\n    if args.ch_mode_ch_delay:\n        ch_mode_config[\"ch_mode_ch_delay\"] = args.ch_mode_ch_delay\n\n    dut = DutMbed(name=\"D%d\" % index, port=port, baudrate=baudrate,\n                  config=conf, ch_mode_config=ch_mode_config,\n                  serial_config=serial_config, params=args)\n    dut.index = index\n\n    # If something to flash, get the allocated device and flash it\n    if binary and not args.skip_flash:\n        if contextlist.check_flashing_need('hardware',\n                                           binary,\n                                           args.forceflash):\n            if dut.flash(binary_location=binary, forceflash=args.forceflash):\n                contextlist.logger.info('flash ready')\n            else:\n                dut.close_dut(False)\n                dut.close_connection()\n                dut = None\n                raise ResourceInitError(\"Dut flashing failed!\")\n\n    dut.platform = dev[\"platform_name\"]\n\n    msg = 'Use board {} as \"{}\" (id: {})'\n    contextlist.logger.info(msg.format(dev['platform_name'],\n                                       dut.get_dut_name(),\n                                       dev['target_id']))\n\n    contextlist.duts.append(dut)\n    contextlist.dutinformations.append(dut.get_info())", "code_tokens": ["def", "init_mbed_dut", "(", "contextlist", ",", "conf", ",", "index", ",", "args", ")", ":", "binary", "=", "None", "try", ":", "binary", "=", "conf", "[", "\"application\"", "]", "[", "'bin'", "]", "except", "KeyError", ":", "pass", "dev", "=", "conf", ".", "get", "(", "'allocated'", ",", "None", ")", "if", "not", "dev", ":", "raise", "ResourceInitError", "(", "\"Allocated device not found.\"", ")", "port", "=", "dev", "[", "'serial_port'", "]", "baudrate", "=", "(", "args", ".", "baudrate", "if", "args", ".", "baudrate", "else", "conf", ".", "get", "(", "\"application\"", ",", "{", "}", ")", ".", "get", "(", "\"baudrate\"", ",", "115200", ")", ")", "serial_config", "=", "{", "}", "if", "args", ".", "serial_rtscts", ":", "serial_config", "[", "\"serial_rtscts\"", "]", "=", "args", ".", "serial_rtscts", "elif", "args", ".", "serial_xonxoff", ":", "serial_config", "[", "\"serial_xonxoff\"", "]", "=", "args", ".", "serial_xonxoff", "if", "args", ".", "serial_timeout", ":", "serial_config", "[", "\"serial_timeout\"", "]", "=", "args", ".", "serial_timeout", "ch_mode_config", "=", "{", "}", "if", "args", ".", "serial_ch_size", ">", "0", ":", "ch_mode_config", "[", "\"ch_mode\"", "]", "=", "True", "ch_mode_config", "[", "\"ch_mode_chunk_size\"", "]", "=", "args", ".", "serial_ch_size", "elif", "args", ".", "serial_ch_size", "is", "0", ":", "ch_mode_config", "[", "\"ch_mode\"", "]", "=", "False", "if", "args", ".", "ch_mode_ch_delay", ":", "ch_mode_config", "[", "\"ch_mode_ch_delay\"", "]", "=", "args", ".", "ch_mode_ch_delay", "dut", "=", "DutMbed", "(", "name", "=", "\"D%d\"", "%", "index", ",", "port", "=", "port", ",", "baudrate", "=", "baudrate", ",", "config", "=", "conf", ",", "ch_mode_config", "=", "ch_mode_config", ",", "serial_config", "=", "serial_config", ",", "params", "=", "args", ")", "dut", ".", "index", "=", "index", "# If something to flash, get the allocated device and flash it", "if", "binary", "and", "not", "args", ".", "skip_flash", ":", "if", "contextlist", ".", "check_flashing_need", "(", "'hardware'", ",", "binary", ",", "args", ".", "forceflash", ")", ":", "if", "dut", ".", "flash", "(", "binary_location", "=", "binary", ",", "forceflash", "=", "args", ".", "forceflash", ")", ":", "contextlist", ".", "logger", ".", "info", "(", "'flash ready'", ")", "else", ":", "dut", ".", "close_dut", "(", "False", ")", "dut", ".", "close_connection", "(", ")", "dut", "=", "None", "raise", "ResourceInitError", "(", "\"Dut flashing failed!\"", ")", "dut", ".", "platform", "=", "dev", "[", "\"platform_name\"", "]", "msg", "=", "'Use board {} as \"{}\" (id: {})'", "contextlist", ".", "logger", ".", "info", "(", "msg", ".", "format", "(", "dev", "[", "'platform_name'", "]", ",", "dut", ".", "get_dut_name", "(", ")", ",", "dev", "[", "'target_id'", "]", ")", ")", "contextlist", ".", "duts", ".", "append", "(", "dut", ")", "contextlist", ".", "dutinformations", ".", "append", "(", "dut", ".", "get_info", "(", ")", ")"], "docstring": "Initializes a local hardware dut", "docstring_tokens": ["Initializes", "a", "local", "hardware", "dut"], "sha": "b2b97ac607429830cf7d62dae2e3903692c7c778", "url": "https://github.com/ARMmbed/icetea/blob/b2b97ac607429830cf7d62dae2e3903692c7c778/icetea_lib/Plugin/plugins/LocalAllocator/LocalAllocator.py#L75-L137", "partition": "train"}
{"repo": "ARMmbed/icetea", "path": "icetea_lib/Plugin/plugins/LocalAllocator/LocalAllocator.py", "func_name": "init_process_dut", "original_string": "def init_process_dut(contextlist, conf, index, args):\n    \"\"\"\n    Initialize process type Dut as DutProcess or DutConsole.\n    \"\"\"\n    if \"subtype\" in conf and conf[\"subtype\"]:\n        if conf[\"subtype\"] != \"console\":\n            msg = \"Unrecognized process subtype: {}\"\n            contextlist.logger.error(msg.format(conf[\"subtype\"]))\n            raise ResourceInitError(\"Unrecognized process subtype: {}\")\n        # This is a specialized 'console' process\n        config = None\n        if \"application\" in conf:\n            config = conf[\"application\"]\n        contextlist.logger.debug(\"Starting a remote console\")\n        dut = DutConsole(name=\"D%d\" % index, conf=config, params=args)\n        dut.index = index\n    else:\n        binary = conf[\"application\"]['bin']\n        app_config = conf[\"application\"]\n        init_cli_cmds = app_config.get(\"init_cli_cmds\", None)\n        post_cli_cmds = app_config.get(\"post_cli_cmds\", None)\n        contextlist.logger.debug(\"Starting process '%s'\" % binary)\n        dut = DutProcess(name=\"D%d\" % index, config=conf, params=args)\n        dut.index = index\n        dut.command = binary\n        if args.valgrind:\n            dut.use_valgrind(args.valgrind_tool,\n                             not args.valgrind_text,\n                             args.valgrind_console,\n                             args.valgrind_track_origins,\n                             args.valgrind_extra_params)\n        if args.gdb == index:\n            dut.use_gdb()\n            contextlist.logger.info(\"GDB is activated for node %i\" % index)\n        if args.gdbs == index:\n            dut.use_gdbs(True, args.gdbs_port)\n            contextlist.logger.info(\"GDBserver is activated for node %i\" % index)\n        if args.vgdb == index:\n            dut.use_vgdb()\n            contextlist.logger.info(\"VGDB is activated for node %i\" % index)\n        if args.nobuf:\n            dut.no_std_buf()\n\n        if init_cli_cmds is not None:\n            dut.set_init_cli_cmds(init_cli_cmds)\n        if post_cli_cmds is not None:\n            dut.set_post_cli_cmds(post_cli_cmds)\n\n    contextlist.duts.append(dut)\n    contextlist.dutinformations.append(dut.get_info())", "language": "python", "code": "def init_process_dut(contextlist, conf, index, args):\n    \"\"\"\n    Initialize process type Dut as DutProcess or DutConsole.\n    \"\"\"\n    if \"subtype\" in conf and conf[\"subtype\"]:\n        if conf[\"subtype\"] != \"console\":\n            msg = \"Unrecognized process subtype: {}\"\n            contextlist.logger.error(msg.format(conf[\"subtype\"]))\n            raise ResourceInitError(\"Unrecognized process subtype: {}\")\n        # This is a specialized 'console' process\n        config = None\n        if \"application\" in conf:\n            config = conf[\"application\"]\n        contextlist.logger.debug(\"Starting a remote console\")\n        dut = DutConsole(name=\"D%d\" % index, conf=config, params=args)\n        dut.index = index\n    else:\n        binary = conf[\"application\"]['bin']\n        app_config = conf[\"application\"]\n        init_cli_cmds = app_config.get(\"init_cli_cmds\", None)\n        post_cli_cmds = app_config.get(\"post_cli_cmds\", None)\n        contextlist.logger.debug(\"Starting process '%s'\" % binary)\n        dut = DutProcess(name=\"D%d\" % index, config=conf, params=args)\n        dut.index = index\n        dut.command = binary\n        if args.valgrind:\n            dut.use_valgrind(args.valgrind_tool,\n                             not args.valgrind_text,\n                             args.valgrind_console,\n                             args.valgrind_track_origins,\n                             args.valgrind_extra_params)\n        if args.gdb == index:\n            dut.use_gdb()\n            contextlist.logger.info(\"GDB is activated for node %i\" % index)\n        if args.gdbs == index:\n            dut.use_gdbs(True, args.gdbs_port)\n            contextlist.logger.info(\"GDBserver is activated for node %i\" % index)\n        if args.vgdb == index:\n            dut.use_vgdb()\n            contextlist.logger.info(\"VGDB is activated for node %i\" % index)\n        if args.nobuf:\n            dut.no_std_buf()\n\n        if init_cli_cmds is not None:\n            dut.set_init_cli_cmds(init_cli_cmds)\n        if post_cli_cmds is not None:\n            dut.set_post_cli_cmds(post_cli_cmds)\n\n    contextlist.duts.append(dut)\n    contextlist.dutinformations.append(dut.get_info())", "code_tokens": ["def", "init_process_dut", "(", "contextlist", ",", "conf", ",", "index", ",", "args", ")", ":", "if", "\"subtype\"", "in", "conf", "and", "conf", "[", "\"subtype\"", "]", ":", "if", "conf", "[", "\"subtype\"", "]", "!=", "\"console\"", ":", "msg", "=", "\"Unrecognized process subtype: {}\"", "contextlist", ".", "logger", ".", "error", "(", "msg", ".", "format", "(", "conf", "[", "\"subtype\"", "]", ")", ")", "raise", "ResourceInitError", "(", "\"Unrecognized process subtype: {}\"", ")", "# This is a specialized 'console' process", "config", "=", "None", "if", "\"application\"", "in", "conf", ":", "config", "=", "conf", "[", "\"application\"", "]", "contextlist", ".", "logger", ".", "debug", "(", "\"Starting a remote console\"", ")", "dut", "=", "DutConsole", "(", "name", "=", "\"D%d\"", "%", "index", ",", "conf", "=", "config", ",", "params", "=", "args", ")", "dut", ".", "index", "=", "index", "else", ":", "binary", "=", "conf", "[", "\"application\"", "]", "[", "'bin'", "]", "app_config", "=", "conf", "[", "\"application\"", "]", "init_cli_cmds", "=", "app_config", ".", "get", "(", "\"init_cli_cmds\"", ",", "None", ")", "post_cli_cmds", "=", "app_config", ".", "get", "(", "\"post_cli_cmds\"", ",", "None", ")", "contextlist", ".", "logger", ".", "debug", "(", "\"Starting process '%s'\"", "%", "binary", ")", "dut", "=", "DutProcess", "(", "name", "=", "\"D%d\"", "%", "index", ",", "config", "=", "conf", ",", "params", "=", "args", ")", "dut", ".", "index", "=", "index", "dut", ".", "command", "=", "binary", "if", "args", ".", "valgrind", ":", "dut", ".", "use_valgrind", "(", "args", ".", "valgrind_tool", ",", "not", "args", ".", "valgrind_text", ",", "args", ".", "valgrind_console", ",", "args", ".", "valgrind_track_origins", ",", "args", ".", "valgrind_extra_params", ")", "if", "args", ".", "gdb", "==", "index", ":", "dut", ".", "use_gdb", "(", ")", "contextlist", ".", "logger", ".", "info", "(", "\"GDB is activated for node %i\"", "%", "index", ")", "if", "args", ".", "gdbs", "==", "index", ":", "dut", ".", "use_gdbs", "(", "True", ",", "args", ".", "gdbs_port", ")", "contextlist", ".", "logger", ".", "info", "(", "\"GDBserver is activated for node %i\"", "%", "index", ")", "if", "args", ".", "vgdb", "==", "index", ":", "dut", ".", "use_vgdb", "(", ")", "contextlist", ".", "logger", ".", "info", "(", "\"VGDB is activated for node %i\"", "%", "index", ")", "if", "args", ".", "nobuf", ":", "dut", ".", "no_std_buf", "(", ")", "if", "init_cli_cmds", "is", "not", "None", ":", "dut", ".", "set_init_cli_cmds", "(", "init_cli_cmds", ")", "if", "post_cli_cmds", "is", "not", "None", ":", "dut", ".", "set_post_cli_cmds", "(", "post_cli_cmds", ")", "contextlist", ".", "duts", ".", "append", "(", "dut", ")", "contextlist", ".", "dutinformations", ".", "append", "(", "dut", ".", "get_info", "(", ")", ")"], "docstring": "Initialize process type Dut as DutProcess or DutConsole.", "docstring_tokens": ["Initialize", "process", "type", "Dut", "as", "DutProcess", "or", "DutConsole", "."], "sha": "b2b97ac607429830cf7d62dae2e3903692c7c778", "url": "https://github.com/ARMmbed/icetea/blob/b2b97ac607429830cf7d62dae2e3903692c7c778/icetea_lib/Plugin/plugins/LocalAllocator/LocalAllocator.py#L140-L189", "partition": "train"}
{"repo": "grst/geos", "path": "geos/mapsource.py", "func_name": "MapSource.parse_xml_layers", "original_string": "def parse_xml_layers(xml_layers):\n        \"\"\"\n        Get the MapLayers from an XML element\n\n        Args:\n            xml_layers (Element): The <layers> tag as XML Element\n\n        Returns:\n            list of MapLayer:\n\n        \"\"\"\n        layers = []\n        for custom_map_source in xml_layers.getchildren():\n            layers.append(MapSource.parse_xml_layer(custom_map_source))\n        return layers", "language": "python", "code": "def parse_xml_layers(xml_layers):\n        \"\"\"\n        Get the MapLayers from an XML element\n\n        Args:\n            xml_layers (Element): The <layers> tag as XML Element\n\n        Returns:\n            list of MapLayer:\n\n        \"\"\"\n        layers = []\n        for custom_map_source in xml_layers.getchildren():\n            layers.append(MapSource.parse_xml_layer(custom_map_source))\n        return layers", "code_tokens": ["def", "parse_xml_layers", "(", "xml_layers", ")", ":", "layers", "=", "[", "]", "for", "custom_map_source", "in", "xml_layers", ".", "getchildren", "(", ")", ":", "layers", ".", "append", "(", "MapSource", ".", "parse_xml_layer", "(", "custom_map_source", ")", ")", "return", "layers"], "docstring": "Get the MapLayers from an XML element\n\n        Args:\n            xml_layers (Element): The <layers> tag as XML Element\n\n        Returns:\n            list of MapLayer:", "docstring_tokens": ["Get", "the", "MapLayers", "from", "an", "XML", "element"], "sha": "ea15abcc5d8f86c9051df55e489b7d941b51a638", "url": "https://github.com/grst/geos/blob/ea15abcc5d8f86c9051df55e489b7d941b51a638/geos/mapsource.py#L212-L226", "partition": "train"}
{"repo": "grst/geos", "path": "geos/mapsource.py", "func_name": "MapSource.parse_xml_layer", "original_string": "def parse_xml_layer(xml_custom_map_source):\n        \"\"\"\n        Get one MapLayer from an XML element\n\n        Args:\n            xml_custom_map_source (Element): The <customMapSource> element tag wrapped\n               in a <layers> tag as XML Element\n\n        Returns:\n            MapLayer:\n\n        \"\"\"\n        map_layer = MapLayer()\n        try:\n            for elem in xml_custom_map_source.getchildren():\n                if elem.tag == 'url':\n                    map_layer.tile_url = elem.text\n                elif elem.tag == 'minZoom':\n                    map_layer.min_zoom = int(elem.text)\n                elif elem.tag == 'maxZoom':\n                    map_layer.max_zoom = int(elem.text)\n        except ValueError:\n            raise MapSourceException(\"minZoom/maxZoom must be an integer. \")\n\n        if map_layer.tile_url is None:\n            raise MapSourceException(\"Layer requires a tile_url parameter. \")\n\n        return map_layer", "language": "python", "code": "def parse_xml_layer(xml_custom_map_source):\n        \"\"\"\n        Get one MapLayer from an XML element\n\n        Args:\n            xml_custom_map_source (Element): The <customMapSource> element tag wrapped\n               in a <layers> tag as XML Element\n\n        Returns:\n            MapLayer:\n\n        \"\"\"\n        map_layer = MapLayer()\n        try:\n            for elem in xml_custom_map_source.getchildren():\n                if elem.tag == 'url':\n                    map_layer.tile_url = elem.text\n                elif elem.tag == 'minZoom':\n                    map_layer.min_zoom = int(elem.text)\n                elif elem.tag == 'maxZoom':\n                    map_layer.max_zoom = int(elem.text)\n        except ValueError:\n            raise MapSourceException(\"minZoom/maxZoom must be an integer. \")\n\n        if map_layer.tile_url is None:\n            raise MapSourceException(\"Layer requires a tile_url parameter. \")\n\n        return map_layer", "code_tokens": ["def", "parse_xml_layer", "(", "xml_custom_map_source", ")", ":", "map_layer", "=", "MapLayer", "(", ")", "try", ":", "for", "elem", "in", "xml_custom_map_source", ".", "getchildren", "(", ")", ":", "if", "elem", ".", "tag", "==", "'url'", ":", "map_layer", ".", "tile_url", "=", "elem", ".", "text", "elif", "elem", ".", "tag", "==", "'minZoom'", ":", "map_layer", ".", "min_zoom", "=", "int", "(", "elem", ".", "text", ")", "elif", "elem", ".", "tag", "==", "'maxZoom'", ":", "map_layer", ".", "max_zoom", "=", "int", "(", "elem", ".", "text", ")", "except", "ValueError", ":", "raise", "MapSourceException", "(", "\"minZoom/maxZoom must be an integer. \"", ")", "if", "map_layer", ".", "tile_url", "is", "None", ":", "raise", "MapSourceException", "(", "\"Layer requires a tile_url parameter. \"", ")", "return", "map_layer"], "docstring": "Get one MapLayer from an XML element\n\n        Args:\n            xml_custom_map_source (Element): The <customMapSource> element tag wrapped\n               in a <layers> tag as XML Element\n\n        Returns:\n            MapLayer:", "docstring_tokens": ["Get", "one", "MapLayer", "from", "an", "XML", "element"], "sha": "ea15abcc5d8f86c9051df55e489b7d941b51a638", "url": "https://github.com/grst/geos/blob/ea15abcc5d8f86c9051df55e489b7d941b51a638/geos/mapsource.py#L229-L256", "partition": "train"}
{"repo": "grst/geos", "path": "geos/mapsource.py", "func_name": "MapSource.from_xml", "original_string": "def from_xml(xml_path, mapsource_prefix=\"\"):\n        \"\"\"\n        Create a MapSource object from a MOBAC\n        mapsource xml.\n\n        Args:\n            xml_path: path to the MOBAC mapsource xml file.\n            mapsource_prefix: root path of the mapsource folder.\n              Used to determine relative path within the maps\n              directory.\n\n        Note:\n            The Meta-Information is read from the xml\n            <id>, <folder>, <name> tags. If <id> is not available it defaults\n            to the xml file basename. If <folder> is not available if defaults to\n            the folder of the xml file with the `mapsource_prefix` removed.\n\n            The function first tries <url>, <minZoom>, <maxZoom> from <customMapSource>\n            tags within the <layers> tag. If the <layers> tag is not available,\n            the function tries to find <url>, <minZoom> and <maxZoom> on the top level.\n            If none of thie information is found, a MapSourceException is raised. \n\n        Returns:\n            MapSource:\n\n        Raises:\n            MapSourceException: when the xml file could not be parsed properly.\n\n        \"\"\"\n        xmldoc = xml.etree.ElementTree.parse(xml_path).getroot()\n\n        map_id = os.path.splitext(os.path.basename(xml_path))[0]\n        map_name = map_id\n        map_folder = re.sub(\"^\" + re.escape(mapsource_prefix), \"\", os.path.dirname(xml_path))\n        bbox = None\n        layers = None\n\n        for elem in xmldoc.getchildren():\n            if elem.tag == 'id':\n                map_id = elem.text\n            elif elem.tag == 'name':\n                map_name = elem.text\n            elif elem.tag == 'folder':\n                map_folder = elem.text\n            elif elem.tag == 'region':\n                bbox = MapSource.parse_xml_boundary(elem)\n            elif elem.tag == 'layers':\n                layers = MapSource.parse_xml_layers(elem)\n\n        if map_folder is None:\n            map_folder = \"\" # fallback if bad specification in xml\n        if layers is None:  # layers tag not found, expect url etc. in main xmldoc\n            layers = [MapSource.parse_xml_layer(xmldoc)]\n\n        ms = MapSource(map_id, map_name, map_folder, bbox=bbox)\n        ms.layers = layers\n        return ms", "language": "python", "code": "def from_xml(xml_path, mapsource_prefix=\"\"):\n        \"\"\"\n        Create a MapSource object from a MOBAC\n        mapsource xml.\n\n        Args:\n            xml_path: path to the MOBAC mapsource xml file.\n            mapsource_prefix: root path of the mapsource folder.\n              Used to determine relative path within the maps\n              directory.\n\n        Note:\n            The Meta-Information is read from the xml\n            <id>, <folder>, <name> tags. If <id> is not available it defaults\n            to the xml file basename. If <folder> is not available if defaults to\n            the folder of the xml file with the `mapsource_prefix` removed.\n\n            The function first tries <url>, <minZoom>, <maxZoom> from <customMapSource>\n            tags within the <layers> tag. If the <layers> tag is not available,\n            the function tries to find <url>, <minZoom> and <maxZoom> on the top level.\n            If none of thie information is found, a MapSourceException is raised. \n\n        Returns:\n            MapSource:\n\n        Raises:\n            MapSourceException: when the xml file could not be parsed properly.\n\n        \"\"\"\n        xmldoc = xml.etree.ElementTree.parse(xml_path).getroot()\n\n        map_id = os.path.splitext(os.path.basename(xml_path))[0]\n        map_name = map_id\n        map_folder = re.sub(\"^\" + re.escape(mapsource_prefix), \"\", os.path.dirname(xml_path))\n        bbox = None\n        layers = None\n\n        for elem in xmldoc.getchildren():\n            if elem.tag == 'id':\n                map_id = elem.text\n            elif elem.tag == 'name':\n                map_name = elem.text\n            elif elem.tag == 'folder':\n                map_folder = elem.text\n            elif elem.tag == 'region':\n                bbox = MapSource.parse_xml_boundary(elem)\n            elif elem.tag == 'layers':\n                layers = MapSource.parse_xml_layers(elem)\n\n        if map_folder is None:\n            map_folder = \"\" # fallback if bad specification in xml\n        if layers is None:  # layers tag not found, expect url etc. in main xmldoc\n            layers = [MapSource.parse_xml_layer(xmldoc)]\n\n        ms = MapSource(map_id, map_name, map_folder, bbox=bbox)\n        ms.layers = layers\n        return ms", "code_tokens": ["def", "from_xml", "(", "xml_path", ",", "mapsource_prefix", "=", "\"\"", ")", ":", "xmldoc", "=", "xml", ".", "etree", ".", "ElementTree", ".", "parse", "(", "xml_path", ")", ".", "getroot", "(", ")", "map_id", "=", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "xml_path", ")", ")", "[", "0", "]", "map_name", "=", "map_id", "map_folder", "=", "re", ".", "sub", "(", "\"^\"", "+", "re", ".", "escape", "(", "mapsource_prefix", ")", ",", "\"\"", ",", "os", ".", "path", ".", "dirname", "(", "xml_path", ")", ")", "bbox", "=", "None", "layers", "=", "None", "for", "elem", "in", "xmldoc", ".", "getchildren", "(", ")", ":", "if", "elem", ".", "tag", "==", "'id'", ":", "map_id", "=", "elem", ".", "text", "elif", "elem", ".", "tag", "==", "'name'", ":", "map_name", "=", "elem", ".", "text", "elif", "elem", ".", "tag", "==", "'folder'", ":", "map_folder", "=", "elem", ".", "text", "elif", "elem", ".", "tag", "==", "'region'", ":", "bbox", "=", "MapSource", ".", "parse_xml_boundary", "(", "elem", ")", "elif", "elem", ".", "tag", "==", "'layers'", ":", "layers", "=", "MapSource", ".", "parse_xml_layers", "(", "elem", ")", "if", "map_folder", "is", "None", ":", "map_folder", "=", "\"\"", "# fallback if bad specification in xml", "if", "layers", "is", "None", ":", "# layers tag not found, expect url etc. in main xmldoc", "layers", "=", "[", "MapSource", ".", "parse_xml_layer", "(", "xmldoc", ")", "]", "ms", "=", "MapSource", "(", "map_id", ",", "map_name", ",", "map_folder", ",", "bbox", "=", "bbox", ")", "ms", ".", "layers", "=", "layers", "return", "ms"], "docstring": "Create a MapSource object from a MOBAC\n        mapsource xml.\n\n        Args:\n            xml_path: path to the MOBAC mapsource xml file.\n            mapsource_prefix: root path of the mapsource folder.\n              Used to determine relative path within the maps\n              directory.\n\n        Note:\n            The Meta-Information is read from the xml\n            <id>, <folder>, <name> tags. If <id> is not available it defaults\n            to the xml file basename. If <folder> is not available if defaults to\n            the folder of the xml file with the `mapsource_prefix` removed.\n\n            The function first tries <url>, <minZoom>, <maxZoom> from <customMapSource>\n            tags within the <layers> tag. If the <layers> tag is not available,\n            the function tries to find <url>, <minZoom> and <maxZoom> on the top level.\n            If none of thie information is found, a MapSourceException is raised. \n\n        Returns:\n            MapSource:\n\n        Raises:\n            MapSourceException: when the xml file could not be parsed properly.", "docstring_tokens": ["Create", "a", "MapSource", "object", "from", "a", "MOBAC", "mapsource", "xml", "."], "sha": "ea15abcc5d8f86c9051df55e489b7d941b51a638", "url": "https://github.com/grst/geos/blob/ea15abcc5d8f86c9051df55e489b7d941b51a638/geos/mapsource.py#L259-L315", "partition": "train"}
{"repo": "python-cas/python-cas", "path": "cas.py", "func_name": "SingleLogoutMixin.get_saml_slos", "original_string": "def get_saml_slos(cls, logout_request):\n        \"\"\"returns saml logout ticket info\"\"\"\n        try:\n            root = etree.fromstring(logout_request)\n            return root.xpath(\n                \"//samlp:SessionIndex\",\n                namespaces={'samlp': \"urn:oasis:names:tc:SAML:2.0:protocol\"})\n        except etree.XMLSyntaxError:\n            return None", "language": "python", "code": "def get_saml_slos(cls, logout_request):\n        \"\"\"returns saml logout ticket info\"\"\"\n        try:\n            root = etree.fromstring(logout_request)\n            return root.xpath(\n                \"//samlp:SessionIndex\",\n                namespaces={'samlp': \"urn:oasis:names:tc:SAML:2.0:protocol\"})\n        except etree.XMLSyntaxError:\n            return None", "code_tokens": ["def", "get_saml_slos", "(", "cls", ",", "logout_request", ")", ":", "try", ":", "root", "=", "etree", ".", "fromstring", "(", "logout_request", ")", "return", "root", ".", "xpath", "(", "\"//samlp:SessionIndex\"", ",", "namespaces", "=", "{", "'samlp'", ":", "\"urn:oasis:names:tc:SAML:2.0:protocol\"", "}", ")", "except", "etree", ".", "XMLSyntaxError", ":", "return", "None"], "docstring": "returns saml logout ticket info", "docstring_tokens": ["returns", "saml", "logout", "ticket", "info"], "sha": "42fc76fbd2e50f167e752eba4bf5b0df74a83978", "url": "https://github.com/python-cas/python-cas/blob/42fc76fbd2e50f167e752eba4bf5b0df74a83978/cas.py#L18-L26", "partition": "train"}
{"repo": "python-cas/python-cas", "path": "cas.py", "func_name": "SingleLogoutMixin.verify_logout_request", "original_string": "def verify_logout_request(cls, logout_request, ticket):\n        \"\"\"verifies the single logout request came from the CAS server\n        returns True if the logout_request is valid, False otherwise\n        \"\"\"\n        try:\n            session_index = cls.get_saml_slos(logout_request)\n            session_index = session_index[0].text\n            if session_index == ticket:\n                return True\n            else:\n                return False\n        except (AttributeError, IndexError):\n            return False", "language": "python", "code": "def verify_logout_request(cls, logout_request, ticket):\n        \"\"\"verifies the single logout request came from the CAS server\n        returns True if the logout_request is valid, False otherwise\n        \"\"\"\n        try:\n            session_index = cls.get_saml_slos(logout_request)\n            session_index = session_index[0].text\n            if session_index == ticket:\n                return True\n            else:\n                return False\n        except (AttributeError, IndexError):\n            return False", "code_tokens": ["def", "verify_logout_request", "(", "cls", ",", "logout_request", ",", "ticket", ")", ":", "try", ":", "session_index", "=", "cls", ".", "get_saml_slos", "(", "logout_request", ")", "session_index", "=", "session_index", "[", "0", "]", ".", "text", "if", "session_index", "==", "ticket", ":", "return", "True", "else", ":", "return", "False", "except", "(", "AttributeError", ",", "IndexError", ")", ":", "return", "False"], "docstring": "verifies the single logout request came from the CAS server\n        returns True if the logout_request is valid, False otherwise", "docstring_tokens": ["verifies", "the", "single", "logout", "request", "came", "from", "the", "CAS", "server", "returns", "True", "if", "the", "logout_request", "is", "valid", "False", "otherwise"], "sha": "42fc76fbd2e50f167e752eba4bf5b0df74a83978", "url": "https://github.com/python-cas/python-cas/blob/42fc76fbd2e50f167e752eba4bf5b0df74a83978/cas.py#L29-L41", "partition": "train"}
{"repo": "python-cas/python-cas", "path": "cas.py", "func_name": "CASClientBase.get_proxy_ticket", "original_string": "def get_proxy_ticket(self, pgt):\n        \"\"\"Returns proxy ticket given the proxy granting ticket\"\"\"\n        response = requests.get(self.get_proxy_url(pgt))\n        if response.status_code == 200:\n            from lxml import etree\n            root = etree.fromstring(response.content)\n            tickets = root.xpath(\n                \"//cas:proxyTicket\",\n                namespaces={\"cas\": \"http://www.yale.edu/tp/cas\"}\n            )\n            if len(tickets) == 1:\n                return tickets[0].text\n            errors = root.xpath(\n                \"//cas:authenticationFailure\",\n                namespaces={\"cas\": \"http://www.yale.edu/tp/cas\"}\n            )\n            if len(errors) == 1:\n                raise CASError(errors[0].attrib['code'], errors[0].text)\n        raise CASError(\"Bad http code %s\" % response.status_code)", "language": "python", "code": "def get_proxy_ticket(self, pgt):\n        \"\"\"Returns proxy ticket given the proxy granting ticket\"\"\"\n        response = requests.get(self.get_proxy_url(pgt))\n        if response.status_code == 200:\n            from lxml import etree\n            root = etree.fromstring(response.content)\n            tickets = root.xpath(\n                \"//cas:proxyTicket\",\n                namespaces={\"cas\": \"http://www.yale.edu/tp/cas\"}\n            )\n            if len(tickets) == 1:\n                return tickets[0].text\n            errors = root.xpath(\n                \"//cas:authenticationFailure\",\n                namespaces={\"cas\": \"http://www.yale.edu/tp/cas\"}\n            )\n            if len(errors) == 1:\n                raise CASError(errors[0].attrib['code'], errors[0].text)\n        raise CASError(\"Bad http code %s\" % response.status_code)", "code_tokens": ["def", "get_proxy_ticket", "(", "self", ",", "pgt", ")", ":", "response", "=", "requests", ".", "get", "(", "self", ".", "get_proxy_url", "(", "pgt", ")", ")", "if", "response", ".", "status_code", "==", "200", ":", "from", "lxml", "import", "etree", "root", "=", "etree", ".", "fromstring", "(", "response", ".", "content", ")", "tickets", "=", "root", ".", "xpath", "(", "\"//cas:proxyTicket\"", ",", "namespaces", "=", "{", "\"cas\"", ":", "\"http://www.yale.edu/tp/cas\"", "}", ")", "if", "len", "(", "tickets", ")", "==", "1", ":", "return", "tickets", "[", "0", "]", ".", "text", "errors", "=", "root", ".", "xpath", "(", "\"//cas:authenticationFailure\"", ",", "namespaces", "=", "{", "\"cas\"", ":", "\"http://www.yale.edu/tp/cas\"", "}", ")", "if", "len", "(", "errors", ")", "==", "1", ":", "raise", "CASError", "(", "errors", "[", "0", "]", ".", "attrib", "[", "'code'", "]", ",", "errors", "[", "0", "]", ".", "text", ")", "raise", "CASError", "(", "\"Bad http code %s\"", "%", "response", ".", "status_code", ")"], "docstring": "Returns proxy ticket given the proxy granting ticket", "docstring_tokens": ["Returns", "proxy", "ticket", "given", "the", "proxy", "granting", "ticket"], "sha": "42fc76fbd2e50f167e752eba4bf5b0df74a83978", "url": "https://github.com/python-cas/python-cas/blob/42fc76fbd2e50f167e752eba4bf5b0df74a83978/cas.py#L102-L120", "partition": "train"}
{"repo": "kvh/ramp", "path": "ramp/model_definition.py", "func_name": "model_definition_factory", "original_string": "def model_definition_factory(base_model_definition, **kwargs):\n    \"\"\"\n    Provides an iterator over passed-in\n    configuration values, allowing for easy\n    exploration of models.\n\n    Parameters:\n    ___________\n\n    base_model_definition:\n        The base `ModelDefinition` to augment\n\n    kwargs:\n        Can be any keyword accepted by `ModelDefinition`.\n        Values should be iterables.\n    \"\"\"\n    if not kwargs:\n        yield config\n    else:\n        for param in kwargs:\n            if not hasattr(base_model_definition, param):\n                raise ValueError(\"'%s' is not a valid configuration parameter\" % param)\n\n        for raw_params in itertools.product(*kwargs.values()):\n            new_definition = copy.copy(base_model_definition)\n            new_definition.update(dict(zip(kwargs.keys(), raw_params)))\n            yield new_definition", "language": "python", "code": "def model_definition_factory(base_model_definition, **kwargs):\n    \"\"\"\n    Provides an iterator over passed-in\n    configuration values, allowing for easy\n    exploration of models.\n\n    Parameters:\n    ___________\n\n    base_model_definition:\n        The base `ModelDefinition` to augment\n\n    kwargs:\n        Can be any keyword accepted by `ModelDefinition`.\n        Values should be iterables.\n    \"\"\"\n    if not kwargs:\n        yield config\n    else:\n        for param in kwargs:\n            if not hasattr(base_model_definition, param):\n                raise ValueError(\"'%s' is not a valid configuration parameter\" % param)\n\n        for raw_params in itertools.product(*kwargs.values()):\n            new_definition = copy.copy(base_model_definition)\n            new_definition.update(dict(zip(kwargs.keys(), raw_params)))\n            yield new_definition", "code_tokens": ["def", "model_definition_factory", "(", "base_model_definition", ",", "*", "*", "kwargs", ")", ":", "if", "not", "kwargs", ":", "yield", "config", "else", ":", "for", "param", "in", "kwargs", ":", "if", "not", "hasattr", "(", "base_model_definition", ",", "param", ")", ":", "raise", "ValueError", "(", "\"'%s' is not a valid configuration parameter\"", "%", "param", ")", "for", "raw_params", "in", "itertools", ".", "product", "(", "*", "kwargs", ".", "values", "(", ")", ")", ":", "new_definition", "=", "copy", ".", "copy", "(", "base_model_definition", ")", "new_definition", ".", "update", "(", "dict", "(", "zip", "(", "kwargs", ".", "keys", "(", ")", ",", "raw_params", ")", ")", ")", "yield", "new_definition"], "docstring": "Provides an iterator over passed-in\n    configuration values, allowing for easy\n    exploration of models.\n\n    Parameters:\n    ___________\n\n    base_model_definition:\n        The base `ModelDefinition` to augment\n\n    kwargs:\n        Can be any keyword accepted by `ModelDefinition`.\n        Values should be iterables.", "docstring_tokens": ["Provides", "an", "iterator", "over", "passed", "-", "in", "configuration", "values", "allowing", "for", "easy", "exploration", "of", "models", "."], "sha": "8618ce673e49b95f40c9659319c3cb72281dacac", "url": "https://github.com/kvh/ramp/blob/8618ce673e49b95f40c9659319c3cb72281dacac/ramp/model_definition.py#L217-L243", "partition": "train"}
{"repo": "kvh/ramp", "path": "ramp/model_definition.py", "func_name": "ModelDefinition.summary", "original_string": "def summary(self):\n        \"\"\"\n        Summary of model definition for labeling. Intended to be somewhat\n        readable but unique to a given model definition.\n        \"\"\"\n        if self.features is not None:\n            feature_count = len(self.features)\n        else:\n            feature_count = 0\n        feature_hash = 'feathash:' + str(hash(tuple(self.features)))\n        return (str(self.estimator), feature_count, feature_hash, self.target)", "language": "python", "code": "def summary(self):\n        \"\"\"\n        Summary of model definition for labeling. Intended to be somewhat\n        readable but unique to a given model definition.\n        \"\"\"\n        if self.features is not None:\n            feature_count = len(self.features)\n        else:\n            feature_count = 0\n        feature_hash = 'feathash:' + str(hash(tuple(self.features)))\n        return (str(self.estimator), feature_count, feature_hash, self.target)", "code_tokens": ["def", "summary", "(", "self", ")", ":", "if", "self", ".", "features", "is", "not", "None", ":", "feature_count", "=", "len", "(", "self", ".", "features", ")", "else", ":", "feature_count", "=", "0", "feature_hash", "=", "'feathash:'", "+", "str", "(", "hash", "(", "tuple", "(", "self", ".", "features", ")", ")", ")", "return", "(", "str", "(", "self", ".", "estimator", ")", ",", "feature_count", ",", "feature_hash", ",", "self", ".", "target", ")"], "docstring": "Summary of model definition for labeling. Intended to be somewhat\n        readable but unique to a given model definition.", "docstring_tokens": ["Summary", "of", "model", "definition", "for", "labeling", ".", "Intended", "to", "be", "somewhat", "readable", "but", "unique", "to", "a", "given", "model", "definition", "."], "sha": "8618ce673e49b95f40c9659319c3cb72281dacac", "url": "https://github.com/kvh/ramp/blob/8618ce673e49b95f40c9659319c3cb72281dacac/ramp/model_definition.py#L197-L207", "partition": "train"}
{"repo": "kvh/ramp", "path": "ramp/model_definition.py", "func_name": "ModelDefinition.update", "original_string": "def update(self, dct):\n        \"\"\"Update the configuration with new parameters. Must use same\n        kwargs as __init__\"\"\"\n        d = self.__dict__.copy()\n        d.update(dct)\n        self.set_attrs(**d)", "language": "python", "code": "def update(self, dct):\n        \"\"\"Update the configuration with new parameters. Must use same\n        kwargs as __init__\"\"\"\n        d = self.__dict__.copy()\n        d.update(dct)\n        self.set_attrs(**d)", "code_tokens": ["def", "update", "(", "self", ",", "dct", ")", ":", "d", "=", "self", ".", "__dict__", ".", "copy", "(", ")", "d", ".", "update", "(", "dct", ")", "self", ".", "set_attrs", "(", "*", "*", "d", ")"], "docstring": "Update the configuration with new parameters. Must use same\n        kwargs as __init__", "docstring_tokens": ["Update", "the", "configuration", "with", "new", "parameters", ".", "Must", "use", "same", "kwargs", "as", "__init__"], "sha": "8618ce673e49b95f40c9659319c3cb72281dacac", "url": "https://github.com/kvh/ramp/blob/8618ce673e49b95f40c9659319c3cb72281dacac/ramp/model_definition.py#L209-L214", "partition": "train"}
{"repo": "kvh/ramp", "path": "ramp/features/base.py", "func_name": "ComboFeature.column_rename", "original_string": "def column_rename(self, existing_name, hsh=None):\n        \"\"\"\n        Like unique_name, but in addition must be unique to each column of this\n        feature. accomplishes this by prepending readable string to existing\n        column name and replacing unique hash at end of column name.\n        \"\"\"\n        try:\n            existing_name = str(existing_name)\n        except UnicodeEncodeError:\n            pass\n        if hsh is None:\n            hsh = self._hash()\n        if self._name:\n            return '%s(%s) [%s]' %(self._name, self._remove_hashes(existing_name),\n                    hsh)\n        return '%s [%s]'%(self._remove_hashes(existing_name),\n                    hsh)", "language": "python", "code": "def column_rename(self, existing_name, hsh=None):\n        \"\"\"\n        Like unique_name, but in addition must be unique to each column of this\n        feature. accomplishes this by prepending readable string to existing\n        column name and replacing unique hash at end of column name.\n        \"\"\"\n        try:\n            existing_name = str(existing_name)\n        except UnicodeEncodeError:\n            pass\n        if hsh is None:\n            hsh = self._hash()\n        if self._name:\n            return '%s(%s) [%s]' %(self._name, self._remove_hashes(existing_name),\n                    hsh)\n        return '%s [%s]'%(self._remove_hashes(existing_name),\n                    hsh)", "code_tokens": ["def", "column_rename", "(", "self", ",", "existing_name", ",", "hsh", "=", "None", ")", ":", "try", ":", "existing_name", "=", "str", "(", "existing_name", ")", "except", "UnicodeEncodeError", ":", "pass", "if", "hsh", "is", "None", ":", "hsh", "=", "self", ".", "_hash", "(", ")", "if", "self", ".", "_name", ":", "return", "'%s(%s) [%s]'", "%", "(", "self", ".", "_name", ",", "self", ".", "_remove_hashes", "(", "existing_name", ")", ",", "hsh", ")", "return", "'%s [%s]'", "%", "(", "self", ".", "_remove_hashes", "(", "existing_name", ")", ",", "hsh", ")"], "docstring": "Like unique_name, but in addition must be unique to each column of this\n        feature. accomplishes this by prepending readable string to existing\n        column name and replacing unique hash at end of column name.", "docstring_tokens": ["Like", "unique_name", "but", "in", "addition", "must", "be", "unique", "to", "each", "column", "of", "this", "feature", ".", "accomplishes", "this", "by", "prepending", "readable", "string", "to", "existing", "column", "name", "and", "replacing", "unique", "hash", "at", "end", "of", "column", "name", "."], "sha": "8618ce673e49b95f40c9659319c3cb72281dacac", "url": "https://github.com/kvh/ramp/blob/8618ce673e49b95f40c9659319c3cb72281dacac/ramp/features/base.py#L228-L244", "partition": "train"}
{"repo": "ludbb/secp256k1-py", "path": "secp256k1/__init__.py", "func_name": "ECDSA.ecdsa_signature_normalize", "original_string": "def ecdsa_signature_normalize(self, raw_sig, check_only=False):\n        \"\"\"\n        Check and optionally convert a signature to a normalized lower-S form.\n        If check_only is True then the normalized signature is not returned.\n\n        This function always return a tuple containing a boolean (True if\n        not previously normalized or False if signature was already\n        normalized), and the normalized signature. When check_only is True,\n        the normalized signature returned is always None.\n        \"\"\"\n        if check_only:\n            sigout = ffi.NULL\n        else:\n            sigout = ffi.new('secp256k1_ecdsa_signature *')\n\n        result = lib.secp256k1_ecdsa_signature_normalize(\n            self.ctx, sigout, raw_sig)\n\n        return (bool(result), sigout if sigout != ffi.NULL else None)", "language": "python", "code": "def ecdsa_signature_normalize(self, raw_sig, check_only=False):\n        \"\"\"\n        Check and optionally convert a signature to a normalized lower-S form.\n        If check_only is True then the normalized signature is not returned.\n\n        This function always return a tuple containing a boolean (True if\n        not previously normalized or False if signature was already\n        normalized), and the normalized signature. When check_only is True,\n        the normalized signature returned is always None.\n        \"\"\"\n        if check_only:\n            sigout = ffi.NULL\n        else:\n            sigout = ffi.new('secp256k1_ecdsa_signature *')\n\n        result = lib.secp256k1_ecdsa_signature_normalize(\n            self.ctx, sigout, raw_sig)\n\n        return (bool(result), sigout if sigout != ffi.NULL else None)", "code_tokens": ["def", "ecdsa_signature_normalize", "(", "self", ",", "raw_sig", ",", "check_only", "=", "False", ")", ":", "if", "check_only", ":", "sigout", "=", "ffi", ".", "NULL", "else", ":", "sigout", "=", "ffi", ".", "new", "(", "'secp256k1_ecdsa_signature *'", ")", "result", "=", "lib", ".", "secp256k1_ecdsa_signature_normalize", "(", "self", ".", "ctx", ",", "sigout", ",", "raw_sig", ")", "return", "(", "bool", "(", "result", ")", ",", "sigout", "if", "sigout", "!=", "ffi", ".", "NULL", "else", "None", ")"], "docstring": "Check and optionally convert a signature to a normalized lower-S form.\n        If check_only is True then the normalized signature is not returned.\n\n        This function always return a tuple containing a boolean (True if\n        not previously normalized or False if signature was already\n        normalized), and the normalized signature. When check_only is True,\n        the normalized signature returned is always None.", "docstring_tokens": ["Check", "and", "optionally", "convert", "a", "signature", "to", "a", "normalized", "lower", "-", "S", "form", ".", "If", "check_only", "is", "True", "then", "the", "normalized", "signature", "is", "not", "returned", "."], "sha": "f5e455227bf1e833128adf80de8ee0ebcebf218c", "url": "https://github.com/ludbb/secp256k1-py/blob/f5e455227bf1e833128adf80de8ee0ebcebf218c/secp256k1/__init__.py#L84-L102", "partition": "train"}
{"repo": "ludbb/secp256k1-py", "path": "secp256k1/__init__.py", "func_name": "Schnorr.schnorr_partial_combine", "original_string": "def schnorr_partial_combine(self, schnorr_sigs):\n        \"\"\"Combine multiple Schnorr partial signatures.\"\"\"\n        if not HAS_SCHNORR:\n            raise Exception(\"secp256k1_schnorr not enabled\")\n        assert len(schnorr_sigs) > 0\n\n        sig64 = ffi.new('char [64]')\n        sig64sin = []\n        for sig in schnorr_sigs:\n            if not isinstance(sig, bytes):\n                raise TypeError('expected bytes, got {}'.format(type(sig)))\n            if len(sig) != 64:\n                raise Exception('invalid signature length')\n            sig64sin.append(ffi.new('char []', sig))\n\n        res = lib.secp256k1_schnorr_partial_combine(\n            self.ctx, sig64, sig64sin, len(sig64sin))\n        if res <= 0:\n            raise Exception('failed to combine signatures ({})'.format(res))\n\n        return bytes(ffi.buffer(sig64, 64))", "language": "python", "code": "def schnorr_partial_combine(self, schnorr_sigs):\n        \"\"\"Combine multiple Schnorr partial signatures.\"\"\"\n        if not HAS_SCHNORR:\n            raise Exception(\"secp256k1_schnorr not enabled\")\n        assert len(schnorr_sigs) > 0\n\n        sig64 = ffi.new('char [64]')\n        sig64sin = []\n        for sig in schnorr_sigs:\n            if not isinstance(sig, bytes):\n                raise TypeError('expected bytes, got {}'.format(type(sig)))\n            if len(sig) != 64:\n                raise Exception('invalid signature length')\n            sig64sin.append(ffi.new('char []', sig))\n\n        res = lib.secp256k1_schnorr_partial_combine(\n            self.ctx, sig64, sig64sin, len(sig64sin))\n        if res <= 0:\n            raise Exception('failed to combine signatures ({})'.format(res))\n\n        return bytes(ffi.buffer(sig64, 64))", "code_tokens": ["def", "schnorr_partial_combine", "(", "self", ",", "schnorr_sigs", ")", ":", "if", "not", "HAS_SCHNORR", ":", "raise", "Exception", "(", "\"secp256k1_schnorr not enabled\"", ")", "assert", "len", "(", "schnorr_sigs", ")", ">", "0", "sig64", "=", "ffi", ".", "new", "(", "'char [64]'", ")", "sig64sin", "=", "[", "]", "for", "sig", "in", "schnorr_sigs", ":", "if", "not", "isinstance", "(", "sig", ",", "bytes", ")", ":", "raise", "TypeError", "(", "'expected bytes, got {}'", ".", "format", "(", "type", "(", "sig", ")", ")", ")", "if", "len", "(", "sig", ")", "!=", "64", ":", "raise", "Exception", "(", "'invalid signature length'", ")", "sig64sin", ".", "append", "(", "ffi", ".", "new", "(", "'char []'", ",", "sig", ")", ")", "res", "=", "lib", ".", "secp256k1_schnorr_partial_combine", "(", "self", ".", "ctx", ",", "sig64", ",", "sig64sin", ",", "len", "(", "sig64sin", ")", ")", "if", "res", "<=", "0", ":", "raise", "Exception", "(", "'failed to combine signatures ({})'", ".", "format", "(", "res", ")", ")", "return", "bytes", "(", "ffi", ".", "buffer", "(", "sig64", ",", "64", ")", ")"], "docstring": "Combine multiple Schnorr partial signatures.", "docstring_tokens": ["Combine", "multiple", "Schnorr", "partial", "signatures", "."], "sha": "f5e455227bf1e833128adf80de8ee0ebcebf218c", "url": "https://github.com/ludbb/secp256k1-py/blob/f5e455227bf1e833128adf80de8ee0ebcebf218c/secp256k1/__init__.py#L179-L199", "partition": "train"}
{"repo": "djungelorm/sphinx-tabs", "path": "sphinx_tabs/tabs.py", "func_name": "TabDirective.run", "original_string": "def run(self):\n        \"\"\" Parse a tab directive \"\"\"\n        self.assert_has_content()\n        env = self.state.document.settings.env\n\n        tabs_id = env.temp_data['tabs_stack'][-1]\n        tabs_key = 'tabs_%d' % tabs_id\n\n        args = self.content[0].strip()\n        if args.startswith('{'):\n            try:\n                args = json.loads(args)\n                self.content.trim_start(1)\n            except ValueError:\n                args = {}\n        else:\n            args = {}\n\n        tab_name = nodes.container()\n        self.state.nested_parse(\n            self.content[:1], self.content_offset, tab_name)\n        args['tab_name'] = tab_name\n\n        include_tabs_id_in_data_tab = False\n        if 'tab_id' not in args:\n            args['tab_id'] = env.new_serialno(tabs_key)\n            include_tabs_id_in_data_tab = True\n        i = 1\n        while args['tab_id'] in env.temp_data[tabs_key]['tab_ids']:\n            args['tab_id'] = '%s-%d' % (args['tab_id'], i)\n            i += 1\n        env.temp_data[tabs_key]['tab_ids'].append(args['tab_id'])\n\n        data_tab = str(args['tab_id'])\n        if include_tabs_id_in_data_tab:\n            data_tab = '%d-%s' % (tabs_id, data_tab)\n        data_tab = \"sphinx-data-tab-{}\".format(data_tab)\n\n        env.temp_data[tabs_key]['tab_titles'].append(\n            (data_tab, args['tab_name']))\n\n        text = '\\n'.join(self.content)\n        node = nodes.container(text)\n\n        classes = 'ui bottom attached sphinx-tab tab segment'\n        node['classes'] = classes.split(' ')\n        node['classes'].extend(args.get('classes', []))\n        node['classes'].append(data_tab)\n\n        if env.temp_data[tabs_key]['is_first_tab']:\n            node['classes'].append('active')\n            env.temp_data[tabs_key]['is_first_tab'] = False\n\n        self.state.nested_parse(self.content[2:], self.content_offset, node)\n\n        if env.app.builder.name not in get_compatible_builders(env.app):\n            outer_node = nodes.container()\n            tab = nodes.container()\n            tab.tagname = 'a'\n            tab['classes'] = ['item']\n            tab += tab_name\n\n            outer_node.append(tab)\n            outer_node.append(node)\n            return [outer_node]\n\n        return [node]", "language": "python", "code": "def run(self):\n        \"\"\" Parse a tab directive \"\"\"\n        self.assert_has_content()\n        env = self.state.document.settings.env\n\n        tabs_id = env.temp_data['tabs_stack'][-1]\n        tabs_key = 'tabs_%d' % tabs_id\n\n        args = self.content[0].strip()\n        if args.startswith('{'):\n            try:\n                args = json.loads(args)\n                self.content.trim_start(1)\n            except ValueError:\n                args = {}\n        else:\n            args = {}\n\n        tab_name = nodes.container()\n        self.state.nested_parse(\n            self.content[:1], self.content_offset, tab_name)\n        args['tab_name'] = tab_name\n\n        include_tabs_id_in_data_tab = False\n        if 'tab_id' not in args:\n            args['tab_id'] = env.new_serialno(tabs_key)\n            include_tabs_id_in_data_tab = True\n        i = 1\n        while args['tab_id'] in env.temp_data[tabs_key]['tab_ids']:\n            args['tab_id'] = '%s-%d' % (args['tab_id'], i)\n            i += 1\n        env.temp_data[tabs_key]['tab_ids'].append(args['tab_id'])\n\n        data_tab = str(args['tab_id'])\n        if include_tabs_id_in_data_tab:\n            data_tab = '%d-%s' % (tabs_id, data_tab)\n        data_tab = \"sphinx-data-tab-{}\".format(data_tab)\n\n        env.temp_data[tabs_key]['tab_titles'].append(\n            (data_tab, args['tab_name']))\n\n        text = '\\n'.join(self.content)\n        node = nodes.container(text)\n\n        classes = 'ui bottom attached sphinx-tab tab segment'\n        node['classes'] = classes.split(' ')\n        node['classes'].extend(args.get('classes', []))\n        node['classes'].append(data_tab)\n\n        if env.temp_data[tabs_key]['is_first_tab']:\n            node['classes'].append('active')\n            env.temp_data[tabs_key]['is_first_tab'] = False\n\n        self.state.nested_parse(self.content[2:], self.content_offset, node)\n\n        if env.app.builder.name not in get_compatible_builders(env.app):\n            outer_node = nodes.container()\n            tab = nodes.container()\n            tab.tagname = 'a'\n            tab['classes'] = ['item']\n            tab += tab_name\n\n            outer_node.append(tab)\n            outer_node.append(node)\n            return [outer_node]\n\n        return [node]", "code_tokens": ["def", "run", "(", "self", ")", ":", "self", ".", "assert_has_content", "(", ")", "env", "=", "self", ".", "state", ".", "document", ".", "settings", ".", "env", "tabs_id", "=", "env", ".", "temp_data", "[", "'tabs_stack'", "]", "[", "-", "1", "]", "tabs_key", "=", "'tabs_%d'", "%", "tabs_id", "args", "=", "self", ".", "content", "[", "0", "]", ".", "strip", "(", ")", "if", "args", ".", "startswith", "(", "'{'", ")", ":", "try", ":", "args", "=", "json", ".", "loads", "(", "args", ")", "self", ".", "content", ".", "trim_start", "(", "1", ")", "except", "ValueError", ":", "args", "=", "{", "}", "else", ":", "args", "=", "{", "}", "tab_name", "=", "nodes", ".", "container", "(", ")", "self", ".", "state", ".", "nested_parse", "(", "self", ".", "content", "[", ":", "1", "]", ",", "self", ".", "content_offset", ",", "tab_name", ")", "args", "[", "'tab_name'", "]", "=", "tab_name", "include_tabs_id_in_data_tab", "=", "False", "if", "'tab_id'", "not", "in", "args", ":", "args", "[", "'tab_id'", "]", "=", "env", ".", "new_serialno", "(", "tabs_key", ")", "include_tabs_id_in_data_tab", "=", "True", "i", "=", "1", "while", "args", "[", "'tab_id'", "]", "in", "env", ".", "temp_data", "[", "tabs_key", "]", "[", "'tab_ids'", "]", ":", "args", "[", "'tab_id'", "]", "=", "'%s-%d'", "%", "(", "args", "[", "'tab_id'", "]", ",", "i", ")", "i", "+=", "1", "env", ".", "temp_data", "[", "tabs_key", "]", "[", "'tab_ids'", "]", ".", "append", "(", "args", "[", "'tab_id'", "]", ")", "data_tab", "=", "str", "(", "args", "[", "'tab_id'", "]", ")", "if", "include_tabs_id_in_data_tab", ":", "data_tab", "=", "'%d-%s'", "%", "(", "tabs_id", ",", "data_tab", ")", "data_tab", "=", "\"sphinx-data-tab-{}\"", ".", "format", "(", "data_tab", ")", "env", ".", "temp_data", "[", "tabs_key", "]", "[", "'tab_titles'", "]", ".", "append", "(", "(", "data_tab", ",", "args", "[", "'tab_name'", "]", ")", ")", "text", "=", "'\\n'", ".", "join", "(", "self", ".", "content", ")", "node", "=", "nodes", ".", "container", "(", "text", ")", "classes", "=", "'ui bottom attached sphinx-tab tab segment'", "node", "[", "'classes'", "]", "=", "classes", ".", "split", "(", "' '", ")", "node", "[", "'classes'", "]", ".", "extend", "(", "args", ".", "get", "(", "'classes'", ",", "[", "]", ")", ")", "node", "[", "'classes'", "]", ".", "append", "(", "data_tab", ")", "if", "env", ".", "temp_data", "[", "tabs_key", "]", "[", "'is_first_tab'", "]", ":", "node", "[", "'classes'", "]", ".", "append", "(", "'active'", ")", "env", ".", "temp_data", "[", "tabs_key", "]", "[", "'is_first_tab'", "]", "=", "False", "self", ".", "state", ".", "nested_parse", "(", "self", ".", "content", "[", "2", ":", "]", ",", "self", ".", "content_offset", ",", "node", ")", "if", "env", ".", "app", ".", "builder", ".", "name", "not", "in", "get_compatible_builders", "(", "env", ".", "app", ")", ":", "outer_node", "=", "nodes", ".", "container", "(", ")", "tab", "=", "nodes", ".", "container", "(", ")", "tab", ".", "tagname", "=", "'a'", "tab", "[", "'classes'", "]", "=", "[", "'item'", "]", "tab", "+=", "tab_name", "outer_node", ".", "append", "(", "tab", ")", "outer_node", ".", "append", "(", "node", ")", "return", "[", "outer_node", "]", "return", "[", "node", "]"], "docstring": "Parse a tab directive", "docstring_tokens": ["Parse", "a", "tab", "directive"], "sha": "2f17b5ca82a91613b42d58d01aafbd484525915c", "url": "https://github.com/djungelorm/sphinx-tabs/blob/2f17b5ca82a91613b42d58d01aafbd484525915c/sphinx_tabs/tabs.py#L97-L163", "partition": "train"}
{"repo": "djungelorm/sphinx-tabs", "path": "sphinx_tabs/tabs.py", "func_name": "GroupTabDirective.run", "original_string": "def run(self):\n        \"\"\" Parse a tab directive \"\"\"\n        self.assert_has_content()\n\n        group_name = self.content[0]\n        self.content.trim_start(2)\n\n        for idx, line in enumerate(self.content.data):\n            self.content.data[idx] = '   ' + line\n\n        tab_args = {\n            'tab_id': base64.b64encode(\n                group_name.encode('utf-8')).decode('utf-8'),\n            'group_tab': True\n        }\n\n        new_content = [\n            '.. tab:: {}'.format(json.dumps(tab_args)),\n            '   {}'.format(group_name),\n            '',\n        ]\n\n        for idx, line in enumerate(new_content):\n            self.content.data.insert(idx, line)\n            self.content.items.insert(idx, (None, idx))\n\n        node = nodes.container()\n        self.state.nested_parse(self.content, self.content_offset, node)\n        return node.children", "language": "python", "code": "def run(self):\n        \"\"\" Parse a tab directive \"\"\"\n        self.assert_has_content()\n\n        group_name = self.content[0]\n        self.content.trim_start(2)\n\n        for idx, line in enumerate(self.content.data):\n            self.content.data[idx] = '   ' + line\n\n        tab_args = {\n            'tab_id': base64.b64encode(\n                group_name.encode('utf-8')).decode('utf-8'),\n            'group_tab': True\n        }\n\n        new_content = [\n            '.. tab:: {}'.format(json.dumps(tab_args)),\n            '   {}'.format(group_name),\n            '',\n        ]\n\n        for idx, line in enumerate(new_content):\n            self.content.data.insert(idx, line)\n            self.content.items.insert(idx, (None, idx))\n\n        node = nodes.container()\n        self.state.nested_parse(self.content, self.content_offset, node)\n        return node.children", "code_tokens": ["def", "run", "(", "self", ")", ":", "self", ".", "assert_has_content", "(", ")", "group_name", "=", "self", ".", "content", "[", "0", "]", "self", ".", "content", ".", "trim_start", "(", "2", ")", "for", "idx", ",", "line", "in", "enumerate", "(", "self", ".", "content", ".", "data", ")", ":", "self", ".", "content", ".", "data", "[", "idx", "]", "=", "'   '", "+", "line", "tab_args", "=", "{", "'tab_id'", ":", "base64", ".", "b64encode", "(", "group_name", ".", "encode", "(", "'utf-8'", ")", ")", ".", "decode", "(", "'utf-8'", ")", ",", "'group_tab'", ":", "True", "}", "new_content", "=", "[", "'.. tab:: {}'", ".", "format", "(", "json", ".", "dumps", "(", "tab_args", ")", ")", ",", "'   {}'", ".", "format", "(", "group_name", ")", ",", "''", ",", "]", "for", "idx", ",", "line", "in", "enumerate", "(", "new_content", ")", ":", "self", ".", "content", ".", "data", ".", "insert", "(", "idx", ",", "line", ")", "self", ".", "content", ".", "items", ".", "insert", "(", "idx", ",", "(", "None", ",", "idx", ")", ")", "node", "=", "nodes", ".", "container", "(", ")", "self", ".", "state", ".", "nested_parse", "(", "self", ".", "content", ",", "self", ".", "content_offset", ",", "node", ")", "return", "node", ".", "children"], "docstring": "Parse a tab directive", "docstring_tokens": ["Parse", "a", "tab", "directive"], "sha": "2f17b5ca82a91613b42d58d01aafbd484525915c", "url": "https://github.com/djungelorm/sphinx-tabs/blob/2f17b5ca82a91613b42d58d01aafbd484525915c/sphinx_tabs/tabs.py#L171-L199", "partition": "train"}
{"repo": "djungelorm/sphinx-tabs", "path": "sphinx_tabs/tabs.py", "func_name": "CodeTabDirective.run", "original_string": "def run(self):\n        \"\"\" Parse a tab directive \"\"\"\n        self.assert_has_content()\n\n        args = self.content[0].strip().split()\n        self.content.trim_start(2)\n\n        lang = args[0]\n        tab_name = ' '.join(args[1:]) if len(args) > 1 else LEXER_MAP[lang]\n\n        for idx, line in enumerate(self.content.data):\n            self.content.data[idx] = '      ' + line\n\n        tab_args = {\n            'tab_id': base64.b64encode(\n                tab_name.encode('utf-8')).decode('utf-8'),\n            'classes': ['code-tab'],\n        }\n\n        new_content = [\n            '.. tab:: {}'.format(json.dumps(tab_args)),\n            '   {}'.format(tab_name),\n            '',\n            '   .. code-block:: {}'.format(lang),\n        ]\n\n        if 'linenos' in self.options:\n            new_content.append('      :linenos:')\n\n        new_content.append('')\n\n        for idx, line in enumerate(new_content):\n            self.content.data.insert(idx, line)\n            self.content.items.insert(idx, (None, idx))\n\n        node = nodes.container()\n        self.state.nested_parse(self.content, self.content_offset, node)\n        return node.children", "language": "python", "code": "def run(self):\n        \"\"\" Parse a tab directive \"\"\"\n        self.assert_has_content()\n\n        args = self.content[0].strip().split()\n        self.content.trim_start(2)\n\n        lang = args[0]\n        tab_name = ' '.join(args[1:]) if len(args) > 1 else LEXER_MAP[lang]\n\n        for idx, line in enumerate(self.content.data):\n            self.content.data[idx] = '      ' + line\n\n        tab_args = {\n            'tab_id': base64.b64encode(\n                tab_name.encode('utf-8')).decode('utf-8'),\n            'classes': ['code-tab'],\n        }\n\n        new_content = [\n            '.. tab:: {}'.format(json.dumps(tab_args)),\n            '   {}'.format(tab_name),\n            '',\n            '   .. code-block:: {}'.format(lang),\n        ]\n\n        if 'linenos' in self.options:\n            new_content.append('      :linenos:')\n\n        new_content.append('')\n\n        for idx, line in enumerate(new_content):\n            self.content.data.insert(idx, line)\n            self.content.items.insert(idx, (None, idx))\n\n        node = nodes.container()\n        self.state.nested_parse(self.content, self.content_offset, node)\n        return node.children", "code_tokens": ["def", "run", "(", "self", ")", ":", "self", ".", "assert_has_content", "(", ")", "args", "=", "self", ".", "content", "[", "0", "]", ".", "strip", "(", ")", ".", "split", "(", ")", "self", ".", "content", ".", "trim_start", "(", "2", ")", "lang", "=", "args", "[", "0", "]", "tab_name", "=", "' '", ".", "join", "(", "args", "[", "1", ":", "]", ")", "if", "len", "(", "args", ")", ">", "1", "else", "LEXER_MAP", "[", "lang", "]", "for", "idx", ",", "line", "in", "enumerate", "(", "self", ".", "content", ".", "data", ")", ":", "self", ".", "content", ".", "data", "[", "idx", "]", "=", "'      '", "+", "line", "tab_args", "=", "{", "'tab_id'", ":", "base64", ".", "b64encode", "(", "tab_name", ".", "encode", "(", "'utf-8'", ")", ")", ".", "decode", "(", "'utf-8'", ")", ",", "'classes'", ":", "[", "'code-tab'", "]", ",", "}", "new_content", "=", "[", "'.. tab:: {}'", ".", "format", "(", "json", ".", "dumps", "(", "tab_args", ")", ")", ",", "'   {}'", ".", "format", "(", "tab_name", ")", ",", "''", ",", "'   .. code-block:: {}'", ".", "format", "(", "lang", ")", ",", "]", "if", "'linenos'", "in", "self", ".", "options", ":", "new_content", ".", "append", "(", "'      :linenos:'", ")", "new_content", ".", "append", "(", "''", ")", "for", "idx", ",", "line", "in", "enumerate", "(", "new_content", ")", ":", "self", ".", "content", ".", "data", ".", "insert", "(", "idx", ",", "line", ")", "self", ".", "content", ".", "items", ".", "insert", "(", "idx", ",", "(", "None", ",", "idx", ")", ")", "node", "=", "nodes", ".", "container", "(", ")", "self", ".", "state", ".", "nested_parse", "(", "self", ".", "content", ",", "self", ".", "content_offset", ",", "node", ")", "return", "node", ".", "children"], "docstring": "Parse a tab directive", "docstring_tokens": ["Parse", "a", "tab", "directive"], "sha": "2f17b5ca82a91613b42d58d01aafbd484525915c", "url": "https://github.com/djungelorm/sphinx-tabs/blob/2f17b5ca82a91613b42d58d01aafbd484525915c/sphinx_tabs/tabs.py#L210-L247", "partition": "train"}
{"repo": "bitly/asyncmongo", "path": "asyncmongo/message.py", "func_name": "__pack_message", "original_string": "def __pack_message(operation, data):\n    \"\"\"Takes message data and adds a message header based on the operation.\n\n    Returns the resultant message string.\n    \"\"\"\n    request_id = random.randint(-2 ** 31 - 1, 2 ** 31)\n    message = struct.pack(\"<i\", 16 + len(data))\n    message += struct.pack(\"<i\", request_id)\n    message += __ZERO  # responseTo\n    message += struct.pack(\"<i\", operation)\n    return (request_id, message + data)", "language": "python", "code": "def __pack_message(operation, data):\n    \"\"\"Takes message data and adds a message header based on the operation.\n\n    Returns the resultant message string.\n    \"\"\"\n    request_id = random.randint(-2 ** 31 - 1, 2 ** 31)\n    message = struct.pack(\"<i\", 16 + len(data))\n    message += struct.pack(\"<i\", request_id)\n    message += __ZERO  # responseTo\n    message += struct.pack(\"<i\", operation)\n    return (request_id, message + data)", "code_tokens": ["def", "__pack_message", "(", "operation", ",", "data", ")", ":", "request_id", "=", "random", ".", "randint", "(", "-", "2", "**", "31", "-", "1", ",", "2", "**", "31", ")", "message", "=", "struct", ".", "pack", "(", "\"<i\"", ",", "16", "+", "len", "(", "data", ")", ")", "message", "+=", "struct", ".", "pack", "(", "\"<i\"", ",", "request_id", ")", "message", "+=", "__ZERO", "# responseTo", "message", "+=", "struct", ".", "pack", "(", "\"<i\"", ",", "operation", ")", "return", "(", "request_id", ",", "message", "+", "data", ")"], "docstring": "Takes message data and adds a message header based on the operation.\n\n    Returns the resultant message string.", "docstring_tokens": ["Takes", "message", "data", "and", "adds", "a", "message", "header", "based", "on", "the", "operation", "."], "sha": "3da47c96d4592ec9e8b3ef5cf1b7d5b439ab3a5b", "url": "https://github.com/bitly/asyncmongo/blob/3da47c96d4592ec9e8b3ef5cf1b7d5b439ab3a5b/asyncmongo/message.py#L47-L57", "partition": "train"}
{"repo": "bitly/asyncmongo", "path": "asyncmongo/message.py", "func_name": "insert", "original_string": "def insert(collection_name, docs, check_keys, safe, last_error_args):\n    \"\"\"Get an **insert** message.\n    \"\"\"\n    data = __ZERO\n    data += bson._make_c_string(collection_name)\n    bson_data = \"\".join([bson.BSON.encode(doc, check_keys) for doc in docs])\n    if not bson_data:\n        raise InvalidOperation(\"cannot do an empty bulk insert\")\n    data += bson_data\n    if safe:\n        (_, insert_message) = __pack_message(2002, data)\n        (request_id, error_message) = __last_error(last_error_args)\n        return (request_id, insert_message + error_message)\n    else:\n        return __pack_message(2002, data)", "language": "python", "code": "def insert(collection_name, docs, check_keys, safe, last_error_args):\n    \"\"\"Get an **insert** message.\n    \"\"\"\n    data = __ZERO\n    data += bson._make_c_string(collection_name)\n    bson_data = \"\".join([bson.BSON.encode(doc, check_keys) for doc in docs])\n    if not bson_data:\n        raise InvalidOperation(\"cannot do an empty bulk insert\")\n    data += bson_data\n    if safe:\n        (_, insert_message) = __pack_message(2002, data)\n        (request_id, error_message) = __last_error(last_error_args)\n        return (request_id, insert_message + error_message)\n    else:\n        return __pack_message(2002, data)", "code_tokens": ["def", "insert", "(", "collection_name", ",", "docs", ",", "check_keys", ",", "safe", ",", "last_error_args", ")", ":", "data", "=", "__ZERO", "data", "+=", "bson", ".", "_make_c_string", "(", "collection_name", ")", "bson_data", "=", "\"\"", ".", "join", "(", "[", "bson", ".", "BSON", ".", "encode", "(", "doc", ",", "check_keys", ")", "for", "doc", "in", "docs", "]", ")", "if", "not", "bson_data", ":", "raise", "InvalidOperation", "(", "\"cannot do an empty bulk insert\"", ")", "data", "+=", "bson_data", "if", "safe", ":", "(", "_", ",", "insert_message", ")", "=", "__pack_message", "(", "2002", ",", "data", ")", "(", "request_id", ",", "error_message", ")", "=", "__last_error", "(", "last_error_args", ")", "return", "(", "request_id", ",", "insert_message", "+", "error_message", ")", "else", ":", "return", "__pack_message", "(", "2002", ",", "data", ")"], "docstring": "Get an **insert** message.", "docstring_tokens": ["Get", "an", "**", "insert", "**", "message", "."], "sha": "3da47c96d4592ec9e8b3ef5cf1b7d5b439ab3a5b", "url": "https://github.com/bitly/asyncmongo/blob/3da47c96d4592ec9e8b3ef5cf1b7d5b439ab3a5b/asyncmongo/message.py#L60-L74", "partition": "train"}
{"repo": "DBuildService/dockerfile-parse", "path": "dockerfile_parse/util.py", "func_name": "Context.set_line_value", "original_string": "def set_line_value(self, context_type, value):\n        \"\"\"\n        Set value defined on this line ('line_envs'/'line_labels')\n        and update 'envs'/'labels'.\n\n        :param context_type: \"ENV\" or \"LABEL\"\n        :param value: new value for this line\n        \"\"\"\n        if context_type.upper() == \"ENV\":\n            self.line_envs = value\n            self.envs.update(value)\n        elif context_type.upper() == \"LABEL\":\n            self.line_labels = value\n            self.labels.update(value)", "language": "python", "code": "def set_line_value(self, context_type, value):\n        \"\"\"\n        Set value defined on this line ('line_envs'/'line_labels')\n        and update 'envs'/'labels'.\n\n        :param context_type: \"ENV\" or \"LABEL\"\n        :param value: new value for this line\n        \"\"\"\n        if context_type.upper() == \"ENV\":\n            self.line_envs = value\n            self.envs.update(value)\n        elif context_type.upper() == \"LABEL\":\n            self.line_labels = value\n            self.labels.update(value)", "code_tokens": ["def", "set_line_value", "(", "self", ",", "context_type", ",", "value", ")", ":", "if", "context_type", ".", "upper", "(", ")", "==", "\"ENV\"", ":", "self", ".", "line_envs", "=", "value", "self", ".", "envs", ".", "update", "(", "value", ")", "elif", "context_type", ".", "upper", "(", ")", "==", "\"LABEL\"", ":", "self", ".", "line_labels", "=", "value", "self", ".", "labels", ".", "update", "(", "value", ")"], "docstring": "Set value defined on this line ('line_envs'/'line_labels')\n        and update 'envs'/'labels'.\n\n        :param context_type: \"ENV\" or \"LABEL\"\n        :param value: new value for this line", "docstring_tokens": ["Set", "value", "defined", "on", "this", "line", "(", "line_envs", "/", "line_labels", ")", "and", "update", "envs", "/", "labels", "."], "sha": "3d7b514d8b8eded1b33529cf0f6a0770a573aee0", "url": "https://github.com/DBuildService/dockerfile-parse/blob/3d7b514d8b8eded1b33529cf0f6a0770a573aee0/dockerfile_parse/util.py#L272-L285", "partition": "train"}
{"repo": "DBuildService/dockerfile-parse", "path": "dockerfile_parse/util.py", "func_name": "Context.get_line_value", "original_string": "def get_line_value(self, context_type):\n        \"\"\"\n        Get the values defined on this line.\n\n        :param context_type: \"ENV\" or \"LABEL\"\n        :return: values of given type defined on this line\n        \"\"\"\n        if context_type.upper() == \"ENV\":\n            return self.line_envs\n        elif context_type.upper() == \"LABEL\":\n            return self.line_labels", "language": "python", "code": "def get_line_value(self, context_type):\n        \"\"\"\n        Get the values defined on this line.\n\n        :param context_type: \"ENV\" or \"LABEL\"\n        :return: values of given type defined on this line\n        \"\"\"\n        if context_type.upper() == \"ENV\":\n            return self.line_envs\n        elif context_type.upper() == \"LABEL\":\n            return self.line_labels", "code_tokens": ["def", "get_line_value", "(", "self", ",", "context_type", ")", ":", "if", "context_type", ".", "upper", "(", ")", "==", "\"ENV\"", ":", "return", "self", ".", "line_envs", "elif", "context_type", ".", "upper", "(", ")", "==", "\"LABEL\"", ":", "return", "self", ".", "line_labels"], "docstring": "Get the values defined on this line.\n\n        :param context_type: \"ENV\" or \"LABEL\"\n        :return: values of given type defined on this line", "docstring_tokens": ["Get", "the", "values", "defined", "on", "this", "line", "."], "sha": "3d7b514d8b8eded1b33529cf0f6a0770a573aee0", "url": "https://github.com/DBuildService/dockerfile-parse/blob/3d7b514d8b8eded1b33529cf0f6a0770a573aee0/dockerfile_parse/util.py#L287-L297", "partition": "train"}
{"repo": "DBuildService/dockerfile-parse", "path": "dockerfile_parse/util.py", "func_name": "Context.get_values", "original_string": "def get_values(self, context_type):\n        \"\"\"\n        Get the values valid on this line.\n\n        :param context_type: \"ENV\" or \"LABEL\"\n        :return: values of given type valid on this line\n        \"\"\"\n        if context_type.upper() == \"ENV\":\n            return self.envs\n        elif context_type.upper() == \"LABEL\":\n            return self.labels", "language": "python", "code": "def get_values(self, context_type):\n        \"\"\"\n        Get the values valid on this line.\n\n        :param context_type: \"ENV\" or \"LABEL\"\n        :return: values of given type valid on this line\n        \"\"\"\n        if context_type.upper() == \"ENV\":\n            return self.envs\n        elif context_type.upper() == \"LABEL\":\n            return self.labels", "code_tokens": ["def", "get_values", "(", "self", ",", "context_type", ")", ":", "if", "context_type", ".", "upper", "(", ")", "==", "\"ENV\"", ":", "return", "self", ".", "envs", "elif", "context_type", ".", "upper", "(", ")", "==", "\"LABEL\"", ":", "return", "self", ".", "labels"], "docstring": "Get the values valid on this line.\n\n        :param context_type: \"ENV\" or \"LABEL\"\n        :return: values of given type valid on this line", "docstring_tokens": ["Get", "the", "values", "valid", "on", "this", "line", "."], "sha": "3d7b514d8b8eded1b33529cf0f6a0770a573aee0", "url": "https://github.com/DBuildService/dockerfile-parse/blob/3d7b514d8b8eded1b33529cf0f6a0770a573aee0/dockerfile_parse/util.py#L299-L309", "partition": "train"}
{"repo": "gvanderheide/discreteMarkovChain", "path": "discreteMarkovChain/usefulFunctions.py", "func_name": "uniqueStates", "original_string": "def uniqueStates(states,rates):\n    \"\"\"\n    Returns unique states and sums up the corresponding rates.\n    States should be a 2d numpy array with on each row a state, and rates a 1d numpy array with length equal to the number of rows in states.      \n    \n    This may be helpful in the transition function for summing up the rates of different transitions that lead to the same state            \n    \"\"\"        \n    order     = np.lexsort(states.T)\n    states    = states[order]\n    diff      = np.ones(len(states), 'bool')\n    diff[1:]  = (states[1:] != states[:-1]).any(-1)\n    sums      = np.bincount(diff.cumsum() - 1, rates[order])\n    return states[diff], sums", "language": "python", "code": "def uniqueStates(states,rates):\n    \"\"\"\n    Returns unique states and sums up the corresponding rates.\n    States should be a 2d numpy array with on each row a state, and rates a 1d numpy array with length equal to the number of rows in states.      \n    \n    This may be helpful in the transition function for summing up the rates of different transitions that lead to the same state            \n    \"\"\"        \n    order     = np.lexsort(states.T)\n    states    = states[order]\n    diff      = np.ones(len(states), 'bool')\n    diff[1:]  = (states[1:] != states[:-1]).any(-1)\n    sums      = np.bincount(diff.cumsum() - 1, rates[order])\n    return states[diff], sums", "code_tokens": ["def", "uniqueStates", "(", "states", ",", "rates", ")", ":", "order", "=", "np", ".", "lexsort", "(", "states", ".", "T", ")", "states", "=", "states", "[", "order", "]", "diff", "=", "np", ".", "ones", "(", "len", "(", "states", ")", ",", "'bool'", ")", "diff", "[", "1", ":", "]", "=", "(", "states", "[", "1", ":", "]", "!=", "states", "[", ":", "-", "1", "]", ")", ".", "any", "(", "-", "1", ")", "sums", "=", "np", ".", "bincount", "(", "diff", ".", "cumsum", "(", ")", "-", "1", ",", "rates", "[", "order", "]", ")", "return", "states", "[", "diff", "]", ",", "sums"], "docstring": "Returns unique states and sums up the corresponding rates.\n    States should be a 2d numpy array with on each row a state, and rates a 1d numpy array with length equal to the number of rows in states.      \n    \n    This may be helpful in the transition function for summing up the rates of different transitions that lead to the same state", "docstring_tokens": ["Returns", "unique", "states", "and", "sums", "up", "the", "corresponding", "rates", ".", "States", "should", "be", "a", "2d", "numpy", "array", "with", "on", "each", "row", "a", "state", "and", "rates", "a", "1d", "numpy", "array", "with", "length", "equal", "to", "the", "number", "of", "rows", "in", "states", ".", "This", "may", "be", "helpful", "in", "the", "transition", "function", "for", "summing", "up", "the", "rates", "of", "different", "transitions", "that", "lead", "to", "the", "same", "state"], "sha": "8325ffdb791c109eee600684ee0dc9126ce80700", "url": "https://github.com/gvanderheide/discreteMarkovChain/blob/8325ffdb791c109eee600684ee0dc9126ce80700/discreteMarkovChain/usefulFunctions.py#L4-L16", "partition": "train"}
{"repo": "gvanderheide/discreteMarkovChain", "path": "discreteMarkovChain/usefulFunctions.py", "func_name": "number_of_partitions", "original_string": "def number_of_partitions(max_range, max_sum):\n    '''\n    Returns an array arr of the same shape as max_range, where\n    arr[j] = number of admissible partitions for \n             j summands bounded by max_range[j:] and with sum <= max_sum\n    '''\n    M = max_sum + 1\n    N = len(max_range) \n    arr = np.zeros(shape=(M,N), dtype = int)    \n    arr[:,-1] = np.where(np.arange(M) <= min(max_range[-1], max_sum), 1, 0)\n    for i in range(N-2,-1,-1):\n        for j in range(max_range[i]+1):\n            arr[j:,i] += arr[:M-j,i+1] \n    return arr.sum(axis = 0)", "language": "python", "code": "def number_of_partitions(max_range, max_sum):\n    '''\n    Returns an array arr of the same shape as max_range, where\n    arr[j] = number of admissible partitions for \n             j summands bounded by max_range[j:] and with sum <= max_sum\n    '''\n    M = max_sum + 1\n    N = len(max_range) \n    arr = np.zeros(shape=(M,N), dtype = int)    \n    arr[:,-1] = np.where(np.arange(M) <= min(max_range[-1], max_sum), 1, 0)\n    for i in range(N-2,-1,-1):\n        for j in range(max_range[i]+1):\n            arr[j:,i] += arr[:M-j,i+1] \n    return arr.sum(axis = 0)", "code_tokens": ["def", "number_of_partitions", "(", "max_range", ",", "max_sum", ")", ":", "M", "=", "max_sum", "+", "1", "N", "=", "len", "(", "max_range", ")", "arr", "=", "np", ".", "zeros", "(", "shape", "=", "(", "M", ",", "N", ")", ",", "dtype", "=", "int", ")", "arr", "[", ":", ",", "-", "1", "]", "=", "np", ".", "where", "(", "np", ".", "arange", "(", "M", ")", "<=", "min", "(", "max_range", "[", "-", "1", "]", ",", "max_sum", ")", ",", "1", ",", "0", ")", "for", "i", "in", "range", "(", "N", "-", "2", ",", "-", "1", ",", "-", "1", ")", ":", "for", "j", "in", "range", "(", "max_range", "[", "i", "]", "+", "1", ")", ":", "arr", "[", "j", ":", ",", "i", "]", "+=", "arr", "[", ":", "M", "-", "j", ",", "i", "+", "1", "]", "return", "arr", ".", "sum", "(", "axis", "=", "0", ")"], "docstring": "Returns an array arr of the same shape as max_range, where\n    arr[j] = number of admissible partitions for \n             j summands bounded by max_range[j:] and with sum <= max_sum", "docstring_tokens": ["Returns", "an", "array", "arr", "of", "the", "same", "shape", "as", "max_range", "where", "arr", "[", "j", "]", "=", "number", "of", "admissible", "partitions", "for", "j", "summands", "bounded", "by", "max_range", "[", "j", ":", "]", "and", "with", "sum", "<", "=", "max_sum"], "sha": "8325ffdb791c109eee600684ee0dc9126ce80700", "url": "https://github.com/gvanderheide/discreteMarkovChain/blob/8325ffdb791c109eee600684ee0dc9126ce80700/discreteMarkovChain/usefulFunctions.py#L18-L31", "partition": "train"}
{"repo": "gvanderheide/discreteMarkovChain", "path": "discreteMarkovChain/usefulFunctions.py", "func_name": "partition_zero", "original_string": "def partition_zero(max_range, max_sum, out = None, n_part = None):\n    '''\n    Function that can be helpful for obtaining the state space of a discrete Markov chain or Markov decision processes. \n    Returns a 2d-array with on the rows all possible partitions of the ranges `0,...,max_range[j]` that add up to at most `max_sum`.\n\n    Code due to ptrj, see http://stackoverflow.com/a/36563744/1479342.  \n   \n    Parameters\n    ----------\n    max_range : array or list of ints\n        Gives the ranges for each element in the output array. Element `j` has range `np.arange(max_range[j]+1)`. \n    max_sum : int\n        The maximum sum for each partition in the output array. \n       \n    Returns\n    -------\n    out : array\n        2d array with all possible partitions of the ranges `0,...,max_range[j]` summing up to at most `max_sum`.\n        \n    Example\n    -------\n    >>> max_range=np.array([1,3,2])    \n    >>> max_sum = 3    \n    >>> partition_zero(max_range,max_sum)    \n    array([[0, 0, 0],\n           [0, 0, 1],\n           [0, 0, 2],\n           [0, 1, 0],\n           [0, 1, 1],\n           [0, 1, 2],\n           [0, 2, 0],\n           [0, 2, 1],\n           [0, 3, 0],\n           [1, 0, 0],\n           [1, 0, 1],\n           [1, 0, 2],\n           [1, 1, 0],\n           [1, 1, 1],\n           [1, 2, 0]])\n    '''\n    if out is None:\n        max_range = np.asarray(max_range, dtype = int).ravel()\n        n_part = number_of_partitions(max_range, max_sum)\n        out = np.zeros(shape = (n_part[0], max_range.size), dtype = int)\n\n    if(max_range.size == 1):\n        out[:] = np.arange(min(max_range[0],max_sum) + 1, dtype = int).reshape(-1,1)\n        return out\n\n    P = partition_zero(max_range[1:], max_sum, out=out[:n_part[1],1:], n_part = n_part[1:])        \n\n    S = np.minimum(max_sum - P.sum(axis = 1), max_range[0])\n    offset, sz  = 0, S.size\n    out[:sz,0] = 0\n    for i in range(1, max_range[0]+1):\n        ind, = np.nonzero(S)\n        offset, sz = offset + sz, ind.size\n        out[offset:offset+sz, 0] = i\n        out[offset:offset+sz, 1:] = P[ind]\n        S[ind] -= 1\n    return out", "language": "python", "code": "def partition_zero(max_range, max_sum, out = None, n_part = None):\n    '''\n    Function that can be helpful for obtaining the state space of a discrete Markov chain or Markov decision processes. \n    Returns a 2d-array with on the rows all possible partitions of the ranges `0,...,max_range[j]` that add up to at most `max_sum`.\n\n    Code due to ptrj, see http://stackoverflow.com/a/36563744/1479342.  \n   \n    Parameters\n    ----------\n    max_range : array or list of ints\n        Gives the ranges for each element in the output array. Element `j` has range `np.arange(max_range[j]+1)`. \n    max_sum : int\n        The maximum sum for each partition in the output array. \n       \n    Returns\n    -------\n    out : array\n        2d array with all possible partitions of the ranges `0,...,max_range[j]` summing up to at most `max_sum`.\n        \n    Example\n    -------\n    >>> max_range=np.array([1,3,2])    \n    >>> max_sum = 3    \n    >>> partition_zero(max_range,max_sum)    \n    array([[0, 0, 0],\n           [0, 0, 1],\n           [0, 0, 2],\n           [0, 1, 0],\n           [0, 1, 1],\n           [0, 1, 2],\n           [0, 2, 0],\n           [0, 2, 1],\n           [0, 3, 0],\n           [1, 0, 0],\n           [1, 0, 1],\n           [1, 0, 2],\n           [1, 1, 0],\n           [1, 1, 1],\n           [1, 2, 0]])\n    '''\n    if out is None:\n        max_range = np.asarray(max_range, dtype = int).ravel()\n        n_part = number_of_partitions(max_range, max_sum)\n        out = np.zeros(shape = (n_part[0], max_range.size), dtype = int)\n\n    if(max_range.size == 1):\n        out[:] = np.arange(min(max_range[0],max_sum) + 1, dtype = int).reshape(-1,1)\n        return out\n\n    P = partition_zero(max_range[1:], max_sum, out=out[:n_part[1],1:], n_part = n_part[1:])        \n\n    S = np.minimum(max_sum - P.sum(axis = 1), max_range[0])\n    offset, sz  = 0, S.size\n    out[:sz,0] = 0\n    for i in range(1, max_range[0]+1):\n        ind, = np.nonzero(S)\n        offset, sz = offset + sz, ind.size\n        out[offset:offset+sz, 0] = i\n        out[offset:offset+sz, 1:] = P[ind]\n        S[ind] -= 1\n    return out", "code_tokens": ["def", "partition_zero", "(", "max_range", ",", "max_sum", ",", "out", "=", "None", ",", "n_part", "=", "None", ")", ":", "if", "out", "is", "None", ":", "max_range", "=", "np", ".", "asarray", "(", "max_range", ",", "dtype", "=", "int", ")", ".", "ravel", "(", ")", "n_part", "=", "number_of_partitions", "(", "max_range", ",", "max_sum", ")", "out", "=", "np", ".", "zeros", "(", "shape", "=", "(", "n_part", "[", "0", "]", ",", "max_range", ".", "size", ")", ",", "dtype", "=", "int", ")", "if", "(", "max_range", ".", "size", "==", "1", ")", ":", "out", "[", ":", "]", "=", "np", ".", "arange", "(", "min", "(", "max_range", "[", "0", "]", ",", "max_sum", ")", "+", "1", ",", "dtype", "=", "int", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "return", "out", "P", "=", "partition_zero", "(", "max_range", "[", "1", ":", "]", ",", "max_sum", ",", "out", "=", "out", "[", ":", "n_part", "[", "1", "]", ",", "1", ":", "]", ",", "n_part", "=", "n_part", "[", "1", ":", "]", ")", "S", "=", "np", ".", "minimum", "(", "max_sum", "-", "P", ".", "sum", "(", "axis", "=", "1", ")", ",", "max_range", "[", "0", "]", ")", "offset", ",", "sz", "=", "0", ",", "S", ".", "size", "out", "[", ":", "sz", ",", "0", "]", "=", "0", "for", "i", "in", "range", "(", "1", ",", "max_range", "[", "0", "]", "+", "1", ")", ":", "ind", ",", "=", "np", ".", "nonzero", "(", "S", ")", "offset", ",", "sz", "=", "offset", "+", "sz", ",", "ind", ".", "size", "out", "[", "offset", ":", "offset", "+", "sz", ",", "0", "]", "=", "i", "out", "[", "offset", ":", "offset", "+", "sz", ",", "1", ":", "]", "=", "P", "[", "ind", "]", "S", "[", "ind", "]", "-=", "1", "return", "out"], "docstring": "Function that can be helpful for obtaining the state space of a discrete Markov chain or Markov decision processes. \n    Returns a 2d-array with on the rows all possible partitions of the ranges `0,...,max_range[j]` that add up to at most `max_sum`.\n\n    Code due to ptrj, see http://stackoverflow.com/a/36563744/1479342.  \n   \n    Parameters\n    ----------\n    max_range : array or list of ints\n        Gives the ranges for each element in the output array. Element `j` has range `np.arange(max_range[j]+1)`. \n    max_sum : int\n        The maximum sum for each partition in the output array. \n       \n    Returns\n    -------\n    out : array\n        2d array with all possible partitions of the ranges `0,...,max_range[j]` summing up to at most `max_sum`.\n        \n    Example\n    -------\n    >>> max_range=np.array([1,3,2])    \n    >>> max_sum = 3    \n    >>> partition_zero(max_range,max_sum)    \n    array([[0, 0, 0],\n           [0, 0, 1],\n           [0, 0, 2],\n           [0, 1, 0],\n           [0, 1, 1],\n           [0, 1, 2],\n           [0, 2, 0],\n           [0, 2, 1],\n           [0, 3, 0],\n           [1, 0, 0],\n           [1, 0, 1],\n           [1, 0, 2],\n           [1, 1, 0],\n           [1, 1, 1],\n           [1, 2, 0]])", "docstring_tokens": ["Function", "that", "can", "be", "helpful", "for", "obtaining", "the", "state", "space", "of", "a", "discrete", "Markov", "chain", "or", "Markov", "decision", "processes", ".", "Returns", "a", "2d", "-", "array", "with", "on", "the", "rows", "all", "possible", "partitions", "of", "the", "ranges", "0", "...", "max_range", "[", "j", "]", "that", "add", "up", "to", "at", "most", "max_sum", "."], "sha": "8325ffdb791c109eee600684ee0dc9126ce80700", "url": "https://github.com/gvanderheide/discreteMarkovChain/blob/8325ffdb791c109eee600684ee0dc9126ce80700/discreteMarkovChain/usefulFunctions.py#L33-L93", "partition": "train"}
{"repo": "gvanderheide/discreteMarkovChain", "path": "discreteMarkovChain/usefulFunctions.py", "func_name": "partition", "original_string": "def partition(min_range,max_range,max_sum=None):\n    '''\n    Function that can be helpful for obtaining the state space of a discrete Markov chain or Markov decision processes. \n    Returns a 2d-array with on the rows all possible partitions of the ranges `min_range[j],...,max_range[j]` for each j. \n    If the argument `max_sum` is specified, the numbers will add up to at most `max_sum`\n    \n    Parameters\n    ----------\n    min_range : array or list of ints\n        `min_range[j]` gives the minimum value for element `j` in the output array. \n    max_range : array or list of ints\n        `max_range[j]` gives the maximum value for element `j` in the output array. \n    max_sum : int\n        Optional argument. The maximum sum for each partition in the output array. \n       \n    Returns\n    -------\n    out : array\n        2d array with all possible partitions of the ranges `min_range[j],...,max_range[j]` summing up to at most `max_sum`.\n        \n    Example\n    -------\n    >>> min_range=np.array([1,2,1])\n    >>> max_range=np.array([2,4,3])    \n    >>> max_sum = 6    \n    >>> partition(min_range,max_range,max_sum)    \n    array([[1, 2, 1],\n           [1, 2, 2],\n           [1, 2, 3],\n           [1, 3, 1],\n           [1, 3, 2],\n           [1, 4, 1],\n           [2, 2, 1],\n           [2, 2, 2],          \n           [2, 3, 1]])\n    '''\n    max_range = np.asarray(max_range, dtype = int).ravel()\n    min_range = np.asarray(min_range, dtype = int).ravel()\n    full_range = max_range-min_range\n    if any(full_range<0):\n        raise ValueError(\"max_range needs to be larger than min_range\")\n    if max_sum == None:\n        max_sum = np.sum(full_range)\n    else:\n        max_sum -= np.sum(min_range)\n    out = partition_zero(full_range,max_sum)\n    out += min_range\n    return out", "language": "python", "code": "def partition(min_range,max_range,max_sum=None):\n    '''\n    Function that can be helpful for obtaining the state space of a discrete Markov chain or Markov decision processes. \n    Returns a 2d-array with on the rows all possible partitions of the ranges `min_range[j],...,max_range[j]` for each j. \n    If the argument `max_sum` is specified, the numbers will add up to at most `max_sum`\n    \n    Parameters\n    ----------\n    min_range : array or list of ints\n        `min_range[j]` gives the minimum value for element `j` in the output array. \n    max_range : array or list of ints\n        `max_range[j]` gives the maximum value for element `j` in the output array. \n    max_sum : int\n        Optional argument. The maximum sum for each partition in the output array. \n       \n    Returns\n    -------\n    out : array\n        2d array with all possible partitions of the ranges `min_range[j],...,max_range[j]` summing up to at most `max_sum`.\n        \n    Example\n    -------\n    >>> min_range=np.array([1,2,1])\n    >>> max_range=np.array([2,4,3])    \n    >>> max_sum = 6    \n    >>> partition(min_range,max_range,max_sum)    \n    array([[1, 2, 1],\n           [1, 2, 2],\n           [1, 2, 3],\n           [1, 3, 1],\n           [1, 3, 2],\n           [1, 4, 1],\n           [2, 2, 1],\n           [2, 2, 2],          \n           [2, 3, 1]])\n    '''\n    max_range = np.asarray(max_range, dtype = int).ravel()\n    min_range = np.asarray(min_range, dtype = int).ravel()\n    full_range = max_range-min_range\n    if any(full_range<0):\n        raise ValueError(\"max_range needs to be larger than min_range\")\n    if max_sum == None:\n        max_sum = np.sum(full_range)\n    else:\n        max_sum -= np.sum(min_range)\n    out = partition_zero(full_range,max_sum)\n    out += min_range\n    return out", "code_tokens": ["def", "partition", "(", "min_range", ",", "max_range", ",", "max_sum", "=", "None", ")", ":", "max_range", "=", "np", ".", "asarray", "(", "max_range", ",", "dtype", "=", "int", ")", ".", "ravel", "(", ")", "min_range", "=", "np", ".", "asarray", "(", "min_range", ",", "dtype", "=", "int", ")", ".", "ravel", "(", ")", "full_range", "=", "max_range", "-", "min_range", "if", "any", "(", "full_range", "<", "0", ")", ":", "raise", "ValueError", "(", "\"max_range needs to be larger than min_range\"", ")", "if", "max_sum", "==", "None", ":", "max_sum", "=", "np", ".", "sum", "(", "full_range", ")", "else", ":", "max_sum", "-=", "np", ".", "sum", "(", "min_range", ")", "out", "=", "partition_zero", "(", "full_range", ",", "max_sum", ")", "out", "+=", "min_range", "return", "out"], "docstring": "Function that can be helpful for obtaining the state space of a discrete Markov chain or Markov decision processes. \n    Returns a 2d-array with on the rows all possible partitions of the ranges `min_range[j],...,max_range[j]` for each j. \n    If the argument `max_sum` is specified, the numbers will add up to at most `max_sum`\n    \n    Parameters\n    ----------\n    min_range : array or list of ints\n        `min_range[j]` gives the minimum value for element `j` in the output array. \n    max_range : array or list of ints\n        `max_range[j]` gives the maximum value for element `j` in the output array. \n    max_sum : int\n        Optional argument. The maximum sum for each partition in the output array. \n       \n    Returns\n    -------\n    out : array\n        2d array with all possible partitions of the ranges `min_range[j],...,max_range[j]` summing up to at most `max_sum`.\n        \n    Example\n    -------\n    >>> min_range=np.array([1,2,1])\n    >>> max_range=np.array([2,4,3])    \n    >>> max_sum = 6    \n    >>> partition(min_range,max_range,max_sum)    \n    array([[1, 2, 1],\n           [1, 2, 2],\n           [1, 2, 3],\n           [1, 3, 1],\n           [1, 3, 2],\n           [1, 4, 1],\n           [2, 2, 1],\n           [2, 2, 2],          \n           [2, 3, 1]])", "docstring_tokens": ["Function", "that", "can", "be", "helpful", "for", "obtaining", "the", "state", "space", "of", "a", "discrete", "Markov", "chain", "or", "Markov", "decision", "processes", ".", "Returns", "a", "2d", "-", "array", "with", "on", "the", "rows", "all", "possible", "partitions", "of", "the", "ranges", "min_range", "[", "j", "]", "...", "max_range", "[", "j", "]", "for", "each", "j", ".", "If", "the", "argument", "max_sum", "is", "specified", "the", "numbers", "will", "add", "up", "to", "at", "most", "max_sum", "Parameters", "----------", "min_range", ":", "array", "or", "list", "of", "ints", "min_range", "[", "j", "]", "gives", "the", "minimum", "value", "for", "element", "j", "in", "the", "output", "array", ".", "max_range", ":", "array", "or", "list", "of", "ints", "max_range", "[", "j", "]", "gives", "the", "maximum", "value", "for", "element", "j", "in", "the", "output", "array", ".", "max_sum", ":", "int", "Optional", "argument", ".", "The", "maximum", "sum", "for", "each", "partition", "in", "the", "output", "array", ".", "Returns", "-------", "out", ":", "array", "2d", "array", "with", "all", "possible", "partitions", "of", "the", "ranges", "min_range", "[", "j", "]", "...", "max_range", "[", "j", "]", "summing", "up", "to", "at", "most", "max_sum", ".", "Example", "-------", ">>>", "min_range", "=", "np", ".", "array", "(", "[", "1", "2", "1", "]", ")", ">>>", "max_range", "=", "np", ".", "array", "(", "[", "2", "4", "3", "]", ")", ">>>", "max_sum", "=", "6", ">>>", "partition", "(", "min_range", "max_range", "max_sum", ")", "array", "(", "[[", "1", "2", "1", "]", "[", "1", "2", "2", "]", "[", "1", "2", "3", "]", "[", "1", "3", "1", "]", "[", "1", "3", "2", "]", "[", "1", "4", "1", "]", "[", "2", "2", "1", "]", "[", "2", "2", "2", "]", "[", "2", "3", "1", "]]", ")"], "sha": "8325ffdb791c109eee600684ee0dc9126ce80700", "url": "https://github.com/gvanderheide/discreteMarkovChain/blob/8325ffdb791c109eee600684ee0dc9126ce80700/discreteMarkovChain/usefulFunctions.py#L95-L142", "partition": "train"}
{"repo": "obsrvbl/flowlogs-reader", "path": "flowlogs_reader/__main__.py", "func_name": "action_findip", "original_string": "def action_findip(reader, *args):\n    \"\"\"Find Flow Log records involving a specific IP or IPs.\"\"\"\n    target_ips = set(args)\n    for record in reader:\n        if (record.srcaddr in target_ips) or (record.dstaddr in target_ips):\n            print(record.to_message())", "language": "python", "code": "def action_findip(reader, *args):\n    \"\"\"Find Flow Log records involving a specific IP or IPs.\"\"\"\n    target_ips = set(args)\n    for record in reader:\n        if (record.srcaddr in target_ips) or (record.dstaddr in target_ips):\n            print(record.to_message())", "code_tokens": ["def", "action_findip", "(", "reader", ",", "*", "args", ")", ":", "target_ips", "=", "set", "(", "args", ")", "for", "record", "in", "reader", ":", "if", "(", "record", ".", "srcaddr", "in", "target_ips", ")", "or", "(", "record", ".", "dstaddr", "in", "target_ips", ")", ":", "print", "(", "record", ".", "to_message", "(", ")", ")"], "docstring": "Find Flow Log records involving a specific IP or IPs.", "docstring_tokens": ["Find", "Flow", "Log", "records", "involving", "a", "specific", "IP", "or", "IPs", "."], "sha": "248d8cb3cc586859b6744d30cebce0f359d9900c", "url": "https://github.com/obsrvbl/flowlogs-reader/blob/248d8cb3cc586859b6744d30cebce0f359d9900c/flowlogs_reader/__main__.py#L66-L71", "partition": "train"}
{"repo": "obsrvbl/flowlogs-reader", "path": "flowlogs_reader/__main__.py", "func_name": "action_aggregate", "original_string": "def action_aggregate(reader, *args):\n    \"\"\"Aggregate flow records by 5-tuple and print a tab-separated stream\"\"\"\n    all_aggregated = aggregated_records(reader)\n    first_row = next(all_aggregated)\n    keys = sorted(first_row.keys())\n    print(*keys, sep='\\t')\n\n    # Join the first row with the rest of the rows and print them\n    iterable = chain([first_row], all_aggregated)\n    for item in iterable:\n        print(*[item[k] for k in keys], sep='\\t')", "language": "python", "code": "def action_aggregate(reader, *args):\n    \"\"\"Aggregate flow records by 5-tuple and print a tab-separated stream\"\"\"\n    all_aggregated = aggregated_records(reader)\n    first_row = next(all_aggregated)\n    keys = sorted(first_row.keys())\n    print(*keys, sep='\\t')\n\n    # Join the first row with the rest of the rows and print them\n    iterable = chain([first_row], all_aggregated)\n    for item in iterable:\n        print(*[item[k] for k in keys], sep='\\t')", "code_tokens": ["def", "action_aggregate", "(", "reader", ",", "*", "args", ")", ":", "all_aggregated", "=", "aggregated_records", "(", "reader", ")", "first_row", "=", "next", "(", "all_aggregated", ")", "keys", "=", "sorted", "(", "first_row", ".", "keys", "(", ")", ")", "print", "(", "*", "keys", ",", "sep", "=", "'\\t'", ")", "# Join the first row with the rest of the rows and print them", "iterable", "=", "chain", "(", "[", "first_row", "]", ",", "all_aggregated", ")", "for", "item", "in", "iterable", ":", "print", "(", "*", "[", "item", "[", "k", "]", "for", "k", "in", "keys", "]", ",", "sep", "=", "'\\t'", ")"], "docstring": "Aggregate flow records by 5-tuple and print a tab-separated stream", "docstring_tokens": ["Aggregate", "flow", "records", "by", "5", "-", "tuple", "and", "print", "a", "tab", "-", "separated", "stream"], "sha": "248d8cb3cc586859b6744d30cebce0f359d9900c", "url": "https://github.com/obsrvbl/flowlogs-reader/blob/248d8cb3cc586859b6744d30cebce0f359d9900c/flowlogs_reader/__main__.py#L77-L87", "partition": "train"}
{"repo": "erdc/RAPIDpy", "path": "RAPIDpy/postprocess/generate_return_periods.py", "func_name": "generate_single_return_period", "original_string": "def generate_single_return_period(args):\n    \"\"\"\n    This function calculates a single return period for a single reach\n    \"\"\"\n    qout_file, return_period_file, rivid_index_list, step, num_years, \\\n        method, mp_lock = args\n\n    skewvals = [-3.0, -2.8, -2.6, -2.4, -2.2, -2.0, -1.8, -1.6, -1.4, -1.2,\n                -1.0, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0,\n                1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0]\n    kfac2 = [0.396, 0.384, 0.368, 0.351, 0.33, 0.307, 0.282, 0.254, 0.225,\n             0.195, 0.164, 0.132, 0.099, 0.066, 0.033, 0, -0.033, -0.066,\n             -0.099, -0.132, -0.164, -0.195, -0.225, -0.254, -0.282, -0.307,\n             -0.33, -0.351, -0.368, -0.384, -0.396]\n    kfac10 = [0.66, 0.702, 0.747, 0.795, 0.844, 0.895, 0.945, 0.994, 1.041,\n              1.086, 1.128, 1.166, 1.2, 1.231, 1.258, 1.282, 1.301, 1.317,\n              1.328, 1.336, 1.34, 1.34, 1.337, 1.329, 1.318, 1.302, 1.284,\n              1.262, 1.238, 1.21, 1.18]\n    kfac25 = [.666, .712, .764, .823, .888, .959, 1.035, 1.116, 1.198, 1.282,\n              1.366, 1.448, 1.528, 1.606, 1.680, 1.751, 1.818, 1.880, 1.939,\n              1.993, 2.043, 2.087, 2.128, 2.163, 2.193, 2.219, 2.240, 2.256,\n              2.267, 2.275, 2.278]\n    kfac50 = [0.666, 0.714, 0.768, 0.83, 0.9, 0.98, 1.069, 1.166, 1.27, 1.379,\n              1.492, 1.606, 1.72, 1.834, 1.945, 2.054, 2.159, 2.261, 2.359,\n              2.453, 2.542, 2.626, 2.706, 2.78, 2.848, 2.912, 2.97, 3.023,\n              3.071, 3.114, 3.152]\n    kfac100 = [0.667, 0.714, 0.769, 0.832, 0.905, 0.99, 1.087, 1.197, 1.318,\n               1.499, 1.588, 1.733, 1.88, 2.029, 2.178, 2.326, 2.472, 2.615,\n               2.755, 2.891, 3.022, 3.149, 3.271, 3.388, 3.499, 3.605, 3.705,\n               3.8, 3.889, 3.973, 4.051]\n\n    with RAPIDDataset(qout_file) as qout_nc_file:\n        # get index of return period data\n        if method == 'weibull':\n            rp_index_20 = int((num_years + 1)/20.0)\n            rp_index_10 = int((num_years + 1)/10.0)\n            rp_index_2 = int((num_years + 1)/2.0)\n\n        if method == 'weibull':\n            return_20_array = np.zeros(len(rivid_index_list))\n        elif method == 'gumble':\n            return_100_array = np.zeros(len(rivid_index_list))\n            return_50_array = np.zeros(len(rivid_index_list))\n            return_20_array = np.zeros(len(rivid_index_list))\n        elif method == 'log_pearson':\n            return_100_array = np.zeros(len(rivid_index_list))\n            return_50_array = np.zeros(len(rivid_index_list))\n            return_25_array = np.zeros(len(rivid_index_list))\n        return_10_array = np.zeros(len(rivid_index_list))\n        return_2_array = np.zeros(len(rivid_index_list))\n        max_flow_array = np.zeros(len(rivid_index_list))\n\n        # iterate through rivids to generate return periods\n        for iter_idx, rivid_index in enumerate(rivid_index_list):\n            filtered_flow_data = qout_nc_file.get_qout_index(\n                rivid_index,\n                pd_filter=\"{0}D\".format(step),\n                filter_mode=\"max\")\n            sorted_flow_data = np.sort(filtered_flow_data)[:num_years:-1]\n            max_flow = sorted_flow_data[0]\n            if max_flow < 0.01:\n                log(\"Return period data < 0.01 generated for rivid {0}\"\n                    .format(qout_nc_file.qout_nc.variables[\n                        qout_nc_file.river_id_dimension][rivid_index]),\n                    \"WARNING\")\n            max_flow_array[iter_idx] = max_flow\n\n            if method == 'weibull':\n                return_20_array[iter_idx] = sorted_flow_data[rp_index_20]\n                return_10_array[iter_idx] = sorted_flow_data[rp_index_10]\n                return_2_array[iter_idx] = sorted_flow_data[rp_index_2]\n\n            elif method == 'gumble':\n                mean_flow = np.mean(filtered_flow_data)\n                stddev = np.std(filtered_flow_data)\n                return_100_array[iter_idx] = mean_flow + 3.14*stddev\n                return_50_array[iter_idx] = mean_flow + 2.59*stddev\n                return_20_array[iter_idx] = mean_flow + 1.87*stddev\n                return_10_array[iter_idx] = mean_flow + 1.3*stddev\n                return_2_array[iter_idx] = mean_flow - .164*stddev\n\n            elif method == 'log_pearson':\n                log_flow = np.log10(filtered_flow_data[filtered_flow_data > 0])\n                if len(log_flow) <= 0:\n                    continue\n                mean_log_flow = np.mean(log_flow)\n                std_log_flow = np.std(log_flow)\n                log_flow_array = np.array(log_flow)\n                skew = (num_years * (np.sum(\n                    np.power((log_flow_array - mean_log_flow), 3)))) / \\\n                    ((num_years - 1) * (num_years - 2) * std_log_flow ** 3)\n                k2 = np.interp(skew, skewvals, kfac2)\n                k10 = np.interp(skew, skewvals, kfac10)\n                k25 = np.interp(skew, skewvals, kfac25)\n                k50 = np.interp(skew, skewvals, kfac50)\n                k100 = np.interp(skew, skewvals, kfac100)\n                return_100_array[iter_idx] = \\\n                    np.power(10, (mean_log_flow + k100*std_log_flow))\n                return_50_array[iter_idx] = \\\n                    np.power(10, (mean_log_flow + k50*std_log_flow))\n                return_25_array[iter_idx] = \\\n                    np.power(10, (mean_log_flow + k25*std_log_flow))\n                return_10_array[iter_idx] = \\\n                    np.power(10, (mean_log_flow + k10*std_log_flow))\n                return_2_array[iter_idx] = \\\n                    np.power(10, (mean_log_flow + k2*std_log_flow))\n\n        mp_lock.acquire()\n        return_period_nc = Dataset(return_period_file, 'a')\n        return_period_nc.variables['max_flow'][rivid_index_list] = \\\n            max_flow_array\n        if method == 'weibull':\n            return_period_nc.variables['return_period_20'][\n                rivid_index_list] = return_20_array\n        elif method in 'gumble':\n            return_period_nc.variables['return_period_100'][\n                rivid_index_list] = return_100_array\n            return_period_nc.variables['return_period_50'][\n                rivid_index_list] = return_50_array\n            return_period_nc.variables['return_period_20'][\n                rivid_index_list] = return_20_array\n        elif method == 'log_pearson':\n            return_period_nc.variables['return_period_100'][\n                rivid_index_list] = return_100_array\n            return_period_nc.variables['return_period_50'][\n                rivid_index_list] = return_50_array\n            return_period_nc.variables['return_period_25'][\n                rivid_index_list] = return_25_array\n        return_period_nc.variables['return_period_10'][\n            rivid_index_list] = return_10_array\n        return_period_nc.variables['return_period_2'][\n            rivid_index_list] = return_2_array\n        return_period_nc.close()\n        mp_lock.release()", "language": "python", "code": "def generate_single_return_period(args):\n    \"\"\"\n    This function calculates a single return period for a single reach\n    \"\"\"\n    qout_file, return_period_file, rivid_index_list, step, num_years, \\\n        method, mp_lock = args\n\n    skewvals = [-3.0, -2.8, -2.6, -2.4, -2.2, -2.0, -1.8, -1.6, -1.4, -1.2,\n                -1.0, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0,\n                1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0]\n    kfac2 = [0.396, 0.384, 0.368, 0.351, 0.33, 0.307, 0.282, 0.254, 0.225,\n             0.195, 0.164, 0.132, 0.099, 0.066, 0.033, 0, -0.033, -0.066,\n             -0.099, -0.132, -0.164, -0.195, -0.225, -0.254, -0.282, -0.307,\n             -0.33, -0.351, -0.368, -0.384, -0.396]\n    kfac10 = [0.66, 0.702, 0.747, 0.795, 0.844, 0.895, 0.945, 0.994, 1.041,\n              1.086, 1.128, 1.166, 1.2, 1.231, 1.258, 1.282, 1.301, 1.317,\n              1.328, 1.336, 1.34, 1.34, 1.337, 1.329, 1.318, 1.302, 1.284,\n              1.262, 1.238, 1.21, 1.18]\n    kfac25 = [.666, .712, .764, .823, .888, .959, 1.035, 1.116, 1.198, 1.282,\n              1.366, 1.448, 1.528, 1.606, 1.680, 1.751, 1.818, 1.880, 1.939,\n              1.993, 2.043, 2.087, 2.128, 2.163, 2.193, 2.219, 2.240, 2.256,\n              2.267, 2.275, 2.278]\n    kfac50 = [0.666, 0.714, 0.768, 0.83, 0.9, 0.98, 1.069, 1.166, 1.27, 1.379,\n              1.492, 1.606, 1.72, 1.834, 1.945, 2.054, 2.159, 2.261, 2.359,\n              2.453, 2.542, 2.626, 2.706, 2.78, 2.848, 2.912, 2.97, 3.023,\n              3.071, 3.114, 3.152]\n    kfac100 = [0.667, 0.714, 0.769, 0.832, 0.905, 0.99, 1.087, 1.197, 1.318,\n               1.499, 1.588, 1.733, 1.88, 2.029, 2.178, 2.326, 2.472, 2.615,\n               2.755, 2.891, 3.022, 3.149, 3.271, 3.388, 3.499, 3.605, 3.705,\n               3.8, 3.889, 3.973, 4.051]\n\n    with RAPIDDataset(qout_file) as qout_nc_file:\n        # get index of return period data\n        if method == 'weibull':\n            rp_index_20 = int((num_years + 1)/20.0)\n            rp_index_10 = int((num_years + 1)/10.0)\n            rp_index_2 = int((num_years + 1)/2.0)\n\n        if method == 'weibull':\n            return_20_array = np.zeros(len(rivid_index_list))\n        elif method == 'gumble':\n            return_100_array = np.zeros(len(rivid_index_list))\n            return_50_array = np.zeros(len(rivid_index_list))\n            return_20_array = np.zeros(len(rivid_index_list))\n        elif method == 'log_pearson':\n            return_100_array = np.zeros(len(rivid_index_list))\n            return_50_array = np.zeros(len(rivid_index_list))\n            return_25_array = np.zeros(len(rivid_index_list))\n        return_10_array = np.zeros(len(rivid_index_list))\n        return_2_array = np.zeros(len(rivid_index_list))\n        max_flow_array = np.zeros(len(rivid_index_list))\n\n        # iterate through rivids to generate return periods\n        for iter_idx, rivid_index in enumerate(rivid_index_list):\n            filtered_flow_data = qout_nc_file.get_qout_index(\n                rivid_index,\n                pd_filter=\"{0}D\".format(step),\n                filter_mode=\"max\")\n            sorted_flow_data = np.sort(filtered_flow_data)[:num_years:-1]\n            max_flow = sorted_flow_data[0]\n            if max_flow < 0.01:\n                log(\"Return period data < 0.01 generated for rivid {0}\"\n                    .format(qout_nc_file.qout_nc.variables[\n                        qout_nc_file.river_id_dimension][rivid_index]),\n                    \"WARNING\")\n            max_flow_array[iter_idx] = max_flow\n\n            if method == 'weibull':\n                return_20_array[iter_idx] = sorted_flow_data[rp_index_20]\n                return_10_array[iter_idx] = sorted_flow_data[rp_index_10]\n                return_2_array[iter_idx] = sorted_flow_data[rp_index_2]\n\n            elif method == 'gumble':\n                mean_flow = np.mean(filtered_flow_data)\n                stddev = np.std(filtered_flow_data)\n                return_100_array[iter_idx] = mean_flow + 3.14*stddev\n                return_50_array[iter_idx] = mean_flow + 2.59*stddev\n                return_20_array[iter_idx] = mean_flow + 1.87*stddev\n                return_10_array[iter_idx] = mean_flow + 1.3*stddev\n                return_2_array[iter_idx] = mean_flow - .164*stddev\n\n            elif method == 'log_pearson':\n                log_flow = np.log10(filtered_flow_data[filtered_flow_data > 0])\n                if len(log_flow) <= 0:\n                    continue\n                mean_log_flow = np.mean(log_flow)\n                std_log_flow = np.std(log_flow)\n                log_flow_array = np.array(log_flow)\n                skew = (num_years * (np.sum(\n                    np.power((log_flow_array - mean_log_flow), 3)))) / \\\n                    ((num_years - 1) * (num_years - 2) * std_log_flow ** 3)\n                k2 = np.interp(skew, skewvals, kfac2)\n                k10 = np.interp(skew, skewvals, kfac10)\n                k25 = np.interp(skew, skewvals, kfac25)\n                k50 = np.interp(skew, skewvals, kfac50)\n                k100 = np.interp(skew, skewvals, kfac100)\n                return_100_array[iter_idx] = \\\n                    np.power(10, (mean_log_flow + k100*std_log_flow))\n                return_50_array[iter_idx] = \\\n                    np.power(10, (mean_log_flow + k50*std_log_flow))\n                return_25_array[iter_idx] = \\\n                    np.power(10, (mean_log_flow + k25*std_log_flow))\n                return_10_array[iter_idx] = \\\n                    np.power(10, (mean_log_flow + k10*std_log_flow))\n                return_2_array[iter_idx] = \\\n                    np.power(10, (mean_log_flow + k2*std_log_flow))\n\n        mp_lock.acquire()\n        return_period_nc = Dataset(return_period_file, 'a')\n        return_period_nc.variables['max_flow'][rivid_index_list] = \\\n            max_flow_array\n        if method == 'weibull':\n            return_period_nc.variables['return_period_20'][\n                rivid_index_list] = return_20_array\n        elif method in 'gumble':\n            return_period_nc.variables['return_period_100'][\n                rivid_index_list] = return_100_array\n            return_period_nc.variables['return_period_50'][\n                rivid_index_list] = return_50_array\n            return_period_nc.variables['return_period_20'][\n                rivid_index_list] = return_20_array\n        elif method == 'log_pearson':\n            return_period_nc.variables['return_period_100'][\n                rivid_index_list] = return_100_array\n            return_period_nc.variables['return_period_50'][\n                rivid_index_list] = return_50_array\n            return_period_nc.variables['return_period_25'][\n                rivid_index_list] = return_25_array\n        return_period_nc.variables['return_period_10'][\n            rivid_index_list] = return_10_array\n        return_period_nc.variables['return_period_2'][\n            rivid_index_list] = return_2_array\n        return_period_nc.close()\n        mp_lock.release()", "code_tokens": ["def", "generate_single_return_period", "(", "args", ")", ":", "qout_file", ",", "return_period_file", ",", "rivid_index_list", ",", "step", ",", "num_years", ",", "method", ",", "mp_lock", "=", "args", "skewvals", "=", "[", "-", "3.0", ",", "-", "2.8", ",", "-", "2.6", ",", "-", "2.4", ",", "-", "2.2", ",", "-", "2.0", ",", "-", "1.8", ",", "-", "1.6", ",", "-", "1.4", ",", "-", "1.2", ",", "-", "1.0", ",", "-", "0.8", ",", "-", "0.6", ",", "-", "0.4", ",", "-", "0.2", ",", "0", ",", "0.2", ",", "0.4", ",", "0.6", ",", "0.8", ",", "1.0", ",", "1.2", ",", "1.4", ",", "1.6", ",", "1.8", ",", "2.0", ",", "2.2", ",", "2.4", ",", "2.6", ",", "2.8", ",", "3.0", "]", "kfac2", "=", "[", "0.396", ",", "0.384", ",", "0.368", ",", "0.351", ",", "0.33", ",", "0.307", ",", "0.282", ",", "0.254", ",", "0.225", ",", "0.195", ",", "0.164", ",", "0.132", ",", "0.099", ",", "0.066", ",", "0.033", ",", "0", ",", "-", "0.033", ",", "-", "0.066", ",", "-", "0.099", ",", "-", "0.132", ",", "-", "0.164", ",", "-", "0.195", ",", "-", "0.225", ",", "-", "0.254", ",", "-", "0.282", ",", "-", "0.307", ",", "-", "0.33", ",", "-", "0.351", ",", "-", "0.368", ",", "-", "0.384", ",", "-", "0.396", "]", "kfac10", "=", "[", "0.66", ",", "0.702", ",", "0.747", ",", "0.795", ",", "0.844", ",", "0.895", ",", "0.945", ",", "0.994", ",", "1.041", ",", "1.086", ",", "1.128", ",", "1.166", ",", "1.2", ",", "1.231", ",", "1.258", ",", "1.282", ",", "1.301", ",", "1.317", ",", "1.328", ",", "1.336", ",", "1.34", ",", "1.34", ",", "1.337", ",", "1.329", ",", "1.318", ",", "1.302", ",", "1.284", ",", "1.262", ",", "1.238", ",", "1.21", ",", "1.18", "]", "kfac25", "=", "[", ".666", ",", ".712", ",", ".764", ",", ".823", ",", ".888", ",", ".959", ",", "1.035", ",", "1.116", ",", "1.198", ",", "1.282", ",", "1.366", ",", "1.448", ",", "1.528", ",", "1.606", ",", "1.680", ",", "1.751", ",", "1.818", ",", "1.880", ",", "1.939", ",", "1.993", ",", "2.043", ",", "2.087", ",", "2.128", ",", "2.163", ",", "2.193", ",", "2.219", ",", "2.240", ",", "2.256", ",", "2.267", ",", "2.275", ",", "2.278", "]", "kfac50", "=", "[", "0.666", ",", "0.714", ",", "0.768", ",", "0.83", ",", "0.9", ",", "0.98", ",", "1.069", ",", "1.166", ",", "1.27", ",", "1.379", ",", "1.492", ",", "1.606", ",", "1.72", ",", "1.834", ",", "1.945", ",", "2.054", ",", "2.159", ",", "2.261", ",", "2.359", ",", "2.453", ",", "2.542", ",", "2.626", ",", "2.706", ",", "2.78", ",", "2.848", ",", "2.912", ",", "2.97", ",", "3.023", ",", "3.071", ",", "3.114", ",", "3.152", "]", "kfac100", "=", "[", "0.667", ",", "0.714", ",", "0.769", ",", "0.832", ",", "0.905", ",", "0.99", ",", "1.087", ",", "1.197", ",", "1.318", ",", "1.499", ",", "1.588", ",", "1.733", ",", "1.88", ",", "2.029", ",", "2.178", ",", "2.326", ",", "2.472", ",", "2.615", ",", "2.755", ",", "2.891", ",", "3.022", ",", "3.149", ",", "3.271", ",", "3.388", ",", "3.499", ",", "3.605", ",", "3.705", ",", "3.8", ",", "3.889", ",", "3.973", ",", "4.051", "]", "with", "RAPIDDataset", "(", "qout_file", ")", "as", "qout_nc_file", ":", "# get index of return period data", "if", "method", "==", "'weibull'", ":", "rp_index_20", "=", "int", "(", "(", "num_years", "+", "1", ")", "/", "20.0", ")", "rp_index_10", "=", "int", "(", "(", "num_years", "+", "1", ")", "/", "10.0", ")", "rp_index_2", "=", "int", "(", "(", "num_years", "+", "1", ")", "/", "2.0", ")", "if", "method", "==", "'weibull'", ":", "return_20_array", "=", "np", ".", "zeros", "(", "len", "(", "rivid_index_list", ")", ")", "elif", "method", "==", "'gumble'", ":", "return_100_array", "=", "np", ".", "zeros", "(", "len", "(", "rivid_index_list", ")", ")", "return_50_array", "=", "np", ".", "zeros", "(", "len", "(", "rivid_index_list", ")", ")", "return_20_array", "=", "np", ".", "zeros", "(", "len", "(", "rivid_index_list", ")", ")", "elif", "method", "==", "'log_pearson'", ":", "return_100_array", "=", "np", ".", "zeros", "(", "len", "(", "rivid_index_list", ")", ")", "return_50_array", "=", "np", ".", "zeros", "(", "len", "(", "rivid_index_list", ")", ")", "return_25_array", "=", "np", ".", "zeros", "(", "len", "(", "rivid_index_list", ")", ")", "return_10_array", "=", "np", ".", "zeros", "(", "len", "(", "rivid_index_list", ")", ")", "return_2_array", "=", "np", ".", "zeros", "(", "len", "(", "rivid_index_list", ")", ")", "max_flow_array", "=", "np", ".", "zeros", "(", "len", "(", "rivid_index_list", ")", ")", "# iterate through rivids to generate return periods", "for", "iter_idx", ",", "rivid_index", "in", "enumerate", "(", "rivid_index_list", ")", ":", "filtered_flow_data", "=", "qout_nc_file", ".", "get_qout_index", "(", "rivid_index", ",", "pd_filter", "=", "\"{0}D\"", ".", "format", "(", "step", ")", ",", "filter_mode", "=", "\"max\"", ")", "sorted_flow_data", "=", "np", ".", "sort", "(", "filtered_flow_data", ")", "[", ":", "num_years", ":", "-", "1", "]", "max_flow", "=", "sorted_flow_data", "[", "0", "]", "if", "max_flow", "<", "0.01", ":", "log", "(", "\"Return period data < 0.01 generated for rivid {0}\"", ".", "format", "(", "qout_nc_file", ".", "qout_nc", ".", "variables", "[", "qout_nc_file", ".", "river_id_dimension", "]", "[", "rivid_index", "]", ")", ",", "\"WARNING\"", ")", "max_flow_array", "[", "iter_idx", "]", "=", "max_flow", "if", "method", "==", "'weibull'", ":", "return_20_array", "[", "iter_idx", "]", "=", "sorted_flow_data", "[", "rp_index_20", "]", "return_10_array", "[", "iter_idx", "]", "=", "sorted_flow_data", "[", "rp_index_10", "]", "return_2_array", "[", "iter_idx", "]", "=", "sorted_flow_data", "[", "rp_index_2", "]", "elif", "method", "==", "'gumble'", ":", "mean_flow", "=", "np", ".", "mean", "(", "filtered_flow_data", ")", "stddev", "=", "np", ".", "std", "(", "filtered_flow_data", ")", "return_100_array", "[", "iter_idx", "]", "=", "mean_flow", "+", "3.14", "*", "stddev", "return_50_array", "[", "iter_idx", "]", "=", "mean_flow", "+", "2.59", "*", "stddev", "return_20_array", "[", "iter_idx", "]", "=", "mean_flow", "+", "1.87", "*", "stddev", "return_10_array", "[", "iter_idx", "]", "=", "mean_flow", "+", "1.3", "*", "stddev", "return_2_array", "[", "iter_idx", "]", "=", "mean_flow", "-", ".164", "*", "stddev", "elif", "method", "==", "'log_pearson'", ":", "log_flow", "=", "np", ".", "log10", "(", "filtered_flow_data", "[", "filtered_flow_data", ">", "0", "]", ")", "if", "len", "(", "log_flow", ")", "<=", "0", ":", "continue", "mean_log_flow", "=", "np", ".", "mean", "(", "log_flow", ")", "std_log_flow", "=", "np", ".", "std", "(", "log_flow", ")", "log_flow_array", "=", "np", ".", "array", "(", "log_flow", ")", "skew", "=", "(", "num_years", "*", "(", "np", ".", "sum", "(", "np", ".", "power", "(", "(", "log_flow_array", "-", "mean_log_flow", ")", ",", "3", ")", ")", ")", ")", "/", "(", "(", "num_years", "-", "1", ")", "*", "(", "num_years", "-", "2", ")", "*", "std_log_flow", "**", "3", ")", "k2", "=", "np", ".", "interp", "(", "skew", ",", "skewvals", ",", "kfac2", ")", "k10", "=", "np", ".", "interp", "(", "skew", ",", "skewvals", ",", "kfac10", ")", "k25", "=", "np", ".", "interp", "(", "skew", ",", "skewvals", ",", "kfac25", ")", "k50", "=", "np", ".", "interp", "(", "skew", ",", "skewvals", ",", "kfac50", ")", "k100", "=", "np", ".", "interp", "(", "skew", ",", "skewvals", ",", "kfac100", ")", "return_100_array", "[", "iter_idx", "]", "=", "np", ".", "power", "(", "10", ",", "(", "mean_log_flow", "+", "k100", "*", "std_log_flow", ")", ")", "return_50_array", "[", "iter_idx", "]", "=", "np", ".", "power", "(", "10", ",", "(", "mean_log_flow", "+", "k50", "*", "std_log_flow", ")", ")", "return_25_array", "[", "iter_idx", "]", "=", "np", ".", "power", "(", "10", ",", "(", "mean_log_flow", "+", "k25", "*", "std_log_flow", ")", ")", "return_10_array", "[", "iter_idx", "]", "=", "np", ".", "power", "(", "10", ",", "(", "mean_log_flow", "+", "k10", "*", "std_log_flow", ")", ")", "return_2_array", "[", "iter_idx", "]", "=", "np", ".", "power", "(", "10", ",", "(", "mean_log_flow", "+", "k2", "*", "std_log_flow", ")", ")", "mp_lock", ".", "acquire", "(", ")", "return_period_nc", "=", "Dataset", "(", "return_period_file", ",", "'a'", ")", "return_period_nc", ".", "variables", "[", "'max_flow'", "]", "[", "rivid_index_list", "]", "=", "max_flow_array", "if", "method", "==", "'weibull'", ":", "return_period_nc", ".", "variables", "[", "'return_period_20'", "]", "[", "rivid_index_list", "]", "=", "return_20_array", "elif", "method", "in", "'gumble'", ":", "return_period_nc", ".", "variables", "[", "'return_period_100'", "]", "[", "rivid_index_list", "]", "=", "return_100_array", "return_period_nc", ".", "variables", "[", "'return_period_50'", "]", "[", "rivid_index_list", "]", "=", "return_50_array", "return_period_nc", ".", "variables", "[", "'return_period_20'", "]", "[", "rivid_index_list", "]", "=", "return_20_array", "elif", "method", "==", "'log_pearson'", ":", "return_period_nc", ".", "variables", "[", "'return_period_100'", "]", "[", "rivid_index_list", "]", "=", "return_100_array", "return_period_nc", ".", "variables", "[", "'return_period_50'", "]", "[", "rivid_index_list", "]", "=", "return_50_array", "return_period_nc", ".", "variables", "[", "'return_period_25'", "]", "[", "rivid_index_list", "]", "=", "return_25_array", "return_period_nc", ".", "variables", "[", "'return_period_10'", "]", "[", "rivid_index_list", "]", "=", "return_10_array", "return_period_nc", ".", "variables", "[", "'return_period_2'", "]", "[", "rivid_index_list", "]", "=", "return_2_array", "return_period_nc", ".", "close", "(", ")", "mp_lock", ".", "release", "(", ")"], "docstring": "This function calculates a single return period for a single reach", "docstring_tokens": ["This", "function", "calculates", "a", "single", "return", "period", "for", "a", "single", "reach"], "sha": "50e14e130554b254a00ff23b226cd7e4c6cfe91a", "url": "https://github.com/erdc/RAPIDpy/blob/50e14e130554b254a00ff23b226cd7e4c6cfe91a/RAPIDpy/postprocess/generate_return_periods.py#L20-L153", "partition": "train"}
{"repo": "erdc/RAPIDpy", "path": "RAPIDpy/postprocess/generate_return_periods.py", "func_name": "generate_return_periods", "original_string": "def generate_return_periods(qout_file,\n                            return_period_file,\n                            num_cpus=multiprocessing.cpu_count(),\n                            storm_duration_days=7,\n                            method='weibull'):\n    \"\"\"\n    Generate return period from RAPID Qout file\n    \"\"\"\n    # get ERA Interim Data Analyzed\n    with RAPIDDataset(qout_file) as qout_nc_file:\n        print(\"Setting up Return Periods File ...\")\n        return_period_nc = Dataset(return_period_file, 'w')\n\n        return_period_nc.createDimension('rivid', qout_nc_file.size_river_id)\n\n        timeSeries_var = \\\n            return_period_nc.createVariable('rivid', 'i4', ('rivid',))\n        timeSeries_var.long_name = (\n            'unique identifier for each river reach')\n\n        max_flow_var = \\\n            return_period_nc.createVariable('max_flow', 'f8', ('rivid',))\n        max_flow_var.long_name = 'maximum streamflow'\n        max_flow_var.units = 'm3/s'\n\n        if method == 'weibull':\n\n            return_period_20_var = \\\n                return_period_nc.createVariable('return_period_20',\n                                                'f8', ('rivid',))\n            return_period_20_var.long_name = '20 year return period flow'\n            return_period_20_var.units = 'm3/s'\n\n        if method == 'gumble':\n\n            return_period_100_var = \\\n                return_period_nc.createVariable('return_period_100',\n                                                'f8', ('rivid',))\n            return_period_100_var.long_name = '100 year return period flow'\n            return_period_100_var.units = 'm3/s'\n\n            return_period_50_var = \\\n                return_period_nc.createVariable('return_period_50',\n                                                'f8', ('rivid',))\n            return_period_50_var.long_name = '50 year return period flow'\n            return_period_50_var.units = 'm3/s'\n\n            return_period_20_var = \\\n                return_period_nc.createVariable('return_period_20',\n                                                'f8', ('rivid',))\n            return_period_20_var.long_name = '20 year return period flow'\n            return_period_20_var.units = 'm3/s'\n\n        if method == 'log_pearson':\n\n            return_period_100_var = \\\n                return_period_nc.createVariable('return_period_100',\n                                                'f8', ('rivid',))\n            return_period_100_var.long_name = '100 year return period flow'\n            return_period_100_var.units = 'm3/s'\n\n            return_period_50_var = \\\n                return_period_nc.createVariable('return_period_50',\n                                                'f8', ('rivid',))\n            return_period_50_var.long_name = '50 year return period flow'\n            return_period_50_var.units = 'm3/s'\n\n            return_period_25_var = \\\n                return_period_nc.createVariable('return_period_25',\n                                                'f8', ('rivid',))\n            return_period_25_var.long_name = '25 year return period flow'\n            return_period_25_var.units = 'm3/s'\n\n        return_period_10_var = \\\n            return_period_nc.createVariable('return_period_10',\n                                            'f8', ('rivid',))\n        return_period_10_var.long_name = '10 year return period flow'\n        return_period_10_var.units = 'm3/s'\n\n        return_period_2_var = \\\n            return_period_nc.createVariable('return_period_2',\n                                            'f8', ('rivid',))\n        return_period_2_var.long_name = '2 year return period flow'\n        return_period_2_var.units = 'm3/s'\n\n        lat_var = return_period_nc.createVariable('lat', 'f8', ('rivid',),\n                                                  fill_value=-9999.0)\n\n        lon_var = return_period_nc.createVariable('lon', 'f8', ('rivid',),\n                                                  fill_value=-9999.0)\n\n        add_latlon_metadata(lat_var, lon_var)\n\n        return_period_nc.variables['lat'][:] = \\\n            qout_nc_file.qout_nc.variables['lat'][:]\n        return_period_nc.variables['lon'][:] = \\\n            qout_nc_file.qout_nc.variables['lon'][:]\n\n        river_id_list = qout_nc_file.get_river_id_array()\n        return_period_nc.variables['rivid'][:] = river_id_list\n\n        return_period_nc.return_period_method = method\n\n        return_period_nc.close()\n\n        time_array = qout_nc_file.get_time_array()\n\n    log(\"Extracting Data and Generating Return Periods ...\")\n    num_years = int((datetime.utcfromtimestamp(time_array[-1]) -\n                     datetime.utcfromtimestamp(time_array[0])).days/365.2425)\n    time_steps_per_day = (24 * 3600) / float(\n        (datetime.utcfromtimestamp(time_array[1]) -\n         datetime.utcfromtimestamp(time_array[0])).total_seconds())\n    step = max(1, int(time_steps_per_day * storm_duration_days))\n\n    # generate multiprocessing jobs\n    # pylint: disable=no-member\n    mp_lock = multiprocessing.Manager().Lock()\n    job_combinations = []\n    partition_index_list = partition(river_id_list, num_cpus*2)[1]\n    for sub_partition_index_list in partition_index_list:\n        # pylint: disable=len-as-condition\n        if len(sub_partition_index_list) > 0:\n            job_combinations.append((qout_file,\n                                     return_period_file,\n                                     sub_partition_index_list,\n                                     step,\n                                     num_years,\n                                     method,\n                                     mp_lock\n                                     ))\n\n    pool = multiprocessing.Pool(num_cpus)\n    pool.map(generate_single_return_period,\n             job_combinations)\n    pool.close()\n    pool.join()", "language": "python", "code": "def generate_return_periods(qout_file,\n                            return_period_file,\n                            num_cpus=multiprocessing.cpu_count(),\n                            storm_duration_days=7,\n                            method='weibull'):\n    \"\"\"\n    Generate return period from RAPID Qout file\n    \"\"\"\n    # get ERA Interim Data Analyzed\n    with RAPIDDataset(qout_file) as qout_nc_file:\n        print(\"Setting up Return Periods File ...\")\n        return_period_nc = Dataset(return_period_file, 'w')\n\n        return_period_nc.createDimension('rivid', qout_nc_file.size_river_id)\n\n        timeSeries_var = \\\n            return_period_nc.createVariable('rivid', 'i4', ('rivid',))\n        timeSeries_var.long_name = (\n            'unique identifier for each river reach')\n\n        max_flow_var = \\\n            return_period_nc.createVariable('max_flow', 'f8', ('rivid',))\n        max_flow_var.long_name = 'maximum streamflow'\n        max_flow_var.units = 'm3/s'\n\n        if method == 'weibull':\n\n            return_period_20_var = \\\n                return_period_nc.createVariable('return_period_20',\n                                                'f8', ('rivid',))\n            return_period_20_var.long_name = '20 year return period flow'\n            return_period_20_var.units = 'm3/s'\n\n        if method == 'gumble':\n\n            return_period_100_var = \\\n                return_period_nc.createVariable('return_period_100',\n                                                'f8', ('rivid',))\n            return_period_100_var.long_name = '100 year return period flow'\n            return_period_100_var.units = 'm3/s'\n\n            return_period_50_var = \\\n                return_period_nc.createVariable('return_period_50',\n                                                'f8', ('rivid',))\n            return_period_50_var.long_name = '50 year return period flow'\n            return_period_50_var.units = 'm3/s'\n\n            return_period_20_var = \\\n                return_period_nc.createVariable('return_period_20',\n                                                'f8', ('rivid',))\n            return_period_20_var.long_name = '20 year return period flow'\n            return_period_20_var.units = 'm3/s'\n\n        if method == 'log_pearson':\n\n            return_period_100_var = \\\n                return_period_nc.createVariable('return_period_100',\n                                                'f8', ('rivid',))\n            return_period_100_var.long_name = '100 year return period flow'\n            return_period_100_var.units = 'm3/s'\n\n            return_period_50_var = \\\n                return_period_nc.createVariable('return_period_50',\n                                                'f8', ('rivid',))\n            return_period_50_var.long_name = '50 year return period flow'\n            return_period_50_var.units = 'm3/s'\n\n            return_period_25_var = \\\n                return_period_nc.createVariable('return_period_25',\n                                                'f8', ('rivid',))\n            return_period_25_var.long_name = '25 year return period flow'\n            return_period_25_var.units = 'm3/s'\n\n        return_period_10_var = \\\n            return_period_nc.createVariable('return_period_10',\n                                            'f8', ('rivid',))\n        return_period_10_var.long_name = '10 year return period flow'\n        return_period_10_var.units = 'm3/s'\n\n        return_period_2_var = \\\n            return_period_nc.createVariable('return_period_2',\n                                            'f8', ('rivid',))\n        return_period_2_var.long_name = '2 year return period flow'\n        return_period_2_var.units = 'm3/s'\n\n        lat_var = return_period_nc.createVariable('lat', 'f8', ('rivid',),\n                                                  fill_value=-9999.0)\n\n        lon_var = return_period_nc.createVariable('lon', 'f8', ('rivid',),\n                                                  fill_value=-9999.0)\n\n        add_latlon_metadata(lat_var, lon_var)\n\n        return_period_nc.variables['lat'][:] = \\\n            qout_nc_file.qout_nc.variables['lat'][:]\n        return_period_nc.variables['lon'][:] = \\\n            qout_nc_file.qout_nc.variables['lon'][:]\n\n        river_id_list = qout_nc_file.get_river_id_array()\n        return_period_nc.variables['rivid'][:] = river_id_list\n\n        return_period_nc.return_period_method = method\n\n        return_period_nc.close()\n\n        time_array = qout_nc_file.get_time_array()\n\n    log(\"Extracting Data and Generating Return Periods ...\")\n    num_years = int((datetime.utcfromtimestamp(time_array[-1]) -\n                     datetime.utcfromtimestamp(time_array[0])).days/365.2425)\n    time_steps_per_day = (24 * 3600) / float(\n        (datetime.utcfromtimestamp(time_array[1]) -\n         datetime.utcfromtimestamp(time_array[0])).total_seconds())\n    step = max(1, int(time_steps_per_day * storm_duration_days))\n\n    # generate multiprocessing jobs\n    # pylint: disable=no-member\n    mp_lock = multiprocessing.Manager().Lock()\n    job_combinations = []\n    partition_index_list = partition(river_id_list, num_cpus*2)[1]\n    for sub_partition_index_list in partition_index_list:\n        # pylint: disable=len-as-condition\n        if len(sub_partition_index_list) > 0:\n            job_combinations.append((qout_file,\n                                     return_period_file,\n                                     sub_partition_index_list,\n                                     step,\n                                     num_years,\n                                     method,\n                                     mp_lock\n                                     ))\n\n    pool = multiprocessing.Pool(num_cpus)\n    pool.map(generate_single_return_period,\n             job_combinations)\n    pool.close()\n    pool.join()", "code_tokens": ["def", "generate_return_periods", "(", "qout_file", ",", "return_period_file", ",", "num_cpus", "=", "multiprocessing", ".", "cpu_count", "(", ")", ",", "storm_duration_days", "=", "7", ",", "method", "=", "'weibull'", ")", ":", "# get ERA Interim Data Analyzed", "with", "RAPIDDataset", "(", "qout_file", ")", "as", "qout_nc_file", ":", "print", "(", "\"Setting up Return Periods File ...\"", ")", "return_period_nc", "=", "Dataset", "(", "return_period_file", ",", "'w'", ")", "return_period_nc", ".", "createDimension", "(", "'rivid'", ",", "qout_nc_file", ".", "size_river_id", ")", "timeSeries_var", "=", "return_period_nc", ".", "createVariable", "(", "'rivid'", ",", "'i4'", ",", "(", "'rivid'", ",", ")", ")", "timeSeries_var", ".", "long_name", "=", "(", "'unique identifier for each river reach'", ")", "max_flow_var", "=", "return_period_nc", ".", "createVariable", "(", "'max_flow'", ",", "'f8'", ",", "(", "'rivid'", ",", ")", ")", "max_flow_var", ".", "long_name", "=", "'maximum streamflow'", "max_flow_var", ".", "units", "=", "'m3/s'", "if", "method", "==", "'weibull'", ":", "return_period_20_var", "=", "return_period_nc", ".", "createVariable", "(", "'return_period_20'", ",", "'f8'", ",", "(", "'rivid'", ",", ")", ")", "return_period_20_var", ".", "long_name", "=", "'20 year return period flow'", "return_period_20_var", ".", "units", "=", "'m3/s'", "if", "method", "==", "'gumble'", ":", "return_period_100_var", "=", "return_period_nc", ".", "createVariable", "(", "'return_period_100'", ",", "'f8'", ",", "(", "'rivid'", ",", ")", ")", "return_period_100_var", ".", "long_name", "=", "'100 year return period flow'", "return_period_100_var", ".", "units", "=", "'m3/s'", "return_period_50_var", "=", "return_period_nc", ".", "createVariable", "(", "'return_period_50'", ",", "'f8'", ",", "(", "'rivid'", ",", ")", ")", "return_period_50_var", ".", "long_name", "=", "'50 year return period flow'", "return_period_50_var", ".", "units", "=", "'m3/s'", "return_period_20_var", "=", "return_period_nc", ".", "createVariable", "(", "'return_period_20'", ",", "'f8'", ",", "(", "'rivid'", ",", ")", ")", "return_period_20_var", ".", "long_name", "=", "'20 year return period flow'", "return_period_20_var", ".", "units", "=", "'m3/s'", "if", "method", "==", "'log_pearson'", ":", "return_period_100_var", "=", "return_period_nc", ".", "createVariable", "(", "'return_period_100'", ",", "'f8'", ",", "(", "'rivid'", ",", ")", ")", "return_period_100_var", ".", "long_name", "=", "'100 year return period flow'", "return_period_100_var", ".", "units", "=", "'m3/s'", "return_period_50_var", "=", "return_period_nc", ".", "createVariable", "(", "'return_period_50'", ",", "'f8'", ",", "(", "'rivid'", ",", ")", ")", "return_period_50_var", ".", "long_name", "=", "'50 year return period flow'", "return_period_50_var", ".", "units", "=", "'m3/s'", "return_period_25_var", "=", "return_period_nc", ".", "createVariable", "(", "'return_period_25'", ",", "'f8'", ",", "(", "'rivid'", ",", ")", ")", "return_period_25_var", ".", "long_name", "=", "'25 year return period flow'", "return_period_25_var", ".", "units", "=", "'m3/s'", "return_period_10_var", "=", "return_period_nc", ".", "createVariable", "(", "'return_period_10'", ",", "'f8'", ",", "(", "'rivid'", ",", ")", ")", "return_period_10_var", ".", "long_name", "=", "'10 year return period flow'", "return_period_10_var", ".", "units", "=", "'m3/s'", "return_period_2_var", "=", "return_period_nc", ".", "createVariable", "(", "'return_period_2'", ",", "'f8'", ",", "(", "'rivid'", ",", ")", ")", "return_period_2_var", ".", "long_name", "=", "'2 year return period flow'", "return_period_2_var", ".", "units", "=", "'m3/s'", "lat_var", "=", "return_period_nc", ".", "createVariable", "(", "'lat'", ",", "'f8'", ",", "(", "'rivid'", ",", ")", ",", "fill_value", "=", "-", "9999.0", ")", "lon_var", "=", "return_period_nc", ".", "createVariable", "(", "'lon'", ",", "'f8'", ",", "(", "'rivid'", ",", ")", ",", "fill_value", "=", "-", "9999.0", ")", "add_latlon_metadata", "(", "lat_var", ",", "lon_var", ")", "return_period_nc", ".", "variables", "[", "'lat'", "]", "[", ":", "]", "=", "qout_nc_file", ".", "qout_nc", ".", "variables", "[", "'lat'", "]", "[", ":", "]", "return_period_nc", ".", "variables", "[", "'lon'", "]", "[", ":", "]", "=", "qout_nc_file", ".", "qout_nc", ".", "variables", "[", "'lon'", "]", "[", ":", "]", "river_id_list", "=", "qout_nc_file", ".", "get_river_id_array", "(", ")", "return_period_nc", ".", "variables", "[", "'rivid'", "]", "[", ":", "]", "=", "river_id_list", "return_period_nc", ".", "return_period_method", "=", "method", "return_period_nc", ".", "close", "(", ")", "time_array", "=", "qout_nc_file", ".", "get_time_array", "(", ")", "log", "(", "\"Extracting Data and Generating Return Periods ...\"", ")", "num_years", "=", "int", "(", "(", "datetime", ".", "utcfromtimestamp", "(", "time_array", "[", "-", "1", "]", ")", "-", "datetime", ".", "utcfromtimestamp", "(", "time_array", "[", "0", "]", ")", ")", ".", "days", "/", "365.2425", ")", "time_steps_per_day", "=", "(", "24", "*", "3600", ")", "/", "float", "(", "(", "datetime", ".", "utcfromtimestamp", "(", "time_array", "[", "1", "]", ")", "-", "datetime", ".", "utcfromtimestamp", "(", "time_array", "[", "0", "]", ")", ")", ".", "total_seconds", "(", ")", ")", "step", "=", "max", "(", "1", ",", "int", "(", "time_steps_per_day", "*", "storm_duration_days", ")", ")", "# generate multiprocessing jobs", "# pylint: disable=no-member", "mp_lock", "=", "multiprocessing", ".", "Manager", "(", ")", ".", "Lock", "(", ")", "job_combinations", "=", "[", "]", "partition_index_list", "=", "partition", "(", "river_id_list", ",", "num_cpus", "*", "2", ")", "[", "1", "]", "for", "sub_partition_index_list", "in", "partition_index_list", ":", "# pylint: disable=len-as-condition", "if", "len", "(", "sub_partition_index_list", ")", ">", "0", ":", "job_combinations", ".", "append", "(", "(", "qout_file", ",", "return_period_file", ",", "sub_partition_index_list", ",", "step", ",", "num_years", ",", "method", ",", "mp_lock", ")", ")", "pool", "=", "multiprocessing", ".", "Pool", "(", "num_cpus", ")", "pool", ".", "map", "(", "generate_single_return_period", ",", "job_combinations", ")", "pool", ".", "close", "(", ")", "pool", ".", "join", "(", ")"], "docstring": "Generate return period from RAPID Qout file", "docstring_tokens": ["Generate", "return", "period", "from", "RAPID", "Qout", "file"], "sha": "50e14e130554b254a00ff23b226cd7e4c6cfe91a", "url": "https://github.com/erdc/RAPIDpy/blob/50e14e130554b254a00ff23b226cd7e4c6cfe91a/RAPIDpy/postprocess/generate_return_periods.py#L156-L292", "partition": "train"}
{"repo": "erdc/RAPIDpy", "path": "RAPIDpy/gis/weight.py", "func_name": "get_poly_area_geo", "original_string": "def get_poly_area_geo(poly):\n    \"\"\"\n    Calculates the area in meters squared of the individual polygon\n    \"\"\"\n    minx, miny, maxx, maxy = poly.bounds\n    # reproject polygon to get area\n    reprojected_for_area = Proj(\"+proj=aea +lat_1={0} +lat_1={1} \"\n                                \"+lat_0={2} +lon_0={3}\"\n                                .format(miny,\n                                        maxy,\n                                        (miny + maxy) / 2.0,\n                                        (minx + maxx) / 2.0))\n    geographic_proj = Proj(init='epsg:4326')\n    project_func = partial(transform,\n                           geographic_proj,\n                           reprojected_for_area)\n    reprojected_poly = shapely_transform(project_func, poly)\n    return reprojected_poly.area", "language": "python", "code": "def get_poly_area_geo(poly):\n    \"\"\"\n    Calculates the area in meters squared of the individual polygon\n    \"\"\"\n    minx, miny, maxx, maxy = poly.bounds\n    # reproject polygon to get area\n    reprojected_for_area = Proj(\"+proj=aea +lat_1={0} +lat_1={1} \"\n                                \"+lat_0={2} +lon_0={3}\"\n                                .format(miny,\n                                        maxy,\n                                        (miny + maxy) / 2.0,\n                                        (minx + maxx) / 2.0))\n    geographic_proj = Proj(init='epsg:4326')\n    project_func = partial(transform,\n                           geographic_proj,\n                           reprojected_for_area)\n    reprojected_poly = shapely_transform(project_func, poly)\n    return reprojected_poly.area", "code_tokens": ["def", "get_poly_area_geo", "(", "poly", ")", ":", "minx", ",", "miny", ",", "maxx", ",", "maxy", "=", "poly", ".", "bounds", "# reproject polygon to get area", "reprojected_for_area", "=", "Proj", "(", "\"+proj=aea +lat_1={0} +lat_1={1} \"", "\"+lat_0={2} +lon_0={3}\"", ".", "format", "(", "miny", ",", "maxy", ",", "(", "miny", "+", "maxy", ")", "/", "2.0", ",", "(", "minx", "+", "maxx", ")", "/", "2.0", ")", ")", "geographic_proj", "=", "Proj", "(", "init", "=", "'epsg:4326'", ")", "project_func", "=", "partial", "(", "transform", ",", "geographic_proj", ",", "reprojected_for_area", ")", "reprojected_poly", "=", "shapely_transform", "(", "project_func", ",", "poly", ")", "return", "reprojected_poly", ".", "area"], "docstring": "Calculates the area in meters squared of the individual polygon", "docstring_tokens": ["Calculates", "the", "area", "in", "meters", "squared", "of", "the", "individual", "polygon"], "sha": "50e14e130554b254a00ff23b226cd7e4c6cfe91a", "url": "https://github.com/erdc/RAPIDpy/blob/50e14e130554b254a00ff23b226cd7e4c6cfe91a/RAPIDpy/gis/weight.py#L30-L47", "partition": "train"}
{"repo": "TUT-ARG/sed_eval", "path": "sed_eval/util/event_list.py", "func_name": "max_event_offset", "original_string": "def max_event_offset(event_list):\n    \"\"\"Find the offset (end-time) of last event\n\n    Parameters\n    ----------\n    event_list : list or dcase_util.containers.MetaDataContainer\n        A list containing event dicts\n\n    Returns\n    -------\n    float > 0\n        maximum offset\n\n    \"\"\"\n\n    if isinstance(event_list, dcase_util.containers.MetaDataContainer):\n        return event_list.max_offset\n\n    else:\n        max_offset = 0\n        for event in event_list:\n            if 'event_offset' in event:\n                if event['event_offset'] > max_offset:\n                    max_offset = event['event_offset']\n\n            elif 'offset' in event:\n                if event['offset'] > max_offset:\n                    max_offset = event['offset']\n\n        return max_offset", "language": "python", "code": "def max_event_offset(event_list):\n    \"\"\"Find the offset (end-time) of last event\n\n    Parameters\n    ----------\n    event_list : list or dcase_util.containers.MetaDataContainer\n        A list containing event dicts\n\n    Returns\n    -------\n    float > 0\n        maximum offset\n\n    \"\"\"\n\n    if isinstance(event_list, dcase_util.containers.MetaDataContainer):\n        return event_list.max_offset\n\n    else:\n        max_offset = 0\n        for event in event_list:\n            if 'event_offset' in event:\n                if event['event_offset'] > max_offset:\n                    max_offset = event['event_offset']\n\n            elif 'offset' in event:\n                if event['offset'] > max_offset:\n                    max_offset = event['offset']\n\n        return max_offset", "code_tokens": ["def", "max_event_offset", "(", "event_list", ")", ":", "if", "isinstance", "(", "event_list", ",", "dcase_util", ".", "containers", ".", "MetaDataContainer", ")", ":", "return", "event_list", ".", "max_offset", "else", ":", "max_offset", "=", "0", "for", "event", "in", "event_list", ":", "if", "'event_offset'", "in", "event", ":", "if", "event", "[", "'event_offset'", "]", ">", "max_offset", ":", "max_offset", "=", "event", "[", "'event_offset'", "]", "elif", "'offset'", "in", "event", ":", "if", "event", "[", "'offset'", "]", ">", "max_offset", ":", "max_offset", "=", "event", "[", "'offset'", "]", "return", "max_offset"], "docstring": "Find the offset (end-time) of last event\n\n    Parameters\n    ----------\n    event_list : list or dcase_util.containers.MetaDataContainer\n        A list containing event dicts\n\n    Returns\n    -------\n    float > 0\n        maximum offset", "docstring_tokens": ["Find", "the", "offset", "(", "end", "-", "time", ")", "of", "last", "event"], "sha": "0cb1b6d11ceec4fe500cc9b31079c9d8666ed6eb", "url": "https://github.com/TUT-ARG/sed_eval/blob/0cb1b6d11ceec4fe500cc9b31079c9d8666ed6eb/sed_eval/util/event_list.py#L106-L135", "partition": "train"}
{"repo": "TUT-ARG/sed_eval", "path": "sed_eval/audio_tag.py", "func_name": "AudioTaggingMetrics.evaluate", "original_string": "def evaluate(self, reference_tag_list, estimated_tag_list=None, estimated_tag_probabilities=None):\n        \"\"\"Evaluate estimated against reference\n\n        Parameters\n        ----------\n\n        reference_tag_list : list of dict or dcase_util.containers.MetaDataContainer\n            Reference tag list\n\n        estimated_tag_list : list of dict or dcase_util.containers.MetaDataContainer\n            Estimated tag list\n\n        estimated_tag_probabilities : list of dict or dcase_util.containers.ProbabilityContainer\n            Estimated tag probabilities\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n\n        if estimated_tag_list is None and estimated_tag_probabilities is None:\n            raise ValueError(\"Nothing to evaluate, give at least estimated_tag_list or estimated_tag_probabilities\")\n\n        # Make sure reference_tag_list is dcase_util.containers.MetaDataContainer\n        if not isinstance(reference_tag_list, dcase_util.containers.MetaDataContainer):\n            reference_tag_list = dcase_util.containers.MetaDataContainer(reference_tag_list)\n\n        # Make sure estimated_tag_list is dcase_util.containers.MetaDataContainer\n        if estimated_tag_list is not None:\n            if not isinstance(estimated_tag_list, dcase_util.containers.MetaDataContainer):\n                estimated_tag_list = dcase_util.containers.MetaDataContainer(estimated_tag_list)\n\n        # Make sure estimated_tag_probabilities is dcase_util.containers.ProbabilityContainer\n        if estimated_tag_probabilities is not None:\n            if not isinstance(estimated_tag_probabilities, dcase_util.containers.ProbabilityContainer):\n                estimated_tag_probabilities = dcase_util.containers.ProbabilityContainer(estimated_tag_probabilities)\n\n        y_true = []\n        y_pred = []\n\n        # Go though reference and estimated list label by label, and file by file\n        for label in self.tag_label_list:\n            for filename in reference_tag_list.unique_files:\n                reference_item = reference_tag_list.filter(filename=filename)[0]\n\n                # Populate y_true based on reference_item\n                if label in reference_item.tags:\n                    self.y_true[label].append(1)\n                    y_true.append(1)\n\n                else:\n                    self.y_true[label].append(0)\n                    y_true.append(0)\n\n                if estimated_tag_list is not None:\n                    # Evaluate based on estimated tags\n\n                    estimated_item = estimated_tag_list.filter(filename=filename)[0]\n\n                    if not estimated_item:\n                        raise ValueError(\n                            \"Not all reference files estimated, please check [{file}]\".format(\n                                file=filename\n                            )\n                        )\n                    # Store nref\n                    if label in reference_item.tags:\n                        self.tag_wise[label]['Nref'] += 1\n\n                    # Populate y_pred based estimated_item\n                    if label in estimated_item.tags:\n                        self.y_pred[label].append(1)\n                        y_pred.append(1)\n                        self.tag_wise[label]['Nsys'] += 1\n\n                    else:\n                        self.y_pred[label].append(0)\n                        y_pred.append(0)\n\n                    # Accumulate intermediate values\n                    # True positives (TP)\n                    if label in reference_item.tags and label in estimated_item.tags:\n                        self.tag_wise[label]['Ntp'] += 1\n\n                    # True negatives (TN)\n                    if label not in reference_item.tags and label not in estimated_item.tags:\n                        self.tag_wise[label]['Ntn'] += 1\n\n                    # False positives (FP)\n                    if label not in reference_item.tags and label in estimated_item.tags:\n                        self.tag_wise[label]['Nfp'] += 1\n\n                    # False negatives (FN)\n                    if label in reference_item.tags and label not in estimated_item.tags:\n                        self.tag_wise[label]['Nfn'] += 1\n\n                if estimated_tag_probabilities is not None:\n                    # Evaluate based on per tag probabilities\n\n                    estimated_item = estimated_tag_probabilities.filter(filename=filename, label=label)[0]\n                    self.y_pred_score[label].append(float(estimated_item['probability']))\n\n        if estimated_tag_list is not None:\n            # Evaluate based on estimated tags\n\n            self.overall['Nref'] += sum(y_true)\n            self.overall['Nsys'] += sum(y_pred)\n\n            y_true = numpy.array(y_true)\n            y_pred = numpy.array(y_pred)\n\n            self.overall['Ntp'] += sum(y_pred + y_true > 1)\n            self.overall['Ntn'] += sum(y_pred + y_true == 0)\n            self.overall['Nfp'] += sum(y_pred - y_true > 0)\n            self.overall['Nfn'] += sum(y_true - y_pred > 0)\n\n        return self", "language": "python", "code": "def evaluate(self, reference_tag_list, estimated_tag_list=None, estimated_tag_probabilities=None):\n        \"\"\"Evaluate estimated against reference\n\n        Parameters\n        ----------\n\n        reference_tag_list : list of dict or dcase_util.containers.MetaDataContainer\n            Reference tag list\n\n        estimated_tag_list : list of dict or dcase_util.containers.MetaDataContainer\n            Estimated tag list\n\n        estimated_tag_probabilities : list of dict or dcase_util.containers.ProbabilityContainer\n            Estimated tag probabilities\n\n        Returns\n        -------\n        self\n\n        \"\"\"\n\n        if estimated_tag_list is None and estimated_tag_probabilities is None:\n            raise ValueError(\"Nothing to evaluate, give at least estimated_tag_list or estimated_tag_probabilities\")\n\n        # Make sure reference_tag_list is dcase_util.containers.MetaDataContainer\n        if not isinstance(reference_tag_list, dcase_util.containers.MetaDataContainer):\n            reference_tag_list = dcase_util.containers.MetaDataContainer(reference_tag_list)\n\n        # Make sure estimated_tag_list is dcase_util.containers.MetaDataContainer\n        if estimated_tag_list is not None:\n            if not isinstance(estimated_tag_list, dcase_util.containers.MetaDataContainer):\n                estimated_tag_list = dcase_util.containers.MetaDataContainer(estimated_tag_list)\n\n        # Make sure estimated_tag_probabilities is dcase_util.containers.ProbabilityContainer\n        if estimated_tag_probabilities is not None:\n            if not isinstance(estimated_tag_probabilities, dcase_util.containers.ProbabilityContainer):\n                estimated_tag_probabilities = dcase_util.containers.ProbabilityContainer(estimated_tag_probabilities)\n\n        y_true = []\n        y_pred = []\n\n        # Go though reference and estimated list label by label, and file by file\n        for label in self.tag_label_list:\n            for filename in reference_tag_list.unique_files:\n                reference_item = reference_tag_list.filter(filename=filename)[0]\n\n                # Populate y_true based on reference_item\n                if label in reference_item.tags:\n                    self.y_true[label].append(1)\n                    y_true.append(1)\n\n                else:\n                    self.y_true[label].append(0)\n                    y_true.append(0)\n\n                if estimated_tag_list is not None:\n                    # Evaluate based on estimated tags\n\n                    estimated_item = estimated_tag_list.filter(filename=filename)[0]\n\n                    if not estimated_item:\n                        raise ValueError(\n                            \"Not all reference files estimated, please check [{file}]\".format(\n                                file=filename\n                            )\n                        )\n                    # Store nref\n                    if label in reference_item.tags:\n                        self.tag_wise[label]['Nref'] += 1\n\n                    # Populate y_pred based estimated_item\n                    if label in estimated_item.tags:\n                        self.y_pred[label].append(1)\n                        y_pred.append(1)\n                        self.tag_wise[label]['Nsys'] += 1\n\n                    else:\n                        self.y_pred[label].append(0)\n                        y_pred.append(0)\n\n                    # Accumulate intermediate values\n                    # True positives (TP)\n                    if label in reference_item.tags and label in estimated_item.tags:\n                        self.tag_wise[label]['Ntp'] += 1\n\n                    # True negatives (TN)\n                    if label not in reference_item.tags and label not in estimated_item.tags:\n                        self.tag_wise[label]['Ntn'] += 1\n\n                    # False positives (FP)\n                    if label not in reference_item.tags and label in estimated_item.tags:\n                        self.tag_wise[label]['Nfp'] += 1\n\n                    # False negatives (FN)\n                    if label in reference_item.tags and label not in estimated_item.tags:\n                        self.tag_wise[label]['Nfn'] += 1\n\n                if estimated_tag_probabilities is not None:\n                    # Evaluate based on per tag probabilities\n\n                    estimated_item = estimated_tag_probabilities.filter(filename=filename, label=label)[0]\n                    self.y_pred_score[label].append(float(estimated_item['probability']))\n\n        if estimated_tag_list is not None:\n            # Evaluate based on estimated tags\n\n            self.overall['Nref'] += sum(y_true)\n            self.overall['Nsys'] += sum(y_pred)\n\n            y_true = numpy.array(y_true)\n            y_pred = numpy.array(y_pred)\n\n            self.overall['Ntp'] += sum(y_pred + y_true > 1)\n            self.overall['Ntn'] += sum(y_pred + y_true == 0)\n            self.overall['Nfp'] += sum(y_pred - y_true > 0)\n            self.overall['Nfn'] += sum(y_true - y_pred > 0)\n\n        return self", "code_tokens": ["def", "evaluate", "(", "self", ",", "reference_tag_list", ",", "estimated_tag_list", "=", "None", ",", "estimated_tag_probabilities", "=", "None", ")", ":", "if", "estimated_tag_list", "is", "None", "and", "estimated_tag_probabilities", "is", "None", ":", "raise", "ValueError", "(", "\"Nothing to evaluate, give at least estimated_tag_list or estimated_tag_probabilities\"", ")", "# Make sure reference_tag_list is dcase_util.containers.MetaDataContainer", "if", "not", "isinstance", "(", "reference_tag_list", ",", "dcase_util", ".", "containers", ".", "MetaDataContainer", ")", ":", "reference_tag_list", "=", "dcase_util", ".", "containers", ".", "MetaDataContainer", "(", "reference_tag_list", ")", "# Make sure estimated_tag_list is dcase_util.containers.MetaDataContainer", "if", "estimated_tag_list", "is", "not", "None", ":", "if", "not", "isinstance", "(", "estimated_tag_list", ",", "dcase_util", ".", "containers", ".", "MetaDataContainer", ")", ":", "estimated_tag_list", "=", "dcase_util", ".", "containers", ".", "MetaDataContainer", "(", "estimated_tag_list", ")", "# Make sure estimated_tag_probabilities is dcase_util.containers.ProbabilityContainer", "if", "estimated_tag_probabilities", "is", "not", "None", ":", "if", "not", "isinstance", "(", "estimated_tag_probabilities", ",", "dcase_util", ".", "containers", ".", "ProbabilityContainer", ")", ":", "estimated_tag_probabilities", "=", "dcase_util", ".", "containers", ".", "ProbabilityContainer", "(", "estimated_tag_probabilities", ")", "y_true", "=", "[", "]", "y_pred", "=", "[", "]", "# Go though reference and estimated list label by label, and file by file", "for", "label", "in", "self", ".", "tag_label_list", ":", "for", "filename", "in", "reference_tag_list", ".", "unique_files", ":", "reference_item", "=", "reference_tag_list", ".", "filter", "(", "filename", "=", "filename", ")", "[", "0", "]", "# Populate y_true based on reference_item", "if", "label", "in", "reference_item", ".", "tags", ":", "self", ".", "y_true", "[", "label", "]", ".", "append", "(", "1", ")", "y_true", ".", "append", "(", "1", ")", "else", ":", "self", ".", "y_true", "[", "label", "]", ".", "append", "(", "0", ")", "y_true", ".", "append", "(", "0", ")", "if", "estimated_tag_list", "is", "not", "None", ":", "# Evaluate based on estimated tags", "estimated_item", "=", "estimated_tag_list", ".", "filter", "(", "filename", "=", "filename", ")", "[", "0", "]", "if", "not", "estimated_item", ":", "raise", "ValueError", "(", "\"Not all reference files estimated, please check [{file}]\"", ".", "format", "(", "file", "=", "filename", ")", ")", "# Store nref", "if", "label", "in", "reference_item", ".", "tags", ":", "self", ".", "tag_wise", "[", "label", "]", "[", "'Nref'", "]", "+=", "1", "# Populate y_pred based estimated_item", "if", "label", "in", "estimated_item", ".", "tags", ":", "self", ".", "y_pred", "[", "label", "]", ".", "append", "(", "1", ")", "y_pred", ".", "append", "(", "1", ")", "self", ".", "tag_wise", "[", "label", "]", "[", "'Nsys'", "]", "+=", "1", "else", ":", "self", ".", "y_pred", "[", "label", "]", ".", "append", "(", "0", ")", "y_pred", ".", "append", "(", "0", ")", "# Accumulate intermediate values", "# True positives (TP)", "if", "label", "in", "reference_item", ".", "tags", "and", "label", "in", "estimated_item", ".", "tags", ":", "self", ".", "tag_wise", "[", "label", "]", "[", "'Ntp'", "]", "+=", "1", "# True negatives (TN)", "if", "label", "not", "in", "reference_item", ".", "tags", "and", "label", "not", "in", "estimated_item", ".", "tags", ":", "self", ".", "tag_wise", "[", "label", "]", "[", "'Ntn'", "]", "+=", "1", "# False positives (FP)", "if", "label", "not", "in", "reference_item", ".", "tags", "and", "label", "in", "estimated_item", ".", "tags", ":", "self", ".", "tag_wise", "[", "label", "]", "[", "'Nfp'", "]", "+=", "1", "# False negatives (FN)", "if", "label", "in", "reference_item", ".", "tags", "and", "label", "not", "in", "estimated_item", ".", "tags", ":", "self", ".", "tag_wise", "[", "label", "]", "[", "'Nfn'", "]", "+=", "1", "if", "estimated_tag_probabilities", "is", "not", "None", ":", "# Evaluate based on per tag probabilities", "estimated_item", "=", "estimated_tag_probabilities", ".", "filter", "(", "filename", "=", "filename", ",", "label", "=", "label", ")", "[", "0", "]", "self", ".", "y_pred_score", "[", "label", "]", ".", "append", "(", "float", "(", "estimated_item", "[", "'probability'", "]", ")", ")", "if", "estimated_tag_list", "is", "not", "None", ":", "# Evaluate based on estimated tags", "self", ".", "overall", "[", "'Nref'", "]", "+=", "sum", "(", "y_true", ")", "self", ".", "overall", "[", "'Nsys'", "]", "+=", "sum", "(", "y_pred", ")", "y_true", "=", "numpy", ".", "array", "(", "y_true", ")", "y_pred", "=", "numpy", ".", "array", "(", "y_pred", ")", "self", ".", "overall", "[", "'Ntp'", "]", "+=", "sum", "(", "y_pred", "+", "y_true", ">", "1", ")", "self", ".", "overall", "[", "'Ntn'", "]", "+=", "sum", "(", "y_pred", "+", "y_true", "==", "0", ")", "self", ".", "overall", "[", "'Nfp'", "]", "+=", "sum", "(", "y_pred", "-", "y_true", ">", "0", ")", "self", ".", "overall", "[", "'Nfn'", "]", "+=", "sum", "(", "y_true", "-", "y_pred", ">", "0", ")", "return", "self"], "docstring": "Evaluate estimated against reference\n\n        Parameters\n        ----------\n\n        reference_tag_list : list of dict or dcase_util.containers.MetaDataContainer\n            Reference tag list\n\n        estimated_tag_list : list of dict or dcase_util.containers.MetaDataContainer\n            Estimated tag list\n\n        estimated_tag_probabilities : list of dict or dcase_util.containers.ProbabilityContainer\n            Estimated tag probabilities\n\n        Returns\n        -------\n        self", "docstring_tokens": ["Evaluate", "estimated", "against", "reference"], "sha": "0cb1b6d11ceec4fe500cc9b31079c9d8666ed6eb", "url": "https://github.com/TUT-ARG/sed_eval/blob/0cb1b6d11ceec4fe500cc9b31079c9d8666ed6eb/sed_eval/audio_tag.py#L304-L421", "partition": "train"}
{"repo": "cisco-sas/kitty", "path": "kitty/fuzzers/server.py", "func_name": "ServerFuzzer._transmit", "original_string": "def _transmit(self, node):\n        '''\n        Transmit node data to target.\n\n        :type node:  Template\n        :param node: node to transmit\n        :return: response if there is any\n        '''\n        payload = node.render().tobytes()\n        self._last_payload = payload\n        try:\n            return self.target.transmit(payload)\n        except Exception as e:\n            self.logger.error('Error in transmit: %s', e)\n            raise", "language": "python", "code": "def _transmit(self, node):\n        '''\n        Transmit node data to target.\n\n        :type node:  Template\n        :param node: node to transmit\n        :return: response if there is any\n        '''\n        payload = node.render().tobytes()\n        self._last_payload = payload\n        try:\n            return self.target.transmit(payload)\n        except Exception as e:\n            self.logger.error('Error in transmit: %s', e)\n            raise", "code_tokens": ["def", "_transmit", "(", "self", ",", "node", ")", ":", "payload", "=", "node", ".", "render", "(", ")", ".", "tobytes", "(", ")", "self", ".", "_last_payload", "=", "payload", "try", ":", "return", "self", ".", "target", ".", "transmit", "(", "payload", ")", "except", "Exception", "as", "e", ":", "self", ".", "logger", ".", "error", "(", "'Error in transmit: %s'", ",", "e", ")", "raise"], "docstring": "Transmit node data to target.\n\n        :type node:  Template\n        :param node: node to transmit\n        :return: response if there is any", "docstring_tokens": ["Transmit", "node", "data", "to", "target", "."], "sha": "cb0760989dcdfe079e43ac574d872d0b18953a32", "url": "https://github.com/cisco-sas/kitty/blob/cb0760989dcdfe079e43ac574d872d0b18953a32/kitty/fuzzers/server.py#L77-L91", "partition": "train"}
{"repo": "cisco-sas/kitty", "path": "kitty/remote/rpc.py", "func_name": "encode_string", "original_string": "def encode_string(data, encoding='hex'):\n    '''\n    Encode string\n\n    :param data: string to encode\n    :param encoding: encoding to use (default: 'hex')\n    :return: encoded string\n    '''\n    if six.PY2:\n        return data.encode(encoding)\n    else:\n        if isinstance(data, str):\n            data = bytes(data, 'utf-8')\n        return codecs.encode(data, encoding).decode('ascii')", "language": "python", "code": "def encode_string(data, encoding='hex'):\n    '''\n    Encode string\n\n    :param data: string to encode\n    :param encoding: encoding to use (default: 'hex')\n    :return: encoded string\n    '''\n    if six.PY2:\n        return data.encode(encoding)\n    else:\n        if isinstance(data, str):\n            data = bytes(data, 'utf-8')\n        return codecs.encode(data, encoding).decode('ascii')", "code_tokens": ["def", "encode_string", "(", "data", ",", "encoding", "=", "'hex'", ")", ":", "if", "six", ".", "PY2", ":", "return", "data", ".", "encode", "(", "encoding", ")", "else", ":", "if", "isinstance", "(", "data", ",", "str", ")", ":", "data", "=", "bytes", "(", "data", ",", "'utf-8'", ")", "return", "codecs", ".", "encode", "(", "data", ",", "encoding", ")", ".", "decode", "(", "'ascii'", ")"], "docstring": "Encode string\n\n    :param data: string to encode\n    :param encoding: encoding to use (default: 'hex')\n    :return: encoded string", "docstring_tokens": ["Encode", "string"], "sha": "cb0760989dcdfe079e43ac574d872d0b18953a32", "url": "https://github.com/cisco-sas/kitty/blob/cb0760989dcdfe079e43ac574d872d0b18953a32/kitty/remote/rpc.py#L39-L52", "partition": "train"}
{"repo": "pirate/mesh-networking", "path": "examples/large_network.py", "func_name": "eigenvalue", "original_string": "def eigenvalue(nodes, node=None):\n    \"\"\"\n    calculate the eigenvalue (number of connections) for a given node in an array of nodes connected by an array of links\n    if no node is given, return the minimum eigenvalue in the whole network\n    \"\"\"\n    if node is None:\n        return sorted([eigenvalue(nodes, n) for n in nodes])[0]  # return lowest eigenvalue\n    else:\n        return len([1 for n in nodes if hops(node, n)])", "language": "python", "code": "def eigenvalue(nodes, node=None):\n    \"\"\"\n    calculate the eigenvalue (number of connections) for a given node in an array of nodes connected by an array of links\n    if no node is given, return the minimum eigenvalue in the whole network\n    \"\"\"\n    if node is None:\n        return sorted([eigenvalue(nodes, n) for n in nodes])[0]  # return lowest eigenvalue\n    else:\n        return len([1 for n in nodes if hops(node, n)])", "code_tokens": ["def", "eigenvalue", "(", "nodes", ",", "node", "=", "None", ")", ":", "if", "node", "is", "None", ":", "return", "sorted", "(", "[", "eigenvalue", "(", "nodes", ",", "n", ")", "for", "n", "in", "nodes", "]", ")", "[", "0", "]", "# return lowest eigenvalue", "else", ":", "return", "len", "(", "[", "1", "for", "n", "in", "nodes", "if", "hops", "(", "node", ",", "n", ")", "]", ")"], "docstring": "calculate the eigenvalue (number of connections) for a given node in an array of nodes connected by an array of links\n    if no node is given, return the minimum eigenvalue in the whole network", "docstring_tokens": ["calculate", "the", "eigenvalue", "(", "number", "of", "connections", ")", "for", "a", "given", "node", "in", "an", "array", "of", "nodes", "connected", "by", "an", "array", "of", "links", "if", "no", "node", "is", "given", "return", "the", "minimum", "eigenvalue", "in", "the", "whole", "network"], "sha": "e8da35d2ecded6930cf2180605bf28479ee555c7", "url": "https://github.com/pirate/mesh-networking/blob/e8da35d2ecded6930cf2180605bf28479ee555c7/examples/large_network.py#L28-L36", "partition": "train"}
{"repo": "sethmlarson/virtualbox-python", "path": "virtualbox/library_ext/console.py", "func_name": "IConsole.register_on_network_adapter_changed", "original_string": "def register_on_network_adapter_changed(self, callback):\n        \"\"\"Set the callback function to consume on network adapter changed\n        events.\n\n        Callback receives a INetworkAdapterChangedEvent object.\n\n        Returns the callback_id\n        \"\"\"\n        event_type = library.VBoxEventType.on_network_adapter_changed\n        return self.event_source.register_callback(callback, event_type)", "language": "python", "code": "def register_on_network_adapter_changed(self, callback):\n        \"\"\"Set the callback function to consume on network adapter changed\n        events.\n\n        Callback receives a INetworkAdapterChangedEvent object.\n\n        Returns the callback_id\n        \"\"\"\n        event_type = library.VBoxEventType.on_network_adapter_changed\n        return self.event_source.register_callback(callback, event_type)", "code_tokens": ["def", "register_on_network_adapter_changed", "(", "self", ",", "callback", ")", ":", "event_type", "=", "library", ".", "VBoxEventType", ".", "on_network_adapter_changed", "return", "self", ".", "event_source", ".", "register_callback", "(", "callback", ",", "event_type", ")"], "docstring": "Set the callback function to consume on network adapter changed\n        events.\n\n        Callback receives a INetworkAdapterChangedEvent object.\n\n        Returns the callback_id", "docstring_tokens": ["Set", "the", "callback", "function", "to", "consume", "on", "network", "adapter", "changed", "events", "."], "sha": "706c8e3f6e3aee17eb06458e73cbb4bc2d37878b", "url": "https://github.com/sethmlarson/virtualbox-python/blob/706c8e3f6e3aee17eb06458e73cbb4bc2d37878b/virtualbox/library_ext/console.py#L13-L22", "partition": "train"}
{"repo": "sethmlarson/virtualbox-python", "path": "virtualbox/library_ext/console.py", "func_name": "IConsole.register_on_serial_port_changed", "original_string": "def register_on_serial_port_changed(self, callback):\n        \"\"\"Set the callback function to consume on serial port changed events.\n\n        Callback receives a ISerialPortChangedEvent object.\n\n        Returns the callback_id\n        \"\"\"\n        event_type = library.VBoxEventType.on_serial_port_changed\n        return self.event_source.register_callback(callback, event_type)", "language": "python", "code": "def register_on_serial_port_changed(self, callback):\n        \"\"\"Set the callback function to consume on serial port changed events.\n\n        Callback receives a ISerialPortChangedEvent object.\n\n        Returns the callback_id\n        \"\"\"\n        event_type = library.VBoxEventType.on_serial_port_changed\n        return self.event_source.register_callback(callback, event_type)", "code_tokens": ["def", "register_on_serial_port_changed", "(", "self", ",", "callback", ")", ":", "event_type", "=", "library", ".", "VBoxEventType", ".", "on_serial_port_changed", "return", "self", ".", "event_source", ".", "register_callback", "(", "callback", ",", "event_type", ")"], "docstring": "Set the callback function to consume on serial port changed events.\n\n        Callback receives a ISerialPortChangedEvent object.\n\n        Returns the callback_id", "docstring_tokens": ["Set", "the", "callback", "function", "to", "consume", "on", "serial", "port", "changed", "events", "."], "sha": "706c8e3f6e3aee17eb06458e73cbb4bc2d37878b", "url": "https://github.com/sethmlarson/virtualbox-python/blob/706c8e3f6e3aee17eb06458e73cbb4bc2d37878b/virtualbox/library_ext/console.py#L24-L32", "partition": "train"}
{"repo": "trombastic/PyScada", "path": "pyscada/mail/worker.py", "func_name": "Process.loop", "original_string": "def loop(self):\n        \"\"\"\n        check for mails and send them\n        \"\"\"\n        for mail in Mail.objects.filter(done=False, send_fail_count__lt=3):\n            # send all emails that are not already send or failed to send less\n            # then three times\n            mail.send_mail()\n\n        for mail in Mail.objects.filter(done=True, timestamp__lt=time() - 60 * 60 * 24 * 7):\n            # delete all done emails older then one week\n            mail.delete()\n        return 1, None", "language": "python", "code": "def loop(self):\n        \"\"\"\n        check for mails and send them\n        \"\"\"\n        for mail in Mail.objects.filter(done=False, send_fail_count__lt=3):\n            # send all emails that are not already send or failed to send less\n            # then three times\n            mail.send_mail()\n\n        for mail in Mail.objects.filter(done=True, timestamp__lt=time() - 60 * 60 * 24 * 7):\n            # delete all done emails older then one week\n            mail.delete()\n        return 1, None", "code_tokens": ["def", "loop", "(", "self", ")", ":", "for", "mail", "in", "Mail", ".", "objects", ".", "filter", "(", "done", "=", "False", ",", "send_fail_count__lt", "=", "3", ")", ":", "# send all emails that are not already send or failed to send less", "# then three times", "mail", ".", "send_mail", "(", ")", "for", "mail", "in", "Mail", ".", "objects", ".", "filter", "(", "done", "=", "True", ",", "timestamp__lt", "=", "time", "(", ")", "-", "60", "*", "60", "*", "24", "*", "7", ")", ":", "# delete all done emails older then one week", "mail", ".", "delete", "(", ")", "return", "1", ",", "None"], "docstring": "check for mails and send them", "docstring_tokens": ["check", "for", "mails", "and", "send", "them"], "sha": "c5fc348a25f0df1340336f694ee9bc1aea62516a", "url": "https://github.com/trombastic/PyScada/blob/c5fc348a25f0df1340336f694ee9bc1aea62516a/pyscada/mail/worker.py#L18-L30", "partition": "train"}
{"repo": "marcharper/python-ternary", "path": "ternary/helpers.py", "func_name": "normalize", "original_string": "def normalize(l):\n    \"\"\"\n    Normalizes input list.\n\n    Parameters\n    ----------\n    l: list\n        The list to be normalized\n\n    Returns\n    -------\n    The normalized list or numpy array\n\n    Raises\n    ------\n    ValueError, if the list sums to zero\n    \"\"\"\n\n    s = float(sum(l))\n    if s == 0:\n        raise ValueError(\"Cannot normalize list with sum 0\")\n    return [x / s for x in l]", "language": "python", "code": "def normalize(l):\n    \"\"\"\n    Normalizes input list.\n\n    Parameters\n    ----------\n    l: list\n        The list to be normalized\n\n    Returns\n    -------\n    The normalized list or numpy array\n\n    Raises\n    ------\n    ValueError, if the list sums to zero\n    \"\"\"\n\n    s = float(sum(l))\n    if s == 0:\n        raise ValueError(\"Cannot normalize list with sum 0\")\n    return [x / s for x in l]", "code_tokens": ["def", "normalize", "(", "l", ")", ":", "s", "=", "float", "(", "sum", "(", "l", ")", ")", "if", "s", "==", "0", ":", "raise", "ValueError", "(", "\"Cannot normalize list with sum 0\"", ")", "return", "[", "x", "/", "s", "for", "x", "in", "l", "]"], "docstring": "Normalizes input list.\n\n    Parameters\n    ----------\n    l: list\n        The list to be normalized\n\n    Returns\n    -------\n    The normalized list or numpy array\n\n    Raises\n    ------\n    ValueError, if the list sums to zero", "docstring_tokens": ["Normalizes", "input", "list", "."], "sha": "a4bef393ec9df130d4b55707293c750498a01843", "url": "https://github.com/marcharper/python-ternary/blob/a4bef393ec9df130d4b55707293c750498a01843/ternary/helpers.py#L21-L42", "partition": "train"}
{"repo": "econ-ark/HARK", "path": "HARK/ConsumptionSaving/ConsIndShockModel.py", "func_name": "KinkedRconsumerType.getRfree", "original_string": "def getRfree(self):\n        '''\n        Returns an array of size self.AgentCount with self.Rboro or self.Rsave in each entry, based\n        on whether self.aNrmNow >< 0.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        RfreeNow : np.array\n             Array of size self.AgentCount with risk free interest rate for each agent.\n        '''\n        RfreeNow = self.Rboro*np.ones(self.AgentCount)\n        RfreeNow[self.aNrmNow > 0] = self.Rsave\n        return RfreeNow", "language": "python", "code": "def getRfree(self):\n        '''\n        Returns an array of size self.AgentCount with self.Rboro or self.Rsave in each entry, based\n        on whether self.aNrmNow >< 0.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        RfreeNow : np.array\n             Array of size self.AgentCount with risk free interest rate for each agent.\n        '''\n        RfreeNow = self.Rboro*np.ones(self.AgentCount)\n        RfreeNow[self.aNrmNow > 0] = self.Rsave\n        return RfreeNow", "code_tokens": ["def", "getRfree", "(", "self", ")", ":", "RfreeNow", "=", "self", ".", "Rboro", "*", "np", ".", "ones", "(", "self", ".", "AgentCount", ")", "RfreeNow", "[", "self", ".", "aNrmNow", ">", "0", "]", "=", "self", ".", "Rsave", "return", "RfreeNow"], "docstring": "Returns an array of size self.AgentCount with self.Rboro or self.Rsave in each entry, based\n        on whether self.aNrmNow >< 0.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        RfreeNow : np.array\n             Array of size self.AgentCount with risk free interest rate for each agent.", "docstring_tokens": ["Returns", "an", "array", "of", "size", "self", ".", "AgentCount", "with", "self", ".", "Rboro", "or", "self", ".", "Rsave", "in", "each", "entry", "based", "on", "whether", "self", ".", "aNrmNow", ">", "<", "0", "."], "sha": "3d184153a189e618a87c9540df1cd12044039cc5", "url": "https://github.com/econ-ark/HARK/blob/3d184153a189e618a87c9540df1cd12044039cc5/HARK/ConsumptionSaving/ConsIndShockModel.py#L2189-L2205", "partition": "train"}
{"repo": "JoelBender/bacpypes", "path": "py34/bacpypes/analysis.py", "func_name": "decode_packet", "original_string": "def decode_packet(data):\n    \"\"\"decode the data, return some kind of PDU.\"\"\"\n    if _debug: decode_packet._debug(\"decode_packet %r\", data)\n\n    # empty strings are some other kind of pcap content\n    if not data:\n        return None\n\n    # assume it is ethernet for now\n    d = decode_ethernet(data)\n    pduSource = Address(d['source_address'])\n    pduDestination = Address(d['destination_address'])\n    data = d['data']\n\n    # there could be a VLAN header\n    if (d['type'] == 0x8100):\n        if _debug: decode_packet._debug(\"    - vlan found\")\n\n        d = decode_vlan(data)\n        data = d['data']\n\n    # look for IP packets\n    if (d['type'] == 0x0800):\n        if _debug: decode_packet._debug(\"    - IP found\")\n\n        d = decode_ip(data)\n        pduSource, pduDestination = d['source_address'], d['destination_address']\n        data = d['data']\n\n        if (d['protocol'] == 'udp'):\n            if _debug: decode_packet._debug(\"    - UDP found\")\n\n            d = decode_udp(data)\n            data = d['data']\n\n            pduSource = Address((pduSource, d['source_port']))\n            pduDestination = Address((pduDestination, d['destination_port']))\n            if _debug:\n                decode_packet._debug(\"    - pduSource: %r\", pduSource)\n                decode_packet._debug(\"    - pduDestination: %r\", pduDestination)\n        else:\n            if _debug: decode_packet._debug(\"    - not a UDP packet\")\n    else:\n        if _debug: decode_packet._debug(\"    - not an IP packet\")\n\n    # check for empty\n    if not data:\n        if _debug: decode_packet._debug(\"    - empty packet\")\n        return None\n\n    # build a PDU\n    pdu = PDU(data, source=pduSource, destination=pduDestination)\n\n    # check for a BVLL header\n    if (pdu.pduData[0] == 0x81):\n        if _debug: decode_packet._debug(\"    - BVLL header found\")\n\n        try:\n            xpdu = BVLPDU()\n            xpdu.decode(pdu)\n            pdu = xpdu\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - BVLPDU decoding error: %r\", err)\n            return pdu\n\n        # make a more focused interpretation\n        atype = bvl_pdu_types.get(pdu.bvlciFunction)\n        if not atype:\n            if _debug: decode_packet._debug(\"    - unknown BVLL type: %r\", pdu.bvlciFunction)\n            return pdu\n\n        # decode it as one of the basic types\n        try:\n            xpdu = pdu\n            bpdu = atype()\n            bpdu.decode(pdu)\n            if _debug: decode_packet._debug(\"    - bpdu: %r\", bpdu)\n\n            pdu = bpdu\n\n            # lift the address for forwarded NPDU's\n            if atype is ForwardedNPDU:\n                pdu.pduSource = bpdu.bvlciAddress\n            # no deeper decoding for some\n            elif atype not in (DistributeBroadcastToNetwork, OriginalUnicastNPDU, OriginalBroadcastNPDU):\n                return pdu\n\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - decoding Error: %r\", err)\n            return xpdu\n\n    # check for version number\n    if (pdu.pduData[0] != 0x01):\n        if _debug: decode_packet._debug(\"    - not a version 1 packet: %s...\", btox(pdu.pduData[:30], '.'))\n        return None\n\n    # it's an NPDU\n    try:\n        npdu = NPDU()\n        npdu.decode(pdu)\n    except Exception as err:\n        if _debug: decode_packet._debug(\"    - decoding Error: %r\", err)\n        return None\n\n    # application or network layer message\n    if npdu.npduNetMessage is None:\n        if _debug: decode_packet._debug(\"    - not a network layer message, try as an APDU\")\n\n        # decode as a generic APDU\n        try:\n            xpdu = APDU()\n            xpdu.decode(npdu)\n            apdu = xpdu\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - decoding Error: %r\", err)\n            return npdu\n\n        # \"lift\" the source and destination address\n        if npdu.npduSADR:\n            apdu.pduSource = npdu.npduSADR\n        else:\n            apdu.pduSource = npdu.pduSource\n        if npdu.npduDADR:\n            apdu.pduDestination = npdu.npduDADR\n        else:\n            apdu.pduDestination = npdu.pduDestination\n\n        # make a more focused interpretation\n        atype = apdu_types.get(apdu.apduType)\n        if not atype:\n            if _debug: decode_packet._debug(\"    - unknown APDU type: %r\", apdu.apduType)\n            return apdu\n\n        # decode it as one of the basic types\n        try:\n            xpdu = apdu\n            apdu = atype()\n            apdu.decode(xpdu)\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - decoding Error: %r\", err)\n            return xpdu\n\n        # decode it at the next level\n        if isinstance(apdu, ConfirmedRequestPDU):\n            atype = confirmed_request_types.get(apdu.apduService)\n            if not atype:\n                if _debug: decode_packet._debug(\"    - no confirmed request decoder: %r\", apdu.apduService)\n                return apdu\n\n        elif isinstance(apdu, UnconfirmedRequestPDU):\n            atype = unconfirmed_request_types.get(apdu.apduService)\n            if not atype:\n                if _debug: decode_packet._debug(\"    - no unconfirmed request decoder: %r\", apdu.apduService)\n                return apdu\n\n        elif isinstance(apdu, SimpleAckPDU):\n            atype = None\n\n        elif isinstance(apdu, ComplexAckPDU):\n            atype = complex_ack_types.get(apdu.apduService)\n            if not atype:\n                if _debug: decode_packet._debug(\"    - no complex ack decoder: %r\", apdu.apduService)\n                return apdu\n\n        elif isinstance(apdu, SegmentAckPDU):\n            atype = None\n\n        elif isinstance(apdu, ErrorPDU):\n            atype = error_types.get(apdu.apduService)\n            if not atype:\n                if _debug: decode_packet._debug(\"    - no error decoder: %r\", apdu.apduService)\n                return apdu\n\n        elif isinstance(apdu, RejectPDU):\n            atype = None\n\n        elif isinstance(apdu, AbortPDU):\n            atype = None\n        if _debug: decode_packet._debug(\"    - atype: %r\", atype)\n\n        # deeper decoding\n        try:\n            if atype:\n                xpdu = apdu\n                apdu = atype()\n                apdu.decode(xpdu)\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - decoding error: %r\", err)\n            return xpdu\n\n        # success\n        return apdu\n\n    else:\n        # make a more focused interpretation\n        ntype = npdu_types.get(npdu.npduNetMessage)\n        if not ntype:\n            if _debug: decode_packet._debug(\"    - no network layer decoder: %r\", npdu.npduNetMessage)\n            return npdu\n        if _debug: decode_packet._debug(\"    - ntype: %r\", ntype)\n\n        # deeper decoding\n        try:\n            xpdu = npdu\n            npdu = ntype()\n            npdu.decode(xpdu)\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - decoding error: %r\", err)\n            return xpdu\n\n        # success\n        return npdu", "language": "python", "code": "def decode_packet(data):\n    \"\"\"decode the data, return some kind of PDU.\"\"\"\n    if _debug: decode_packet._debug(\"decode_packet %r\", data)\n\n    # empty strings are some other kind of pcap content\n    if not data:\n        return None\n\n    # assume it is ethernet for now\n    d = decode_ethernet(data)\n    pduSource = Address(d['source_address'])\n    pduDestination = Address(d['destination_address'])\n    data = d['data']\n\n    # there could be a VLAN header\n    if (d['type'] == 0x8100):\n        if _debug: decode_packet._debug(\"    - vlan found\")\n\n        d = decode_vlan(data)\n        data = d['data']\n\n    # look for IP packets\n    if (d['type'] == 0x0800):\n        if _debug: decode_packet._debug(\"    - IP found\")\n\n        d = decode_ip(data)\n        pduSource, pduDestination = d['source_address'], d['destination_address']\n        data = d['data']\n\n        if (d['protocol'] == 'udp'):\n            if _debug: decode_packet._debug(\"    - UDP found\")\n\n            d = decode_udp(data)\n            data = d['data']\n\n            pduSource = Address((pduSource, d['source_port']))\n            pduDestination = Address((pduDestination, d['destination_port']))\n            if _debug:\n                decode_packet._debug(\"    - pduSource: %r\", pduSource)\n                decode_packet._debug(\"    - pduDestination: %r\", pduDestination)\n        else:\n            if _debug: decode_packet._debug(\"    - not a UDP packet\")\n    else:\n        if _debug: decode_packet._debug(\"    - not an IP packet\")\n\n    # check for empty\n    if not data:\n        if _debug: decode_packet._debug(\"    - empty packet\")\n        return None\n\n    # build a PDU\n    pdu = PDU(data, source=pduSource, destination=pduDestination)\n\n    # check for a BVLL header\n    if (pdu.pduData[0] == 0x81):\n        if _debug: decode_packet._debug(\"    - BVLL header found\")\n\n        try:\n            xpdu = BVLPDU()\n            xpdu.decode(pdu)\n            pdu = xpdu\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - BVLPDU decoding error: %r\", err)\n            return pdu\n\n        # make a more focused interpretation\n        atype = bvl_pdu_types.get(pdu.bvlciFunction)\n        if not atype:\n            if _debug: decode_packet._debug(\"    - unknown BVLL type: %r\", pdu.bvlciFunction)\n            return pdu\n\n        # decode it as one of the basic types\n        try:\n            xpdu = pdu\n            bpdu = atype()\n            bpdu.decode(pdu)\n            if _debug: decode_packet._debug(\"    - bpdu: %r\", bpdu)\n\n            pdu = bpdu\n\n            # lift the address for forwarded NPDU's\n            if atype is ForwardedNPDU:\n                pdu.pduSource = bpdu.bvlciAddress\n            # no deeper decoding for some\n            elif atype not in (DistributeBroadcastToNetwork, OriginalUnicastNPDU, OriginalBroadcastNPDU):\n                return pdu\n\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - decoding Error: %r\", err)\n            return xpdu\n\n    # check for version number\n    if (pdu.pduData[0] != 0x01):\n        if _debug: decode_packet._debug(\"    - not a version 1 packet: %s...\", btox(pdu.pduData[:30], '.'))\n        return None\n\n    # it's an NPDU\n    try:\n        npdu = NPDU()\n        npdu.decode(pdu)\n    except Exception as err:\n        if _debug: decode_packet._debug(\"    - decoding Error: %r\", err)\n        return None\n\n    # application or network layer message\n    if npdu.npduNetMessage is None:\n        if _debug: decode_packet._debug(\"    - not a network layer message, try as an APDU\")\n\n        # decode as a generic APDU\n        try:\n            xpdu = APDU()\n            xpdu.decode(npdu)\n            apdu = xpdu\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - decoding Error: %r\", err)\n            return npdu\n\n        # \"lift\" the source and destination address\n        if npdu.npduSADR:\n            apdu.pduSource = npdu.npduSADR\n        else:\n            apdu.pduSource = npdu.pduSource\n        if npdu.npduDADR:\n            apdu.pduDestination = npdu.npduDADR\n        else:\n            apdu.pduDestination = npdu.pduDestination\n\n        # make a more focused interpretation\n        atype = apdu_types.get(apdu.apduType)\n        if not atype:\n            if _debug: decode_packet._debug(\"    - unknown APDU type: %r\", apdu.apduType)\n            return apdu\n\n        # decode it as one of the basic types\n        try:\n            xpdu = apdu\n            apdu = atype()\n            apdu.decode(xpdu)\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - decoding Error: %r\", err)\n            return xpdu\n\n        # decode it at the next level\n        if isinstance(apdu, ConfirmedRequestPDU):\n            atype = confirmed_request_types.get(apdu.apduService)\n            if not atype:\n                if _debug: decode_packet._debug(\"    - no confirmed request decoder: %r\", apdu.apduService)\n                return apdu\n\n        elif isinstance(apdu, UnconfirmedRequestPDU):\n            atype = unconfirmed_request_types.get(apdu.apduService)\n            if not atype:\n                if _debug: decode_packet._debug(\"    - no unconfirmed request decoder: %r\", apdu.apduService)\n                return apdu\n\n        elif isinstance(apdu, SimpleAckPDU):\n            atype = None\n\n        elif isinstance(apdu, ComplexAckPDU):\n            atype = complex_ack_types.get(apdu.apduService)\n            if not atype:\n                if _debug: decode_packet._debug(\"    - no complex ack decoder: %r\", apdu.apduService)\n                return apdu\n\n        elif isinstance(apdu, SegmentAckPDU):\n            atype = None\n\n        elif isinstance(apdu, ErrorPDU):\n            atype = error_types.get(apdu.apduService)\n            if not atype:\n                if _debug: decode_packet._debug(\"    - no error decoder: %r\", apdu.apduService)\n                return apdu\n\n        elif isinstance(apdu, RejectPDU):\n            atype = None\n\n        elif isinstance(apdu, AbortPDU):\n            atype = None\n        if _debug: decode_packet._debug(\"    - atype: %r\", atype)\n\n        # deeper decoding\n        try:\n            if atype:\n                xpdu = apdu\n                apdu = atype()\n                apdu.decode(xpdu)\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - decoding error: %r\", err)\n            return xpdu\n\n        # success\n        return apdu\n\n    else:\n        # make a more focused interpretation\n        ntype = npdu_types.get(npdu.npduNetMessage)\n        if not ntype:\n            if _debug: decode_packet._debug(\"    - no network layer decoder: %r\", npdu.npduNetMessage)\n            return npdu\n        if _debug: decode_packet._debug(\"    - ntype: %r\", ntype)\n\n        # deeper decoding\n        try:\n            xpdu = npdu\n            npdu = ntype()\n            npdu.decode(xpdu)\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - decoding error: %r\", err)\n            return xpdu\n\n        # success\n        return npdu", "code_tokens": ["def", "decode_packet", "(", "data", ")", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"decode_packet %r\"", ",", "data", ")", "# empty strings are some other kind of pcap content", "if", "not", "data", ":", "return", "None", "# assume it is ethernet for now", "d", "=", "decode_ethernet", "(", "data", ")", "pduSource", "=", "Address", "(", "d", "[", "'source_address'", "]", ")", "pduDestination", "=", "Address", "(", "d", "[", "'destination_address'", "]", ")", "data", "=", "d", "[", "'data'", "]", "# there could be a VLAN header", "if", "(", "d", "[", "'type'", "]", "==", "0x8100", ")", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - vlan found\"", ")", "d", "=", "decode_vlan", "(", "data", ")", "data", "=", "d", "[", "'data'", "]", "# look for IP packets", "if", "(", "d", "[", "'type'", "]", "==", "0x0800", ")", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - IP found\"", ")", "d", "=", "decode_ip", "(", "data", ")", "pduSource", ",", "pduDestination", "=", "d", "[", "'source_address'", "]", ",", "d", "[", "'destination_address'", "]", "data", "=", "d", "[", "'data'", "]", "if", "(", "d", "[", "'protocol'", "]", "==", "'udp'", ")", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - UDP found\"", ")", "d", "=", "decode_udp", "(", "data", ")", "data", "=", "d", "[", "'data'", "]", "pduSource", "=", "Address", "(", "(", "pduSource", ",", "d", "[", "'source_port'", "]", ")", ")", "pduDestination", "=", "Address", "(", "(", "pduDestination", ",", "d", "[", "'destination_port'", "]", ")", ")", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - pduSource: %r\"", ",", "pduSource", ")", "decode_packet", ".", "_debug", "(", "\"    - pduDestination: %r\"", ",", "pduDestination", ")", "else", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - not a UDP packet\"", ")", "else", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - not an IP packet\"", ")", "# check for empty", "if", "not", "data", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - empty packet\"", ")", "return", "None", "# build a PDU", "pdu", "=", "PDU", "(", "data", ",", "source", "=", "pduSource", ",", "destination", "=", "pduDestination", ")", "# check for a BVLL header", "if", "(", "pdu", ".", "pduData", "[", "0", "]", "==", "0x81", ")", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - BVLL header found\"", ")", "try", ":", "xpdu", "=", "BVLPDU", "(", ")", "xpdu", ".", "decode", "(", "pdu", ")", "pdu", "=", "xpdu", "except", "Exception", "as", "err", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - BVLPDU decoding error: %r\"", ",", "err", ")", "return", "pdu", "# make a more focused interpretation", "atype", "=", "bvl_pdu_types", ".", "get", "(", "pdu", ".", "bvlciFunction", ")", "if", "not", "atype", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - unknown BVLL type: %r\"", ",", "pdu", ".", "bvlciFunction", ")", "return", "pdu", "# decode it as one of the basic types", "try", ":", "xpdu", "=", "pdu", "bpdu", "=", "atype", "(", ")", "bpdu", ".", "decode", "(", "pdu", ")", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - bpdu: %r\"", ",", "bpdu", ")", "pdu", "=", "bpdu", "# lift the address for forwarded NPDU's", "if", "atype", "is", "ForwardedNPDU", ":", "pdu", ".", "pduSource", "=", "bpdu", ".", "bvlciAddress", "# no deeper decoding for some", "elif", "atype", "not", "in", "(", "DistributeBroadcastToNetwork", ",", "OriginalUnicastNPDU", ",", "OriginalBroadcastNPDU", ")", ":", "return", "pdu", "except", "Exception", "as", "err", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - decoding Error: %r\"", ",", "err", ")", "return", "xpdu", "# check for version number", "if", "(", "pdu", ".", "pduData", "[", "0", "]", "!=", "0x01", ")", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - not a version 1 packet: %s...\"", ",", "btox", "(", "pdu", ".", "pduData", "[", ":", "30", "]", ",", "'.'", ")", ")", "return", "None", "# it's an NPDU", "try", ":", "npdu", "=", "NPDU", "(", ")", "npdu", ".", "decode", "(", "pdu", ")", "except", "Exception", "as", "err", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - decoding Error: %r\"", ",", "err", ")", "return", "None", "# application or network layer message", "if", "npdu", ".", "npduNetMessage", "is", "None", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - not a network layer message, try as an APDU\"", ")", "# decode as a generic APDU", "try", ":", "xpdu", "=", "APDU", "(", ")", "xpdu", ".", "decode", "(", "npdu", ")", "apdu", "=", "xpdu", "except", "Exception", "as", "err", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - decoding Error: %r\"", ",", "err", ")", "return", "npdu", "# \"lift\" the source and destination address", "if", "npdu", ".", "npduSADR", ":", "apdu", ".", "pduSource", "=", "npdu", ".", "npduSADR", "else", ":", "apdu", ".", "pduSource", "=", "npdu", ".", "pduSource", "if", "npdu", ".", "npduDADR", ":", "apdu", ".", "pduDestination", "=", "npdu", ".", "npduDADR", "else", ":", "apdu", ".", "pduDestination", "=", "npdu", ".", "pduDestination", "# make a more focused interpretation", "atype", "=", "apdu_types", ".", "get", "(", "apdu", ".", "apduType", ")", "if", "not", "atype", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - unknown APDU type: %r\"", ",", "apdu", ".", "apduType", ")", "return", "apdu", "# decode it as one of the basic types", "try", ":", "xpdu", "=", "apdu", "apdu", "=", "atype", "(", ")", "apdu", ".", "decode", "(", "xpdu", ")", "except", "Exception", "as", "err", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - decoding Error: %r\"", ",", "err", ")", "return", "xpdu", "# decode it at the next level", "if", "isinstance", "(", "apdu", ",", "ConfirmedRequestPDU", ")", ":", "atype", "=", "confirmed_request_types", ".", "get", "(", "apdu", ".", "apduService", ")", "if", "not", "atype", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - no confirmed request decoder: %r\"", ",", "apdu", ".", "apduService", ")", "return", "apdu", "elif", "isinstance", "(", "apdu", ",", "UnconfirmedRequestPDU", ")", ":", "atype", "=", "unconfirmed_request_types", ".", "get", "(", "apdu", ".", "apduService", ")", "if", "not", "atype", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - no unconfirmed request decoder: %r\"", ",", "apdu", ".", "apduService", ")", "return", "apdu", "elif", "isinstance", "(", "apdu", ",", "SimpleAckPDU", ")", ":", "atype", "=", "None", "elif", "isinstance", "(", "apdu", ",", "ComplexAckPDU", ")", ":", "atype", "=", "complex_ack_types", ".", "get", "(", "apdu", ".", "apduService", ")", "if", "not", "atype", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - no complex ack decoder: %r\"", ",", "apdu", ".", "apduService", ")", "return", "apdu", "elif", "isinstance", "(", "apdu", ",", "SegmentAckPDU", ")", ":", "atype", "=", "None", "elif", "isinstance", "(", "apdu", ",", "ErrorPDU", ")", ":", "atype", "=", "error_types", ".", "get", "(", "apdu", ".", "apduService", ")", "if", "not", "atype", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - no error decoder: %r\"", ",", "apdu", ".", "apduService", ")", "return", "apdu", "elif", "isinstance", "(", "apdu", ",", "RejectPDU", ")", ":", "atype", "=", "None", "elif", "isinstance", "(", "apdu", ",", "AbortPDU", ")", ":", "atype", "=", "None", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - atype: %r\"", ",", "atype", ")", "# deeper decoding", "try", ":", "if", "atype", ":", "xpdu", "=", "apdu", "apdu", "=", "atype", "(", ")", "apdu", ".", "decode", "(", "xpdu", ")", "except", "Exception", "as", "err", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - decoding error: %r\"", ",", "err", ")", "return", "xpdu", "# success", "return", "apdu", "else", ":", "# make a more focused interpretation", "ntype", "=", "npdu_types", ".", "get", "(", "npdu", ".", "npduNetMessage", ")", "if", "not", "ntype", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - no network layer decoder: %r\"", ",", "npdu", ".", "npduNetMessage", ")", "return", "npdu", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - ntype: %r\"", ",", "ntype", ")", "# deeper decoding", "try", ":", "xpdu", "=", "npdu", "npdu", "=", "ntype", "(", ")", "npdu", ".", "decode", "(", "xpdu", ")", "except", "Exception", "as", "err", ":", "if", "_debug", ":", "decode_packet", ".", "_debug", "(", "\"    - decoding error: %r\"", ",", "err", ")", "return", "xpdu", "# success", "return", "npdu"], "docstring": "decode the data, return some kind of PDU.", "docstring_tokens": ["decode", "the", "data", "return", "some", "kind", "of", "PDU", "."], "sha": "4111b8604a16fa2b7f80d8104a43b9f3e28dfc78", "url": "https://github.com/JoelBender/bacpypes/blob/4111b8604a16fa2b7f80d8104a43b9f3e28dfc78/py34/bacpypes/analysis.py#L137-L348", "partition": "train"}
{"repo": "plivo/sharq", "path": "sharq/utils.py", "func_name": "is_valid_requeue_limit", "original_string": "def is_valid_requeue_limit(requeue_limit):\n    \"\"\"Checks if the given requeue limit is valid.\n    A valid requeue limit is always greater than\n    or equal to -1.\n    \"\"\"\n    if not isinstance(requeue_limit, (int, long)):\n        return False\n\n    if requeue_limit <= -2:\n        return False\n\n    return True", "language": "python", "code": "def is_valid_requeue_limit(requeue_limit):\n    \"\"\"Checks if the given requeue limit is valid.\n    A valid requeue limit is always greater than\n    or equal to -1.\n    \"\"\"\n    if not isinstance(requeue_limit, (int, long)):\n        return False\n\n    if requeue_limit <= -2:\n        return False\n\n    return True", "code_tokens": ["def", "is_valid_requeue_limit", "(", "requeue_limit", ")", ":", "if", "not", "isinstance", "(", "requeue_limit", ",", "(", "int", ",", "long", ")", ")", ":", "return", "False", "if", "requeue_limit", "<=", "-", "2", ":", "return", "False", "return", "True"], "docstring": "Checks if the given requeue limit is valid.\n    A valid requeue limit is always greater than\n    or equal to -1.", "docstring_tokens": ["Checks", "if", "the", "given", "requeue", "limit", "is", "valid", ".", "A", "valid", "requeue", "limit", "is", "always", "greater", "than", "or", "equal", "to", "-", "1", "."], "sha": "32bbfbdcbbaa8e154271ffd125ac4500382f3d19", "url": "https://github.com/plivo/sharq/blob/32bbfbdcbbaa8e154271ffd125ac4500382f3d19/sharq/utils.py#L44-L55", "partition": "train"}
{"repo": "mvantellingen/localshop", "path": "src/localshop/apps/packages/pypi.py", "func_name": "get_search_names", "original_string": "def get_search_names(name):\n    \"\"\"Return a list of values to search on when we are looking for a package\n    with the given name.\n\n    This is required to search on both pyramid_debugtoolbar and\n    pyramid-debugtoolbar.\n\n    \"\"\"\n    parts = re.split('[-_.]', name)\n    if len(parts) == 1:\n        return parts\n\n    result = set()\n    for i in range(len(parts) - 1, 0, -1):\n        for s1 in '-_.':\n            prefix = s1.join(parts[:i])\n            for s2 in '-_.':\n                suffix = s2.join(parts[i:])\n                for s3 in '-_.':\n                    result.add(s3.join([prefix, suffix]))\n    return list(result)", "language": "python", "code": "def get_search_names(name):\n    \"\"\"Return a list of values to search on when we are looking for a package\n    with the given name.\n\n    This is required to search on both pyramid_debugtoolbar and\n    pyramid-debugtoolbar.\n\n    \"\"\"\n    parts = re.split('[-_.]', name)\n    if len(parts) == 1:\n        return parts\n\n    result = set()\n    for i in range(len(parts) - 1, 0, -1):\n        for s1 in '-_.':\n            prefix = s1.join(parts[:i])\n            for s2 in '-_.':\n                suffix = s2.join(parts[i:])\n                for s3 in '-_.':\n                    result.add(s3.join([prefix, suffix]))\n    return list(result)", "code_tokens": ["def", "get_search_names", "(", "name", ")", ":", "parts", "=", "re", ".", "split", "(", "'[-_.]'", ",", "name", ")", "if", "len", "(", "parts", ")", "==", "1", ":", "return", "parts", "result", "=", "set", "(", ")", "for", "i", "in", "range", "(", "len", "(", "parts", ")", "-", "1", ",", "0", ",", "-", "1", ")", ":", "for", "s1", "in", "'-_.'", ":", "prefix", "=", "s1", ".", "join", "(", "parts", "[", ":", "i", "]", ")", "for", "s2", "in", "'-_.'", ":", "suffix", "=", "s2", ".", "join", "(", "parts", "[", "i", ":", "]", ")", "for", "s3", "in", "'-_.'", ":", "result", ".", "add", "(", "s3", ".", "join", "(", "[", "prefix", ",", "suffix", "]", ")", ")", "return", "list", "(", "result", ")"], "docstring": "Return a list of values to search on when we are looking for a package\n    with the given name.\n\n    This is required to search on both pyramid_debugtoolbar and\n    pyramid-debugtoolbar.", "docstring_tokens": ["Return", "a", "list", "of", "values", "to", "search", "on", "when", "we", "are", "looking", "for", "a", "package", "with", "the", "given", "name", "."], "sha": "32310dc454720aefdea5bf4cea7f78a38c183954", "url": "https://github.com/mvantellingen/localshop/blob/32310dc454720aefdea5bf4cea7f78a38c183954/src/localshop/apps/packages/pypi.py#L6-L26", "partition": "train"}
{"repo": "captin411/ofxclient", "path": "ofxclient/client.py", "func_name": "Client._do_post", "original_string": "def _do_post(self, query, extra_headers=[]):\n        \"\"\"\n        Do a POST to the Institution.\n\n        :param query: Body content to POST (OFX Query)\n        :type query: str\n        :param extra_headers: Extra headers to send with the request, as a list\n          of (Name, Value) header 2-tuples.\n        :type extra_headers: list\n        :return: 2-tuple of (HTTPResponse, str response body)\n        :rtype: tuple\n        \"\"\"\n        i = self.institution\n        logging.debug('posting data to %s' % i.url)\n        garbage, path = splittype(i.url)\n        host, selector = splithost(path)\n        h = HTTPSConnection(host, timeout=60)\n        # Discover requires a particular ordering of headers, so send the\n        # request step by step.\n        h.putrequest('POST', selector, skip_host=True,\n                     skip_accept_encoding=True)\n        headers = [\n            ('Content-Type', 'application/x-ofx'),\n            ('Host', host),\n            ('Content-Length', len(query)),\n            ('Connection', 'Keep-Alive')\n        ]\n        if self.accept:\n            headers.append(('Accept', self.accept))\n        if self.user_agent:\n            headers.append(('User-Agent', self.user_agent))\n        for ehname, ehval in extra_headers:\n            headers.append((ehname, ehval))\n        logging.debug('---- request headers ----')\n        for hname, hval in headers:\n            logging.debug('%s: %s', hname, hval)\n            h.putheader(hname, hval)\n        logging.debug('---- request body (query) ----')\n        logging.debug(query)\n        h.endheaders(query.encode())\n        res = h.getresponse()\n        response = res.read().decode('ascii', 'ignore')\n        logging.debug('---- response ----')\n        logging.debug(res.__dict__)\n        logging.debug('Headers: %s', res.getheaders())\n        logging.debug(response)\n        res.close()\n        return res, response", "language": "python", "code": "def _do_post(self, query, extra_headers=[]):\n        \"\"\"\n        Do a POST to the Institution.\n\n        :param query: Body content to POST (OFX Query)\n        :type query: str\n        :param extra_headers: Extra headers to send with the request, as a list\n          of (Name, Value) header 2-tuples.\n        :type extra_headers: list\n        :return: 2-tuple of (HTTPResponse, str response body)\n        :rtype: tuple\n        \"\"\"\n        i = self.institution\n        logging.debug('posting data to %s' % i.url)\n        garbage, path = splittype(i.url)\n        host, selector = splithost(path)\n        h = HTTPSConnection(host, timeout=60)\n        # Discover requires a particular ordering of headers, so send the\n        # request step by step.\n        h.putrequest('POST', selector, skip_host=True,\n                     skip_accept_encoding=True)\n        headers = [\n            ('Content-Type', 'application/x-ofx'),\n            ('Host', host),\n            ('Content-Length', len(query)),\n            ('Connection', 'Keep-Alive')\n        ]\n        if self.accept:\n            headers.append(('Accept', self.accept))\n        if self.user_agent:\n            headers.append(('User-Agent', self.user_agent))\n        for ehname, ehval in extra_headers:\n            headers.append((ehname, ehval))\n        logging.debug('---- request headers ----')\n        for hname, hval in headers:\n            logging.debug('%s: %s', hname, hval)\n            h.putheader(hname, hval)\n        logging.debug('---- request body (query) ----')\n        logging.debug(query)\n        h.endheaders(query.encode())\n        res = h.getresponse()\n        response = res.read().decode('ascii', 'ignore')\n        logging.debug('---- response ----')\n        logging.debug(res.__dict__)\n        logging.debug('Headers: %s', res.getheaders())\n        logging.debug(response)\n        res.close()\n        return res, response", "code_tokens": ["def", "_do_post", "(", "self", ",", "query", ",", "extra_headers", "=", "[", "]", ")", ":", "i", "=", "self", ".", "institution", "logging", ".", "debug", "(", "'posting data to %s'", "%", "i", ".", "url", ")", "garbage", ",", "path", "=", "splittype", "(", "i", ".", "url", ")", "host", ",", "selector", "=", "splithost", "(", "path", ")", "h", "=", "HTTPSConnection", "(", "host", ",", "timeout", "=", "60", ")", "# Discover requires a particular ordering of headers, so send the", "# request step by step.", "h", ".", "putrequest", "(", "'POST'", ",", "selector", ",", "skip_host", "=", "True", ",", "skip_accept_encoding", "=", "True", ")", "headers", "=", "[", "(", "'Content-Type'", ",", "'application/x-ofx'", ")", ",", "(", "'Host'", ",", "host", ")", ",", "(", "'Content-Length'", ",", "len", "(", "query", ")", ")", ",", "(", "'Connection'", ",", "'Keep-Alive'", ")", "]", "if", "self", ".", "accept", ":", "headers", ".", "append", "(", "(", "'Accept'", ",", "self", ".", "accept", ")", ")", "if", "self", ".", "user_agent", ":", "headers", ".", "append", "(", "(", "'User-Agent'", ",", "self", ".", "user_agent", ")", ")", "for", "ehname", ",", "ehval", "in", "extra_headers", ":", "headers", ".", "append", "(", "(", "ehname", ",", "ehval", ")", ")", "logging", ".", "debug", "(", "'---- request headers ----'", ")", "for", "hname", ",", "hval", "in", "headers", ":", "logging", ".", "debug", "(", "'%s: %s'", ",", "hname", ",", "hval", ")", "h", ".", "putheader", "(", "hname", ",", "hval", ")", "logging", ".", "debug", "(", "'---- request body (query) ----'", ")", "logging", ".", "debug", "(", "query", ")", "h", ".", "endheaders", "(", "query", ".", "encode", "(", ")", ")", "res", "=", "h", ".", "getresponse", "(", ")", "response", "=", "res", ".", "read", "(", ")", ".", "decode", "(", "'ascii'", ",", "'ignore'", ")", "logging", ".", "debug", "(", "'---- response ----'", ")", "logging", ".", "debug", "(", "res", ".", "__dict__", ")", "logging", ".", "debug", "(", "'Headers: %s'", ",", "res", ".", "getheaders", "(", ")", ")", "logging", ".", "debug", "(", "response", ")", "res", ".", "close", "(", ")", "return", "res", ",", "response"], "docstring": "Do a POST to the Institution.\n\n        :param query: Body content to POST (OFX Query)\n        :type query: str\n        :param extra_headers: Extra headers to send with the request, as a list\n          of (Name, Value) header 2-tuples.\n        :type extra_headers: list\n        :return: 2-tuple of (HTTPResponse, str response body)\n        :rtype: tuple", "docstring_tokens": ["Do", "a", "POST", "to", "the", "Institution", "."], "sha": "4da2719f0ecbbf5eee62fb82c1b3b34ec955ee5e", "url": "https://github.com/captin411/ofxclient/blob/4da2719f0ecbbf5eee62fb82c1b3b34ec955ee5e/ofxclient/client.py#L141-L188", "partition": "train"}
{"repo": "pnuckowski/aioresponses", "path": "aioresponses/core.py", "func_name": "RequestMatch._build_raw_headers", "original_string": "def _build_raw_headers(self, headers: Dict) -> Tuple:\n        \"\"\"\n        Convert a dict of headers to a tuple of tuples\n\n        Mimics the format of ClientResponse.\n        \"\"\"\n        raw_headers = []\n        for k, v in headers.items():\n            raw_headers.append((k.encode('utf8'), v.encode('utf8')))\n        return tuple(raw_headers)", "language": "python", "code": "def _build_raw_headers(self, headers: Dict) -> Tuple:\n        \"\"\"\n        Convert a dict of headers to a tuple of tuples\n\n        Mimics the format of ClientResponse.\n        \"\"\"\n        raw_headers = []\n        for k, v in headers.items():\n            raw_headers.append((k.encode('utf8'), v.encode('utf8')))\n        return tuple(raw_headers)", "code_tokens": ["def", "_build_raw_headers", "(", "self", ",", "headers", ":", "Dict", ")", "->", "Tuple", ":", "raw_headers", "=", "[", "]", "for", "k", ",", "v", "in", "headers", ".", "items", "(", ")", ":", "raw_headers", ".", "append", "(", "(", "k", ".", "encode", "(", "'utf8'", ")", ",", "v", ".", "encode", "(", "'utf8'", ")", ")", ")", "return", "tuple", "(", "raw_headers", ")"], "docstring": "Convert a dict of headers to a tuple of tuples\n\n        Mimics the format of ClientResponse.", "docstring_tokens": ["Convert", "a", "dict", "of", "headers", "to", "a", "tuple", "of", "tuples"], "sha": "566461a21a25757e313e0d4afaf338d53d66db03", "url": "https://github.com/pnuckowski/aioresponses/blob/566461a21a25757e313e0d4afaf338d53d66db03/aioresponses/core.py#L102-L111", "partition": "train"}
{"repo": "TheHive-Project/TheHive4py", "path": "thehive4py/api.py", "func_name": "TheHiveApi.run_analyzer", "original_string": "def run_analyzer(self, cortex_id, artifact_id, analyzer_id):\n\n        \"\"\"\n        :param cortex_id: identifier of the Cortex server\n        :param artifact_id: identifier of the artifact as found with an artifact search\n        :param analyzer_id: name of the analyzer used by the job\n        :rtype: json\n        \"\"\"\n\n        req = self.url + \"/api/connector/cortex/job\"\n\n        try:\n            data = json.dumps({ \"cortexId\": cortex_id,\n                \"artifactId\": artifact_id,\n                \"analyzerId\": analyzer_id\n                })\n            return requests.post(req, headers={'Content-Type': 'application/json'}, data=data, proxies=self.proxies, auth=self.auth, verify=self.cert)\n        except requests.exceptions.RequestException as e:\n            raise TheHiveException(\"Analyzer run error: {}\".format(e))", "language": "python", "code": "def run_analyzer(self, cortex_id, artifact_id, analyzer_id):\n\n        \"\"\"\n        :param cortex_id: identifier of the Cortex server\n        :param artifact_id: identifier of the artifact as found with an artifact search\n        :param analyzer_id: name of the analyzer used by the job\n        :rtype: json\n        \"\"\"\n\n        req = self.url + \"/api/connector/cortex/job\"\n\n        try:\n            data = json.dumps({ \"cortexId\": cortex_id,\n                \"artifactId\": artifact_id,\n                \"analyzerId\": analyzer_id\n                })\n            return requests.post(req, headers={'Content-Type': 'application/json'}, data=data, proxies=self.proxies, auth=self.auth, verify=self.cert)\n        except requests.exceptions.RequestException as e:\n            raise TheHiveException(\"Analyzer run error: {}\".format(e))", "code_tokens": ["def", "run_analyzer", "(", "self", ",", "cortex_id", ",", "artifact_id", ",", "analyzer_id", ")", ":", "req", "=", "self", ".", "url", "+", "\"/api/connector/cortex/job\"", "try", ":", "data", "=", "json", ".", "dumps", "(", "{", "\"cortexId\"", ":", "cortex_id", ",", "\"artifactId\"", ":", "artifact_id", ",", "\"analyzerId\"", ":", "analyzer_id", "}", ")", "return", "requests", ".", "post", "(", "req", ",", "headers", "=", "{", "'Content-Type'", ":", "'application/json'", "}", ",", "data", "=", "data", ",", "proxies", "=", "self", ".", "proxies", ",", "auth", "=", "self", ".", "auth", ",", "verify", "=", "self", ".", "cert", ")", "except", "requests", ".", "exceptions", ".", "RequestException", "as", "e", ":", "raise", "TheHiveException", "(", "\"Analyzer run error: {}\"", ".", "format", "(", "e", ")", ")"], "docstring": ":param cortex_id: identifier of the Cortex server\n        :param artifact_id: identifier of the artifact as found with an artifact search\n        :param analyzer_id: name of the analyzer used by the job\n        :rtype: json", "docstring_tokens": [":", "param", "cortex_id", ":", "identifier", "of", "the", "Cortex", "server", ":", "param", "artifact_id", ":", "identifier", "of", "the", "artifact", "as", "found", "with", "an", "artifact", "search", ":", "param", "analyzer_id", ":", "name", "of", "the", "analyzer", "used", "by", "the", "job", ":", "rtype", ":", "json"], "sha": "35762bbd50d8376943268464326b59c752d6241b", "url": "https://github.com/TheHive-Project/TheHive4py/blob/35762bbd50d8376943268464326b59c752d6241b/thehive4py/api.py#L472-L490", "partition": "train"}
{"repo": "bigchaindb/bigchaindb-driver", "path": "bigchaindb_driver/driver.py", "func_name": "BigchainDB.info", "original_string": "def info(self, headers=None):\n        \"\"\"Retrieves information of the node being connected to via the\n        root endpoint ``'/'``.\n\n        Args:\n            headers (dict): Optional headers to pass to the request.\n\n        Returns:\n            dict: Details of the node that this instance is connected\n            to. Some information that may be interesting:\n\n                * the server version and\n                * an overview of all the endpoints\n\n        Note:\n            Currently limited to one node, and will be expanded to\n            return information for each node that this instance is\n            connected to.\n\n        \"\"\"\n        return self.transport.forward_request(\n            method='GET', path='/', headers=headers)", "language": "python", "code": "def info(self, headers=None):\n        \"\"\"Retrieves information of the node being connected to via the\n        root endpoint ``'/'``.\n\n        Args:\n            headers (dict): Optional headers to pass to the request.\n\n        Returns:\n            dict: Details of the node that this instance is connected\n            to. Some information that may be interesting:\n\n                * the server version and\n                * an overview of all the endpoints\n\n        Note:\n            Currently limited to one node, and will be expanded to\n            return information for each node that this instance is\n            connected to.\n\n        \"\"\"\n        return self.transport.forward_request(\n            method='GET', path='/', headers=headers)", "code_tokens": ["def", "info", "(", "self", ",", "headers", "=", "None", ")", ":", "return", "self", ".", "transport", ".", "forward_request", "(", "method", "=", "'GET'", ",", "path", "=", "'/'", ",", "headers", "=", "headers", ")"], "docstring": "Retrieves information of the node being connected to via the\n        root endpoint ``'/'``.\n\n        Args:\n            headers (dict): Optional headers to pass to the request.\n\n        Returns:\n            dict: Details of the node that this instance is connected\n            to. Some information that may be interesting:\n\n                * the server version and\n                * an overview of all the endpoints\n\n        Note:\n            Currently limited to one node, and will be expanded to\n            return information for each node that this instance is\n            connected to.", "docstring_tokens": ["Retrieves", "information", "of", "the", "node", "being", "connected", "to", "via", "the", "root", "endpoint", "/", "."], "sha": "c294a535f0696bd19483ae11a4882b74e6fc061e", "url": "https://github.com/bigchaindb/bigchaindb-driver/blob/c294a535f0696bd19483ae11a4882b74e6fc061e/bigchaindb_driver/driver.py#L97-L118", "partition": "train"}
{"repo": "bigchaindb/bigchaindb-driver", "path": "bigchaindb_driver/common/transaction.py", "func_name": "Transaction.from_dict", "original_string": "def from_dict(cls, tx):\n        \"\"\"Transforms a Python dictionary to a Transaction object.\n\n            Args:\n                tx_body (dict): The Transaction to be transformed.\n\n            Returns:\n                :class:`~bigchaindb.common.transaction.Transaction`\n        \"\"\"\n        inputs = [Input.from_dict(input_) for input_ in tx['inputs']]\n        outputs = [Output.from_dict(output) for output in tx['outputs']]\n        return cls(tx['operation'], tx['asset'], inputs, outputs,\n                   tx['metadata'], tx['version'], hash_id=tx['id'])", "language": "python", "code": "def from_dict(cls, tx):\n        \"\"\"Transforms a Python dictionary to a Transaction object.\n\n            Args:\n                tx_body (dict): The Transaction to be transformed.\n\n            Returns:\n                :class:`~bigchaindb.common.transaction.Transaction`\n        \"\"\"\n        inputs = [Input.from_dict(input_) for input_ in tx['inputs']]\n        outputs = [Output.from_dict(output) for output in tx['outputs']]\n        return cls(tx['operation'], tx['asset'], inputs, outputs,\n                   tx['metadata'], tx['version'], hash_id=tx['id'])", "code_tokens": ["def", "from_dict", "(", "cls", ",", "tx", ")", ":", "inputs", "=", "[", "Input", ".", "from_dict", "(", "input_", ")", "for", "input_", "in", "tx", "[", "'inputs'", "]", "]", "outputs", "=", "[", "Output", ".", "from_dict", "(", "output", ")", "for", "output", "in", "tx", "[", "'outputs'", "]", "]", "return", "cls", "(", "tx", "[", "'operation'", "]", ",", "tx", "[", "'asset'", "]", ",", "inputs", ",", "outputs", ",", "tx", "[", "'metadata'", "]", ",", "tx", "[", "'version'", "]", ",", "hash_id", "=", "tx", "[", "'id'", "]", ")"], "docstring": "Transforms a Python dictionary to a Transaction object.\n\n            Args:\n                tx_body (dict): The Transaction to be transformed.\n\n            Returns:\n                :class:`~bigchaindb.common.transaction.Transaction`", "docstring_tokens": ["Transforms", "a", "Python", "dictionary", "to", "a", "Transaction", "object", "."], "sha": "c294a535f0696bd19483ae11a4882b74e6fc061e", "url": "https://github.com/bigchaindb/bigchaindb-driver/blob/c294a535f0696bd19483ae11a4882b74e6fc061e/bigchaindb_driver/common/transaction.py#L1174-L1186", "partition": "train"}
{"repo": "Kong/unirest-python", "path": "unirest/utils.py", "func_name": "dict2query", "original_string": "def dict2query(dictionary):\n    \"\"\"\n    We want post vars of form:\n    {'foo': 'bar', 'nested': {'a': 'b', 'c': 'd'}}\n    to become:\n    foo=bar&nested[a]=b&nested[c]=d\n    \"\"\"\n    query = []\n    encoders = {dict: _dictionary_encoder}\n    for k, v in dictionary.iteritems():\n        if v.__class__ in encoders:\n            nested_query = encoders[v.__class__](k, v)\n            query += nested_query\n        else:\n            key = to_utf8(k)\n            value = to_utf8(v)\n            query.append('{}={}'.format(key, value))\n\n    return '&'.join(query)", "language": "python", "code": "def dict2query(dictionary):\n    \"\"\"\n    We want post vars of form:\n    {'foo': 'bar', 'nested': {'a': 'b', 'c': 'd'}}\n    to become:\n    foo=bar&nested[a]=b&nested[c]=d\n    \"\"\"\n    query = []\n    encoders = {dict: _dictionary_encoder}\n    for k, v in dictionary.iteritems():\n        if v.__class__ in encoders:\n            nested_query = encoders[v.__class__](k, v)\n            query += nested_query\n        else:\n            key = to_utf8(k)\n            value = to_utf8(v)\n            query.append('{}={}'.format(key, value))\n\n    return '&'.join(query)", "code_tokens": ["def", "dict2query", "(", "dictionary", ")", ":", "query", "=", "[", "]", "encoders", "=", "{", "dict", ":", "_dictionary_encoder", "}", "for", "k", ",", "v", "in", "dictionary", ".", "iteritems", "(", ")", ":", "if", "v", ".", "__class__", "in", "encoders", ":", "nested_query", "=", "encoders", "[", "v", ".", "__class__", "]", "(", "k", ",", "v", ")", "query", "+=", "nested_query", "else", ":", "key", "=", "to_utf8", "(", "k", ")", "value", "=", "to_utf8", "(", "v", ")", "query", ".", "append", "(", "'{}={}'", ".", "format", "(", "key", ",", "value", ")", ")", "return", "'&'", ".", "join", "(", "query", ")"], "docstring": "We want post vars of form:\n    {'foo': 'bar', 'nested': {'a': 'b', 'c': 'd'}}\n    to become:\n    foo=bar&nested[a]=b&nested[c]=d", "docstring_tokens": ["We", "want", "post", "vars", "of", "form", ":", "{", "foo", ":", "bar", "nested", ":", "{", "a", ":", "b", "c", ":", "d", "}}", "to", "become", ":", "foo", "=", "bar&nested", "[", "a", "]", "=", "b&nested", "[", "c", "]", "=", "d"], "sha": "e3fc0d4ffd6aa87630477e1bd378f7ee047385b4", "url": "https://github.com/Kong/unirest-python/blob/e3fc0d4ffd6aa87630477e1bd378f7ee047385b4/unirest/utils.py#L24-L42", "partition": "train"}
{"repo": "zapier/email-reply-parser", "path": "email_reply_parser/__init__.py", "func_name": "EmailMessage.read", "original_string": "def read(self):\n        \"\"\" Creates new fragment for each line\n            and labels as a signature, quote, or hidden.\n\n            Returns EmailMessage instance\n        \"\"\"\n\n        self.found_visible = False\n\n        is_multi_quote_header = self.MULTI_QUOTE_HDR_REGEX_MULTILINE.search(self.text)\n        if is_multi_quote_header:\n            self.text = self.MULTI_QUOTE_HDR_REGEX.sub(is_multi_quote_header.groups()[0].replace('\\n', ''), self.text)\n\n        # Fix any outlook style replies, with the reply immediately above the signature boundary line\n        #   See email_2_2.txt for an example\n        self.text = re.sub('([^\\n])(?=\\n ?[_-]{7,})', '\\\\1\\n', self.text, re.MULTILINE)\n\n        self.lines = self.text.split('\\n')\n        self.lines.reverse()\n\n        for line in self.lines:\n            self._scan_line(line)\n\n        self._finish_fragment()\n\n        self.fragments.reverse()\n\n        return self", "language": "python", "code": "def read(self):\n        \"\"\" Creates new fragment for each line\n            and labels as a signature, quote, or hidden.\n\n            Returns EmailMessage instance\n        \"\"\"\n\n        self.found_visible = False\n\n        is_multi_quote_header = self.MULTI_QUOTE_HDR_REGEX_MULTILINE.search(self.text)\n        if is_multi_quote_header:\n            self.text = self.MULTI_QUOTE_HDR_REGEX.sub(is_multi_quote_header.groups()[0].replace('\\n', ''), self.text)\n\n        # Fix any outlook style replies, with the reply immediately above the signature boundary line\n        #   See email_2_2.txt for an example\n        self.text = re.sub('([^\\n])(?=\\n ?[_-]{7,})', '\\\\1\\n', self.text, re.MULTILINE)\n\n        self.lines = self.text.split('\\n')\n        self.lines.reverse()\n\n        for line in self.lines:\n            self._scan_line(line)\n\n        self._finish_fragment()\n\n        self.fragments.reverse()\n\n        return self", "code_tokens": ["def", "read", "(", "self", ")", ":", "self", ".", "found_visible", "=", "False", "is_multi_quote_header", "=", "self", ".", "MULTI_QUOTE_HDR_REGEX_MULTILINE", ".", "search", "(", "self", ".", "text", ")", "if", "is_multi_quote_header", ":", "self", ".", "text", "=", "self", ".", "MULTI_QUOTE_HDR_REGEX", ".", "sub", "(", "is_multi_quote_header", ".", "groups", "(", ")", "[", "0", "]", ".", "replace", "(", "'\\n'", ",", "''", ")", ",", "self", ".", "text", ")", "# Fix any outlook style replies, with the reply immediately above the signature boundary line", "#   See email_2_2.txt for an example", "self", ".", "text", "=", "re", ".", "sub", "(", "'([^\\n])(?=\\n ?[_-]{7,})'", ",", "'\\\\1\\n'", ",", "self", ".", "text", ",", "re", ".", "MULTILINE", ")", "self", ".", "lines", "=", "self", ".", "text", ".", "split", "(", "'\\n'", ")", "self", ".", "lines", ".", "reverse", "(", ")", "for", "line", "in", "self", ".", "lines", ":", "self", ".", "_scan_line", "(", "line", ")", "self", ".", "_finish_fragment", "(", ")", "self", ".", "fragments", ".", "reverse", "(", ")", "return", "self"], "docstring": "Creates new fragment for each line\n            and labels as a signature, quote, or hidden.\n\n            Returns EmailMessage instance", "docstring_tokens": ["Creates", "new", "fragment", "for", "each", "line", "and", "labels", "as", "a", "signature", "quote", "or", "hidden", "."], "sha": "0c0b73a9bf2188b079a191417b273fc2cf695bf2", "url": "https://github.com/zapier/email-reply-parser/blob/0c0b73a9bf2188b079a191417b273fc2cf695bf2/email_reply_parser/__init__.py#L53-L80", "partition": "train"}
{"repo": "OCA/vertical-hotel", "path": "hotel_reservation/models/hotel_reservation.py", "func_name": "QuickRoomReservation.room_reserve", "original_string": "def room_reserve(self):\n        \"\"\"\n        This method create a new record for hotel.reservation\n        -----------------------------------------------------\n        @param self: The object pointer\n        @return: new record set for hotel reservation.\n        \"\"\"\n        hotel_res_obj = self.env['hotel.reservation']\n        for res in self:\n            rec = (hotel_res_obj.create\n                   ({'partner_id': res.partner_id.id,\n                     'partner_invoice_id': res.partner_invoice_id.id,\n                     'partner_order_id': res.partner_order_id.id,\n                     'partner_shipping_id': res.partner_shipping_id.id,\n                     'checkin': res.check_in,\n                     'checkout': res.check_out,\n                     'warehouse_id': res.warehouse_id.id,\n                     'pricelist_id': res.pricelist_id.id,\n                     'adults': res.adults,\n                     'reservation_line': [(0, 0,\n                                           {'reserve': [(6, 0,\n                                                         [res.room_id.id])],\n                                            'name': (res.room_id and\n                                                     res.room_id.name or '')\n                                            })]\n                     }))\n        return rec", "language": "python", "code": "def room_reserve(self):\n        \"\"\"\n        This method create a new record for hotel.reservation\n        -----------------------------------------------------\n        @param self: The object pointer\n        @return: new record set for hotel reservation.\n        \"\"\"\n        hotel_res_obj = self.env['hotel.reservation']\n        for res in self:\n            rec = (hotel_res_obj.create\n                   ({'partner_id': res.partner_id.id,\n                     'partner_invoice_id': res.partner_invoice_id.id,\n                     'partner_order_id': res.partner_order_id.id,\n                     'partner_shipping_id': res.partner_shipping_id.id,\n                     'checkin': res.check_in,\n                     'checkout': res.check_out,\n                     'warehouse_id': res.warehouse_id.id,\n                     'pricelist_id': res.pricelist_id.id,\n                     'adults': res.adults,\n                     'reservation_line': [(0, 0,\n                                           {'reserve': [(6, 0,\n                                                         [res.room_id.id])],\n                                            'name': (res.room_id and\n                                                     res.room_id.name or '')\n                                            })]\n                     }))\n        return rec", "code_tokens": ["def", "room_reserve", "(", "self", ")", ":", "hotel_res_obj", "=", "self", ".", "env", "[", "'hotel.reservation'", "]", "for", "res", "in", "self", ":", "rec", "=", "(", "hotel_res_obj", ".", "create", "(", "{", "'partner_id'", ":", "res", ".", "partner_id", ".", "id", ",", "'partner_invoice_id'", ":", "res", ".", "partner_invoice_id", ".", "id", ",", "'partner_order_id'", ":", "res", ".", "partner_order_id", ".", "id", ",", "'partner_shipping_id'", ":", "res", ".", "partner_shipping_id", ".", "id", ",", "'checkin'", ":", "res", ".", "check_in", ",", "'checkout'", ":", "res", ".", "check_out", ",", "'warehouse_id'", ":", "res", ".", "warehouse_id", ".", "id", ",", "'pricelist_id'", ":", "res", ".", "pricelist_id", ".", "id", ",", "'adults'", ":", "res", ".", "adults", ",", "'reservation_line'", ":", "[", "(", "0", ",", "0", ",", "{", "'reserve'", ":", "[", "(", "6", ",", "0", ",", "[", "res", ".", "room_id", ".", "id", "]", ")", "]", ",", "'name'", ":", "(", "res", ".", "room_id", "and", "res", ".", "room_id", ".", "name", "or", "''", ")", "}", ")", "]", "}", ")", ")", "return", "rec"], "docstring": "This method create a new record for hotel.reservation\n        -----------------------------------------------------\n        @param self: The object pointer\n        @return: new record set for hotel reservation.", "docstring_tokens": ["This", "method", "create", "a", "new", "record", "for", "hotel", ".", "reservation", "-----------------------------------------------------"], "sha": "a01442e92b5ea1fda7fb9e6180b3211e8749a35a", "url": "https://github.com/OCA/vertical-hotel/blob/a01442e92b5ea1fda7fb9e6180b3211e8749a35a/hotel_reservation/models/hotel_reservation.py#L1039-L1065", "partition": "train"}
{"repo": "jmcgeheeiv/pyfakefs", "path": "pyfakefs/fake_scandir.py", "func_name": "_classify_directory_contents", "original_string": "def _classify_directory_contents(filesystem, root):\n    \"\"\"Classify contents of a directory as files/directories.\n\n    Args:\n        filesystem: The fake filesystem used for implementation\n        root: (str) Directory to examine.\n\n    Returns:\n        (tuple) A tuple consisting of three values: the directory examined,\n        a list containing all of the directory entries, and a list\n        containing all of the non-directory entries.\n        (This is the same format as returned by the `os.walk` generator.)\n\n    Raises:\n        Nothing on its own, but be ready to catch exceptions generated by\n        underlying mechanisms like `os.listdir`.\n    \"\"\"\n    dirs = []\n    files = []\n    for entry in filesystem.listdir(root):\n        if filesystem.isdir(filesystem.joinpaths(root, entry)):\n            dirs.append(entry)\n        else:\n            files.append(entry)\n    return root, dirs, files", "language": "python", "code": "def _classify_directory_contents(filesystem, root):\n    \"\"\"Classify contents of a directory as files/directories.\n\n    Args:\n        filesystem: The fake filesystem used for implementation\n        root: (str) Directory to examine.\n\n    Returns:\n        (tuple) A tuple consisting of three values: the directory examined,\n        a list containing all of the directory entries, and a list\n        containing all of the non-directory entries.\n        (This is the same format as returned by the `os.walk` generator.)\n\n    Raises:\n        Nothing on its own, but be ready to catch exceptions generated by\n        underlying mechanisms like `os.listdir`.\n    \"\"\"\n    dirs = []\n    files = []\n    for entry in filesystem.listdir(root):\n        if filesystem.isdir(filesystem.joinpaths(root, entry)):\n            dirs.append(entry)\n        else:\n            files.append(entry)\n    return root, dirs, files", "code_tokens": ["def", "_classify_directory_contents", "(", "filesystem", ",", "root", ")", ":", "dirs", "=", "[", "]", "files", "=", "[", "]", "for", "entry", "in", "filesystem", ".", "listdir", "(", "root", ")", ":", "if", "filesystem", ".", "isdir", "(", "filesystem", ".", "joinpaths", "(", "root", ",", "entry", ")", ")", ":", "dirs", ".", "append", "(", "entry", ")", "else", ":", "files", ".", "append", "(", "entry", ")", "return", "root", ",", "dirs", ",", "files"], "docstring": "Classify contents of a directory as files/directories.\n\n    Args:\n        filesystem: The fake filesystem used for implementation\n        root: (str) Directory to examine.\n\n    Returns:\n        (tuple) A tuple consisting of three values: the directory examined,\n        a list containing all of the directory entries, and a list\n        containing all of the non-directory entries.\n        (This is the same format as returned by the `os.walk` generator.)\n\n    Raises:\n        Nothing on its own, but be ready to catch exceptions generated by\n        underlying mechanisms like `os.listdir`.", "docstring_tokens": ["Classify", "contents", "of", "a", "directory", "as", "files", "/", "directories", "."], "sha": "6c36fb8987108107fc861fc3013620d46c7d2f9c", "url": "https://github.com/jmcgeheeiv/pyfakefs/blob/6c36fb8987108107fc861fc3013620d46c7d2f9c/pyfakefs/fake_scandir.py#L176-L200", "partition": "train"}
{"repo": "scopus-api/scopus", "path": "scopus/utils/get_encoded_text.py", "func_name": "get_encoded_text", "original_string": "def get_encoded_text(container, xpath):\n    \"\"\"Return text for element at xpath in the container xml if it is there.\n\n    Parameters\n    ----------\n    container : xml.etree.ElementTree.Element\n        The element to be searched in.\n\n    xpath : str\n        The path to be looked for.\n\n    Returns\n    -------\n    result : str\n    \"\"\"\n    try:\n        return \"\".join(container.find(xpath, ns).itertext())\n    except AttributeError:\n        return None", "language": "python", "code": "def get_encoded_text(container, xpath):\n    \"\"\"Return text for element at xpath in the container xml if it is there.\n\n    Parameters\n    ----------\n    container : xml.etree.ElementTree.Element\n        The element to be searched in.\n\n    xpath : str\n        The path to be looked for.\n\n    Returns\n    -------\n    result : str\n    \"\"\"\n    try:\n        return \"\".join(container.find(xpath, ns).itertext())\n    except AttributeError:\n        return None", "code_tokens": ["def", "get_encoded_text", "(", "container", ",", "xpath", ")", ":", "try", ":", "return", "\"\"", ".", "join", "(", "container", ".", "find", "(", "xpath", ",", "ns", ")", ".", "itertext", "(", ")", ")", "except", "AttributeError", ":", "return", "None"], "docstring": "Return text for element at xpath in the container xml if it is there.\n\n    Parameters\n    ----------\n    container : xml.etree.ElementTree.Element\n        The element to be searched in.\n\n    xpath : str\n        The path to be looked for.\n\n    Returns\n    -------\n    result : str", "docstring_tokens": ["Return", "text", "for", "element", "at", "xpath", "in", "the", "container", "xml", "if", "it", "is", "there", "."], "sha": "27ce02dd3095bfdab9d3e8475543d7c17767d1ab", "url": "https://github.com/scopus-api/scopus/blob/27ce02dd3095bfdab9d3e8475543d7c17767d1ab/scopus/utils/get_encoded_text.py#L15-L33", "partition": "train"}
{"repo": "sixty-north/cosmic-ray", "path": "src/cosmic_ray/cli.py", "func_name": "handle_new_config", "original_string": "def handle_new_config(args):\n    \"\"\"usage: cosmic-ray new-config <config-file>\n\n    Create a new config file.\n    \"\"\"\n    config = cosmic_ray.commands.new_config()\n    config_str = serialize_config(config)\n    with open(args['<config-file>'], mode='wt') as handle:\n        handle.write(config_str)\n\n    return ExitCode.OK", "language": "python", "code": "def handle_new_config(args):\n    \"\"\"usage: cosmic-ray new-config <config-file>\n\n    Create a new config file.\n    \"\"\"\n    config = cosmic_ray.commands.new_config()\n    config_str = serialize_config(config)\n    with open(args['<config-file>'], mode='wt') as handle:\n        handle.write(config_str)\n\n    return ExitCode.OK", "code_tokens": ["def", "handle_new_config", "(", "args", ")", ":", "config", "=", "cosmic_ray", ".", "commands", ".", "new_config", "(", ")", "config_str", "=", "serialize_config", "(", "config", ")", "with", "open", "(", "args", "[", "'<config-file>'", "]", ",", "mode", "=", "'wt'", ")", "as", "handle", ":", "handle", ".", "write", "(", "config_str", ")", "return", "ExitCode", ".", "OK"], "docstring": "usage: cosmic-ray new-config <config-file>\n\n    Create a new config file.", "docstring_tokens": ["usage", ":", "cosmic", "-", "ray", "new", "-", "config", "<config", "-", "file", ">"], "sha": "c654e074afbb7b7fcbc23359083c1287c0d3e991", "url": "https://github.com/sixty-north/cosmic-ray/blob/c654e074afbb7b7fcbc23359083c1287c0d3e991/src/cosmic_ray/cli.py#L71-L81", "partition": "train"}
{"repo": "isislovecruft/python-gnupg", "path": "pretty_bad_protocol/gnupg.py", "func_name": "GPGUtilities.encrypted_to", "original_string": "def encrypted_to(self, raw_data):\n        \"\"\"Return the key to which raw_data is encrypted to.\"\"\"\n        # TODO: make this support multiple keys.\n        result = self._gpg.list_packets(raw_data)\n        if not result.key:\n            raise LookupError(\n                \"Content is not encrypted to a GnuPG key!\")\n        try:\n            return self.find_key_by_keyid(result.key)\n        except:\n            return self.find_key_by_subkey(result.key)", "language": "python", "code": "def encrypted_to(self, raw_data):\n        \"\"\"Return the key to which raw_data is encrypted to.\"\"\"\n        # TODO: make this support multiple keys.\n        result = self._gpg.list_packets(raw_data)\n        if not result.key:\n            raise LookupError(\n                \"Content is not encrypted to a GnuPG key!\")\n        try:\n            return self.find_key_by_keyid(result.key)\n        except:\n            return self.find_key_by_subkey(result.key)", "code_tokens": ["def", "encrypted_to", "(", "self", ",", "raw_data", ")", ":", "# TODO: make this support multiple keys.", "result", "=", "self", ".", "_gpg", ".", "list_packets", "(", "raw_data", ")", "if", "not", "result", ".", "key", ":", "raise", "LookupError", "(", "\"Content is not encrypted to a GnuPG key!\"", ")", "try", ":", "return", "self", ".", "find_key_by_keyid", "(", "result", ".", "key", ")", "except", ":", "return", "self", ".", "find_key_by_subkey", "(", "result", ".", "key", ")"], "docstring": "Return the key to which raw_data is encrypted to.", "docstring_tokens": ["Return", "the", "key", "to", "which", "raw_data", "is", "encrypted", "to", "."], "sha": "784571449032e811587249743e183fc5e908a673", "url": "https://github.com/isislovecruft/python-gnupg/blob/784571449032e811587249743e183fc5e908a673/pretty_bad_protocol/gnupg.py#L1153-L1163", "partition": "train"}
{"repo": "NetEaseGame/aircv", "path": "aircv/__init__.py", "func_name": "imread", "original_string": "def imread(filename):\n    ''' \n    Like cv2.imread\n    This function will make sure filename exists \n    '''\n    im = cv2.imread(filename)\n    if im is None:\n        raise RuntimeError(\"file: '%s' not exists\" % filename)\n    return im", "language": "python", "code": "def imread(filename):\n    ''' \n    Like cv2.imread\n    This function will make sure filename exists \n    '''\n    im = cv2.imread(filename)\n    if im is None:\n        raise RuntimeError(\"file: '%s' not exists\" % filename)\n    return im", "code_tokens": ["def", "imread", "(", "filename", ")", ":", "im", "=", "cv2", ".", "imread", "(", "filename", ")", "if", "im", "is", "None", ":", "raise", "RuntimeError", "(", "\"file: '%s' not exists\"", "%", "filename", ")", "return", "im"], "docstring": "Like cv2.imread\n    This function will make sure filename exists", "docstring_tokens": ["Like", "cv2", ".", "imread", "This", "function", "will", "make", "sure", "filename", "exists"], "sha": "d479e49ef98347bbbe6ac2e4aa9119cecc15c8ca", "url": "https://github.com/NetEaseGame/aircv/blob/d479e49ef98347bbbe6ac2e4aa9119cecc15c8ca/aircv/__init__.py#L80-L88", "partition": "train"}
{"repo": "trolldbois/ctypeslib", "path": "ctypeslib/codegen/codegenerator.py", "func_name": "Generator._generate", "original_string": "def _generate(self, item, *args):\n        \"\"\" wraps execution of specific methods.\"\"\"\n        if item in self.done:\n            return\n        # verbose output with location.\n        if self.generate_locations and item.location:\n            print(\"# %s:%d\" % item.location, file=self.stream)\n        if self.generate_comments:\n            self.print_comment(item)\n        log.debug(\"generate %s, %s\", item.__class__.__name__, item.name)\n        #\n        #log.debug('generate: %s( %s )', type(item).__name__, name)\n        #if name in self.known_symbols:\n        #    log.debug('item is in known_symbols %s'% name )\n        #    mod = self.known_symbols[name]\n        #    print >> self.imports, \"from %s import %s\" % (mod, name)\n        #    self.done.add(item)\n        #    if isinstance(item, typedesc.Structure):\n        #        self.done.add(item.get_head())\n        #        self.done.add(item.get_body())\n        #    return\n        #\n        # to avoid infinite recursion, we have to mark it as done\n        # before actually generating the code.\n        self.done.add(item)\n        # go to specific treatment\n        mth = getattr(self, type(item).__name__)\n        mth(item, *args)\n        return", "language": "python", "code": "def _generate(self, item, *args):\n        \"\"\" wraps execution of specific methods.\"\"\"\n        if item in self.done:\n            return\n        # verbose output with location.\n        if self.generate_locations and item.location:\n            print(\"# %s:%d\" % item.location, file=self.stream)\n        if self.generate_comments:\n            self.print_comment(item)\n        log.debug(\"generate %s, %s\", item.__class__.__name__, item.name)\n        #\n        #log.debug('generate: %s( %s )', type(item).__name__, name)\n        #if name in self.known_symbols:\n        #    log.debug('item is in known_symbols %s'% name )\n        #    mod = self.known_symbols[name]\n        #    print >> self.imports, \"from %s import %s\" % (mod, name)\n        #    self.done.add(item)\n        #    if isinstance(item, typedesc.Structure):\n        #        self.done.add(item.get_head())\n        #        self.done.add(item.get_body())\n        #    return\n        #\n        # to avoid infinite recursion, we have to mark it as done\n        # before actually generating the code.\n        self.done.add(item)\n        # go to specific treatment\n        mth = getattr(self, type(item).__name__)\n        mth(item, *args)\n        return", "code_tokens": ["def", "_generate", "(", "self", ",", "item", ",", "*", "args", ")", ":", "if", "item", "in", "self", ".", "done", ":", "return", "# verbose output with location.", "if", "self", ".", "generate_locations", "and", "item", ".", "location", ":", "print", "(", "\"# %s:%d\"", "%", "item", ".", "location", ",", "file", "=", "self", ".", "stream", ")", "if", "self", ".", "generate_comments", ":", "self", ".", "print_comment", "(", "item", ")", "log", ".", "debug", "(", "\"generate %s, %s\"", ",", "item", ".", "__class__", ".", "__name__", ",", "item", ".", "name", ")", "#", "#log.debug('generate: %s( %s )', type(item).__name__, name)", "#if name in self.known_symbols:", "#    log.debug('item is in known_symbols %s'% name )", "#    mod = self.known_symbols[name]", "#    print >> self.imports, \"from %s import %s\" % (mod, name)", "#    self.done.add(item)", "#    if isinstance(item, typedesc.Structure):", "#        self.done.add(item.get_head())", "#        self.done.add(item.get_body())", "#    return", "#", "# to avoid infinite recursion, we have to mark it as done", "# before actually generating the code.", "self", ".", "done", ".", "add", "(", "item", ")", "# go to specific treatment", "mth", "=", "getattr", "(", "self", ",", "type", "(", "item", ")", ".", "__name__", ")", "mth", "(", "item", ",", "*", "args", ")", "return"], "docstring": "wraps execution of specific methods.", "docstring_tokens": ["wraps", "execution", "of", "specific", "methods", "."], "sha": "2aeb1942a5a32a5cc798c287cd0d9e684a0181a8", "url": "https://github.com/trolldbois/ctypeslib/blob/2aeb1942a5a32a5cc798c287cd0d9e684a0181a8/ctypeslib/codegen/codegenerator.py#L738-L766", "partition": "train"}
{"repo": "SHTOOLS/SHTOOLS", "path": "examples/python/ClassInterface/exact_power.py", "func_name": "example", "original_string": "def example():\n    \"\"\"Plot random phase and Gaussian random variable spectra.\"\"\"\n    ldata = 200\n    degrees = np.arange(ldata+1, dtype=float)\n    degrees[0] = np.inf\n    power = degrees**(-1)\n\n    clm1 = pyshtools.SHCoeffs.from_random(power, exact_power=False)\n    clm2 = pyshtools.SHCoeffs.from_random(power, exact_power=True)\n\n    fig, ax = plt.subplots()\n    ax.plot(clm1.spectrum(unit='per_l'), label='Normal distributed power')\n    ax.plot(clm2.spectrum(unit='per_l'), label='Exact power')\n    ax.set(xscale='log', yscale='log', xlabel='degree l',\n           ylabel='power per degree l')\n    ax.grid(which='both')\n    ax.legend()\n\n    plt.show()", "language": "python", "code": "def example():\n    \"\"\"Plot random phase and Gaussian random variable spectra.\"\"\"\n    ldata = 200\n    degrees = np.arange(ldata+1, dtype=float)\n    degrees[0] = np.inf\n    power = degrees**(-1)\n\n    clm1 = pyshtools.SHCoeffs.from_random(power, exact_power=False)\n    clm2 = pyshtools.SHCoeffs.from_random(power, exact_power=True)\n\n    fig, ax = plt.subplots()\n    ax.plot(clm1.spectrum(unit='per_l'), label='Normal distributed power')\n    ax.plot(clm2.spectrum(unit='per_l'), label='Exact power')\n    ax.set(xscale='log', yscale='log', xlabel='degree l',\n           ylabel='power per degree l')\n    ax.grid(which='both')\n    ax.legend()\n\n    plt.show()", "code_tokens": ["def", "example", "(", ")", ":", "ldata", "=", "200", "degrees", "=", "np", ".", "arange", "(", "ldata", "+", "1", ",", "dtype", "=", "float", ")", "degrees", "[", "0", "]", "=", "np", ".", "inf", "power", "=", "degrees", "**", "(", "-", "1", ")", "clm1", "=", "pyshtools", ".", "SHCoeffs", ".", "from_random", "(", "power", ",", "exact_power", "=", "False", ")", "clm2", "=", "pyshtools", ".", "SHCoeffs", ".", "from_random", "(", "power", ",", "exact_power", "=", "True", ")", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "ax", ".", "plot", "(", "clm1", ".", "spectrum", "(", "unit", "=", "'per_l'", ")", ",", "label", "=", "'Normal distributed power'", ")", "ax", ".", "plot", "(", "clm2", ".", "spectrum", "(", "unit", "=", "'per_l'", ")", ",", "label", "=", "'Exact power'", ")", "ax", ".", "set", "(", "xscale", "=", "'log'", ",", "yscale", "=", "'log'", ",", "xlabel", "=", "'degree l'", ",", "ylabel", "=", "'power per degree l'", ")", "ax", ".", "grid", "(", "which", "=", "'both'", ")", "ax", ".", "legend", "(", ")", "plt", ".", "show", "(", ")"], "docstring": "Plot random phase and Gaussian random variable spectra.", "docstring_tokens": ["Plot", "random", "phase", "and", "Gaussian", "random", "variable", "spectra", "."], "sha": "9a115cf83002df2ddec6b7f41aeb6be688e285de", "url": "https://github.com/SHTOOLS/SHTOOLS/blob/9a115cf83002df2ddec6b7f41aeb6be688e285de/examples/python/ClassInterface/exact_power.py#L9-L27", "partition": "train"}
{"repo": "toidi/hadoop-yarn-api-python-client", "path": "yarn_api_client/history_server.py", "func_name": "HistoryServer.task_attempt", "original_string": "def task_attempt(self, job_id, task_id, attempt_id):\n        \"\"\"\n        A Task Attempt resource contains information about a particular task\n        attempt within a job.\n\n        :param str job_id: The job id\n        :param str task_id: The task id\n        :param str attempt_id: The attempt id\n        :returns: API response object with JSON data\n        :rtype: :py:class:`yarn_api_client.base.Response`\n        \"\"\"\n        path = '/ws/v1/history/mapreduce/jobs/{jobid}/tasks/{taskid}/attempts/{attemptid}'.format(\n            jobid=job_id, taskid=task_id, attemptid=attempt_id)\n\n        return self.request(path)", "language": "python", "code": "def task_attempt(self, job_id, task_id, attempt_id):\n        \"\"\"\n        A Task Attempt resource contains information about a particular task\n        attempt within a job.\n\n        :param str job_id: The job id\n        :param str task_id: The task id\n        :param str attempt_id: The attempt id\n        :returns: API response object with JSON data\n        :rtype: :py:class:`yarn_api_client.base.Response`\n        \"\"\"\n        path = '/ws/v1/history/mapreduce/jobs/{jobid}/tasks/{taskid}/attempts/{attemptid}'.format(\n            jobid=job_id, taskid=task_id, attemptid=attempt_id)\n\n        return self.request(path)", "code_tokens": ["def", "task_attempt", "(", "self", ",", "job_id", ",", "task_id", ",", "attempt_id", ")", ":", "path", "=", "'/ws/v1/history/mapreduce/jobs/{jobid}/tasks/{taskid}/attempts/{attemptid}'", ".", "format", "(", "jobid", "=", "job_id", ",", "taskid", "=", "task_id", ",", "attemptid", "=", "attempt_id", ")", "return", "self", ".", "request", "(", "path", ")"], "docstring": "A Task Attempt resource contains information about a particular task\n        attempt within a job.\n\n        :param str job_id: The job id\n        :param str task_id: The task id\n        :param str attempt_id: The attempt id\n        :returns: API response object with JSON data\n        :rtype: :py:class:`yarn_api_client.base.Response`", "docstring_tokens": ["A", "Task", "Attempt", "resource", "contains", "information", "about", "a", "particular", "task", "attempt", "within", "a", "job", "."], "sha": "d245bd41808879be6637acfd7460633c0c7dfdd6", "url": "https://github.com/toidi/hadoop-yarn-api-python-client/blob/d245bd41808879be6637acfd7460633c0c7dfdd6/yarn_api_client/history_server.py#L209-L223", "partition": "train"}
{"repo": "mattiaslinnap/django-partial-index", "path": "partial_index/query.py", "func_name": "q_mentioned_fields", "original_string": "def q_mentioned_fields(q, model):\n    \"\"\"Returns list of field names mentioned in Q object.\n\n    Q(a__isnull=True, b=F('c')) -> ['a', 'b', 'c']\n    \"\"\"\n    query = Query(model)\n    where = query._add_q(q, used_aliases=set(), allow_joins=False)[0]\n    return list(sorted(set(expression_mentioned_fields(where))))", "language": "python", "code": "def q_mentioned_fields(q, model):\n    \"\"\"Returns list of field names mentioned in Q object.\n\n    Q(a__isnull=True, b=F('c')) -> ['a', 'b', 'c']\n    \"\"\"\n    query = Query(model)\n    where = query._add_q(q, used_aliases=set(), allow_joins=False)[0]\n    return list(sorted(set(expression_mentioned_fields(where))))", "code_tokens": ["def", "q_mentioned_fields", "(", "q", ",", "model", ")", ":", "query", "=", "Query", "(", "model", ")", "where", "=", "query", ".", "_add_q", "(", "q", ",", "used_aliases", "=", "set", "(", ")", ",", "allow_joins", "=", "False", ")", "[", "0", "]", "return", "list", "(", "sorted", "(", "set", "(", "expression_mentioned_fields", "(", "where", ")", ")", ")", ")"], "docstring": "Returns list of field names mentioned in Q object.\n\n    Q(a__isnull=True, b=F('c')) -> ['a', 'b', 'c']", "docstring_tokens": ["Returns", "list", "of", "field", "names", "mentioned", "in", "Q", "object", "."], "sha": "6e60fd9484f95499587365fda34a881050bcd804", "url": "https://github.com/mattiaslinnap/django-partial-index/blob/6e60fd9484f95499587365fda34a881050bcd804/partial_index/query.py#L107-L114", "partition": "train"}
{"repo": "martin-majlis/Wikipedia-API", "path": "wikipediaapi/__init__.py", "func_name": "WikipediaPage.text", "original_string": "def text(self) -> str:\n        \"\"\"\n        Returns text of the current page.\n\n        :return: text of the current page\n        \"\"\"\n        txt = self.summary\n        if len(txt) > 0:\n            txt += \"\\n\\n\"\n        for sec in self.sections:\n            txt += sec.full_text(level=2)\n        return txt.strip()", "language": "python", "code": "def text(self) -> str:\n        \"\"\"\n        Returns text of the current page.\n\n        :return: text of the current page\n        \"\"\"\n        txt = self.summary\n        if len(txt) > 0:\n            txt += \"\\n\\n\"\n        for sec in self.sections:\n            txt += sec.full_text(level=2)\n        return txt.strip()", "code_tokens": ["def", "text", "(", "self", ")", "->", "str", ":", "txt", "=", "self", ".", "summary", "if", "len", "(", "txt", ")", ">", "0", ":", "txt", "+=", "\"\\n\\n\"", "for", "sec", "in", "self", ".", "sections", ":", "txt", "+=", "sec", ".", "full_text", "(", "level", "=", "2", ")", "return", "txt", ".", "strip", "(", ")"], "docstring": "Returns text of the current page.\n\n        :return: text of the current page", "docstring_tokens": ["Returns", "text", "of", "the", "current", "page", "."], "sha": "714445857f6bdc3d2c4c8450218841fcc70ce499", "url": "https://github.com/martin-majlis/Wikipedia-API/blob/714445857f6bdc3d2c4c8450218841fcc70ce499/wikipediaapi/__init__.py#L1036-L1047", "partition": "train"}
{"repo": "aio-libs/aiohttp-security", "path": "demo/dictionary_auth/authz.py", "func_name": "DictionaryAuthorizationPolicy.permits", "original_string": "async def permits(self, identity, permission, context=None):\n        \"\"\"Check user permissions.\n        Return True if the identity is allowed the permission in the\n        current context, else return False.\n        \"\"\"\n        # pylint: disable=unused-argument\n        user = self.user_map.get(identity)\n        if not user:\n            return False\n        return permission in user.permissions", "language": "python", "code": "async def permits(self, identity, permission, context=None):\n        \"\"\"Check user permissions.\n        Return True if the identity is allowed the permission in the\n        current context, else return False.\n        \"\"\"\n        # pylint: disable=unused-argument\n        user = self.user_map.get(identity)\n        if not user:\n            return False\n        return permission in user.permissions", "code_tokens": ["async", "def", "permits", "(", "self", ",", "identity", ",", "permission", ",", "context", "=", "None", ")", ":", "# pylint: disable=unused-argument", "user", "=", "self", ".", "user_map", ".", "get", "(", "identity", ")", "if", "not", "user", ":", "return", "False", "return", "permission", "in", "user", ".", "permissions"], "docstring": "Check user permissions.\n        Return True if the identity is allowed the permission in the\n        current context, else return False.", "docstring_tokens": ["Check", "user", "permissions", ".", "Return", "True", "if", "the", "identity", "is", "allowed", "the", "permission", "in", "the", "current", "context", "else", "return", "False", "."], "sha": "901cf1a7e0d884313966d6c569b118a0c6cb12b3", "url": "https://github.com/aio-libs/aiohttp-security/blob/901cf1a7e0d884313966d6c569b118a0c6cb12b3/demo/dictionary_auth/authz.py#L17-L26", "partition": "train"}
{"repo": "louisun/iSearch", "path": "iSearch/isearch.py", "func_name": "count_word", "original_string": "def count_word(arg):\n    '''count the number of words'''\n\n    conn = sqlite3.connect(os.path.join(DEFAULT_PATH, 'word.db'))\n    curs = conn.cursor()\n    if arg[0].isdigit():\n        if len(arg) == 1:\n            curs.execute('SELECT count(*) FROM Word WHERE pr ==  %d' % (int(arg[0])))\n        elif len(arg) == 2 and arg[1] == '+':\n            curs.execute('SELECT count(*) FROM Word WHERE pr >=  %d' % (int(arg[0])))\n        elif len(arg) == 3 and arg[1] == '-':\n            curs.execute('SELECT count(*) FROM Word WHERE pr >=  %d AND pr<=  % d' % (int(arg[0]), int(arg[2])))\n    elif arg[0].isalpha():\n        if arg == 'all':\n            curs.execute('SELECT count(*) FROM Word')\n        elif len(arg) == 1:\n            curs.execute('SELECT count(*) FROM Word WHERE aset == \"%s\"' % arg.upper())\n    res = curs.fetchall()\n    print(res[0][0])\n    curs.close()\n    conn.close()", "language": "python", "code": "def count_word(arg):\n    '''count the number of words'''\n\n    conn = sqlite3.connect(os.path.join(DEFAULT_PATH, 'word.db'))\n    curs = conn.cursor()\n    if arg[0].isdigit():\n        if len(arg) == 1:\n            curs.execute('SELECT count(*) FROM Word WHERE pr ==  %d' % (int(arg[0])))\n        elif len(arg) == 2 and arg[1] == '+':\n            curs.execute('SELECT count(*) FROM Word WHERE pr >=  %d' % (int(arg[0])))\n        elif len(arg) == 3 and arg[1] == '-':\n            curs.execute('SELECT count(*) FROM Word WHERE pr >=  %d AND pr<=  % d' % (int(arg[0]), int(arg[2])))\n    elif arg[0].isalpha():\n        if arg == 'all':\n            curs.execute('SELECT count(*) FROM Word')\n        elif len(arg) == 1:\n            curs.execute('SELECT count(*) FROM Word WHERE aset == \"%s\"' % arg.upper())\n    res = curs.fetchall()\n    print(res[0][0])\n    curs.close()\n    conn.close()", "code_tokens": ["def", "count_word", "(", "arg", ")", ":", "conn", "=", "sqlite3", ".", "connect", "(", "os", ".", "path", ".", "join", "(", "DEFAULT_PATH", ",", "'word.db'", ")", ")", "curs", "=", "conn", ".", "cursor", "(", ")", "if", "arg", "[", "0", "]", ".", "isdigit", "(", ")", ":", "if", "len", "(", "arg", ")", "==", "1", ":", "curs", ".", "execute", "(", "'SELECT count(*) FROM Word WHERE pr ==  %d'", "%", "(", "int", "(", "arg", "[", "0", "]", ")", ")", ")", "elif", "len", "(", "arg", ")", "==", "2", "and", "arg", "[", "1", "]", "==", "'+'", ":", "curs", ".", "execute", "(", "'SELECT count(*) FROM Word WHERE pr >=  %d'", "%", "(", "int", "(", "arg", "[", "0", "]", ")", ")", ")", "elif", "len", "(", "arg", ")", "==", "3", "and", "arg", "[", "1", "]", "==", "'-'", ":", "curs", ".", "execute", "(", "'SELECT count(*) FROM Word WHERE pr >=  %d AND pr<=  % d'", "%", "(", "int", "(", "arg", "[", "0", "]", ")", ",", "int", "(", "arg", "[", "2", "]", ")", ")", ")", "elif", "arg", "[", "0", "]", ".", "isalpha", "(", ")", ":", "if", "arg", "==", "'all'", ":", "curs", ".", "execute", "(", "'SELECT count(*) FROM Word'", ")", "elif", "len", "(", "arg", ")", "==", "1", ":", "curs", ".", "execute", "(", "'SELECT count(*) FROM Word WHERE aset == \"%s\"'", "%", "arg", ".", "upper", "(", ")", ")", "res", "=", "curs", ".", "fetchall", "(", ")", "print", "(", "res", "[", "0", "]", "[", "0", "]", ")", "curs", ".", "close", "(", ")", "conn", ".", "close", "(", ")"], "docstring": "count the number of words", "docstring_tokens": ["count", "the", "number", "of", "words"], "sha": "06013d610338397f8cdd69f330b43e1ee8d29f1b", "url": "https://github.com/louisun/iSearch/blob/06013d610338397f8cdd69f330b43e1ee8d29f1b/iSearch/isearch.py#L467-L487", "partition": "train"}
{"repo": "cenkalti/putio.py", "path": "putiopy.py", "func_name": "strptime", "original_string": "def strptime(date):\n    \"\"\"Returns datetime object from the given date, which is in a specific format: YYYY-MM-ddTHH:mm:ss\"\"\"\n    d = {\n        'year': date[0:4],\n        'month': date[5:7],\n        'day': date[8:10],\n        'hour': date[11:13],\n        'minute': date[14:16],\n        'second': date[17:],\n    }\n\n    d = dict((k, int(v)) for k, v in d.items())\n\n    return datetime(**d)", "language": "python", "code": "def strptime(date):\n    \"\"\"Returns datetime object from the given date, which is in a specific format: YYYY-MM-ddTHH:mm:ss\"\"\"\n    d = {\n        'year': date[0:4],\n        'month': date[5:7],\n        'day': date[8:10],\n        'hour': date[11:13],\n        'minute': date[14:16],\n        'second': date[17:],\n    }\n\n    d = dict((k, int(v)) for k, v in d.items())\n\n    return datetime(**d)", "code_tokens": ["def", "strptime", "(", "date", ")", ":", "d", "=", "{", "'year'", ":", "date", "[", "0", ":", "4", "]", ",", "'month'", ":", "date", "[", "5", ":", "7", "]", ",", "'day'", ":", "date", "[", "8", ":", "10", "]", ",", "'hour'", ":", "date", "[", "11", ":", "13", "]", ",", "'minute'", ":", "date", "[", "14", ":", "16", "]", ",", "'second'", ":", "date", "[", "17", ":", "]", ",", "}", "d", "=", "dict", "(", "(", "k", ",", "int", "(", "v", ")", ")", "for", "k", ",", "v", "in", "d", ".", "items", "(", ")", ")", "return", "datetime", "(", "*", "*", "d", ")"], "docstring": "Returns datetime object from the given date, which is in a specific format: YYYY-MM-ddTHH:mm:ss", "docstring_tokens": ["Returns", "datetime", "object", "from", "the", "given", "date", "which", "is", "in", "a", "specific", "format", ":", "YYYY", "-", "MM", "-", "ddTHH", ":", "mm", ":", "ss"], "sha": "6ffe73002795f7362f54fab059e633c0c2620cfc", "url": "https://github.com/cenkalti/putio.py/blob/6ffe73002795f7362f54fab059e633c0c2620cfc/putiopy.py#L596-L609", "partition": "train"}
{"repo": "PX4/pyulog", "path": "pyulog/px4.py", "func_name": "PX4ULog.get_configured_rc_input_names", "original_string": "def get_configured_rc_input_names(self, channel):\n        \"\"\"\n        find all RC mappings to a given channel and return their names\n\n        :param channel: input channel (0=first)\n        :return: list of strings or None\n        \"\"\"\n        ret_val = []\n        for key in self._ulog.initial_parameters:\n            param_val = self._ulog.initial_parameters[key]\n            if key.startswith('RC_MAP_') and param_val == channel + 1:\n                ret_val.append(key[7:].capitalize())\n\n        if len(ret_val) > 0:\n            return ret_val\n        return None", "language": "python", "code": "def get_configured_rc_input_names(self, channel):\n        \"\"\"\n        find all RC mappings to a given channel and return their names\n\n        :param channel: input channel (0=first)\n        :return: list of strings or None\n        \"\"\"\n        ret_val = []\n        for key in self._ulog.initial_parameters:\n            param_val = self._ulog.initial_parameters[key]\n            if key.startswith('RC_MAP_') and param_val == channel + 1:\n                ret_val.append(key[7:].capitalize())\n\n        if len(ret_val) > 0:\n            return ret_val\n        return None", "code_tokens": ["def", "get_configured_rc_input_names", "(", "self", ",", "channel", ")", ":", "ret_val", "=", "[", "]", "for", "key", "in", "self", ".", "_ulog", ".", "initial_parameters", ":", "param_val", "=", "self", ".", "_ulog", ".", "initial_parameters", "[", "key", "]", "if", "key", ".", "startswith", "(", "'RC_MAP_'", ")", "and", "param_val", "==", "channel", "+", "1", ":", "ret_val", ".", "append", "(", "key", "[", "7", ":", "]", ".", "capitalize", "(", ")", ")", "if", "len", "(", "ret_val", ")", ">", "0", ":", "return", "ret_val", "return", "None"], "docstring": "find all RC mappings to a given channel and return their names\n\n        :param channel: input channel (0=first)\n        :return: list of strings or None", "docstring_tokens": ["find", "all", "RC", "mappings", "to", "a", "given", "channel", "and", "return", "their", "names"], "sha": "3bc4f9338d30e2e0a0dfbed58f54d200967e5056", "url": "https://github.com/PX4/pyulog/blob/3bc4f9338d30e2e0a0dfbed58f54d200967e5056/pyulog/px4.py#L96-L111", "partition": "train"}
{"repo": "hawkowl/towncrier", "path": "src/towncrier/__init__.py", "func_name": "__main", "original_string": "def __main(draft, directory, project_name, project_version, project_date, answer_yes):\n    \"\"\"\n    The main entry point.\n    \"\"\"\n    directory = os.path.abspath(directory)\n    config = load_config(directory)\n    to_err = draft\n\n    click.echo(\"Loading template...\", err=to_err)\n    if config[\"template\"] is None:\n        template = pkg_resources.resource_string(\n            __name__, \"templates/template.rst\"\n        ).decode(\"utf8\")\n    else:\n        with open(config[\"template\"], \"rb\") as tmpl:\n            template = tmpl.read().decode(\"utf8\")\n\n    click.echo(\"Finding news fragments...\", err=to_err)\n\n    definitions = config[\"types\"]\n\n    if config.get(\"directory\"):\n        base_directory = os.path.abspath(config[\"directory\"])\n        fragment_directory = None\n    else:\n        base_directory = os.path.abspath(\n            os.path.join(directory, config[\"package_dir\"], config[\"package\"])\n        )\n        fragment_directory = \"newsfragments\"\n\n    fragments, fragment_filenames = find_fragments(\n        base_directory, config[\"sections\"], fragment_directory, definitions\n    )\n\n    click.echo(\"Rendering news fragments...\", err=to_err)\n    fragments = split_fragments(fragments, definitions)\n    rendered = render_fragments(\n        # The 0th underline is used for the top line\n        template,\n        config[\"issue_format\"],\n        fragments,\n        definitions,\n        config[\"underlines\"][1:],\n        config[\"wrap\"],\n    )\n\n    if project_version is None:\n        project_version = get_version(\n            os.path.join(directory, config[\"package_dir\"]), config[\"package\"]\n        )\n\n    if project_name is None:\n        package = config.get(\"package\")\n        if package:\n            project_name = get_project_name(\n                os.path.abspath(os.path.join(directory, config[\"package_dir\"])), package\n            )\n        else:\n            # Can't determine a project_name, but maybe it is not needed.\n            project_name = \"\"\n\n    if project_date is None:\n        project_date = _get_date()\n\n    top_line = config[\"title_format\"].format(\n        name=project_name, version=project_version, project_date=project_date\n    )\n    top_line += u\"\\n\" + (config[\"underlines\"][0] * len(top_line)) + u\"\\n\"\n\n    if draft:\n        click.echo(\n            \"Draft only -- nothing has been written.\\n\"\n            \"What is seen below is what would be written.\\n\",\n            err=to_err,\n        )\n        click.echo(\"%s\\n%s\" % (top_line, rendered))\n    else:\n        click.echo(\"Writing to newsfile...\", err=to_err)\n        start_line = config[\"start_line\"]\n        append_to_newsfile(\n            directory, config[\"filename\"], start_line, top_line, rendered\n        )\n\n        click.echo(\"Staging newsfile...\", err=to_err)\n        stage_newsfile(directory, config[\"filename\"])\n\n        click.echo(\"Removing news fragments...\", err=to_err)\n        remove_files(fragment_filenames, answer_yes)\n\n        click.echo(\"Done!\", err=to_err)", "language": "python", "code": "def __main(draft, directory, project_name, project_version, project_date, answer_yes):\n    \"\"\"\n    The main entry point.\n    \"\"\"\n    directory = os.path.abspath(directory)\n    config = load_config(directory)\n    to_err = draft\n\n    click.echo(\"Loading template...\", err=to_err)\n    if config[\"template\"] is None:\n        template = pkg_resources.resource_string(\n            __name__, \"templates/template.rst\"\n        ).decode(\"utf8\")\n    else:\n        with open(config[\"template\"], \"rb\") as tmpl:\n            template = tmpl.read().decode(\"utf8\")\n\n    click.echo(\"Finding news fragments...\", err=to_err)\n\n    definitions = config[\"types\"]\n\n    if config.get(\"directory\"):\n        base_directory = os.path.abspath(config[\"directory\"])\n        fragment_directory = None\n    else:\n        base_directory = os.path.abspath(\n            os.path.join(directory, config[\"package_dir\"], config[\"package\"])\n        )\n        fragment_directory = \"newsfragments\"\n\n    fragments, fragment_filenames = find_fragments(\n        base_directory, config[\"sections\"], fragment_directory, definitions\n    )\n\n    click.echo(\"Rendering news fragments...\", err=to_err)\n    fragments = split_fragments(fragments, definitions)\n    rendered = render_fragments(\n        # The 0th underline is used for the top line\n        template,\n        config[\"issue_format\"],\n        fragments,\n        definitions,\n        config[\"underlines\"][1:],\n        config[\"wrap\"],\n    )\n\n    if project_version is None:\n        project_version = get_version(\n            os.path.join(directory, config[\"package_dir\"]), config[\"package\"]\n        )\n\n    if project_name is None:\n        package = config.get(\"package\")\n        if package:\n            project_name = get_project_name(\n                os.path.abspath(os.path.join(directory, config[\"package_dir\"])), package\n            )\n        else:\n            # Can't determine a project_name, but maybe it is not needed.\n            project_name = \"\"\n\n    if project_date is None:\n        project_date = _get_date()\n\n    top_line = config[\"title_format\"].format(\n        name=project_name, version=project_version, project_date=project_date\n    )\n    top_line += u\"\\n\" + (config[\"underlines\"][0] * len(top_line)) + u\"\\n\"\n\n    if draft:\n        click.echo(\n            \"Draft only -- nothing has been written.\\n\"\n            \"What is seen below is what would be written.\\n\",\n            err=to_err,\n        )\n        click.echo(\"%s\\n%s\" % (top_line, rendered))\n    else:\n        click.echo(\"Writing to newsfile...\", err=to_err)\n        start_line = config[\"start_line\"]\n        append_to_newsfile(\n            directory, config[\"filename\"], start_line, top_line, rendered\n        )\n\n        click.echo(\"Staging newsfile...\", err=to_err)\n        stage_newsfile(directory, config[\"filename\"])\n\n        click.echo(\"Removing news fragments...\", err=to_err)\n        remove_files(fragment_filenames, answer_yes)\n\n        click.echo(\"Done!\", err=to_err)", "code_tokens": ["def", "__main", "(", "draft", ",", "directory", ",", "project_name", ",", "project_version", ",", "project_date", ",", "answer_yes", ")", ":", "directory", "=", "os", ".", "path", ".", "abspath", "(", "directory", ")", "config", "=", "load_config", "(", "directory", ")", "to_err", "=", "draft", "click", ".", "echo", "(", "\"Loading template...\"", ",", "err", "=", "to_err", ")", "if", "config", "[", "\"template\"", "]", "is", "None", ":", "template", "=", "pkg_resources", ".", "resource_string", "(", "__name__", ",", "\"templates/template.rst\"", ")", ".", "decode", "(", "\"utf8\"", ")", "else", ":", "with", "open", "(", "config", "[", "\"template\"", "]", ",", "\"rb\"", ")", "as", "tmpl", ":", "template", "=", "tmpl", ".", "read", "(", ")", ".", "decode", "(", "\"utf8\"", ")", "click", ".", "echo", "(", "\"Finding news fragments...\"", ",", "err", "=", "to_err", ")", "definitions", "=", "config", "[", "\"types\"", "]", "if", "config", ".", "get", "(", "\"directory\"", ")", ":", "base_directory", "=", "os", ".", "path", ".", "abspath", "(", "config", "[", "\"directory\"", "]", ")", "fragment_directory", "=", "None", "else", ":", "base_directory", "=", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "join", "(", "directory", ",", "config", "[", "\"package_dir\"", "]", ",", "config", "[", "\"package\"", "]", ")", ")", "fragment_directory", "=", "\"newsfragments\"", "fragments", ",", "fragment_filenames", "=", "find_fragments", "(", "base_directory", ",", "config", "[", "\"sections\"", "]", ",", "fragment_directory", ",", "definitions", ")", "click", ".", "echo", "(", "\"Rendering news fragments...\"", ",", "err", "=", "to_err", ")", "fragments", "=", "split_fragments", "(", "fragments", ",", "definitions", ")", "rendered", "=", "render_fragments", "(", "# The 0th underline is used for the top line", "template", ",", "config", "[", "\"issue_format\"", "]", ",", "fragments", ",", "definitions", ",", "config", "[", "\"underlines\"", "]", "[", "1", ":", "]", ",", "config", "[", "\"wrap\"", "]", ",", ")", "if", "project_version", "is", "None", ":", "project_version", "=", "get_version", "(", "os", ".", "path", ".", "join", "(", "directory", ",", "config", "[", "\"package_dir\"", "]", ")", ",", "config", "[", "\"package\"", "]", ")", "if", "project_name", "is", "None", ":", "package", "=", "config", ".", "get", "(", "\"package\"", ")", "if", "package", ":", "project_name", "=", "get_project_name", "(", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "join", "(", "directory", ",", "config", "[", "\"package_dir\"", "]", ")", ")", ",", "package", ")", "else", ":", "# Can't determine a project_name, but maybe it is not needed.", "project_name", "=", "\"\"", "if", "project_date", "is", "None", ":", "project_date", "=", "_get_date", "(", ")", "top_line", "=", "config", "[", "\"title_format\"", "]", ".", "format", "(", "name", "=", "project_name", ",", "version", "=", "project_version", ",", "project_date", "=", "project_date", ")", "top_line", "+=", "u\"\\n\"", "+", "(", "config", "[", "\"underlines\"", "]", "[", "0", "]", "*", "len", "(", "top_line", ")", ")", "+", "u\"\\n\"", "if", "draft", ":", "click", ".", "echo", "(", "\"Draft only -- nothing has been written.\\n\"", "\"What is seen below is what would be written.\\n\"", ",", "err", "=", "to_err", ",", ")", "click", ".", "echo", "(", "\"%s\\n%s\"", "%", "(", "top_line", ",", "rendered", ")", ")", "else", ":", "click", ".", "echo", "(", "\"Writing to newsfile...\"", ",", "err", "=", "to_err", ")", "start_line", "=", "config", "[", "\"start_line\"", "]", "append_to_newsfile", "(", "directory", ",", "config", "[", "\"filename\"", "]", ",", "start_line", ",", "top_line", ",", "rendered", ")", "click", ".", "echo", "(", "\"Staging newsfile...\"", ",", "err", "=", "to_err", ")", "stage_newsfile", "(", "directory", ",", "config", "[", "\"filename\"", "]", ")", "click", ".", "echo", "(", "\"Removing news fragments...\"", ",", "err", "=", "to_err", ")", "remove_files", "(", "fragment_filenames", ",", "answer_yes", ")", "click", ".", "echo", "(", "\"Done!\"", ",", "err", "=", "to_err", ")"], "docstring": "The main entry point.", "docstring_tokens": ["The", "main", "entry", "point", "."], "sha": "ecd438c9c0ef132a92aba2eecc4dc672ccf9ec63", "url": "https://github.com/hawkowl/towncrier/blob/ecd438c9c0ef132a92aba2eecc4dc672ccf9ec63/src/towncrier/__init__.py#L58-L147", "partition": "train"}
{"repo": "raphaelvallat/pingouin", "path": "pingouin/power.py", "func_name": "power_corr", "original_string": "def power_corr(r=None, n=None, power=None, alpha=0.05, tail='two-sided'):\n    \"\"\"\n    Evaluate power, sample size, correlation coefficient or\n    significance level of a correlation test.\n\n    Parameters\n    ----------\n    r : float\n        Correlation coefficient.\n    n : int\n        Number of observations (sample size).\n    power : float\n        Test power (= 1 - type II error).\n    alpha : float\n        Significance level (type I error probability).\n        The default is 0.05.\n    tail : str\n        Indicates whether the test is \"two-sided\" or \"one-sided\".\n\n    Notes\n    -----\n    Exactly ONE of the parameters ``r``, ``n``, ``power`` and ``alpha`` must\n    be passed as None, and that parameter is determined from the others.\n\n    Notice that ``alpha`` has a default value of 0.05 so None must be\n    explicitly passed if you want to compute it.\n\n    :py:func:`scipy.optimize.brenth` is used to solve power equations for other\n    variables (i.e. sample size, effect size, or significance level). If the\n    solving fails, a nan value is returned.\n\n    This function is a mere Python translation of the original `pwr.r.test`\n    function implemented in the `pwr` R package.\n    All credit goes to the author, Stephane Champely.\n\n    References\n    ----------\n\n    .. [1] Cohen, J. (1988). Statistical power analysis for the behavioral\n           sciences (2nd ed.). Hillsdale,NJ: Lawrence Erlbaum.\n\n    .. [2] https://cran.r-project.org/web/packages/pwr/pwr.pdf\n\n    Examples\n    --------\n    1. Compute achieved power given ``r``, ``n`` and ``alpha``\n\n    >>> from pingouin import power_corr\n    >>> print('power: %.4f' % power_corr(r=0.5, n=20))\n    power: 0.6379\n\n    2. Compute required sample size given ``r``, ``power`` and ``alpha``\n\n    >>> print('n: %.4f' % power_corr(r=0.5, power=0.80,\n    ...                                tail='one-sided'))\n    n: 22.6091\n\n    3. Compute achieved ``r`` given ``n``, ``power`` and ``alpha`` level\n\n    >>> print('r: %.4f' % power_corr(n=20, power=0.80, alpha=0.05))\n    r: 0.5822\n\n    4. Compute achieved alpha level given ``r``, ``n`` and ``power``\n\n    >>> print('alpha: %.4f' % power_corr(r=0.5, n=20, power=0.80,\n    ...                                    alpha=None))\n    alpha: 0.1377\n    \"\"\"\n    # Check the number of arguments that are None\n    n_none = sum([v is None for v in [r, n, power, alpha]])\n    if n_none != 1:\n        raise ValueError('Exactly one of n, r, power, and alpha must be None')\n\n    # Safety checks\n    if r is not None:\n        assert -1 <= r <= 1\n        r = abs(r)\n    if alpha is not None:\n        assert 0 < alpha <= 1\n    if power is not None:\n        assert 0 < power <= 1\n    if n is not None:\n        assert n > 4\n\n    # Define main function\n    if tail == 'two-sided':\n\n        def func(r, n, power, alpha):\n            dof = n - 2\n            ttt = stats.t.ppf(1 - alpha / 2, dof)\n            rc = np.sqrt(ttt**2 / (ttt**2 + dof))\n            zr = np.arctanh(r) + r / (2 * (n - 1))\n            zrc = np.arctanh(rc)\n            power = stats.norm.cdf((zr - zrc) * np.sqrt(n - 3)) + \\\n                stats.norm.cdf((-zr - zrc) * np.sqrt(n - 3))\n            return power\n\n    else:\n\n        def func(r, n, power, alpha):\n            dof = n - 2\n            ttt = stats.t.ppf(1 - alpha, dof)\n            rc = np.sqrt(ttt**2 / (ttt**2 + dof))\n            zr = np.arctanh(r) + r / (2 * (n - 1))\n            zrc = np.arctanh(rc)\n            power = stats.norm.cdf((zr - zrc) * np.sqrt(n - 3))\n            return power\n\n    # Evaluate missing variable\n    if power is None and n is not None and r is not None:\n        # Compute achieved power given r, n and alpha\n        return func(r, n, power=None, alpha=alpha)\n\n    elif n is None and power is not None and r is not None:\n        # Compute required sample size given r, power and alpha\n\n        def _eval_n(n, r, power, alpha):\n            return func(r, n, power, alpha) - power\n\n        try:\n            return brenth(_eval_n, 4 + 1e-10, 1e+09, args=(r, power, alpha))\n        except ValueError:  # pragma: no cover\n            return np.nan\n\n    elif r is None and power is not None and n is not None:\n        # Compute achieved r given sample size, power and alpha level\n\n        def _eval_r(r, n, power, alpha):\n            return func(r, n, power, alpha) - power\n\n        try:\n            return brenth(_eval_r, 1e-10, 1 - 1e-10, args=(n, power, alpha))\n        except ValueError:  # pragma: no cover\n            return np.nan\n\n    else:\n        # Compute achieved alpha (significance) level given r, n and power\n\n        def _eval_alpha(alpha, r, n, power):\n            return func(r, n, power, alpha) - power\n\n        try:\n            return brenth(_eval_alpha, 1e-10, 1 - 1e-10, args=(r, n, power))\n        except ValueError:  # pragma: no cover\n            return np.nan", "language": "python", "code": "def power_corr(r=None, n=None, power=None, alpha=0.05, tail='two-sided'):\n    \"\"\"\n    Evaluate power, sample size, correlation coefficient or\n    significance level of a correlation test.\n\n    Parameters\n    ----------\n    r : float\n        Correlation coefficient.\n    n : int\n        Number of observations (sample size).\n    power : float\n        Test power (= 1 - type II error).\n    alpha : float\n        Significance level (type I error probability).\n        The default is 0.05.\n    tail : str\n        Indicates whether the test is \"two-sided\" or \"one-sided\".\n\n    Notes\n    -----\n    Exactly ONE of the parameters ``r``, ``n``, ``power`` and ``alpha`` must\n    be passed as None, and that parameter is determined from the others.\n\n    Notice that ``alpha`` has a default value of 0.05 so None must be\n    explicitly passed if you want to compute it.\n\n    :py:func:`scipy.optimize.brenth` is used to solve power equations for other\n    variables (i.e. sample size, effect size, or significance level). If the\n    solving fails, a nan value is returned.\n\n    This function is a mere Python translation of the original `pwr.r.test`\n    function implemented in the `pwr` R package.\n    All credit goes to the author, Stephane Champely.\n\n    References\n    ----------\n\n    .. [1] Cohen, J. (1988). Statistical power analysis for the behavioral\n           sciences (2nd ed.). Hillsdale,NJ: Lawrence Erlbaum.\n\n    .. [2] https://cran.r-project.org/web/packages/pwr/pwr.pdf\n\n    Examples\n    --------\n    1. Compute achieved power given ``r``, ``n`` and ``alpha``\n\n    >>> from pingouin import power_corr\n    >>> print('power: %.4f' % power_corr(r=0.5, n=20))\n    power: 0.6379\n\n    2. Compute required sample size given ``r``, ``power`` and ``alpha``\n\n    >>> print('n: %.4f' % power_corr(r=0.5, power=0.80,\n    ...                                tail='one-sided'))\n    n: 22.6091\n\n    3. Compute achieved ``r`` given ``n``, ``power`` and ``alpha`` level\n\n    >>> print('r: %.4f' % power_corr(n=20, power=0.80, alpha=0.05))\n    r: 0.5822\n\n    4. Compute achieved alpha level given ``r``, ``n`` and ``power``\n\n    >>> print('alpha: %.4f' % power_corr(r=0.5, n=20, power=0.80,\n    ...                                    alpha=None))\n    alpha: 0.1377\n    \"\"\"\n    # Check the number of arguments that are None\n    n_none = sum([v is None for v in [r, n, power, alpha]])\n    if n_none != 1:\n        raise ValueError('Exactly one of n, r, power, and alpha must be None')\n\n    # Safety checks\n    if r is not None:\n        assert -1 <= r <= 1\n        r = abs(r)\n    if alpha is not None:\n        assert 0 < alpha <= 1\n    if power is not None:\n        assert 0 < power <= 1\n    if n is not None:\n        assert n > 4\n\n    # Define main function\n    if tail == 'two-sided':\n\n        def func(r, n, power, alpha):\n            dof = n - 2\n            ttt = stats.t.ppf(1 - alpha / 2, dof)\n            rc = np.sqrt(ttt**2 / (ttt**2 + dof))\n            zr = np.arctanh(r) + r / (2 * (n - 1))\n            zrc = np.arctanh(rc)\n            power = stats.norm.cdf((zr - zrc) * np.sqrt(n - 3)) + \\\n                stats.norm.cdf((-zr - zrc) * np.sqrt(n - 3))\n            return power\n\n    else:\n\n        def func(r, n, power, alpha):\n            dof = n - 2\n            ttt = stats.t.ppf(1 - alpha, dof)\n            rc = np.sqrt(ttt**2 / (ttt**2 + dof))\n            zr = np.arctanh(r) + r / (2 * (n - 1))\n            zrc = np.arctanh(rc)\n            power = stats.norm.cdf((zr - zrc) * np.sqrt(n - 3))\n            return power\n\n    # Evaluate missing variable\n    if power is None and n is not None and r is not None:\n        # Compute achieved power given r, n and alpha\n        return func(r, n, power=None, alpha=alpha)\n\n    elif n is None and power is not None and r is not None:\n        # Compute required sample size given r, power and alpha\n\n        def _eval_n(n, r, power, alpha):\n            return func(r, n, power, alpha) - power\n\n        try:\n            return brenth(_eval_n, 4 + 1e-10, 1e+09, args=(r, power, alpha))\n        except ValueError:  # pragma: no cover\n            return np.nan\n\n    elif r is None and power is not None and n is not None:\n        # Compute achieved r given sample size, power and alpha level\n\n        def _eval_r(r, n, power, alpha):\n            return func(r, n, power, alpha) - power\n\n        try:\n            return brenth(_eval_r, 1e-10, 1 - 1e-10, args=(n, power, alpha))\n        except ValueError:  # pragma: no cover\n            return np.nan\n\n    else:\n        # Compute achieved alpha (significance) level given r, n and power\n\n        def _eval_alpha(alpha, r, n, power):\n            return func(r, n, power, alpha) - power\n\n        try:\n            return brenth(_eval_alpha, 1e-10, 1 - 1e-10, args=(r, n, power))\n        except ValueError:  # pragma: no cover\n            return np.nan", "code_tokens": ["def", "power_corr", "(", "r", "=", "None", ",", "n", "=", "None", ",", "power", "=", "None", ",", "alpha", "=", "0.05", ",", "tail", "=", "'two-sided'", ")", ":", "# Check the number of arguments that are None", "n_none", "=", "sum", "(", "[", "v", "is", "None", "for", "v", "in", "[", "r", ",", "n", ",", "power", ",", "alpha", "]", "]", ")", "if", "n_none", "!=", "1", ":", "raise", "ValueError", "(", "'Exactly one of n, r, power, and alpha must be None'", ")", "# Safety checks", "if", "r", "is", "not", "None", ":", "assert", "-", "1", "<=", "r", "<=", "1", "r", "=", "abs", "(", "r", ")", "if", "alpha", "is", "not", "None", ":", "assert", "0", "<", "alpha", "<=", "1", "if", "power", "is", "not", "None", ":", "assert", "0", "<", "power", "<=", "1", "if", "n", "is", "not", "None", ":", "assert", "n", ">", "4", "# Define main function", "if", "tail", "==", "'two-sided'", ":", "def", "func", "(", "r", ",", "n", ",", "power", ",", "alpha", ")", ":", "dof", "=", "n", "-", "2", "ttt", "=", "stats", ".", "t", ".", "ppf", "(", "1", "-", "alpha", "/", "2", ",", "dof", ")", "rc", "=", "np", ".", "sqrt", "(", "ttt", "**", "2", "/", "(", "ttt", "**", "2", "+", "dof", ")", ")", "zr", "=", "np", ".", "arctanh", "(", "r", ")", "+", "r", "/", "(", "2", "*", "(", "n", "-", "1", ")", ")", "zrc", "=", "np", ".", "arctanh", "(", "rc", ")", "power", "=", "stats", ".", "norm", ".", "cdf", "(", "(", "zr", "-", "zrc", ")", "*", "np", ".", "sqrt", "(", "n", "-", "3", ")", ")", "+", "stats", ".", "norm", ".", "cdf", "(", "(", "-", "zr", "-", "zrc", ")", "*", "np", ".", "sqrt", "(", "n", "-", "3", ")", ")", "return", "power", "else", ":", "def", "func", "(", "r", ",", "n", ",", "power", ",", "alpha", ")", ":", "dof", "=", "n", "-", "2", "ttt", "=", "stats", ".", "t", ".", "ppf", "(", "1", "-", "alpha", ",", "dof", ")", "rc", "=", "np", ".", "sqrt", "(", "ttt", "**", "2", "/", "(", "ttt", "**", "2", "+", "dof", ")", ")", "zr", "=", "np", ".", "arctanh", "(", "r", ")", "+", "r", "/", "(", "2", "*", "(", "n", "-", "1", ")", ")", "zrc", "=", "np", ".", "arctanh", "(", "rc", ")", "power", "=", "stats", ".", "norm", ".", "cdf", "(", "(", "zr", "-", "zrc", ")", "*", "np", ".", "sqrt", "(", "n", "-", "3", ")", ")", "return", "power", "# Evaluate missing variable", "if", "power", "is", "None", "and", "n", "is", "not", "None", "and", "r", "is", "not", "None", ":", "# Compute achieved power given r, n and alpha", "return", "func", "(", "r", ",", "n", ",", "power", "=", "None", ",", "alpha", "=", "alpha", ")", "elif", "n", "is", "None", "and", "power", "is", "not", "None", "and", "r", "is", "not", "None", ":", "# Compute required sample size given r, power and alpha", "def", "_eval_n", "(", "n", ",", "r", ",", "power", ",", "alpha", ")", ":", "return", "func", "(", "r", ",", "n", ",", "power", ",", "alpha", ")", "-", "power", "try", ":", "return", "brenth", "(", "_eval_n", ",", "4", "+", "1e-10", ",", "1e+09", ",", "args", "=", "(", "r", ",", "power", ",", "alpha", ")", ")", "except", "ValueError", ":", "# pragma: no cover", "return", "np", ".", "nan", "elif", "r", "is", "None", "and", "power", "is", "not", "None", "and", "n", "is", "not", "None", ":", "# Compute achieved r given sample size, power and alpha level", "def", "_eval_r", "(", "r", ",", "n", ",", "power", ",", "alpha", ")", ":", "return", "func", "(", "r", ",", "n", ",", "power", ",", "alpha", ")", "-", "power", "try", ":", "return", "brenth", "(", "_eval_r", ",", "1e-10", ",", "1", "-", "1e-10", ",", "args", "=", "(", "n", ",", "power", ",", "alpha", ")", ")", "except", "ValueError", ":", "# pragma: no cover", "return", "np", ".", "nan", "else", ":", "# Compute achieved alpha (significance) level given r, n and power", "def", "_eval_alpha", "(", "alpha", ",", "r", ",", "n", ",", "power", ")", ":", "return", "func", "(", "r", ",", "n", ",", "power", ",", "alpha", ")", "-", "power", "try", ":", "return", "brenth", "(", "_eval_alpha", ",", "1e-10", ",", "1", "-", "1e-10", ",", "args", "=", "(", "r", ",", "n", ",", "power", ")", ")", "except", "ValueError", ":", "# pragma: no cover", "return", "np", ".", "nan"], "docstring": "Evaluate power, sample size, correlation coefficient or\n    significance level of a correlation test.\n\n    Parameters\n    ----------\n    r : float\n        Correlation coefficient.\n    n : int\n        Number of observations (sample size).\n    power : float\n        Test power (= 1 - type II error).\n    alpha : float\n        Significance level (type I error probability).\n        The default is 0.05.\n    tail : str\n        Indicates whether the test is \"two-sided\" or \"one-sided\".\n\n    Notes\n    -----\n    Exactly ONE of the parameters ``r``, ``n``, ``power`` and ``alpha`` must\n    be passed as None, and that parameter is determined from the others.\n\n    Notice that ``alpha`` has a default value of 0.05 so None must be\n    explicitly passed if you want to compute it.\n\n    :py:func:`scipy.optimize.brenth` is used to solve power equations for other\n    variables (i.e. sample size, effect size, or significance level). If the\n    solving fails, a nan value is returned.\n\n    This function is a mere Python translation of the original `pwr.r.test`\n    function implemented in the `pwr` R package.\n    All credit goes to the author, Stephane Champely.\n\n    References\n    ----------\n\n    .. [1] Cohen, J. (1988). Statistical power analysis for the behavioral\n           sciences (2nd ed.). Hillsdale,NJ: Lawrence Erlbaum.\n\n    .. [2] https://cran.r-project.org/web/packages/pwr/pwr.pdf\n\n    Examples\n    --------\n    1. Compute achieved power given ``r``, ``n`` and ``alpha``\n\n    >>> from pingouin import power_corr\n    >>> print('power: %.4f' % power_corr(r=0.5, n=20))\n    power: 0.6379\n\n    2. Compute required sample size given ``r``, ``power`` and ``alpha``\n\n    >>> print('n: %.4f' % power_corr(r=0.5, power=0.80,\n    ...                                tail='one-sided'))\n    n: 22.6091\n\n    3. Compute achieved ``r`` given ``n``, ``power`` and ``alpha`` level\n\n    >>> print('r: %.4f' % power_corr(n=20, power=0.80, alpha=0.05))\n    r: 0.5822\n\n    4. Compute achieved alpha level given ``r``, ``n`` and ``power``\n\n    >>> print('alpha: %.4f' % power_corr(r=0.5, n=20, power=0.80,\n    ...                                    alpha=None))\n    alpha: 0.1377", "docstring_tokens": ["Evaluate", "power", "sample", "size", "correlation", "coefficient", "or", "significance", "level", "of", "a", "correlation", "test", "."], "sha": "58b19fa4fffbfe09d58b456e3926a148249e4d9b", "url": "https://github.com/raphaelvallat/pingouin/blob/58b19fa4fffbfe09d58b456e3926a148249e4d9b/pingouin/power.py#L523-L667", "partition": "train"}
{"repo": "prawn-cake/vk-requests", "path": "vk_requests/streaming.py", "func_name": "Stream.consumer", "original_string": "def consumer(self, fn):\n        \"\"\"Consumer decorator\n\n        :param fn: coroutine consumer function\n\n        Example:\n\n        >>> api = StreamingAPI('my_service_key')\n        >>> stream = api.get_stream()\n\n        >>> @stream.consumer\n        >>> @asyncio.coroutine\n        >>> def handle_event(payload):\n        >>>     print(payload)\n\n        \"\"\"\n        if self._consumer_fn is not None:\n            raise ValueError('Consumer function is already defined for this '\n                             'Stream instance')\n        if not any([asyncio.iscoroutine(fn), asyncio.iscoroutinefunction(fn)]):\n            raise ValueError('Consumer function must be a coroutine')\n        self._consumer_fn = fn", "language": "python", "code": "def consumer(self, fn):\n        \"\"\"Consumer decorator\n\n        :param fn: coroutine consumer function\n\n        Example:\n\n        >>> api = StreamingAPI('my_service_key')\n        >>> stream = api.get_stream()\n\n        >>> @stream.consumer\n        >>> @asyncio.coroutine\n        >>> def handle_event(payload):\n        >>>     print(payload)\n\n        \"\"\"\n        if self._consumer_fn is not None:\n            raise ValueError('Consumer function is already defined for this '\n                             'Stream instance')\n        if not any([asyncio.iscoroutine(fn), asyncio.iscoroutinefunction(fn)]):\n            raise ValueError('Consumer function must be a coroutine')\n        self._consumer_fn = fn", "code_tokens": ["def", "consumer", "(", "self", ",", "fn", ")", ":", "if", "self", ".", "_consumer_fn", "is", "not", "None", ":", "raise", "ValueError", "(", "'Consumer function is already defined for this '", "'Stream instance'", ")", "if", "not", "any", "(", "[", "asyncio", ".", "iscoroutine", "(", "fn", ")", ",", "asyncio", ".", "iscoroutinefunction", "(", "fn", ")", "]", ")", ":", "raise", "ValueError", "(", "'Consumer function must be a coroutine'", ")", "self", ".", "_consumer_fn", "=", "fn"], "docstring": "Consumer decorator\n\n        :param fn: coroutine consumer function\n\n        Example:\n\n        >>> api = StreamingAPI('my_service_key')\n        >>> stream = api.get_stream()\n\n        >>> @stream.consumer\n        >>> @asyncio.coroutine\n        >>> def handle_event(payload):\n        >>>     print(payload)", "docstring_tokens": ["Consumer", "decorator"], "sha": "dde01c1ed06f13de912506163a35d8c7e06a8f62", "url": "https://github.com/prawn-cake/vk-requests/blob/dde01c1ed06f13de912506163a35d8c7e06a8f62/vk_requests/streaming.py#L26-L47", "partition": "train"}
{"repo": "jrief/django-sass-processor", "path": "sass_processor/utils.py", "func_name": "get_custom_functions", "original_string": "def get_custom_functions():\n    \"\"\"\n    Return a dict of function names, to be used from inside SASS\n    \"\"\"\n    def get_setting(*args):\n        try:\n            return getattr(settings, args[0])\n        except AttributeError as e:\n            raise TemplateSyntaxError(str(e))\n\n    if hasattr(get_custom_functions, '_custom_functions'):\n        return get_custom_functions._custom_functions\n    get_custom_functions._custom_functions = {sass.SassFunction('get-setting', ('key',), get_setting)}\n    for name, func in getattr(settings, 'SASS_PROCESSOR_CUSTOM_FUNCTIONS', {}).items():\n        try:\n            if isinstance(func, six.string_types):\n                func = import_string(func)\n        except Exception as e:\n            raise TemplateSyntaxError(str(e))\n        else:\n            if not inspect.isfunction(func):\n                raise TemplateSyntaxError(\"{} is not a Python function\".format(func))\n            if six.PY2:\n                func_args = inspect.getargspec(func).args\n            else:\n                func_args = inspect.getfullargspec(func).args\n            sass_func = sass.SassFunction(name, func_args, func)\n            get_custom_functions._custom_functions.add(sass_func)\n    return get_custom_functions._custom_functions", "language": "python", "code": "def get_custom_functions():\n    \"\"\"\n    Return a dict of function names, to be used from inside SASS\n    \"\"\"\n    def get_setting(*args):\n        try:\n            return getattr(settings, args[0])\n        except AttributeError as e:\n            raise TemplateSyntaxError(str(e))\n\n    if hasattr(get_custom_functions, '_custom_functions'):\n        return get_custom_functions._custom_functions\n    get_custom_functions._custom_functions = {sass.SassFunction('get-setting', ('key',), get_setting)}\n    for name, func in getattr(settings, 'SASS_PROCESSOR_CUSTOM_FUNCTIONS', {}).items():\n        try:\n            if isinstance(func, six.string_types):\n                func = import_string(func)\n        except Exception as e:\n            raise TemplateSyntaxError(str(e))\n        else:\n            if not inspect.isfunction(func):\n                raise TemplateSyntaxError(\"{} is not a Python function\".format(func))\n            if six.PY2:\n                func_args = inspect.getargspec(func).args\n            else:\n                func_args = inspect.getfullargspec(func).args\n            sass_func = sass.SassFunction(name, func_args, func)\n            get_custom_functions._custom_functions.add(sass_func)\n    return get_custom_functions._custom_functions", "code_tokens": ["def", "get_custom_functions", "(", ")", ":", "def", "get_setting", "(", "*", "args", ")", ":", "try", ":", "return", "getattr", "(", "settings", ",", "args", "[", "0", "]", ")", "except", "AttributeError", "as", "e", ":", "raise", "TemplateSyntaxError", "(", "str", "(", "e", ")", ")", "if", "hasattr", "(", "get_custom_functions", ",", "'_custom_functions'", ")", ":", "return", "get_custom_functions", ".", "_custom_functions", "get_custom_functions", ".", "_custom_functions", "=", "{", "sass", ".", "SassFunction", "(", "'get-setting'", ",", "(", "'key'", ",", ")", ",", "get_setting", ")", "}", "for", "name", ",", "func", "in", "getattr", "(", "settings", ",", "'SASS_PROCESSOR_CUSTOM_FUNCTIONS'", ",", "{", "}", ")", ".", "items", "(", ")", ":", "try", ":", "if", "isinstance", "(", "func", ",", "six", ".", "string_types", ")", ":", "func", "=", "import_string", "(", "func", ")", "except", "Exception", "as", "e", ":", "raise", "TemplateSyntaxError", "(", "str", "(", "e", ")", ")", "else", ":", "if", "not", "inspect", ".", "isfunction", "(", "func", ")", ":", "raise", "TemplateSyntaxError", "(", "\"{} is not a Python function\"", ".", "format", "(", "func", ")", ")", "if", "six", ".", "PY2", ":", "func_args", "=", "inspect", ".", "getargspec", "(", "func", ")", ".", "args", "else", ":", "func_args", "=", "inspect", ".", "getfullargspec", "(", "func", ")", ".", "args", "sass_func", "=", "sass", ".", "SassFunction", "(", "name", ",", "func_args", ",", "func", ")", "get_custom_functions", ".", "_custom_functions", ".", "add", "(", "sass_func", ")", "return", "get_custom_functions", ".", "_custom_functions"], "docstring": "Return a dict of function names, to be used from inside SASS", "docstring_tokens": ["Return", "a", "dict", "of", "function", "names", "to", "be", "used", "from", "inside", "SASS"], "sha": "3ca746258432b1428daee9a2b2f7e05a1e327492", "url": "https://github.com/jrief/django-sass-processor/blob/3ca746258432b1428daee9a2b2f7e05a1e327492/sass_processor/utils.py#L14-L42", "partition": "train"}
{"repo": "chakki-works/chakin", "path": "chakin/downloader.py", "func_name": "download", "original_string": "def download(number=-1, name=\"\", save_dir='./'):\n    \"\"\"Download pre-trained word vector\n    :param number: integer, default ``None``\n    :param save_dir: str, default './'\n    :return: file path for downloaded file\n    \"\"\"\n    df = load_datasets()\n\n    if number > -1:\n        row = df.iloc[[number]]\n    elif name:\n        row = df.loc[df[\"Name\"] == name]\n\n    url = ''.join(row.URL)\n    if not url:\n        print('The word vector you specified was not found. Please specify correct name.')\n\n    widgets = ['Test: ', Percentage(), ' ', Bar(marker=RotatingMarker()), ' ', ETA(), ' ', FileTransferSpeed()]\n    pbar = ProgressBar(widgets=widgets)\n\n    def dlProgress(count, blockSize, totalSize):\n        if pbar.max_value is None:\n            pbar.max_value = totalSize\n            pbar.start()\n\n        pbar.update(min(count * blockSize, totalSize))\n\n    file_name = url.split('/')[-1]\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, file_name)\n    path, _ = urlretrieve(url, save_path, reporthook=dlProgress)\n    pbar.finish()\n    return path", "language": "python", "code": "def download(number=-1, name=\"\", save_dir='./'):\n    \"\"\"Download pre-trained word vector\n    :param number: integer, default ``None``\n    :param save_dir: str, default './'\n    :return: file path for downloaded file\n    \"\"\"\n    df = load_datasets()\n\n    if number > -1:\n        row = df.iloc[[number]]\n    elif name:\n        row = df.loc[df[\"Name\"] == name]\n\n    url = ''.join(row.URL)\n    if not url:\n        print('The word vector you specified was not found. Please specify correct name.')\n\n    widgets = ['Test: ', Percentage(), ' ', Bar(marker=RotatingMarker()), ' ', ETA(), ' ', FileTransferSpeed()]\n    pbar = ProgressBar(widgets=widgets)\n\n    def dlProgress(count, blockSize, totalSize):\n        if pbar.max_value is None:\n            pbar.max_value = totalSize\n            pbar.start()\n\n        pbar.update(min(count * blockSize, totalSize))\n\n    file_name = url.split('/')[-1]\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, file_name)\n    path, _ = urlretrieve(url, save_path, reporthook=dlProgress)\n    pbar.finish()\n    return path", "code_tokens": ["def", "download", "(", "number", "=", "-", "1", ",", "name", "=", "\"\"", ",", "save_dir", "=", "'./'", ")", ":", "df", "=", "load_datasets", "(", ")", "if", "number", ">", "-", "1", ":", "row", "=", "df", ".", "iloc", "[", "[", "number", "]", "]", "elif", "name", ":", "row", "=", "df", ".", "loc", "[", "df", "[", "\"Name\"", "]", "==", "name", "]", "url", "=", "''", ".", "join", "(", "row", ".", "URL", ")", "if", "not", "url", ":", "print", "(", "'The word vector you specified was not found. Please specify correct name.'", ")", "widgets", "=", "[", "'Test: '", ",", "Percentage", "(", ")", ",", "' '", ",", "Bar", "(", "marker", "=", "RotatingMarker", "(", ")", ")", ",", "' '", ",", "ETA", "(", ")", ",", "' '", ",", "FileTransferSpeed", "(", ")", "]", "pbar", "=", "ProgressBar", "(", "widgets", "=", "widgets", ")", "def", "dlProgress", "(", "count", ",", "blockSize", ",", "totalSize", ")", ":", "if", "pbar", ".", "max_value", "is", "None", ":", "pbar", ".", "max_value", "=", "totalSize", "pbar", ".", "start", "(", ")", "pbar", ".", "update", "(", "min", "(", "count", "*", "blockSize", ",", "totalSize", ")", ")", "file_name", "=", "url", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "if", "not", "os", ".", "path", ".", "exists", "(", "save_dir", ")", ":", "os", ".", "makedirs", "(", "save_dir", ")", "save_path", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "file_name", ")", "path", ",", "_", "=", "urlretrieve", "(", "url", ",", "save_path", ",", "reporthook", "=", "dlProgress", ")", "pbar", ".", "finish", "(", ")", "return", "path"], "docstring": "Download pre-trained word vector\n    :param number: integer, default ``None``\n    :param save_dir: str, default './'\n    :return: file path for downloaded file", "docstring_tokens": ["Download", "pre", "-", "trained", "word", "vector", ":", "param", "number", ":", "integer", "default", "None", ":", "param", "save_dir", ":", "str", "default", ".", "/", ":", "return", ":", "file", "path", "for", "downloaded", "file"], "sha": "8cca89336fbc1e90f95063003b2241c26da0e5bc", "url": "https://github.com/chakki-works/chakin/blob/8cca89336fbc1e90f95063003b2241c26da0e5bc/chakin/downloader.py#L14-L47", "partition": "train"}
{"repo": "chakki-works/chakin", "path": "chakin/downloader.py", "func_name": "search", "original_string": "def search(lang=''):\n    \"\"\"Search pre-trained word vectors by their language\n    :param lang: str, default ''\n    :return: None\n        print search result as pandas DataFrame\n    \"\"\"\n    df = load_datasets()\n    if lang == '':\n        print(df[['Name', 'Dimension', 'Corpus', 'VocabularySize', 'Method', 'Language', 'Author']])\n    else:\n        rows = df[df.Language==lang]\n        print(rows[['Name', 'Dimension', 'Corpus', 'VocabularySize', 'Method', 'Language', 'Author']])", "language": "python", "code": "def search(lang=''):\n    \"\"\"Search pre-trained word vectors by their language\n    :param lang: str, default ''\n    :return: None\n        print search result as pandas DataFrame\n    \"\"\"\n    df = load_datasets()\n    if lang == '':\n        print(df[['Name', 'Dimension', 'Corpus', 'VocabularySize', 'Method', 'Language', 'Author']])\n    else:\n        rows = df[df.Language==lang]\n        print(rows[['Name', 'Dimension', 'Corpus', 'VocabularySize', 'Method', 'Language', 'Author']])", "code_tokens": ["def", "search", "(", "lang", "=", "''", ")", ":", "df", "=", "load_datasets", "(", ")", "if", "lang", "==", "''", ":", "print", "(", "df", "[", "[", "'Name'", ",", "'Dimension'", ",", "'Corpus'", ",", "'VocabularySize'", ",", "'Method'", ",", "'Language'", ",", "'Author'", "]", "]", ")", "else", ":", "rows", "=", "df", "[", "df", ".", "Language", "==", "lang", "]", "print", "(", "rows", "[", "[", "'Name'", ",", "'Dimension'", ",", "'Corpus'", ",", "'VocabularySize'", ",", "'Method'", ",", "'Language'", ",", "'Author'", "]", "]", ")"], "docstring": "Search pre-trained word vectors by their language\n    :param lang: str, default ''\n    :return: None\n        print search result as pandas DataFrame", "docstring_tokens": ["Search", "pre", "-", "trained", "word", "vectors", "by", "their", "language", ":", "param", "lang", ":", "str", "default", ":", "return", ":", "None", "print", "search", "result", "as", "pandas", "DataFrame"], "sha": "8cca89336fbc1e90f95063003b2241c26da0e5bc", "url": "https://github.com/chakki-works/chakin/blob/8cca89336fbc1e90f95063003b2241c26da0e5bc/chakin/downloader.py#L50-L61", "partition": "train"}
{"repo": "Yelp/yelp-python", "path": "yelp/endpoint/business.py", "func_name": "BusinessEndpoints.get_by_id", "original_string": "def get_by_id(self, business_id, **url_params):\n        \"\"\"Make a request to the business details endpoint. More info at\n        https://www.yelp.com/developers/documentation/v3/business\n\n        Args:\n            business_id (str): The business alias (i.e. yelp-san-francisco) or\n                ID (i.e. 4kMBvIEWPxWkWKFN__8SxQ.\n            **url_params: Dict corresponding to business API params\n                https://www.yelp.com/developers/documentation/v3/business\n\n        Returns:\n            yelp.obj.business.Business object that wraps the response.\n\n        \"\"\"\n        business_path = BUSINESS_PATH.format(business_id=business_id)\n        response = self.client._make_request(business_path, url_params=url_params)\n        return Business(response)", "language": "python", "code": "def get_by_id(self, business_id, **url_params):\n        \"\"\"Make a request to the business details endpoint. More info at\n        https://www.yelp.com/developers/documentation/v3/business\n\n        Args:\n            business_id (str): The business alias (i.e. yelp-san-francisco) or\n                ID (i.e. 4kMBvIEWPxWkWKFN__8SxQ.\n            **url_params: Dict corresponding to business API params\n                https://www.yelp.com/developers/documentation/v3/business\n\n        Returns:\n            yelp.obj.business.Business object that wraps the response.\n\n        \"\"\"\n        business_path = BUSINESS_PATH.format(business_id=business_id)\n        response = self.client._make_request(business_path, url_params=url_params)\n        return Business(response)", "code_tokens": ["def", "get_by_id", "(", "self", ",", "business_id", ",", "*", "*", "url_params", ")", ":", "business_path", "=", "BUSINESS_PATH", ".", "format", "(", "business_id", "=", "business_id", ")", "response", "=", "self", ".", "client", ".", "_make_request", "(", "business_path", ",", "url_params", "=", "url_params", ")", "return", "Business", "(", "response", ")"], "docstring": "Make a request to the business details endpoint. More info at\n        https://www.yelp.com/developers/documentation/v3/business\n\n        Args:\n            business_id (str): The business alias (i.e. yelp-san-francisco) or\n                ID (i.e. 4kMBvIEWPxWkWKFN__8SxQ.\n            **url_params: Dict corresponding to business API params\n                https://www.yelp.com/developers/documentation/v3/business\n\n        Returns:\n            yelp.obj.business.Business object that wraps the response.", "docstring_tokens": ["Make", "a", "request", "to", "the", "business", "details", "endpoint", ".", "More", "info", "at", "https", ":", "//", "www", ".", "yelp", ".", "com", "/", "developers", "/", "documentation", "/", "v3", "/", "business"], "sha": "12d611bc2344bbc1c93c83775aa71b7b01b36ad6", "url": "https://github.com/Yelp/yelp-python/blob/12d611bc2344bbc1c93c83775aa71b7b01b36ad6/yelp/endpoint/business.py#L13-L29", "partition": "train"}
{"repo": "Yelp/yelp-python", "path": "yelp/errors.py", "func_name": "YelpError.from_response", "original_string": "def from_response(raw_response):\n        \"\"\"The Yelp Fusion API returns error messages with a json body\n        like:\n        {\n            'error': {\n                'code': 'ALL_CAPS_CODE',\n                'description': 'Human readable description.'\n            }\n        }\n\n        Some errors may have additional fields. For example, a\n        validation error:\n        {\n            'error': {\n                'code': 'VALIDATION_ERROR',\n                'description': \"'en_USS' does not match '^[a-z]{2,3}_[A-Z]{2}$'\",\n                'field': 'locale',\n                'instance': 'en_USS'\n            }\n        }\n        \"\"\"\n        json_response = raw_response.json()\n        error_info = json_response[\"error\"]\n        code = error_info[\"code\"]\n\n        try:\n            error_cls = _error_map[code]\n        except KeyError:\n            raise NotImplementedError(\n                \"Unknown error code '{}' returned in Yelp API response. \"\n                \"This code may have been newly added. Please ensure you are \"\n                \"using the latest version of the yelp-python library, and if \"\n                \"so, create a new issue at https://github.com/Yelp/yelp-python \"\n                \"to add support for this error.\".format(code)\n            )\n        else:\n            return error_cls(raw_response, **error_info)", "language": "python", "code": "def from_response(raw_response):\n        \"\"\"The Yelp Fusion API returns error messages with a json body\n        like:\n        {\n            'error': {\n                'code': 'ALL_CAPS_CODE',\n                'description': 'Human readable description.'\n            }\n        }\n\n        Some errors may have additional fields. For example, a\n        validation error:\n        {\n            'error': {\n                'code': 'VALIDATION_ERROR',\n                'description': \"'en_USS' does not match '^[a-z]{2,3}_[A-Z]{2}$'\",\n                'field': 'locale',\n                'instance': 'en_USS'\n            }\n        }\n        \"\"\"\n        json_response = raw_response.json()\n        error_info = json_response[\"error\"]\n        code = error_info[\"code\"]\n\n        try:\n            error_cls = _error_map[code]\n        except KeyError:\n            raise NotImplementedError(\n                \"Unknown error code '{}' returned in Yelp API response. \"\n                \"This code may have been newly added. Please ensure you are \"\n                \"using the latest version of the yelp-python library, and if \"\n                \"so, create a new issue at https://github.com/Yelp/yelp-python \"\n                \"to add support for this error.\".format(code)\n            )\n        else:\n            return error_cls(raw_response, **error_info)", "code_tokens": ["def", "from_response", "(", "raw_response", ")", ":", "json_response", "=", "raw_response", ".", "json", "(", ")", "error_info", "=", "json_response", "[", "\"error\"", "]", "code", "=", "error_info", "[", "\"code\"", "]", "try", ":", "error_cls", "=", "_error_map", "[", "code", "]", "except", "KeyError", ":", "raise", "NotImplementedError", "(", "\"Unknown error code '{}' returned in Yelp API response. \"", "\"This code may have been newly added. Please ensure you are \"", "\"using the latest version of the yelp-python library, and if \"", "\"so, create a new issue at https://github.com/Yelp/yelp-python \"", "\"to add support for this error.\"", ".", "format", "(", "code", ")", ")", "else", ":", "return", "error_cls", "(", "raw_response", ",", "*", "*", "error_info", ")"], "docstring": "The Yelp Fusion API returns error messages with a json body\n        like:\n        {\n            'error': {\n                'code': 'ALL_CAPS_CODE',\n                'description': 'Human readable description.'\n            }\n        }\n\n        Some errors may have additional fields. For example, a\n        validation error:\n        {\n            'error': {\n                'code': 'VALIDATION_ERROR',\n                'description': \"'en_USS' does not match '^[a-z]{2,3}_[A-Z]{2}$'\",\n                'field': 'locale',\n                'instance': 'en_USS'\n            }\n        }", "docstring_tokens": ["The", "Yelp", "Fusion", "API", "returns", "error", "messages", "with", "a", "json", "body", "like", ":", "{", "error", ":", "{", "code", ":", "ALL_CAPS_CODE", "description", ":", "Human", "readable", "description", ".", "}", "}"], "sha": "12d611bc2344bbc1c93c83775aa71b7b01b36ad6", "url": "https://github.com/Yelp/yelp-python/blob/12d611bc2344bbc1c93c83775aa71b7b01b36ad6/yelp/errors.py#L21-L57", "partition": "train"}
{"repo": "adafruit/Adafruit_CircuitPython_ADS1x15", "path": "adafruit_ads1x15/ads1x15.py", "func_name": "ADS1x15.read", "original_string": "def read(self, pin, is_differential=False):\n        \"\"\"I2C Interface for ADS1x15-based ADCs reads.\n\n        params:\n            :param pin: individual or differential pin.\n            :param bool is_differential: single-ended or differential read.\n        \"\"\"\n        pin = pin if is_differential else pin + 0x04\n        return self._read(pin)", "language": "python", "code": "def read(self, pin, is_differential=False):\n        \"\"\"I2C Interface for ADS1x15-based ADCs reads.\n\n        params:\n            :param pin: individual or differential pin.\n            :param bool is_differential: single-ended or differential read.\n        \"\"\"\n        pin = pin if is_differential else pin + 0x04\n        return self._read(pin)", "code_tokens": ["def", "read", "(", "self", ",", "pin", ",", "is_differential", "=", "False", ")", ":", "pin", "=", "pin", "if", "is_differential", "else", "pin", "+", "0x04", "return", "self", ".", "_read", "(", "pin", ")"], "docstring": "I2C Interface for ADS1x15-based ADCs reads.\n\n        params:\n            :param pin: individual or differential pin.\n            :param bool is_differential: single-ended or differential read.", "docstring_tokens": ["I2C", "Interface", "for", "ADS1x15", "-", "based", "ADCs", "reads", "."], "sha": "5ba760c6de40824386f1df343603eab77d3e336c", "url": "https://github.com/adafruit/Adafruit_CircuitPython_ADS1x15/blob/5ba760c6de40824386f1df343603eab77d3e336c/adafruit_ads1x15/ads1x15.py#L129-L137", "partition": "train"}
{"repo": "sixpack/sixpack", "path": "sixpack/db.py", "func_name": "sequential_id", "original_string": "def sequential_id(k, identifier):\n    \"\"\"Map an arbitrary string identifier to a set of sequential ids\"\"\"\n    key = _key(k)\n    return int(monotonic_zadd(keys=[key], args=[identifier]))", "language": "python", "code": "def sequential_id(k, identifier):\n    \"\"\"Map an arbitrary string identifier to a set of sequential ids\"\"\"\n    key = _key(k)\n    return int(monotonic_zadd(keys=[key], args=[identifier]))", "code_tokens": ["def", "sequential_id", "(", "k", ",", "identifier", ")", ":", "key", "=", "_key", "(", "k", ")", "return", "int", "(", "monotonic_zadd", "(", "keys", "=", "[", "key", "]", ",", "args", "=", "[", "identifier", "]", ")", ")"], "docstring": "Map an arbitrary string identifier to a set of sequential ids", "docstring_tokens": ["Map", "an", "arbitrary", "string", "identifier", "to", "a", "set", "of", "sequential", "ids"], "sha": "fec044a35eea79dd7b9af73fafe1b7d15f1d9ef8", "url": "https://github.com/sixpack/sixpack/blob/fec044a35eea79dd7b9af73fafe1b7d15f1d9ef8/sixpack/db.py#L46-L49", "partition": "train"}
{"repo": "wandb/client", "path": "wandb/vendor/prompt_toolkit/key_binding/bindings/emacs.py", "func_name": "load_emacs_bindings", "original_string": "def load_emacs_bindings():\n    \"\"\"\n    Some e-macs extensions.\n    \"\"\"\n    # Overview of Readline emacs commands:\n    # http://www.catonmat.net/download/readline-emacs-editing-mode-cheat-sheet.pdf\n    registry = ConditionalRegistry(Registry(), EmacsMode())\n    handle = registry.add_binding\n\n    insert_mode = EmacsInsertMode()\n    has_selection = HasSelection()\n\n    @handle(Keys.Escape)\n    def _(event):\n        \"\"\"\n        By default, ignore escape key.\n\n        (If we don't put this here, and Esc is followed by a key which sequence\n        is not handled, we'll insert an Escape character in the input stream.\n        Something we don't want and happens to easily in emacs mode.\n        Further, people can always use ControlQ to do a quoted insert.)\n        \"\"\"\n        pass\n\n    handle(Keys.ControlA)(get_by_name('beginning-of-line'))\n    handle(Keys.ControlB)(get_by_name('backward-char'))\n    handle(Keys.ControlDelete, filter=insert_mode)(get_by_name('kill-word'))\n    handle(Keys.ControlE)(get_by_name('end-of-line'))\n    handle(Keys.ControlF)(get_by_name('forward-char'))\n    handle(Keys.ControlLeft)(get_by_name('backward-word'))\n    handle(Keys.ControlRight)(get_by_name('forward-word'))\n    handle(Keys.ControlX, 'r', 'y', filter=insert_mode)(get_by_name('yank'))\n    handle(Keys.ControlY, filter=insert_mode)(get_by_name('yank'))\n    handle(Keys.Escape, 'b')(get_by_name('backward-word'))\n    handle(Keys.Escape, 'c', filter=insert_mode)(get_by_name('capitalize-word'))\n    handle(Keys.Escape, 'd', filter=insert_mode)(get_by_name('kill-word'))\n    handle(Keys.Escape, 'f')(get_by_name('forward-word'))\n    handle(Keys.Escape, 'l', filter=insert_mode)(get_by_name('downcase-word'))\n    handle(Keys.Escape, 'u', filter=insert_mode)(get_by_name('uppercase-word'))\n    handle(Keys.Escape, 'y', filter=insert_mode)(get_by_name('yank-pop'))\n    handle(Keys.Escape, Keys.ControlH, filter=insert_mode)(get_by_name('backward-kill-word'))\n    handle(Keys.Escape, Keys.Backspace, filter=insert_mode)(get_by_name('backward-kill-word'))\n    handle(Keys.Escape, '\\\\', filter=insert_mode)(get_by_name('delete-horizontal-space'))\n\n    handle(Keys.ControlUnderscore, save_before=(lambda e: False), filter=insert_mode)(\n        get_by_name('undo'))\n\n    handle(Keys.ControlX, Keys.ControlU, save_before=(lambda e: False), filter=insert_mode)(\n        get_by_name('undo'))\n\n\n    handle(Keys.Escape, '<', filter= ~has_selection)(get_by_name('beginning-of-history'))\n    handle(Keys.Escape, '>', filter= ~has_selection)(get_by_name('end-of-history'))\n\n    handle(Keys.Escape, '.', filter=insert_mode)(get_by_name('yank-last-arg'))\n    handle(Keys.Escape, '_', filter=insert_mode)(get_by_name('yank-last-arg'))\n    handle(Keys.Escape, Keys.ControlY, filter=insert_mode)(get_by_name('yank-nth-arg'))\n    handle(Keys.Escape, '#', filter=insert_mode)(get_by_name('insert-comment'))\n    handle(Keys.ControlO)(get_by_name('operate-and-get-next'))\n\n    # ControlQ does a quoted insert. Not that for vt100 terminals, you have to\n    # disable flow control by running ``stty -ixon``, otherwise Ctrl-Q and\n    # Ctrl-S are captured by the terminal.\n    handle(Keys.ControlQ, filter= ~has_selection)(get_by_name('quoted-insert'))\n\n    handle(Keys.ControlX, '(')(get_by_name('start-kbd-macro'))\n    handle(Keys.ControlX, ')')(get_by_name('end-kbd-macro'))\n    handle(Keys.ControlX, 'e')(get_by_name('call-last-kbd-macro'))\n\n    @handle(Keys.ControlN)\n    def _(event):\n        \" Next line. \"\n        event.current_buffer.auto_down()\n\n    @handle(Keys.ControlP)\n    def _(event):\n        \" Previous line. \"\n        event.current_buffer.auto_up(count=event.arg)\n\n    def handle_digit(c):\n        \"\"\"\n        Handle input of arguments.\n        The first number needs to be preceeded by escape.\n        \"\"\"\n        @handle(c, filter=HasArg())\n        @handle(Keys.Escape, c)\n        def _(event):\n            event.append_to_arg_count(c)\n\n    for c in '0123456789':\n        handle_digit(c)\n\n    @handle(Keys.Escape, '-', filter=~HasArg())\n    def _(event):\n        \"\"\"\n        \"\"\"\n        if event._arg is None:\n            event.append_to_arg_count('-')\n\n    @handle('-', filter=Condition(lambda cli: cli.input_processor.arg == '-'))\n    def _(event):\n        \"\"\"\n        When '-' is typed again, after exactly '-' has been given as an\n        argument, ignore this.\n        \"\"\"\n        event.cli.input_processor.arg = '-'\n\n    is_returnable = Condition(\n        lambda cli: cli.current_buffer.accept_action.is_returnable)\n\n    # Meta + Newline: always accept input.\n    handle(Keys.Escape, Keys.ControlJ, filter=insert_mode & is_returnable)(\n        get_by_name('accept-line'))\n\n    def character_search(buff, char, count):\n        if count < 0:\n            match = buff.document.find_backwards(char, in_current_line=True, count=-count)\n        else:\n            match = buff.document.find(char, in_current_line=True, count=count)\n\n        if match is not None:\n            buff.cursor_position += match\n\n    @handle(Keys.ControlSquareClose, Keys.Any)\n    def _(event):\n        \" When Ctl-] + a character is pressed. go to that character. \"\n        # Also named 'character-search'\n        character_search(event.current_buffer, event.data, event.arg)\n\n    @handle(Keys.Escape, Keys.ControlSquareClose, Keys.Any)\n    def _(event):\n        \" Like Ctl-], but backwards. \"\n        # Also named 'character-search-backward'\n        character_search(event.current_buffer, event.data, -event.arg)\n\n    @handle(Keys.Escape, 'a')\n    def _(event):\n        \" Previous sentence. \"\n        # TODO:\n\n    @handle(Keys.Escape, 'e')\n    def _(event):\n        \" Move to end of sentence. \"\n        # TODO:\n\n    @handle(Keys.Escape, 't', filter=insert_mode)\n    def _(event):\n        \"\"\"\n        Swap the last two words before the cursor.\n        \"\"\"\n        # TODO\n\n    @handle(Keys.Escape, '*', filter=insert_mode)\n    def _(event):\n        \"\"\"\n        `meta-*`: Insert all possible completions of the preceding text.\n        \"\"\"\n        buff = event.current_buffer\n\n        # List all completions.\n        complete_event = CompleteEvent(text_inserted=False, completion_requested=True)\n        completions = list(buff.completer.get_completions(buff.document, complete_event))\n\n        # Insert them.\n        text_to_insert = ' '.join(c.text for c in completions)\n        buff.insert_text(text_to_insert)\n\n    @handle(Keys.ControlX, Keys.ControlX)\n    def _(event):\n        \"\"\"\n        Move cursor back and forth between the start and end of the current\n        line.\n        \"\"\"\n        buffer = event.current_buffer\n\n        if buffer.document.is_cursor_at_the_end_of_line:\n            buffer.cursor_position += buffer.document.get_start_of_line_position(after_whitespace=False)\n        else:\n            buffer.cursor_position += buffer.document.get_end_of_line_position()\n\n    @handle(Keys.ControlSpace)\n    def _(event):\n        \"\"\"\n        Start of the selection (if the current buffer is not empty).\n        \"\"\"\n        # Take the current cursor position as the start of this selection.\n        buff = event.current_buffer\n        if buff.text:\n            buff.start_selection(selection_type=SelectionType.CHARACTERS)\n\n    @handle(Keys.ControlG, filter= ~has_selection)\n    def _(event):\n        \"\"\"\n        Control + G: Cancel completion menu and validation state.\n        \"\"\"\n        event.current_buffer.complete_state = None\n        event.current_buffer.validation_error = None\n\n    @handle(Keys.ControlG, filter=has_selection)\n    def _(event):\n        \"\"\"\n        Cancel selection.\n        \"\"\"\n        event.current_buffer.exit_selection()\n\n    @handle(Keys.ControlW, filter=has_selection)\n    @handle(Keys.ControlX, 'r', 'k', filter=has_selection)\n    def _(event):\n        \"\"\"\n        Cut selected text.\n        \"\"\"\n        data = event.current_buffer.cut_selection()\n        event.cli.clipboard.set_data(data)\n\n    @handle(Keys.Escape, 'w', filter=has_selection)\n    def _(event):\n        \"\"\"\n        Copy selected text.\n        \"\"\"\n        data = event.current_buffer.copy_selection()\n        event.cli.clipboard.set_data(data)\n\n    @handle(Keys.Escape, Keys.Left)\n    def _(event):\n        \"\"\"\n        Cursor to start of previous word.\n        \"\"\"\n        buffer = event.current_buffer\n        buffer.cursor_position += buffer.document.find_previous_word_beginning(count=event.arg) or 0\n\n    @handle(Keys.Escape, Keys.Right)\n    def _(event):\n        \"\"\"\n        Cursor to start of next word.\n        \"\"\"\n        buffer = event.current_buffer\n        buffer.cursor_position += buffer.document.find_next_word_beginning(count=event.arg) or \\\n            buffer.document.get_end_of_document_position()\n\n    @handle(Keys.Escape, '/', filter=insert_mode)\n    def _(event):\n        \"\"\"\n        M-/: Complete.\n        \"\"\"\n        b = event.current_buffer\n        if b.complete_state:\n            b.complete_next()\n        else:\n            event.cli.start_completion(select_first=True)\n\n    @handle(Keys.ControlC, '>', filter=has_selection)\n    def _(event):\n        \"\"\"\n        Indent selected text.\n        \"\"\"\n        buffer = event.current_buffer\n\n        buffer.cursor_position += buffer.document.get_start_of_line_position(after_whitespace=True)\n\n        from_, to = buffer.document.selection_range()\n        from_, _ = buffer.document.translate_index_to_position(from_)\n        to, _ = buffer.document.translate_index_to_position(to)\n\n        indent(buffer, from_, to + 1, count=event.arg)\n\n    @handle(Keys.ControlC, '<', filter=has_selection)\n    def _(event):\n        \"\"\"\n        Unindent selected text.\n        \"\"\"\n        buffer = event.current_buffer\n\n        from_, to = buffer.document.selection_range()\n        from_, _ = buffer.document.translate_index_to_position(from_)\n        to, _ = buffer.document.translate_index_to_position(to)\n\n        unindent(buffer, from_, to + 1, count=event.arg)\n\n    return registry", "language": "python", "code": "def load_emacs_bindings():\n    \"\"\"\n    Some e-macs extensions.\n    \"\"\"\n    # Overview of Readline emacs commands:\n    # http://www.catonmat.net/download/readline-emacs-editing-mode-cheat-sheet.pdf\n    registry = ConditionalRegistry(Registry(), EmacsMode())\n    handle = registry.add_binding\n\n    insert_mode = EmacsInsertMode()\n    has_selection = HasSelection()\n\n    @handle(Keys.Escape)\n    def _(event):\n        \"\"\"\n        By default, ignore escape key.\n\n        (If we don't put this here, and Esc is followed by a key which sequence\n        is not handled, we'll insert an Escape character in the input stream.\n        Something we don't want and happens to easily in emacs mode.\n        Further, people can always use ControlQ to do a quoted insert.)\n        \"\"\"\n        pass\n\n    handle(Keys.ControlA)(get_by_name('beginning-of-line'))\n    handle(Keys.ControlB)(get_by_name('backward-char'))\n    handle(Keys.ControlDelete, filter=insert_mode)(get_by_name('kill-word'))\n    handle(Keys.ControlE)(get_by_name('end-of-line'))\n    handle(Keys.ControlF)(get_by_name('forward-char'))\n    handle(Keys.ControlLeft)(get_by_name('backward-word'))\n    handle(Keys.ControlRight)(get_by_name('forward-word'))\n    handle(Keys.ControlX, 'r', 'y', filter=insert_mode)(get_by_name('yank'))\n    handle(Keys.ControlY, filter=insert_mode)(get_by_name('yank'))\n    handle(Keys.Escape, 'b')(get_by_name('backward-word'))\n    handle(Keys.Escape, 'c', filter=insert_mode)(get_by_name('capitalize-word'))\n    handle(Keys.Escape, 'd', filter=insert_mode)(get_by_name('kill-word'))\n    handle(Keys.Escape, 'f')(get_by_name('forward-word'))\n    handle(Keys.Escape, 'l', filter=insert_mode)(get_by_name('downcase-word'))\n    handle(Keys.Escape, 'u', filter=insert_mode)(get_by_name('uppercase-word'))\n    handle(Keys.Escape, 'y', filter=insert_mode)(get_by_name('yank-pop'))\n    handle(Keys.Escape, Keys.ControlH, filter=insert_mode)(get_by_name('backward-kill-word'))\n    handle(Keys.Escape, Keys.Backspace, filter=insert_mode)(get_by_name('backward-kill-word'))\n    handle(Keys.Escape, '\\\\', filter=insert_mode)(get_by_name('delete-horizontal-space'))\n\n    handle(Keys.ControlUnderscore, save_before=(lambda e: False), filter=insert_mode)(\n        get_by_name('undo'))\n\n    handle(Keys.ControlX, Keys.ControlU, save_before=(lambda e: False), filter=insert_mode)(\n        get_by_name('undo'))\n\n\n    handle(Keys.Escape, '<', filter= ~has_selection)(get_by_name('beginning-of-history'))\n    handle(Keys.Escape, '>', filter= ~has_selection)(get_by_name('end-of-history'))\n\n    handle(Keys.Escape, '.', filter=insert_mode)(get_by_name('yank-last-arg'))\n    handle(Keys.Escape, '_', filter=insert_mode)(get_by_name('yank-last-arg'))\n    handle(Keys.Escape, Keys.ControlY, filter=insert_mode)(get_by_name('yank-nth-arg'))\n    handle(Keys.Escape, '#', filter=insert_mode)(get_by_name('insert-comment'))\n    handle(Keys.ControlO)(get_by_name('operate-and-get-next'))\n\n    # ControlQ does a quoted insert. Not that for vt100 terminals, you have to\n    # disable flow control by running ``stty -ixon``, otherwise Ctrl-Q and\n    # Ctrl-S are captured by the terminal.\n    handle(Keys.ControlQ, filter= ~has_selection)(get_by_name('quoted-insert'))\n\n    handle(Keys.ControlX, '(')(get_by_name('start-kbd-macro'))\n    handle(Keys.ControlX, ')')(get_by_name('end-kbd-macro'))\n    handle(Keys.ControlX, 'e')(get_by_name('call-last-kbd-macro'))\n\n    @handle(Keys.ControlN)\n    def _(event):\n        \" Next line. \"\n        event.current_buffer.auto_down()\n\n    @handle(Keys.ControlP)\n    def _(event):\n        \" Previous line. \"\n        event.current_buffer.auto_up(count=event.arg)\n\n    def handle_digit(c):\n        \"\"\"\n        Handle input of arguments.\n        The first number needs to be preceeded by escape.\n        \"\"\"\n        @handle(c, filter=HasArg())\n        @handle(Keys.Escape, c)\n        def _(event):\n            event.append_to_arg_count(c)\n\n    for c in '0123456789':\n        handle_digit(c)\n\n    @handle(Keys.Escape, '-', filter=~HasArg())\n    def _(event):\n        \"\"\"\n        \"\"\"\n        if event._arg is None:\n            event.append_to_arg_count('-')\n\n    @handle('-', filter=Condition(lambda cli: cli.input_processor.arg == '-'))\n    def _(event):\n        \"\"\"\n        When '-' is typed again, after exactly '-' has been given as an\n        argument, ignore this.\n        \"\"\"\n        event.cli.input_processor.arg = '-'\n\n    is_returnable = Condition(\n        lambda cli: cli.current_buffer.accept_action.is_returnable)\n\n    # Meta + Newline: always accept input.\n    handle(Keys.Escape, Keys.ControlJ, filter=insert_mode & is_returnable)(\n        get_by_name('accept-line'))\n\n    def character_search(buff, char, count):\n        if count < 0:\n            match = buff.document.find_backwards(char, in_current_line=True, count=-count)\n        else:\n            match = buff.document.find(char, in_current_line=True, count=count)\n\n        if match is not None:\n            buff.cursor_position += match\n\n    @handle(Keys.ControlSquareClose, Keys.Any)\n    def _(event):\n        \" When Ctl-] + a character is pressed. go to that character. \"\n        # Also named 'character-search'\n        character_search(event.current_buffer, event.data, event.arg)\n\n    @handle(Keys.Escape, Keys.ControlSquareClose, Keys.Any)\n    def _(event):\n        \" Like Ctl-], but backwards. \"\n        # Also named 'character-search-backward'\n        character_search(event.current_buffer, event.data, -event.arg)\n\n    @handle(Keys.Escape, 'a')\n    def _(event):\n        \" Previous sentence. \"\n        # TODO:\n\n    @handle(Keys.Escape, 'e')\n    def _(event):\n        \" Move to end of sentence. \"\n        # TODO:\n\n    @handle(Keys.Escape, 't', filter=insert_mode)\n    def _(event):\n        \"\"\"\n        Swap the last two words before the cursor.\n        \"\"\"\n        # TODO\n\n    @handle(Keys.Escape, '*', filter=insert_mode)\n    def _(event):\n        \"\"\"\n        `meta-*`: Insert all possible completions of the preceding text.\n        \"\"\"\n        buff = event.current_buffer\n\n        # List all completions.\n        complete_event = CompleteEvent(text_inserted=False, completion_requested=True)\n        completions = list(buff.completer.get_completions(buff.document, complete_event))\n\n        # Insert them.\n        text_to_insert = ' '.join(c.text for c in completions)\n        buff.insert_text(text_to_insert)\n\n    @handle(Keys.ControlX, Keys.ControlX)\n    def _(event):\n        \"\"\"\n        Move cursor back and forth between the start and end of the current\n        line.\n        \"\"\"\n        buffer = event.current_buffer\n\n        if buffer.document.is_cursor_at_the_end_of_line:\n            buffer.cursor_position += buffer.document.get_start_of_line_position(after_whitespace=False)\n        else:\n            buffer.cursor_position += buffer.document.get_end_of_line_position()\n\n    @handle(Keys.ControlSpace)\n    def _(event):\n        \"\"\"\n        Start of the selection (if the current buffer is not empty).\n        \"\"\"\n        # Take the current cursor position as the start of this selection.\n        buff = event.current_buffer\n        if buff.text:\n            buff.start_selection(selection_type=SelectionType.CHARACTERS)\n\n    @handle(Keys.ControlG, filter= ~has_selection)\n    def _(event):\n        \"\"\"\n        Control + G: Cancel completion menu and validation state.\n        \"\"\"\n        event.current_buffer.complete_state = None\n        event.current_buffer.validation_error = None\n\n    @handle(Keys.ControlG, filter=has_selection)\n    def _(event):\n        \"\"\"\n        Cancel selection.\n        \"\"\"\n        event.current_buffer.exit_selection()\n\n    @handle(Keys.ControlW, filter=has_selection)\n    @handle(Keys.ControlX, 'r', 'k', filter=has_selection)\n    def _(event):\n        \"\"\"\n        Cut selected text.\n        \"\"\"\n        data = event.current_buffer.cut_selection()\n        event.cli.clipboard.set_data(data)\n\n    @handle(Keys.Escape, 'w', filter=has_selection)\n    def _(event):\n        \"\"\"\n        Copy selected text.\n        \"\"\"\n        data = event.current_buffer.copy_selection()\n        event.cli.clipboard.set_data(data)\n\n    @handle(Keys.Escape, Keys.Left)\n    def _(event):\n        \"\"\"\n        Cursor to start of previous word.\n        \"\"\"\n        buffer = event.current_buffer\n        buffer.cursor_position += buffer.document.find_previous_word_beginning(count=event.arg) or 0\n\n    @handle(Keys.Escape, Keys.Right)\n    def _(event):\n        \"\"\"\n        Cursor to start of next word.\n        \"\"\"\n        buffer = event.current_buffer\n        buffer.cursor_position += buffer.document.find_next_word_beginning(count=event.arg) or \\\n            buffer.document.get_end_of_document_position()\n\n    @handle(Keys.Escape, '/', filter=insert_mode)\n    def _(event):\n        \"\"\"\n        M-/: Complete.\n        \"\"\"\n        b = event.current_buffer\n        if b.complete_state:\n            b.complete_next()\n        else:\n            event.cli.start_completion(select_first=True)\n\n    @handle(Keys.ControlC, '>', filter=has_selection)\n    def _(event):\n        \"\"\"\n        Indent selected text.\n        \"\"\"\n        buffer = event.current_buffer\n\n        buffer.cursor_position += buffer.document.get_start_of_line_position(after_whitespace=True)\n\n        from_, to = buffer.document.selection_range()\n        from_, _ = buffer.document.translate_index_to_position(from_)\n        to, _ = buffer.document.translate_index_to_position(to)\n\n        indent(buffer, from_, to + 1, count=event.arg)\n\n    @handle(Keys.ControlC, '<', filter=has_selection)\n    def _(event):\n        \"\"\"\n        Unindent selected text.\n        \"\"\"\n        buffer = event.current_buffer\n\n        from_, to = buffer.document.selection_range()\n        from_, _ = buffer.document.translate_index_to_position(from_)\n        to, _ = buffer.document.translate_index_to_position(to)\n\n        unindent(buffer, from_, to + 1, count=event.arg)\n\n    return registry", "code_tokens": ["def", "load_emacs_bindings", "(", ")", ":", "# Overview of Readline emacs commands:", "# http://www.catonmat.net/download/readline-emacs-editing-mode-cheat-sheet.pdf", "registry", "=", "ConditionalRegistry", "(", "Registry", "(", ")", ",", "EmacsMode", "(", ")", ")", "handle", "=", "registry", ".", "add_binding", "insert_mode", "=", "EmacsInsertMode", "(", ")", "has_selection", "=", "HasSelection", "(", ")", "@", "handle", "(", "Keys", ".", "Escape", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        By default, ignore escape key.\n\n        (If we don't put this here, and Esc is followed by a key which sequence\n        is not handled, we'll insert an Escape character in the input stream.\n        Something we don't want and happens to easily in emacs mode.\n        Further, people can always use ControlQ to do a quoted insert.)\n        \"\"\"", "pass", "handle", "(", "Keys", ".", "ControlA", ")", "(", "get_by_name", "(", "'beginning-of-line'", ")", ")", "handle", "(", "Keys", ".", "ControlB", ")", "(", "get_by_name", "(", "'backward-char'", ")", ")", "handle", "(", "Keys", ".", "ControlDelete", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'kill-word'", ")", ")", "handle", "(", "Keys", ".", "ControlE", ")", "(", "get_by_name", "(", "'end-of-line'", ")", ")", "handle", "(", "Keys", ".", "ControlF", ")", "(", "get_by_name", "(", "'forward-char'", ")", ")", "handle", "(", "Keys", ".", "ControlLeft", ")", "(", "get_by_name", "(", "'backward-word'", ")", ")", "handle", "(", "Keys", ".", "ControlRight", ")", "(", "get_by_name", "(", "'forward-word'", ")", ")", "handle", "(", "Keys", ".", "ControlX", ",", "'r'", ",", "'y'", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'yank'", ")", ")", "handle", "(", "Keys", ".", "ControlY", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'yank'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "'b'", ")", "(", "get_by_name", "(", "'backward-word'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "'c'", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'capitalize-word'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "'d'", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'kill-word'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "'f'", ")", "(", "get_by_name", "(", "'forward-word'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "'l'", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'downcase-word'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "'u'", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'uppercase-word'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "'y'", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'yank-pop'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "Keys", ".", "ControlH", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'backward-kill-word'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "Keys", ".", "Backspace", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'backward-kill-word'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "'\\\\'", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'delete-horizontal-space'", ")", ")", "handle", "(", "Keys", ".", "ControlUnderscore", ",", "save_before", "=", "(", "lambda", "e", ":", "False", ")", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'undo'", ")", ")", "handle", "(", "Keys", ".", "ControlX", ",", "Keys", ".", "ControlU", ",", "save_before", "=", "(", "lambda", "e", ":", "False", ")", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'undo'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "'<'", ",", "filter", "=", "~", "has_selection", ")", "(", "get_by_name", "(", "'beginning-of-history'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "'>'", ",", "filter", "=", "~", "has_selection", ")", "(", "get_by_name", "(", "'end-of-history'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "'.'", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'yank-last-arg'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "'_'", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'yank-last-arg'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "Keys", ".", "ControlY", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'yank-nth-arg'", ")", ")", "handle", "(", "Keys", ".", "Escape", ",", "'#'", ",", "filter", "=", "insert_mode", ")", "(", "get_by_name", "(", "'insert-comment'", ")", ")", "handle", "(", "Keys", ".", "ControlO", ")", "(", "get_by_name", "(", "'operate-and-get-next'", ")", ")", "# ControlQ does a quoted insert. Not that for vt100 terminals, you have to", "# disable flow control by running ``stty -ixon``, otherwise Ctrl-Q and", "# Ctrl-S are captured by the terminal.", "handle", "(", "Keys", ".", "ControlQ", ",", "filter", "=", "~", "has_selection", ")", "(", "get_by_name", "(", "'quoted-insert'", ")", ")", "handle", "(", "Keys", ".", "ControlX", ",", "'('", ")", "(", "get_by_name", "(", "'start-kbd-macro'", ")", ")", "handle", "(", "Keys", ".", "ControlX", ",", "')'", ")", "(", "get_by_name", "(", "'end-kbd-macro'", ")", ")", "handle", "(", "Keys", ".", "ControlX", ",", "'e'", ")", "(", "get_by_name", "(", "'call-last-kbd-macro'", ")", ")", "@", "handle", "(", "Keys", ".", "ControlN", ")", "def", "_", "(", "event", ")", ":", "\" Next line. \"", "event", ".", "current_buffer", ".", "auto_down", "(", ")", "@", "handle", "(", "Keys", ".", "ControlP", ")", "def", "_", "(", "event", ")", ":", "\" Previous line. \"", "event", ".", "current_buffer", ".", "auto_up", "(", "count", "=", "event", ".", "arg", ")", "def", "handle_digit", "(", "c", ")", ":", "\"\"\"\n        Handle input of arguments.\n        The first number needs to be preceeded by escape.\n        \"\"\"", "@", "handle", "(", "c", ",", "filter", "=", "HasArg", "(", ")", ")", "@", "handle", "(", "Keys", ".", "Escape", ",", "c", ")", "def", "_", "(", "event", ")", ":", "event", ".", "append_to_arg_count", "(", "c", ")", "for", "c", "in", "'0123456789'", ":", "handle_digit", "(", "c", ")", "@", "handle", "(", "Keys", ".", "Escape", ",", "'-'", ",", "filter", "=", "~", "HasArg", "(", ")", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        \"\"\"", "if", "event", ".", "_arg", "is", "None", ":", "event", ".", "append_to_arg_count", "(", "'-'", ")", "@", "handle", "(", "'-'", ",", "filter", "=", "Condition", "(", "lambda", "cli", ":", "cli", ".", "input_processor", ".", "arg", "==", "'-'", ")", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        When '-' is typed again, after exactly '-' has been given as an\n        argument, ignore this.\n        \"\"\"", "event", ".", "cli", ".", "input_processor", ".", "arg", "=", "'-'", "is_returnable", "=", "Condition", "(", "lambda", "cli", ":", "cli", ".", "current_buffer", ".", "accept_action", ".", "is_returnable", ")", "# Meta + Newline: always accept input.", "handle", "(", "Keys", ".", "Escape", ",", "Keys", ".", "ControlJ", ",", "filter", "=", "insert_mode", "&", "is_returnable", ")", "(", "get_by_name", "(", "'accept-line'", ")", ")", "def", "character_search", "(", "buff", ",", "char", ",", "count", ")", ":", "if", "count", "<", "0", ":", "match", "=", "buff", ".", "document", ".", "find_backwards", "(", "char", ",", "in_current_line", "=", "True", ",", "count", "=", "-", "count", ")", "else", ":", "match", "=", "buff", ".", "document", ".", "find", "(", "char", ",", "in_current_line", "=", "True", ",", "count", "=", "count", ")", "if", "match", "is", "not", "None", ":", "buff", ".", "cursor_position", "+=", "match", "@", "handle", "(", "Keys", ".", "ControlSquareClose", ",", "Keys", ".", "Any", ")", "def", "_", "(", "event", ")", ":", "\" When Ctl-] + a character is pressed. go to that character. \"", "# Also named 'character-search'", "character_search", "(", "event", ".", "current_buffer", ",", "event", ".", "data", ",", "event", ".", "arg", ")", "@", "handle", "(", "Keys", ".", "Escape", ",", "Keys", ".", "ControlSquareClose", ",", "Keys", ".", "Any", ")", "def", "_", "(", "event", ")", ":", "\" Like Ctl-], but backwards. \"", "# Also named 'character-search-backward'", "character_search", "(", "event", ".", "current_buffer", ",", "event", ".", "data", ",", "-", "event", ".", "arg", ")", "@", "handle", "(", "Keys", ".", "Escape", ",", "'a'", ")", "def", "_", "(", "event", ")", ":", "\" Previous sentence. \"", "# TODO:", "@", "handle", "(", "Keys", ".", "Escape", ",", "'e'", ")", "def", "_", "(", "event", ")", ":", "\" Move to end of sentence. \"", "# TODO:", "@", "handle", "(", "Keys", ".", "Escape", ",", "'t'", ",", "filter", "=", "insert_mode", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        Swap the last two words before the cursor.\n        \"\"\"", "# TODO", "@", "handle", "(", "Keys", ".", "Escape", ",", "'*'", ",", "filter", "=", "insert_mode", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        `meta-*`: Insert all possible completions of the preceding text.\n        \"\"\"", "buff", "=", "event", ".", "current_buffer", "# List all completions.", "complete_event", "=", "CompleteEvent", "(", "text_inserted", "=", "False", ",", "completion_requested", "=", "True", ")", "completions", "=", "list", "(", "buff", ".", "completer", ".", "get_completions", "(", "buff", ".", "document", ",", "complete_event", ")", ")", "# Insert them.", "text_to_insert", "=", "' '", ".", "join", "(", "c", ".", "text", "for", "c", "in", "completions", ")", "buff", ".", "insert_text", "(", "text_to_insert", ")", "@", "handle", "(", "Keys", ".", "ControlX", ",", "Keys", ".", "ControlX", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        Move cursor back and forth between the start and end of the current\n        line.\n        \"\"\"", "buffer", "=", "event", ".", "current_buffer", "if", "buffer", ".", "document", ".", "is_cursor_at_the_end_of_line", ":", "buffer", ".", "cursor_position", "+=", "buffer", ".", "document", ".", "get_start_of_line_position", "(", "after_whitespace", "=", "False", ")", "else", ":", "buffer", ".", "cursor_position", "+=", "buffer", ".", "document", ".", "get_end_of_line_position", "(", ")", "@", "handle", "(", "Keys", ".", "ControlSpace", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        Start of the selection (if the current buffer is not empty).\n        \"\"\"", "# Take the current cursor position as the start of this selection.", "buff", "=", "event", ".", "current_buffer", "if", "buff", ".", "text", ":", "buff", ".", "start_selection", "(", "selection_type", "=", "SelectionType", ".", "CHARACTERS", ")", "@", "handle", "(", "Keys", ".", "ControlG", ",", "filter", "=", "~", "has_selection", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        Control + G: Cancel completion menu and validation state.\n        \"\"\"", "event", ".", "current_buffer", ".", "complete_state", "=", "None", "event", ".", "current_buffer", ".", "validation_error", "=", "None", "@", "handle", "(", "Keys", ".", "ControlG", ",", "filter", "=", "has_selection", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        Cancel selection.\n        \"\"\"", "event", ".", "current_buffer", ".", "exit_selection", "(", ")", "@", "handle", "(", "Keys", ".", "ControlW", ",", "filter", "=", "has_selection", ")", "@", "handle", "(", "Keys", ".", "ControlX", ",", "'r'", ",", "'k'", ",", "filter", "=", "has_selection", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        Cut selected text.\n        \"\"\"", "data", "=", "event", ".", "current_buffer", ".", "cut_selection", "(", ")", "event", ".", "cli", ".", "clipboard", ".", "set_data", "(", "data", ")", "@", "handle", "(", "Keys", ".", "Escape", ",", "'w'", ",", "filter", "=", "has_selection", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        Copy selected text.\n        \"\"\"", "data", "=", "event", ".", "current_buffer", ".", "copy_selection", "(", ")", "event", ".", "cli", ".", "clipboard", ".", "set_data", "(", "data", ")", "@", "handle", "(", "Keys", ".", "Escape", ",", "Keys", ".", "Left", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        Cursor to start of previous word.\n        \"\"\"", "buffer", "=", "event", ".", "current_buffer", "buffer", ".", "cursor_position", "+=", "buffer", ".", "document", ".", "find_previous_word_beginning", "(", "count", "=", "event", ".", "arg", ")", "or", "0", "@", "handle", "(", "Keys", ".", "Escape", ",", "Keys", ".", "Right", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        Cursor to start of next word.\n        \"\"\"", "buffer", "=", "event", ".", "current_buffer", "buffer", ".", "cursor_position", "+=", "buffer", ".", "document", ".", "find_next_word_beginning", "(", "count", "=", "event", ".", "arg", ")", "or", "buffer", ".", "document", ".", "get_end_of_document_position", "(", ")", "@", "handle", "(", "Keys", ".", "Escape", ",", "'/'", ",", "filter", "=", "insert_mode", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        M-/: Complete.\n        \"\"\"", "b", "=", "event", ".", "current_buffer", "if", "b", ".", "complete_state", ":", "b", ".", "complete_next", "(", ")", "else", ":", "event", ".", "cli", ".", "start_completion", "(", "select_first", "=", "True", ")", "@", "handle", "(", "Keys", ".", "ControlC", ",", "'>'", ",", "filter", "=", "has_selection", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        Indent selected text.\n        \"\"\"", "buffer", "=", "event", ".", "current_buffer", "buffer", ".", "cursor_position", "+=", "buffer", ".", "document", ".", "get_start_of_line_position", "(", "after_whitespace", "=", "True", ")", "from_", ",", "to", "=", "buffer", ".", "document", ".", "selection_range", "(", ")", "from_", ",", "_", "=", "buffer", ".", "document", ".", "translate_index_to_position", "(", "from_", ")", "to", ",", "_", "=", "buffer", ".", "document", ".", "translate_index_to_position", "(", "to", ")", "indent", "(", "buffer", ",", "from_", ",", "to", "+", "1", ",", "count", "=", "event", ".", "arg", ")", "@", "handle", "(", "Keys", ".", "ControlC", ",", "'<'", ",", "filter", "=", "has_selection", ")", "def", "_", "(", "event", ")", ":", "\"\"\"\n        Unindent selected text.\n        \"\"\"", "buffer", "=", "event", ".", "current_buffer", "from_", ",", "to", "=", "buffer", ".", "document", ".", "selection_range", "(", ")", "from_", ",", "_", "=", "buffer", ".", "document", ".", "translate_index_to_position", "(", "from_", ")", "to", ",", "_", "=", "buffer", ".", "document", ".", "translate_index_to_position", "(", "to", ")", "unindent", "(", "buffer", ",", "from_", ",", "to", "+", "1", ",", "count", "=", "event", ".", "arg", ")", "return", "registry"], "docstring": "Some e-macs extensions.", "docstring_tokens": ["Some", "e", "-", "macs", "extensions", "."], "sha": "7d08954ed5674fee223cd85ed0d8518fe47266b2", "url": "https://github.com/wandb/client/blob/7d08954ed5674fee223cd85ed0d8518fe47266b2/wandb/vendor/prompt_toolkit/key_binding/bindings/emacs.py#L21-L299", "partition": "train"}
{"repo": "FNNDSC/med2image", "path": "med2image/med2image.py", "func_name": "med2image_nii.run", "original_string": "def run(self):\n        '''\n        Runs the NIfTI conversion based on internal state.\n        '''\n\n        self._log('About to perform NifTI to %s conversion...\\n' %\n                  self._str_outputFileType)\n\n        frames     = 1\n        frameStart = 0\n        frameEnd   = 0\n\n        sliceStart = 0\n        sliceEnd   = 0\n\n        if self._b_4D:\n            self._log('4D volume detected.\\n')\n            frames = self._Vnp_4DVol.shape[3]\n        if self._b_3D:\n            self._log('3D volume detected.\\n')\n\n        if self._b_convertMiddleFrame:\n            self._frameToConvert = int(frames/2)\n\n        if self._frameToConvert == -1:\n            frameEnd    = frames\n        else:\n            frameStart  = self._frameToConvert\n            frameEnd    = self._frameToConvert + 1\n\n        for f in range(frameStart, frameEnd):\n            if self._b_4D:\n                self._Vnp_3DVol = self._Vnp_4DVol[:,:,:,f]\n            slices     = self._Vnp_3DVol.shape[2]\n            if self._b_convertMiddleSlice:\n                self._sliceToConvert = int(slices/2)\n\n            if self._sliceToConvert == -1:\n                sliceEnd    = -1\n            else:\n                sliceStart  = self._sliceToConvert\n                sliceEnd    = self._sliceToConvert + 1\n\n            misc.mkdir(self._str_outputDir)\n            if self._b_reslice:\n                for dim in ['x', 'y', 'z']:\n                    self.dim_save(dimension = dim, makeSubDir = True, indexStart = sliceStart, indexStop = sliceEnd, rot90 = True)\n            else:\n                self.dim_save(dimension = 'z', makeSubDir = False, indexStart = sliceStart, indexStop = sliceEnd, rot90 = True)", "language": "python", "code": "def run(self):\n        '''\n        Runs the NIfTI conversion based on internal state.\n        '''\n\n        self._log('About to perform NifTI to %s conversion...\\n' %\n                  self._str_outputFileType)\n\n        frames     = 1\n        frameStart = 0\n        frameEnd   = 0\n\n        sliceStart = 0\n        sliceEnd   = 0\n\n        if self._b_4D:\n            self._log('4D volume detected.\\n')\n            frames = self._Vnp_4DVol.shape[3]\n        if self._b_3D:\n            self._log('3D volume detected.\\n')\n\n        if self._b_convertMiddleFrame:\n            self._frameToConvert = int(frames/2)\n\n        if self._frameToConvert == -1:\n            frameEnd    = frames\n        else:\n            frameStart  = self._frameToConvert\n            frameEnd    = self._frameToConvert + 1\n\n        for f in range(frameStart, frameEnd):\n            if self._b_4D:\n                self._Vnp_3DVol = self._Vnp_4DVol[:,:,:,f]\n            slices     = self._Vnp_3DVol.shape[2]\n            if self._b_convertMiddleSlice:\n                self._sliceToConvert = int(slices/2)\n\n            if self._sliceToConvert == -1:\n                sliceEnd    = -1\n            else:\n                sliceStart  = self._sliceToConvert\n                sliceEnd    = self._sliceToConvert + 1\n\n            misc.mkdir(self._str_outputDir)\n            if self._b_reslice:\n                for dim in ['x', 'y', 'z']:\n                    self.dim_save(dimension = dim, makeSubDir = True, indexStart = sliceStart, indexStop = sliceEnd, rot90 = True)\n            else:\n                self.dim_save(dimension = 'z', makeSubDir = False, indexStart = sliceStart, indexStop = sliceEnd, rot90 = True)", "code_tokens": ["def", "run", "(", "self", ")", ":", "self", ".", "_log", "(", "'About to perform NifTI to %s conversion...\\n'", "%", "self", ".", "_str_outputFileType", ")", "frames", "=", "1", "frameStart", "=", "0", "frameEnd", "=", "0", "sliceStart", "=", "0", "sliceEnd", "=", "0", "if", "self", ".", "_b_4D", ":", "self", ".", "_log", "(", "'4D volume detected.\\n'", ")", "frames", "=", "self", ".", "_Vnp_4DVol", ".", "shape", "[", "3", "]", "if", "self", ".", "_b_3D", ":", "self", ".", "_log", "(", "'3D volume detected.\\n'", ")", "if", "self", ".", "_b_convertMiddleFrame", ":", "self", ".", "_frameToConvert", "=", "int", "(", "frames", "/", "2", ")", "if", "self", ".", "_frameToConvert", "==", "-", "1", ":", "frameEnd", "=", "frames", "else", ":", "frameStart", "=", "self", ".", "_frameToConvert", "frameEnd", "=", "self", ".", "_frameToConvert", "+", "1", "for", "f", "in", "range", "(", "frameStart", ",", "frameEnd", ")", ":", "if", "self", ".", "_b_4D", ":", "self", ".", "_Vnp_3DVol", "=", "self", ".", "_Vnp_4DVol", "[", ":", ",", ":", ",", ":", ",", "f", "]", "slices", "=", "self", ".", "_Vnp_3DVol", ".", "shape", "[", "2", "]", "if", "self", ".", "_b_convertMiddleSlice", ":", "self", ".", "_sliceToConvert", "=", "int", "(", "slices", "/", "2", ")", "if", "self", ".", "_sliceToConvert", "==", "-", "1", ":", "sliceEnd", "=", "-", "1", "else", ":", "sliceStart", "=", "self", ".", "_sliceToConvert", "sliceEnd", "=", "self", ".", "_sliceToConvert", "+", "1", "misc", ".", "mkdir", "(", "self", ".", "_str_outputDir", ")", "if", "self", ".", "_b_reslice", ":", "for", "dim", "in", "[", "'x'", ",", "'y'", ",", "'z'", "]", ":", "self", ".", "dim_save", "(", "dimension", "=", "dim", ",", "makeSubDir", "=", "True", ",", "indexStart", "=", "sliceStart", ",", "indexStop", "=", "sliceEnd", ",", "rot90", "=", "True", ")", "else", ":", "self", ".", "dim_save", "(", "dimension", "=", "'z'", ",", "makeSubDir", "=", "False", ",", "indexStart", "=", "sliceStart", ",", "indexStop", "=", "sliceEnd", ",", "rot90", "=", "True", ")"], "docstring": "Runs the NIfTI conversion based on internal state.", "docstring_tokens": ["Runs", "the", "NIfTI", "conversion", "based", "on", "internal", "state", "."], "sha": "638d5d230de47608af20f9764acf8e382c2bf2ff", "url": "https://github.com/FNNDSC/med2image/blob/638d5d230de47608af20f9764acf8e382c2bf2ff/med2image/med2image.py#L489-L537", "partition": "train"}
{"repo": "ziwenxie/netease-dl", "path": "netease/logger.py", "func_name": "get_logger", "original_string": "def get_logger(name):\n    \"\"\"Return a logger with a file handler.\"\"\"\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n\n    # File output handler\n    file_handler = logging.FileHandler(log_path)\n    file_handler.setLevel(logging.INFO)\n    formatter = logging.Formatter(\n        '%(asctime)s %(name)12s %(levelname)8s %(lineno)s %(message)s',\n        datefmt='%m/%d/%Y %I:%M:%S %p')\n    file_handler.setFormatter(formatter)\n\n    logger.addHandler(file_handler)\n    return logger", "language": "python", "code": "def get_logger(name):\n    \"\"\"Return a logger with a file handler.\"\"\"\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n\n    # File output handler\n    file_handler = logging.FileHandler(log_path)\n    file_handler.setLevel(logging.INFO)\n    formatter = logging.Formatter(\n        '%(asctime)s %(name)12s %(levelname)8s %(lineno)s %(message)s',\n        datefmt='%m/%d/%Y %I:%M:%S %p')\n    file_handler.setFormatter(formatter)\n\n    logger.addHandler(file_handler)\n    return logger", "code_tokens": ["def", "get_logger", "(", "name", ")", ":", "logger", "=", "logging", ".", "getLogger", "(", "name", ")", "logger", ".", "setLevel", "(", "logging", ".", "INFO", ")", "# File output handler", "file_handler", "=", "logging", ".", "FileHandler", "(", "log_path", ")", "file_handler", ".", "setLevel", "(", "logging", ".", "INFO", ")", "formatter", "=", "logging", ".", "Formatter", "(", "'%(asctime)s %(name)12s %(levelname)8s %(lineno)s %(message)s'", ",", "datefmt", "=", "'%m/%d/%Y %I:%M:%S %p'", ")", "file_handler", ".", "setFormatter", "(", "formatter", ")", "logger", ".", "addHandler", "(", "file_handler", ")", "return", "logger"], "docstring": "Return a logger with a file handler.", "docstring_tokens": ["Return", "a", "logger", "with", "a", "file", "handler", "."], "sha": "84b226fc07b10f7f66580f0fc69f10356f66b5c3", "url": "https://github.com/ziwenxie/netease-dl/blob/84b226fc07b10f7f66580f0fc69f10356f66b5c3/netease/logger.py#L24-L38", "partition": "train"}
{"repo": "djangonauts/django-rest-framework-gis", "path": "rest_framework_gis/filters.py", "func_name": "DistanceToPointFilter.dist_to_deg", "original_string": "def dist_to_deg(self, distance, latitude):\n        \"\"\"\n        distance = distance in meters\n        latitude = latitude in degrees\n\n        at the equator, the distance of one degree is equal in latitude and longitude.\n        at higher latitudes, a degree longitude is shorter in length, proportional to cos(latitude)\n        http://en.wikipedia.org/wiki/Decimal_degrees\n\n        This function is part of a distance filter where the database 'distance' is in degrees.\n        There's no good single-valued answer to this problem.\n        The distance/ degree is quite constant N/S around the earth (latitude),\n        but varies over a huge range E/W (longitude).\n\n        Split the difference: I'm going to average the the degrees latitude and degrees longitude\n        corresponding to the given distance. At high latitudes, this will be too short N/S\n        and too long E/W. It splits the errors between the two axes.\n\n        Errors are < 25 percent for latitudes < 60 degrees N/S.\n        \"\"\"\n        #   d * (180 / pi) / earthRadius   ==> degrees longitude\n        #   (degrees longitude) / cos(latitude)  ==> degrees latitude\n        lat = latitude if latitude >= 0 else -1 * latitude\n        rad2deg = 180 / pi\n        earthRadius = 6378160.0\n        latitudeCorrection = 0.5 * (1 + cos(lat * pi / 180))\n        return (distance / (earthRadius * latitudeCorrection) * rad2deg)", "language": "python", "code": "def dist_to_deg(self, distance, latitude):\n        \"\"\"\n        distance = distance in meters\n        latitude = latitude in degrees\n\n        at the equator, the distance of one degree is equal in latitude and longitude.\n        at higher latitudes, a degree longitude is shorter in length, proportional to cos(latitude)\n        http://en.wikipedia.org/wiki/Decimal_degrees\n\n        This function is part of a distance filter where the database 'distance' is in degrees.\n        There's no good single-valued answer to this problem.\n        The distance/ degree is quite constant N/S around the earth (latitude),\n        but varies over a huge range E/W (longitude).\n\n        Split the difference: I'm going to average the the degrees latitude and degrees longitude\n        corresponding to the given distance. At high latitudes, this will be too short N/S\n        and too long E/W. It splits the errors between the two axes.\n\n        Errors are < 25 percent for latitudes < 60 degrees N/S.\n        \"\"\"\n        #   d * (180 / pi) / earthRadius   ==> degrees longitude\n        #   (degrees longitude) / cos(latitude)  ==> degrees latitude\n        lat = latitude if latitude >= 0 else -1 * latitude\n        rad2deg = 180 / pi\n        earthRadius = 6378160.0\n        latitudeCorrection = 0.5 * (1 + cos(lat * pi / 180))\n        return (distance / (earthRadius * latitudeCorrection) * rad2deg)", "code_tokens": ["def", "dist_to_deg", "(", "self", ",", "distance", ",", "latitude", ")", ":", "#   d * (180 / pi) / earthRadius   ==> degrees longitude", "#   (degrees longitude) / cos(latitude)  ==> degrees latitude", "lat", "=", "latitude", "if", "latitude", ">=", "0", "else", "-", "1", "*", "latitude", "rad2deg", "=", "180", "/", "pi", "earthRadius", "=", "6378160.0", "latitudeCorrection", "=", "0.5", "*", "(", "1", "+", "cos", "(", "lat", "*", "pi", "/", "180", ")", ")", "return", "(", "distance", "/", "(", "earthRadius", "*", "latitudeCorrection", ")", "*", "rad2deg", ")"], "docstring": "distance = distance in meters\n        latitude = latitude in degrees\n\n        at the equator, the distance of one degree is equal in latitude and longitude.\n        at higher latitudes, a degree longitude is shorter in length, proportional to cos(latitude)\n        http://en.wikipedia.org/wiki/Decimal_degrees\n\n        This function is part of a distance filter where the database 'distance' is in degrees.\n        There's no good single-valued answer to this problem.\n        The distance/ degree is quite constant N/S around the earth (latitude),\n        but varies over a huge range E/W (longitude).\n\n        Split the difference: I'm going to average the the degrees latitude and degrees longitude\n        corresponding to the given distance. At high latitudes, this will be too short N/S\n        and too long E/W. It splits the errors between the two axes.\n\n        Errors are < 25 percent for latitudes < 60 degrees N/S.", "docstring_tokens": ["distance", "=", "distance", "in", "meters", "latitude", "=", "latitude", "in", "degrees"], "sha": "7430d6b1ca480222c1b837748f4c60ea10c85f22", "url": "https://github.com/djangonauts/django-rest-framework-gis/blob/7430d6b1ca480222c1b837748f4c60ea10c85f22/rest_framework_gis/filters.py#L139-L165", "partition": "train"}
{"repo": "google/apitools", "path": "apitools/base/py/base_api.py", "func_name": "_urljoin", "original_string": "def _urljoin(base, url):  # pylint: disable=invalid-name\n    \"\"\"Custom urljoin replacement supporting : before / in url.\"\"\"\n    # In general, it's unsafe to simply join base and url. However, for\n    # the case of discovery documents, we know:\n    #  * base will never contain params, query, or fragment\n    #  * url will never contain a scheme or net_loc.\n    # In general, this means we can safely join on /; we just need to\n    # ensure we end up with precisely one / joining base and url. The\n    # exception here is the case of media uploads, where url will be an\n    # absolute url.\n    if url.startswith('http://') or url.startswith('https://'):\n        return urllib.parse.urljoin(base, url)\n    new_base = base if base.endswith('/') else base + '/'\n    new_url = url[1:] if url.startswith('/') else url\n    return new_base + new_url", "language": "python", "code": "def _urljoin(base, url):  # pylint: disable=invalid-name\n    \"\"\"Custom urljoin replacement supporting : before / in url.\"\"\"\n    # In general, it's unsafe to simply join base and url. However, for\n    # the case of discovery documents, we know:\n    #  * base will never contain params, query, or fragment\n    #  * url will never contain a scheme or net_loc.\n    # In general, this means we can safely join on /; we just need to\n    # ensure we end up with precisely one / joining base and url. The\n    # exception here is the case of media uploads, where url will be an\n    # absolute url.\n    if url.startswith('http://') or url.startswith('https://'):\n        return urllib.parse.urljoin(base, url)\n    new_base = base if base.endswith('/') else base + '/'\n    new_url = url[1:] if url.startswith('/') else url\n    return new_base + new_url", "code_tokens": ["def", "_urljoin", "(", "base", ",", "url", ")", ":", "# pylint: disable=invalid-name", "# In general, it's unsafe to simply join base and url. However, for", "# the case of discovery documents, we know:", "#  * base will never contain params, query, or fragment", "#  * url will never contain a scheme or net_loc.", "# In general, this means we can safely join on /; we just need to", "# ensure we end up with precisely one / joining base and url. The", "# exception here is the case of media uploads, where url will be an", "# absolute url.", "if", "url", ".", "startswith", "(", "'http://'", ")", "or", "url", ".", "startswith", "(", "'https://'", ")", ":", "return", "urllib", ".", "parse", ".", "urljoin", "(", "base", ",", "url", ")", "new_base", "=", "base", "if", "base", ".", "endswith", "(", "'/'", ")", "else", "base", "+", "'/'", "new_url", "=", "url", "[", "1", ":", "]", "if", "url", ".", "startswith", "(", "'/'", ")", "else", "url", "return", "new_base", "+", "new_url"], "docstring": "Custom urljoin replacement supporting : before / in url.", "docstring_tokens": ["Custom", "urljoin", "replacement", "supporting", ":", "before", "/", "in", "url", "."], "sha": "f3745a7ea535aa0e88b0650c16479b696d6fd446", "url": "https://github.com/google/apitools/blob/f3745a7ea535aa0e88b0650c16479b696d6fd446/apitools/base/py/base_api.py#L148-L162", "partition": "train"}
{"repo": "patarapolw/AnkiTools", "path": "AnkiTools/tools/create.py", "func_name": "AnkiContentCreator.new_template", "original_string": "def new_template(template_name: str, ordering: int, formatting: dict=None, **kwargs):\n        \"\"\"\n        Templates have no unique ID.\n        :param template_name:\n        :param ordering:\n        :param formatting:\n        :param kwargs:\n        :return:\n        \"\"\"\n        if formatting is not None:\n            kwargs.update(formatting)\n\n        template = dict([\n            ('name', template_name),\n            ('qfmt', DEFAULT_TEMPLATE['qfmt']),\n            ('did', None),\n            ('bafmt', DEFAULT_TEMPLATE['bafmt']),\n            ('afmt', DEFAULT_TEMPLATE['afmt']),\n            ('ord', ordering),\n            ('bqfmt', DEFAULT_TEMPLATE['bqfmt'])\n        ])\n\n        for k, v in template.items():\n            if k in kwargs.keys():\n                template[k] = kwargs[k]\n\n        return template", "language": "python", "code": "def new_template(template_name: str, ordering: int, formatting: dict=None, **kwargs):\n        \"\"\"\n        Templates have no unique ID.\n        :param template_name:\n        :param ordering:\n        :param formatting:\n        :param kwargs:\n        :return:\n        \"\"\"\n        if formatting is not None:\n            kwargs.update(formatting)\n\n        template = dict([\n            ('name', template_name),\n            ('qfmt', DEFAULT_TEMPLATE['qfmt']),\n            ('did', None),\n            ('bafmt', DEFAULT_TEMPLATE['bafmt']),\n            ('afmt', DEFAULT_TEMPLATE['afmt']),\n            ('ord', ordering),\n            ('bqfmt', DEFAULT_TEMPLATE['bqfmt'])\n        ])\n\n        for k, v in template.items():\n            if k in kwargs.keys():\n                template[k] = kwargs[k]\n\n        return template", "code_tokens": ["def", "new_template", "(", "template_name", ":", "str", ",", "ordering", ":", "int", ",", "formatting", ":", "dict", "=", "None", ",", "*", "*", "kwargs", ")", ":", "if", "formatting", "is", "not", "None", ":", "kwargs", ".", "update", "(", "formatting", ")", "template", "=", "dict", "(", "[", "(", "'name'", ",", "template_name", ")", ",", "(", "'qfmt'", ",", "DEFAULT_TEMPLATE", "[", "'qfmt'", "]", ")", ",", "(", "'did'", ",", "None", ")", ",", "(", "'bafmt'", ",", "DEFAULT_TEMPLATE", "[", "'bafmt'", "]", ")", ",", "(", "'afmt'", ",", "DEFAULT_TEMPLATE", "[", "'afmt'", "]", ")", ",", "(", "'ord'", ",", "ordering", ")", ",", "(", "'bqfmt'", ",", "DEFAULT_TEMPLATE", "[", "'bqfmt'", "]", ")", "]", ")", "for", "k", ",", "v", "in", "template", ".", "items", "(", ")", ":", "if", "k", "in", "kwargs", ".", "keys", "(", ")", ":", "template", "[", "k", "]", "=", "kwargs", "[", "k", "]", "return", "template"], "docstring": "Templates have no unique ID.\n        :param template_name:\n        :param ordering:\n        :param formatting:\n        :param kwargs:\n        :return:", "docstring_tokens": ["Templates", "have", "no", "unique", "ID", ".", ":", "param", "template_name", ":", ":", "param", "ordering", ":", ":", "param", "formatting", ":", ":", "param", "kwargs", ":", ":", "return", ":"], "sha": "fab6836dfd9cf5171d9cbff5c55fbb14d2786f05", "url": "https://github.com/patarapolw/AnkiTools/blob/fab6836dfd9cf5171d9cbff5c55fbb14d2786f05/AnkiTools/tools/create.py#L110-L136", "partition": "train"}
{"repo": "Miserlou/zappa-django-utils", "path": "zappa_django_utils/db/backends/s3sqlite/base.py", "func_name": "DatabaseWrapper.load_remote_db", "original_string": "def load_remote_db(self):\n        \"\"\"\n        Load remote S3 DB\n        \"\"\"\n\n        signature_version = self.settings_dict.get(\"SIGNATURE_VERSION\", \"s3v4\")\n        s3 = boto3.resource(\n            's3',\n            config=botocore.client.Config(signature_version=signature_version),\n        )\n\n        if '/tmp/' not in self.settings_dict['NAME']:\n            try:\n                etag = ''\n                if os.path.isfile('/tmp/' + self.settings_dict['NAME']):\n                    m = hashlib.md5()\n                    with open('/tmp/' + self.settings_dict['NAME'], 'rb') as f:\n                        m.update(f.read())\n\n                    # In general the ETag is the md5 of the file, in some cases it's not,\n                    # and in that case we will just need to reload the file, I don't see any other way\n                    etag = m.hexdigest()\n\n                obj = s3.Object(self.settings_dict['BUCKET'], self.settings_dict['NAME'])\n                obj_bytes = obj.get(IfNoneMatch=etag)[\"Body\"]  # Will throw E on 304 or 404\n\n                with open('/tmp/' + self.settings_dict['NAME'], 'wb') as f:\n                    f.write(obj_bytes.read())\n\n                m = hashlib.md5()\n                with open('/tmp/' + self.settings_dict['NAME'], 'rb') as f:\n                    m.update(f.read())\n\n                self.db_hash = m.hexdigest()\n\n            except botocore.exceptions.ClientError as e:\n                if e.response['Error']['Code'] == \"304\":\n                    logging.debug(\"ETag matches md5 of local copy, using local copy of DB!\")\n                    self.db_hash = etag\n                else:\n                    logging.debug(\"Couldn't load remote DB object.\")\n            except Exception as e:\n                # Weird one\n                logging.debug(e)\n\n        # SQLite DatabaseWrapper will treat our tmp as normal now\n        # Check because Django likes to call this function a lot more than it should\n        if '/tmp/' not in self.settings_dict['NAME']:\n            self.settings_dict['REMOTE_NAME'] = self.settings_dict['NAME']\n            self.settings_dict['NAME'] = '/tmp/' + self.settings_dict['NAME']\n\n        # Make sure it exists if it doesn't yet\n        if not os.path.isfile(self.settings_dict['NAME']):\n            open(self.settings_dict['NAME'], 'a').close()\n\n        logging.debug(\"Loaded remote DB!\")", "language": "python", "code": "def load_remote_db(self):\n        \"\"\"\n        Load remote S3 DB\n        \"\"\"\n\n        signature_version = self.settings_dict.get(\"SIGNATURE_VERSION\", \"s3v4\")\n        s3 = boto3.resource(\n            's3',\n            config=botocore.client.Config(signature_version=signature_version),\n        )\n\n        if '/tmp/' not in self.settings_dict['NAME']:\n            try:\n                etag = ''\n                if os.path.isfile('/tmp/' + self.settings_dict['NAME']):\n                    m = hashlib.md5()\n                    with open('/tmp/' + self.settings_dict['NAME'], 'rb') as f:\n                        m.update(f.read())\n\n                    # In general the ETag is the md5 of the file, in some cases it's not,\n                    # and in that case we will just need to reload the file, I don't see any other way\n                    etag = m.hexdigest()\n\n                obj = s3.Object(self.settings_dict['BUCKET'], self.settings_dict['NAME'])\n                obj_bytes = obj.get(IfNoneMatch=etag)[\"Body\"]  # Will throw E on 304 or 404\n\n                with open('/tmp/' + self.settings_dict['NAME'], 'wb') as f:\n                    f.write(obj_bytes.read())\n\n                m = hashlib.md5()\n                with open('/tmp/' + self.settings_dict['NAME'], 'rb') as f:\n                    m.update(f.read())\n\n                self.db_hash = m.hexdigest()\n\n            except botocore.exceptions.ClientError as e:\n                if e.response['Error']['Code'] == \"304\":\n                    logging.debug(\"ETag matches md5 of local copy, using local copy of DB!\")\n                    self.db_hash = etag\n                else:\n                    logging.debug(\"Couldn't load remote DB object.\")\n            except Exception as e:\n                # Weird one\n                logging.debug(e)\n\n        # SQLite DatabaseWrapper will treat our tmp as normal now\n        # Check because Django likes to call this function a lot more than it should\n        if '/tmp/' not in self.settings_dict['NAME']:\n            self.settings_dict['REMOTE_NAME'] = self.settings_dict['NAME']\n            self.settings_dict['NAME'] = '/tmp/' + self.settings_dict['NAME']\n\n        # Make sure it exists if it doesn't yet\n        if not os.path.isfile(self.settings_dict['NAME']):\n            open(self.settings_dict['NAME'], 'a').close()\n\n        logging.debug(\"Loaded remote DB!\")", "code_tokens": ["def", "load_remote_db", "(", "self", ")", ":", "signature_version", "=", "self", ".", "settings_dict", ".", "get", "(", "\"SIGNATURE_VERSION\"", ",", "\"s3v4\"", ")", "s3", "=", "boto3", ".", "resource", "(", "'s3'", ",", "config", "=", "botocore", ".", "client", ".", "Config", "(", "signature_version", "=", "signature_version", ")", ",", ")", "if", "'/tmp/'", "not", "in", "self", ".", "settings_dict", "[", "'NAME'", "]", ":", "try", ":", "etag", "=", "''", "if", "os", ".", "path", ".", "isfile", "(", "'/tmp/'", "+", "self", ".", "settings_dict", "[", "'NAME'", "]", ")", ":", "m", "=", "hashlib", ".", "md5", "(", ")", "with", "open", "(", "'/tmp/'", "+", "self", ".", "settings_dict", "[", "'NAME'", "]", ",", "'rb'", ")", "as", "f", ":", "m", ".", "update", "(", "f", ".", "read", "(", ")", ")", "# In general the ETag is the md5 of the file, in some cases it's not,", "# and in that case we will just need to reload the file, I don't see any other way", "etag", "=", "m", ".", "hexdigest", "(", ")", "obj", "=", "s3", ".", "Object", "(", "self", ".", "settings_dict", "[", "'BUCKET'", "]", ",", "self", ".", "settings_dict", "[", "'NAME'", "]", ")", "obj_bytes", "=", "obj", ".", "get", "(", "IfNoneMatch", "=", "etag", ")", "[", "\"Body\"", "]", "# Will throw E on 304 or 404", "with", "open", "(", "'/tmp/'", "+", "self", ".", "settings_dict", "[", "'NAME'", "]", ",", "'wb'", ")", "as", "f", ":", "f", ".", "write", "(", "obj_bytes", ".", "read", "(", ")", ")", "m", "=", "hashlib", ".", "md5", "(", ")", "with", "open", "(", "'/tmp/'", "+", "self", ".", "settings_dict", "[", "'NAME'", "]", ",", "'rb'", ")", "as", "f", ":", "m", ".", "update", "(", "f", ".", "read", "(", ")", ")", "self", ".", "db_hash", "=", "m", ".", "hexdigest", "(", ")", "except", "botocore", ".", "exceptions", ".", "ClientError", "as", "e", ":", "if", "e", ".", "response", "[", "'Error'", "]", "[", "'Code'", "]", "==", "\"304\"", ":", "logging", ".", "debug", "(", "\"ETag matches md5 of local copy, using local copy of DB!\"", ")", "self", ".", "db_hash", "=", "etag", "else", ":", "logging", ".", "debug", "(", "\"Couldn't load remote DB object.\"", ")", "except", "Exception", "as", "e", ":", "# Weird one", "logging", ".", "debug", "(", "e", ")", "# SQLite DatabaseWrapper will treat our tmp as normal now", "# Check because Django likes to call this function a lot more than it should", "if", "'/tmp/'", "not", "in", "self", ".", "settings_dict", "[", "'NAME'", "]", ":", "self", ".", "settings_dict", "[", "'REMOTE_NAME'", "]", "=", "self", ".", "settings_dict", "[", "'NAME'", "]", "self", ".", "settings_dict", "[", "'NAME'", "]", "=", "'/tmp/'", "+", "self", ".", "settings_dict", "[", "'NAME'", "]", "# Make sure it exists if it doesn't yet", "if", "not", "os", ".", "path", ".", "isfile", "(", "self", ".", "settings_dict", "[", "'NAME'", "]", ")", ":", "open", "(", "self", ".", "settings_dict", "[", "'NAME'", "]", ",", "'a'", ")", ".", "close", "(", ")", "logging", ".", "debug", "(", "\"Loaded remote DB!\"", ")"], "docstring": "Load remote S3 DB", "docstring_tokens": ["Load", "remote", "S3", "DB"], "sha": "33ec0dd4334ceb37bece829a2f01188d5ed00f31", "url": "https://github.com/Miserlou/zappa-django-utils/blob/33ec0dd4334ceb37bece829a2f01188d5ed00f31/zappa_django_utils/db/backends/s3sqlite/base.py#L18-L73", "partition": "train"}
{"repo": "adafruit/Adafruit_Python_PlatformDetect", "path": "adafruit_platformdetect/__init__.py", "func_name": "Detector.get_armbian_release_field", "original_string": "def get_armbian_release_field(self, field):\n        \"\"\"\n        Search /etc/armbian-release, if it exists, for a field and return its\n        value, if found, otherwise None.\n        \"\"\"\n        field_value = None\n        pattern = r'^' + field + r'=(.*)'\n        try:\n            with open(\"/etc/armbian-release\", 'r') as release_file:\n                armbian = release_file.read().split('\\n')\n                for line in armbian:\n                    match = re.search(pattern, line)\n                    if match:\n                        field_value = match.group(1)\n        except FileNotFoundError:\n            pass\n\n        return field_value", "language": "python", "code": "def get_armbian_release_field(self, field):\n        \"\"\"\n        Search /etc/armbian-release, if it exists, for a field and return its\n        value, if found, otherwise None.\n        \"\"\"\n        field_value = None\n        pattern = r'^' + field + r'=(.*)'\n        try:\n            with open(\"/etc/armbian-release\", 'r') as release_file:\n                armbian = release_file.read().split('\\n')\n                for line in armbian:\n                    match = re.search(pattern, line)\n                    if match:\n                        field_value = match.group(1)\n        except FileNotFoundError:\n            pass\n\n        return field_value", "code_tokens": ["def", "get_armbian_release_field", "(", "self", ",", "field", ")", ":", "field_value", "=", "None", "pattern", "=", "r'^'", "+", "field", "+", "r'=(.*)'", "try", ":", "with", "open", "(", "\"/etc/armbian-release\"", ",", "'r'", ")", "as", "release_file", ":", "armbian", "=", "release_file", ".", "read", "(", ")", ".", "split", "(", "'\\n'", ")", "for", "line", "in", "armbian", ":", "match", "=", "re", ".", "search", "(", "pattern", ",", "line", ")", "if", "match", ":", "field_value", "=", "match", ".", "group", "(", "1", ")", "except", "FileNotFoundError", ":", "pass", "return", "field_value"], "docstring": "Search /etc/armbian-release, if it exists, for a field and return its\n        value, if found, otherwise None.", "docstring_tokens": ["Search", "/", "etc", "/", "armbian", "-", "release", "if", "it", "exists", "for", "a", "field", "and", "return", "its", "value", "if", "found", "otherwise", "None", "."], "sha": "cddd4d47e530026778dc4e3c3ccabad14e6eac46", "url": "https://github.com/adafruit/Adafruit_Python_PlatformDetect/blob/cddd4d47e530026778dc4e3c3ccabad14e6eac46/adafruit_platformdetect/__init__.py#L58-L75", "partition": "train"}
{"repo": "aws/aws-xray-sdk-python", "path": "aws_xray_sdk/ext/aiohttp/middleware.py", "func_name": "middleware", "original_string": "async def middleware(request, handler):\n    \"\"\"\n    Main middleware function, deals with all the X-Ray segment logic\n    \"\"\"\n    # Create X-Ray headers\n    xray_header = construct_xray_header(request.headers)\n    # Get name of service or generate a dynamic one from host\n    name = calculate_segment_name(request.headers['host'].split(':', 1)[0], xray_recorder)\n\n    sampling_req = {\n        'host': request.headers['host'],\n        'method': request.method,\n        'path': request.path,\n        'service': name,\n    }\n\n    sampling_decision = calculate_sampling_decision(\n        trace_header=xray_header,\n        recorder=xray_recorder,\n        sampling_req=sampling_req,\n    )\n\n    # Start a segment\n    segment = xray_recorder.begin_segment(\n        name=name,\n        traceid=xray_header.root,\n        parent_id=xray_header.parent,\n        sampling=sampling_decision,\n    )\n\n    segment.save_origin_trace_header(xray_header)\n    # Store request metadata in the current segment\n    segment.put_http_meta(http.URL, str(request.url))\n    segment.put_http_meta(http.METHOD, request.method)\n\n    if 'User-Agent' in request.headers:\n        segment.put_http_meta(http.USER_AGENT, request.headers['User-Agent'])\n\n    if 'X-Forwarded-For' in request.headers:\n        segment.put_http_meta(http.CLIENT_IP, request.headers['X-Forwarded-For'])\n        segment.put_http_meta(http.X_FORWARDED_FOR, True)\n    elif 'remote_addr' in request.headers:\n        segment.put_http_meta(http.CLIENT_IP, request.headers['remote_addr'])\n    else:\n        segment.put_http_meta(http.CLIENT_IP, request.remote)\n\n    try:\n        # Call next middleware or request handler\n        response = await handler(request)\n    except HTTPException as exc:\n        # Non 2XX responses are raised as HTTPExceptions\n        response = exc\n        raise\n    except Exception as err:\n        # Store exception information including the stacktrace to the segment\n        response = None\n        segment.put_http_meta(http.STATUS, 500)\n        stack = stacktrace.get_stacktrace(limit=xray_recorder.max_trace_back)\n        segment.add_exception(err, stack)\n        raise\n    finally:\n        if response is not None:\n            segment.put_http_meta(http.STATUS, response.status)\n            if 'Content-Length' in response.headers:\n                length = int(response.headers['Content-Length'])\n                segment.put_http_meta(http.CONTENT_LENGTH, length)\n\n            header_str = prepare_response_header(xray_header, segment)\n            response.headers[http.XRAY_HEADER] = header_str\n\n        xray_recorder.end_segment()\n\n    return response", "language": "python", "code": "async def middleware(request, handler):\n    \"\"\"\n    Main middleware function, deals with all the X-Ray segment logic\n    \"\"\"\n    # Create X-Ray headers\n    xray_header = construct_xray_header(request.headers)\n    # Get name of service or generate a dynamic one from host\n    name = calculate_segment_name(request.headers['host'].split(':', 1)[0], xray_recorder)\n\n    sampling_req = {\n        'host': request.headers['host'],\n        'method': request.method,\n        'path': request.path,\n        'service': name,\n    }\n\n    sampling_decision = calculate_sampling_decision(\n        trace_header=xray_header,\n        recorder=xray_recorder,\n        sampling_req=sampling_req,\n    )\n\n    # Start a segment\n    segment = xray_recorder.begin_segment(\n        name=name,\n        traceid=xray_header.root,\n        parent_id=xray_header.parent,\n        sampling=sampling_decision,\n    )\n\n    segment.save_origin_trace_header(xray_header)\n    # Store request metadata in the current segment\n    segment.put_http_meta(http.URL, str(request.url))\n    segment.put_http_meta(http.METHOD, request.method)\n\n    if 'User-Agent' in request.headers:\n        segment.put_http_meta(http.USER_AGENT, request.headers['User-Agent'])\n\n    if 'X-Forwarded-For' in request.headers:\n        segment.put_http_meta(http.CLIENT_IP, request.headers['X-Forwarded-For'])\n        segment.put_http_meta(http.X_FORWARDED_FOR, True)\n    elif 'remote_addr' in request.headers:\n        segment.put_http_meta(http.CLIENT_IP, request.headers['remote_addr'])\n    else:\n        segment.put_http_meta(http.CLIENT_IP, request.remote)\n\n    try:\n        # Call next middleware or request handler\n        response = await handler(request)\n    except HTTPException as exc:\n        # Non 2XX responses are raised as HTTPExceptions\n        response = exc\n        raise\n    except Exception as err:\n        # Store exception information including the stacktrace to the segment\n        response = None\n        segment.put_http_meta(http.STATUS, 500)\n        stack = stacktrace.get_stacktrace(limit=xray_recorder.max_trace_back)\n        segment.add_exception(err, stack)\n        raise\n    finally:\n        if response is not None:\n            segment.put_http_meta(http.STATUS, response.status)\n            if 'Content-Length' in response.headers:\n                length = int(response.headers['Content-Length'])\n                segment.put_http_meta(http.CONTENT_LENGTH, length)\n\n            header_str = prepare_response_header(xray_header, segment)\n            response.headers[http.XRAY_HEADER] = header_str\n\n        xray_recorder.end_segment()\n\n    return response", "code_tokens": ["async", "def", "middleware", "(", "request", ",", "handler", ")", ":", "# Create X-Ray headers", "xray_header", "=", "construct_xray_header", "(", "request", ".", "headers", ")", "# Get name of service or generate a dynamic one from host", "name", "=", "calculate_segment_name", "(", "request", ".", "headers", "[", "'host'", "]", ".", "split", "(", "':'", ",", "1", ")", "[", "0", "]", ",", "xray_recorder", ")", "sampling_req", "=", "{", "'host'", ":", "request", ".", "headers", "[", "'host'", "]", ",", "'method'", ":", "request", ".", "method", ",", "'path'", ":", "request", ".", "path", ",", "'service'", ":", "name", ",", "}", "sampling_decision", "=", "calculate_sampling_decision", "(", "trace_header", "=", "xray_header", ",", "recorder", "=", "xray_recorder", ",", "sampling_req", "=", "sampling_req", ",", ")", "# Start a segment", "segment", "=", "xray_recorder", ".", "begin_segment", "(", "name", "=", "name", ",", "traceid", "=", "xray_header", ".", "root", ",", "parent_id", "=", "xray_header", ".", "parent", ",", "sampling", "=", "sampling_decision", ",", ")", "segment", ".", "save_origin_trace_header", "(", "xray_header", ")", "# Store request metadata in the current segment", "segment", ".", "put_http_meta", "(", "http", ".", "URL", ",", "str", "(", "request", ".", "url", ")", ")", "segment", ".", "put_http_meta", "(", "http", ".", "METHOD", ",", "request", ".", "method", ")", "if", "'User-Agent'", "in", "request", ".", "headers", ":", "segment", ".", "put_http_meta", "(", "http", ".", "USER_AGENT", ",", "request", ".", "headers", "[", "'User-Agent'", "]", ")", "if", "'X-Forwarded-For'", "in", "request", ".", "headers", ":", "segment", ".", "put_http_meta", "(", "http", ".", "CLIENT_IP", ",", "request", ".", "headers", "[", "'X-Forwarded-For'", "]", ")", "segment", ".", "put_http_meta", "(", "http", ".", "X_FORWARDED_FOR", ",", "True", ")", "elif", "'remote_addr'", "in", "request", ".", "headers", ":", "segment", ".", "put_http_meta", "(", "http", ".", "CLIENT_IP", ",", "request", ".", "headers", "[", "'remote_addr'", "]", ")", "else", ":", "segment", ".", "put_http_meta", "(", "http", ".", "CLIENT_IP", ",", "request", ".", "remote", ")", "try", ":", "# Call next middleware or request handler", "response", "=", "await", "handler", "(", "request", ")", "except", "HTTPException", "as", "exc", ":", "# Non 2XX responses are raised as HTTPExceptions", "response", "=", "exc", "raise", "except", "Exception", "as", "err", ":", "# Store exception information including the stacktrace to the segment", "response", "=", "None", "segment", ".", "put_http_meta", "(", "http", ".", "STATUS", ",", "500", ")", "stack", "=", "stacktrace", ".", "get_stacktrace", "(", "limit", "=", "xray_recorder", ".", "max_trace_back", ")", "segment", ".", "add_exception", "(", "err", ",", "stack", ")", "raise", "finally", ":", "if", "response", "is", "not", "None", ":", "segment", ".", "put_http_meta", "(", "http", ".", "STATUS", ",", "response", ".", "status", ")", "if", "'Content-Length'", "in", "response", ".", "headers", ":", "length", "=", "int", "(", "response", ".", "headers", "[", "'Content-Length'", "]", ")", "segment", ".", "put_http_meta", "(", "http", ".", "CONTENT_LENGTH", ",", "length", ")", "header_str", "=", "prepare_response_header", "(", "xray_header", ",", "segment", ")", "response", ".", "headers", "[", "http", ".", "XRAY_HEADER", "]", "=", "header_str", "xray_recorder", ".", "end_segment", "(", ")", "return", "response"], "docstring": "Main middleware function, deals with all the X-Ray segment logic", "docstring_tokens": ["Main", "middleware", "function", "deals", "with", "all", "the", "X", "-", "Ray", "segment", "logic"], "sha": "707358cd3a516d51f2ebf71cf34f00e8d906a667", "url": "https://github.com/aws/aws-xray-sdk-python/blob/707358cd3a516d51f2ebf71cf34f00e8d906a667/aws_xray_sdk/ext/aiohttp/middleware.py#L15-L87", "partition": "train"}
{"repo": "gem/oq-engine", "path": "openquake/hazardlib/shakemap.py", "func_name": "cross_correlation_matrix", "original_string": "def cross_correlation_matrix(imts, corr='yes'):\n    \"\"\"\n    :param imts: M intensity measure types\n    :param corr: 'yes', 'no' or 'full'\n    :returns: an array of shape (M, M)\n    \"\"\"\n    assert corr in 'yes no full', corr\n    # if there is only PGA this is a 1x1 identity matrix\n    M = len(imts)\n    cross_matrix = numpy.zeros((M, M))\n    for i, im in enumerate(imts):\n        T1 = im.period or 0.05\n\n        for j in range(M):\n            T2 = imts[j].period or 0.05\n            if i == j:\n                cross_matrix[i, j] = 1\n            else:\n                Tmax = max([T1, T2])\n                Tmin = min([T1, T2])\n                II = 1 if Tmin < 0.189 else 0\n                if corr == 'full':\n                    cross_matrix[i, j] = 0.99999\n                elif corr == 'yes':\n                    cross_matrix[i, j] = 1 - math.cos(math.pi / 2 - (\n                        0.359 + 0.163 * II * math.log(Tmin / 0.189)\n                    ) * math.log(Tmax / Tmin))\n\n    return cross_matrix", "language": "python", "code": "def cross_correlation_matrix(imts, corr='yes'):\n    \"\"\"\n    :param imts: M intensity measure types\n    :param corr: 'yes', 'no' or 'full'\n    :returns: an array of shape (M, M)\n    \"\"\"\n    assert corr in 'yes no full', corr\n    # if there is only PGA this is a 1x1 identity matrix\n    M = len(imts)\n    cross_matrix = numpy.zeros((M, M))\n    for i, im in enumerate(imts):\n        T1 = im.period or 0.05\n\n        for j in range(M):\n            T2 = imts[j].period or 0.05\n            if i == j:\n                cross_matrix[i, j] = 1\n            else:\n                Tmax = max([T1, T2])\n                Tmin = min([T1, T2])\n                II = 1 if Tmin < 0.189 else 0\n                if corr == 'full':\n                    cross_matrix[i, j] = 0.99999\n                elif corr == 'yes':\n                    cross_matrix[i, j] = 1 - math.cos(math.pi / 2 - (\n                        0.359 + 0.163 * II * math.log(Tmin / 0.189)\n                    ) * math.log(Tmax / Tmin))\n\n    return cross_matrix", "code_tokens": ["def", "cross_correlation_matrix", "(", "imts", ",", "corr", "=", "'yes'", ")", ":", "assert", "corr", "in", "'yes no full'", ",", "corr", "# if there is only PGA this is a 1x1 identity matrix", "M", "=", "len", "(", "imts", ")", "cross_matrix", "=", "numpy", ".", "zeros", "(", "(", "M", ",", "M", ")", ")", "for", "i", ",", "im", "in", "enumerate", "(", "imts", ")", ":", "T1", "=", "im", ".", "period", "or", "0.05", "for", "j", "in", "range", "(", "M", ")", ":", "T2", "=", "imts", "[", "j", "]", ".", "period", "or", "0.05", "if", "i", "==", "j", ":", "cross_matrix", "[", "i", ",", "j", "]", "=", "1", "else", ":", "Tmax", "=", "max", "(", "[", "T1", ",", "T2", "]", ")", "Tmin", "=", "min", "(", "[", "T1", ",", "T2", "]", ")", "II", "=", "1", "if", "Tmin", "<", "0.189", "else", "0", "if", "corr", "==", "'full'", ":", "cross_matrix", "[", "i", ",", "j", "]", "=", "0.99999", "elif", "corr", "==", "'yes'", ":", "cross_matrix", "[", "i", ",", "j", "]", "=", "1", "-", "math", ".", "cos", "(", "math", ".", "pi", "/", "2", "-", "(", "0.359", "+", "0.163", "*", "II", "*", "math", ".", "log", "(", "Tmin", "/", "0.189", ")", ")", "*", "math", ".", "log", "(", "Tmax", "/", "Tmin", ")", ")", "return", "cross_matrix"], "docstring": ":param imts: M intensity measure types\n    :param corr: 'yes', 'no' or 'full'\n    :returns: an array of shape (M, M)", "docstring_tokens": [":", "param", "imts", ":", "M", "intensity", "measure", "types", ":", "param", "corr", ":", "yes", "no", "or", "full", ":", "returns", ":", "an", "array", "of", "shape", "(", "M", "M", ")"], "sha": "8294553a0b8aba33fd96437a35065d03547d0040", "url": "https://github.com/gem/oq-engine/blob/8294553a0b8aba33fd96437a35065d03547d0040/openquake/hazardlib/shakemap.py#L189-L217", "partition": "train"}
{"repo": "aaugustin/websockets", "path": "src/websockets/extensions/base.py", "func_name": "ServerExtensionFactory.process_request_params", "original_string": "def process_request_params(\n        self,\n        params: Sequence[ExtensionParameter],\n        accepted_extensions: Sequence[Extension],\n    ) -> Tuple[List[ExtensionParameter], Extension]:\n        \"\"\"\n        Process request parameters received from the client.\n\n        ``params`` is a list of (name, value) pairs.\n\n        ``accepted_extensions`` is a list of previously accepted extensions.\n\n        To accept the offer, return a 2-uple containing:\n\n        - response parameters: a list of (name, value) pairs\n        - an extension: an instance of a subclass of :class:`Extension`\n\n        To reject the offer, raise\n        :exc:`~websockets.exceptions.NegotiationError`.\n\n        \"\"\"", "language": "python", "code": "def process_request_params(\n        self,\n        params: Sequence[ExtensionParameter],\n        accepted_extensions: Sequence[Extension],\n    ) -> Tuple[List[ExtensionParameter], Extension]:\n        \"\"\"\n        Process request parameters received from the client.\n\n        ``params`` is a list of (name, value) pairs.\n\n        ``accepted_extensions`` is a list of previously accepted extensions.\n\n        To accept the offer, return a 2-uple containing:\n\n        - response parameters: a list of (name, value) pairs\n        - an extension: an instance of a subclass of :class:`Extension`\n\n        To reject the offer, raise\n        :exc:`~websockets.exceptions.NegotiationError`.\n\n        \"\"\"", "code_tokens": ["def", "process_request_params", "(", "self", ",", "params", ":", "Sequence", "[", "ExtensionParameter", "]", ",", "accepted_extensions", ":", "Sequence", "[", "Extension", "]", ",", ")", "->", "Tuple", "[", "List", "[", "ExtensionParameter", "]", ",", "Extension", "]", ":"], "docstring": "Process request parameters received from the client.\n\n        ``params`` is a list of (name, value) pairs.\n\n        ``accepted_extensions`` is a list of previously accepted extensions.\n\n        To accept the offer, return a 2-uple containing:\n\n        - response parameters: a list of (name, value) pairs\n        - an extension: an instance of a subclass of :class:`Extension`\n\n        To reject the offer, raise\n        :exc:`~websockets.exceptions.NegotiationError`.", "docstring_tokens": ["Process", "request", "parameters", "received", "from", "the", "client", "."], "sha": "17b3f47549b6f752a1be07fa1ba3037cb59c7d56", "url": "https://github.com/aaugustin/websockets/blob/17b3f47549b6f752a1be07fa1ba3037cb59c7d56/src/websockets/extensions/base.py#L104-L124", "partition": "train"}
{"repo": "nameko/nameko", "path": "nameko/runners.py", "func_name": "run_services", "original_string": "def run_services(config, *services, **kwargs):\n    \"\"\" Serves a number of services for a contextual block.\n    The caller can specify a number of service classes then serve them either\n    stopping (default) or killing them on exiting the contextual block.\n\n\n    Example::\n\n        with run_services(config, Foobar, Spam) as runner:\n            # interact with services and stop them on exiting the block\n\n        # services stopped\n\n\n    Additional configuration available to :class:``ServiceRunner`` instances\n    can be specified through keyword arguments::\n\n        with run_services(config, Foobar, Spam, kill_on_exit=True):\n            # interact with services\n\n        # services killed\n\n    :Parameters:\n        config : dict\n            Configuration to instantiate the service containers with\n        services : service definitions\n            Services to be served for the contextual block\n        kill_on_exit : bool (default=False)\n            If ``True``, run ``kill()`` on the service containers when exiting\n            the contextual block. Otherwise ``stop()`` will be called on the\n            service containers on exiting the block.\n\n    :Returns: The configured :class:`ServiceRunner` instance\n\n    \"\"\"\n    kill_on_exit = kwargs.pop('kill_on_exit', False)\n\n    runner = ServiceRunner(config)\n    for service in services:\n        runner.add_service(service)\n\n    runner.start()\n\n    yield runner\n\n    if kill_on_exit:\n        runner.kill()\n    else:\n        runner.stop()", "language": "python", "code": "def run_services(config, *services, **kwargs):\n    \"\"\" Serves a number of services for a contextual block.\n    The caller can specify a number of service classes then serve them either\n    stopping (default) or killing them on exiting the contextual block.\n\n\n    Example::\n\n        with run_services(config, Foobar, Spam) as runner:\n            # interact with services and stop them on exiting the block\n\n        # services stopped\n\n\n    Additional configuration available to :class:``ServiceRunner`` instances\n    can be specified through keyword arguments::\n\n        with run_services(config, Foobar, Spam, kill_on_exit=True):\n            # interact with services\n\n        # services killed\n\n    :Parameters:\n        config : dict\n            Configuration to instantiate the service containers with\n        services : service definitions\n            Services to be served for the contextual block\n        kill_on_exit : bool (default=False)\n            If ``True``, run ``kill()`` on the service containers when exiting\n            the contextual block. Otherwise ``stop()`` will be called on the\n            service containers on exiting the block.\n\n    :Returns: The configured :class:`ServiceRunner` instance\n\n    \"\"\"\n    kill_on_exit = kwargs.pop('kill_on_exit', False)\n\n    runner = ServiceRunner(config)\n    for service in services:\n        runner.add_service(service)\n\n    runner.start()\n\n    yield runner\n\n    if kill_on_exit:\n        runner.kill()\n    else:\n        runner.stop()", "code_tokens": ["def", "run_services", "(", "config", ",", "*", "services", ",", "*", "*", "kwargs", ")", ":", "kill_on_exit", "=", "kwargs", ".", "pop", "(", "'kill_on_exit'", ",", "False", ")", "runner", "=", "ServiceRunner", "(", "config", ")", "for", "service", "in", "services", ":", "runner", ".", "add_service", "(", "service", ")", "runner", ".", "start", "(", ")", "yield", "runner", "if", "kill_on_exit", ":", "runner", ".", "kill", "(", ")", "else", ":", "runner", ".", "stop", "(", ")"], "docstring": "Serves a number of services for a contextual block.\n    The caller can specify a number of service classes then serve them either\n    stopping (default) or killing them on exiting the contextual block.\n\n\n    Example::\n\n        with run_services(config, Foobar, Spam) as runner:\n            # interact with services and stop them on exiting the block\n\n        # services stopped\n\n\n    Additional configuration available to :class:``ServiceRunner`` instances\n    can be specified through keyword arguments::\n\n        with run_services(config, Foobar, Spam, kill_on_exit=True):\n            # interact with services\n\n        # services killed\n\n    :Parameters:\n        config : dict\n            Configuration to instantiate the service containers with\n        services : service definitions\n            Services to be served for the contextual block\n        kill_on_exit : bool (default=False)\n            If ``True``, run ``kill()`` on the service containers when exiting\n            the contextual block. Otherwise ``stop()`` will be called on the\n            service containers on exiting the block.\n\n    :Returns: The configured :class:`ServiceRunner` instance", "docstring_tokens": ["Serves", "a", "number", "of", "services", "for", "a", "contextual", "block", ".", "The", "caller", "can", "specify", "a", "number", "of", "service", "classes", "then", "serve", "them", "either", "stopping", "(", "default", ")", "or", "killing", "them", "on", "exiting", "the", "contextual", "block", "."], "sha": "88d7e5211de4fcc1c34cd7f84d7c77f0619c5f5d", "url": "https://github.com/nameko/nameko/blob/88d7e5211de4fcc1c34cd7f84d7c77f0619c5f5d/nameko/runners.py#L105-L153", "partition": "train"}
{"repo": "MechanicalSoup/MechanicalSoup", "path": "setup.py", "func_name": "read", "original_string": "def read(fname, URL, URLImage):\n    \"\"\"Read the content of a file.\"\"\"\n    readme = open(path.join(path.dirname(__file__), fname)).read()\n    if hasattr(readme, 'decode'):\n        # In Python 3, turn bytes into str.\n        readme = readme.decode('utf8')\n    # turn relative links into absolute ones\n    readme = re.sub(r'`<([^>]*)>`__',\n                    r'`\\1 <' + URL + r\"/blob/master/\\1>`__\",\n                    readme)\n    readme = re.sub(r\"\\.\\. image:: /\", \".. image:: \" + URLImage + \"/\", readme)\n\n    return readme", "language": "python", "code": "def read(fname, URL, URLImage):\n    \"\"\"Read the content of a file.\"\"\"\n    readme = open(path.join(path.dirname(__file__), fname)).read()\n    if hasattr(readme, 'decode'):\n        # In Python 3, turn bytes into str.\n        readme = readme.decode('utf8')\n    # turn relative links into absolute ones\n    readme = re.sub(r'`<([^>]*)>`__',\n                    r'`\\1 <' + URL + r\"/blob/master/\\1>`__\",\n                    readme)\n    readme = re.sub(r\"\\.\\. image:: /\", \".. image:: \" + URLImage + \"/\", readme)\n\n    return readme", "code_tokens": ["def", "read", "(", "fname", ",", "URL", ",", "URLImage", ")", ":", "readme", "=", "open", "(", "path", ".", "join", "(", "path", ".", "dirname", "(", "__file__", ")", ",", "fname", ")", ")", ".", "read", "(", ")", "if", "hasattr", "(", "readme", ",", "'decode'", ")", ":", "# In Python 3, turn bytes into str.", "readme", "=", "readme", ".", "decode", "(", "'utf8'", ")", "# turn relative links into absolute ones", "readme", "=", "re", ".", "sub", "(", "r'`<([^>]*)>`__'", ",", "r'`\\1 <'", "+", "URL", "+", "r\"/blob/master/\\1>`__\"", ",", "readme", ")", "readme", "=", "re", ".", "sub", "(", "r\"\\.\\. image:: /\"", ",", "\".. image:: \"", "+", "URLImage", "+", "\"/\"", ",", "readme", ")", "return", "readme"], "docstring": "Read the content of a file.", "docstring_tokens": ["Read", "the", "content", "of", "a", "file", "."], "sha": "027a270febf5bcda6a75db60ea9838d631370f4b", "url": "https://github.com/MechanicalSoup/MechanicalSoup/blob/027a270febf5bcda6a75db60ea9838d631370f4b/setup.py#L15-L27", "partition": "train"}
{"repo": "DmitryUlyanov/Multicore-TSNE", "path": "tsne-embedding.py", "func_name": "imscatter", "original_string": "def imscatter(images, positions):\n    '''\n        Creates a scatter plot, where each plot is shown by corresponding image\n    '''\n    positions = np.array(positions)\n\n    bottoms = positions[:, 1] - np.array([im.shape[1] / 2.0 for im in images])\n    tops = bottoms + np.array([im.shape[1] for im in images])\n\n    lefts = positions[:, 0] - np.array([im.shape[0] / 2.0 for im in images])\n    rigths = lefts + np.array([im.shape[0] for im in images])\n\n    most_bottom = int(np.floor(bottoms.min()))\n    most_top = int(np.ceil(tops.max()))\n\n    most_left = int(np.floor(lefts.min()))\n    most_right = int(np.ceil(rigths.max()))\n\n    scatter_image = np.zeros(\n        [most_right - most_left, most_top - most_bottom, 3], dtype=imgs[0].dtype)\n\n    # shift, now all from zero\n    positions -= [most_left, most_bottom]\n\n    for im, pos in zip(images, positions):\n\n        xl = int(pos[0] - im.shape[0] / 2)\n        xr = xl + im.shape[0]\n\n        yb = int(pos[1] - im.shape[1] / 2)\n        yt = yb + im.shape[1]\n\n        scatter_image[xl:xr, yb:yt, :] = im\n    return scatter_image", "language": "python", "code": "def imscatter(images, positions):\n    '''\n        Creates a scatter plot, where each plot is shown by corresponding image\n    '''\n    positions = np.array(positions)\n\n    bottoms = positions[:, 1] - np.array([im.shape[1] / 2.0 for im in images])\n    tops = bottoms + np.array([im.shape[1] for im in images])\n\n    lefts = positions[:, 0] - np.array([im.shape[0] / 2.0 for im in images])\n    rigths = lefts + np.array([im.shape[0] for im in images])\n\n    most_bottom = int(np.floor(bottoms.min()))\n    most_top = int(np.ceil(tops.max()))\n\n    most_left = int(np.floor(lefts.min()))\n    most_right = int(np.ceil(rigths.max()))\n\n    scatter_image = np.zeros(\n        [most_right - most_left, most_top - most_bottom, 3], dtype=imgs[0].dtype)\n\n    # shift, now all from zero\n    positions -= [most_left, most_bottom]\n\n    for im, pos in zip(images, positions):\n\n        xl = int(pos[0] - im.shape[0] / 2)\n        xr = xl + im.shape[0]\n\n        yb = int(pos[1] - im.shape[1] / 2)\n        yt = yb + im.shape[1]\n\n        scatter_image[xl:xr, yb:yt, :] = im\n    return scatter_image", "code_tokens": ["def", "imscatter", "(", "images", ",", "positions", ")", ":", "positions", "=", "np", ".", "array", "(", "positions", ")", "bottoms", "=", "positions", "[", ":", ",", "1", "]", "-", "np", ".", "array", "(", "[", "im", ".", "shape", "[", "1", "]", "/", "2.0", "for", "im", "in", "images", "]", ")", "tops", "=", "bottoms", "+", "np", ".", "array", "(", "[", "im", ".", "shape", "[", "1", "]", "for", "im", "in", "images", "]", ")", "lefts", "=", "positions", "[", ":", ",", "0", "]", "-", "np", ".", "array", "(", "[", "im", ".", "shape", "[", "0", "]", "/", "2.0", "for", "im", "in", "images", "]", ")", "rigths", "=", "lefts", "+", "np", ".", "array", "(", "[", "im", ".", "shape", "[", "0", "]", "for", "im", "in", "images", "]", ")", "most_bottom", "=", "int", "(", "np", ".", "floor", "(", "bottoms", ".", "min", "(", ")", ")", ")", "most_top", "=", "int", "(", "np", ".", "ceil", "(", "tops", ".", "max", "(", ")", ")", ")", "most_left", "=", "int", "(", "np", ".", "floor", "(", "lefts", ".", "min", "(", ")", ")", ")", "most_right", "=", "int", "(", "np", ".", "ceil", "(", "rigths", ".", "max", "(", ")", ")", ")", "scatter_image", "=", "np", ".", "zeros", "(", "[", "most_right", "-", "most_left", ",", "most_top", "-", "most_bottom", ",", "3", "]", ",", "dtype", "=", "imgs", "[", "0", "]", ".", "dtype", ")", "# shift, now all from zero", "positions", "-=", "[", "most_left", ",", "most_bottom", "]", "for", "im", ",", "pos", "in", "zip", "(", "images", ",", "positions", ")", ":", "xl", "=", "int", "(", "pos", "[", "0", "]", "-", "im", ".", "shape", "[", "0", "]", "/", "2", ")", "xr", "=", "xl", "+", "im", ".", "shape", "[", "0", "]", "yb", "=", "int", "(", "pos", "[", "1", "]", "-", "im", ".", "shape", "[", "1", "]", "/", "2", ")", "yt", "=", "yb", "+", "im", ".", "shape", "[", "1", "]", "scatter_image", "[", "xl", ":", "xr", ",", "yb", ":", "yt", ",", ":", "]", "=", "im", "return", "scatter_image"], "docstring": "Creates a scatter plot, where each plot is shown by corresponding image", "docstring_tokens": ["Creates", "a", "scatter", "plot", "where", "each", "plot", "is", "shown", "by", "corresponding", "image"], "sha": "62dedde52469f3a0aeb22fdd7bce2538f17f77ef", "url": "https://github.com/DmitryUlyanov/Multicore-TSNE/blob/62dedde52469f3a0aeb22fdd7bce2538f17f77ef/tsne-embedding.py#L9-L42", "partition": "train"}
{"repo": "Blueqat/Blueqat", "path": "blueqat/opt.py", "func_name": "pauli", "original_string": "def pauli(qubo):\n\t\"\"\"\n\tConvert to pauli operators of universal gate model.\n\tRequires blueqat.\n\t\"\"\"\n\tfrom blueqat.pauli import qubo_bit\n\th = 0.0\n\tassert all(len(q) == len(qubo) for q in qubo)\n\tfor i in range(len(qubo)):\n\t\th += qubo_bit(i) * qubo[i][i]\n\t\tfor j in range(i + 1, len(qubo)):\n\t\t\th += qubo_bit(i)*qubo_bit(j) * (qubo[i][j] + qubo[j][i])\n\treturn h", "language": "python", "code": "def pauli(qubo):\n\t\"\"\"\n\tConvert to pauli operators of universal gate model.\n\tRequires blueqat.\n\t\"\"\"\n\tfrom blueqat.pauli import qubo_bit\n\th = 0.0\n\tassert all(len(q) == len(qubo) for q in qubo)\n\tfor i in range(len(qubo)):\n\t\th += qubo_bit(i) * qubo[i][i]\n\t\tfor j in range(i + 1, len(qubo)):\n\t\t\th += qubo_bit(i)*qubo_bit(j) * (qubo[i][j] + qubo[j][i])\n\treturn h", "code_tokens": ["def", "pauli", "(", "qubo", ")", ":", "from", "blueqat", ".", "pauli", "import", "qubo_bit", "h", "=", "0.0", "assert", "all", "(", "len", "(", "q", ")", "==", "len", "(", "qubo", ")", "for", "q", "in", "qubo", ")", "for", "i", "in", "range", "(", "len", "(", "qubo", ")", ")", ":", "h", "+=", "qubo_bit", "(", "i", ")", "*", "qubo", "[", "i", "]", "[", "i", "]", "for", "j", "in", "range", "(", "i", "+", "1", ",", "len", "(", "qubo", ")", ")", ":", "h", "+=", "qubo_bit", "(", "i", ")", "*", "qubo_bit", "(", "j", ")", "*", "(", "qubo", "[", "i", "]", "[", "j", "]", "+", "qubo", "[", "j", "]", "[", "i", "]", ")", "return", "h"], "docstring": "Convert to pauli operators of universal gate model.\n\tRequires blueqat.", "docstring_tokens": ["Convert", "to", "pauli", "operators", "of", "universal", "gate", "model", ".", "Requires", "blueqat", "."], "sha": "2ac8592c79e7acf4f385d982af82fbd68dafa5cc", "url": "https://github.com/Blueqat/Blueqat/blob/2ac8592c79e7acf4f385d982af82fbd68dafa5cc/blueqat/opt.py#L18-L30", "partition": "train"}
{"repo": "dddomodossola/remi", "path": "examples/examples_from_contributors/remi_ext.py", "func_name": "MultiRowSelectionTable.on_table_row_click", "original_string": "def on_table_row_click(self, row, item):\n        ''' Highlight selected row(s)\n            and put the result of a muti_row selection\n            in the list \"self.selected_row_list\".\n        '''\n        if not self.multi_selection_enabled:\n            self.remove_selection()\n        if row not in self.selected_row_list:\n            self.selected_row_list.append(row)\n            row.style['outline'] = \"2px dotted blue\"\n        return (row, item)", "language": "python", "code": "def on_table_row_click(self, row, item):\n        ''' Highlight selected row(s)\n            and put the result of a muti_row selection\n            in the list \"self.selected_row_list\".\n        '''\n        if not self.multi_selection_enabled:\n            self.remove_selection()\n        if row not in self.selected_row_list:\n            self.selected_row_list.append(row)\n            row.style['outline'] = \"2px dotted blue\"\n        return (row, item)", "code_tokens": ["def", "on_table_row_click", "(", "self", ",", "row", ",", "item", ")", ":", "if", "not", "self", ".", "multi_selection_enabled", ":", "self", ".", "remove_selection", "(", ")", "if", "row", "not", "in", "self", ".", "selected_row_list", ":", "self", ".", "selected_row_list", ".", "append", "(", "row", ")", "row", ".", "style", "[", "'outline'", "]", "=", "\"2px dotted blue\"", "return", "(", "row", ",", "item", ")"], "docstring": "Highlight selected row(s)\n            and put the result of a muti_row selection\n            in the list \"self.selected_row_list\".", "docstring_tokens": ["Highlight", "selected", "row", "(", "s", ")", "and", "put", "the", "result", "of", "a", "muti_row", "selection", "in", "the", "list", "self", ".", "selected_row_list", "."], "sha": "85206f62220662bb7ecd471042268def71ccad28", "url": "https://github.com/dddomodossola/remi/blob/85206f62220662bb7ecd471042268def71ccad28/examples/examples_from_contributors/remi_ext.py#L47-L57", "partition": "train"}
{"repo": "RaRe-Technologies/smart_open", "path": "smart_open/doctools.py", "func_name": "extract_kwargs", "original_string": "def extract_kwargs(docstring):\n    \"\"\"Extract keyword argument documentation from a function's docstring.\n\n    Parameters\n    ----------\n    docstring: str\n        The docstring to extract keyword arguments from.\n\n    Returns\n    -------\n    list of (str, str, list str)\n\n    str\n        The name of the keyword argument.\n    str\n        Its type.\n    str\n        Its documentation as a list of lines.\n\n    Notes\n    -----\n    The implementation is rather fragile.  It expects the following:\n\n    1. The parameters are under an underlined Parameters section\n    2. Keyword parameters have the literal \", optional\" after the type\n    3. Names and types are not indented\n    4. Descriptions are indented with 4 spaces\n    5. The Parameters section ends with an empty line.\n\n    Examples\n    --------\n\n    >>> docstring = '''The foo function.\n    ... Parameters\n    ... ----------\n    ... bar: str, optional\n    ...     This parameter is the bar.\n    ... baz: int, optional\n    ...     This parameter is the baz.\n    ...\n    ... '''\n    >>> kwargs = extract_kwargs(docstring)\n    >>> kwargs[0]\n    ('bar', 'str, optional', ['This parameter is the bar.'])\n\n    \"\"\"\n    lines = inspect.cleandoc(docstring).split('\\n')\n    retval = []\n\n    #\n    # 1. Find the underlined 'Parameters' section\n    # 2. Once there, continue parsing parameters until we hit an empty line\n    #\n    while lines[0] != 'Parameters':\n        lines.pop(0)\n    lines.pop(0)\n    lines.pop(0)\n\n    while lines and lines[0]:\n        name, type_ = lines.pop(0).split(':', 1)\n        description = []\n        while lines and lines[0].startswith('    '):\n            description.append(lines.pop(0).strip())\n        if 'optional' in type_:\n            retval.append((name.strip(), type_.strip(), description))\n\n    return retval", "language": "python", "code": "def extract_kwargs(docstring):\n    \"\"\"Extract keyword argument documentation from a function's docstring.\n\n    Parameters\n    ----------\n    docstring: str\n        The docstring to extract keyword arguments from.\n\n    Returns\n    -------\n    list of (str, str, list str)\n\n    str\n        The name of the keyword argument.\n    str\n        Its type.\n    str\n        Its documentation as a list of lines.\n\n    Notes\n    -----\n    The implementation is rather fragile.  It expects the following:\n\n    1. The parameters are under an underlined Parameters section\n    2. Keyword parameters have the literal \", optional\" after the type\n    3. Names and types are not indented\n    4. Descriptions are indented with 4 spaces\n    5. The Parameters section ends with an empty line.\n\n    Examples\n    --------\n\n    >>> docstring = '''The foo function.\n    ... Parameters\n    ... ----------\n    ... bar: str, optional\n    ...     This parameter is the bar.\n    ... baz: int, optional\n    ...     This parameter is the baz.\n    ...\n    ... '''\n    >>> kwargs = extract_kwargs(docstring)\n    >>> kwargs[0]\n    ('bar', 'str, optional', ['This parameter is the bar.'])\n\n    \"\"\"\n    lines = inspect.cleandoc(docstring).split('\\n')\n    retval = []\n\n    #\n    # 1. Find the underlined 'Parameters' section\n    # 2. Once there, continue parsing parameters until we hit an empty line\n    #\n    while lines[0] != 'Parameters':\n        lines.pop(0)\n    lines.pop(0)\n    lines.pop(0)\n\n    while lines and lines[0]:\n        name, type_ = lines.pop(0).split(':', 1)\n        description = []\n        while lines and lines[0].startswith('    '):\n            description.append(lines.pop(0).strip())\n        if 'optional' in type_:\n            retval.append((name.strip(), type_.strip(), description))\n\n    return retval", "code_tokens": ["def", "extract_kwargs", "(", "docstring", ")", ":", "lines", "=", "inspect", ".", "cleandoc", "(", "docstring", ")", ".", "split", "(", "'\\n'", ")", "retval", "=", "[", "]", "#", "# 1. Find the underlined 'Parameters' section", "# 2. Once there, continue parsing parameters until we hit an empty line", "#", "while", "lines", "[", "0", "]", "!=", "'Parameters'", ":", "lines", ".", "pop", "(", "0", ")", "lines", ".", "pop", "(", "0", ")", "lines", ".", "pop", "(", "0", ")", "while", "lines", "and", "lines", "[", "0", "]", ":", "name", ",", "type_", "=", "lines", ".", "pop", "(", "0", ")", ".", "split", "(", "':'", ",", "1", ")", "description", "=", "[", "]", "while", "lines", "and", "lines", "[", "0", "]", ".", "startswith", "(", "'    '", ")", ":", "description", ".", "append", "(", "lines", ".", "pop", "(", "0", ")", ".", "strip", "(", ")", ")", "if", "'optional'", "in", "type_", ":", "retval", ".", "append", "(", "(", "name", ".", "strip", "(", ")", ",", "type_", ".", "strip", "(", ")", ",", "description", ")", ")", "return", "retval"], "docstring": "Extract keyword argument documentation from a function's docstring.\n\n    Parameters\n    ----------\n    docstring: str\n        The docstring to extract keyword arguments from.\n\n    Returns\n    -------\n    list of (str, str, list str)\n\n    str\n        The name of the keyword argument.\n    str\n        Its type.\n    str\n        Its documentation as a list of lines.\n\n    Notes\n    -----\n    The implementation is rather fragile.  It expects the following:\n\n    1. The parameters are under an underlined Parameters section\n    2. Keyword parameters have the literal \", optional\" after the type\n    3. Names and types are not indented\n    4. Descriptions are indented with 4 spaces\n    5. The Parameters section ends with an empty line.\n\n    Examples\n    --------\n\n    >>> docstring = '''The foo function.\n    ... Parameters\n    ... ----------\n    ... bar: str, optional\n    ...     This parameter is the bar.\n    ... baz: int, optional\n    ...     This parameter is the baz.\n    ...\n    ... '''\n    >>> kwargs = extract_kwargs(docstring)\n    >>> kwargs[0]\n    ('bar', 'str, optional', ['This parameter is the bar.'])", "docstring_tokens": ["Extract", "keyword", "argument", "documentation", "from", "a", "function", "s", "docstring", "."], "sha": "2dc8d60f223fc7b00a2000c56362a7bd6cd0850e", "url": "https://github.com/RaRe-Technologies/smart_open/blob/2dc8d60f223fc7b00a2000c56362a7bd6cd0850e/smart_open/doctools.py#L20-L86", "partition": "train"}
{"repo": "marcgibbons/django-rest-swagger", "path": "rest_framework_swagger/settings.py", "func_name": "reload_settings", "original_string": "def reload_settings(*args, **kwargs):  # pragma: no cover\n    \"\"\"\n    Reloads settings during unit tests if override_settings decorator\n    is used. (Taken from DRF)\n    \"\"\"\n    # pylint: disable=W0603\n    global swagger_settings\n\n    if kwargs['setting'] == 'LOGIN_URL':\n        swagger_settings.LOGIN_URL = kwargs['value']\n    if kwargs['setting'] == 'LOGOUT_URL':\n        swagger_settings.LOGOUT_URL = kwargs['value']\n    if kwargs['setting'] != 'SWAGGER_SETTINGS':\n        return\n\n    swagger_settings = APISettings(\n        kwargs['value'],\n        DEFAULTS,\n        IMPORT_STRINGS\n    )", "language": "python", "code": "def reload_settings(*args, **kwargs):  # pragma: no cover\n    \"\"\"\n    Reloads settings during unit tests if override_settings decorator\n    is used. (Taken from DRF)\n    \"\"\"\n    # pylint: disable=W0603\n    global swagger_settings\n\n    if kwargs['setting'] == 'LOGIN_URL':\n        swagger_settings.LOGIN_URL = kwargs['value']\n    if kwargs['setting'] == 'LOGOUT_URL':\n        swagger_settings.LOGOUT_URL = kwargs['value']\n    if kwargs['setting'] != 'SWAGGER_SETTINGS':\n        return\n\n    swagger_settings = APISettings(\n        kwargs['value'],\n        DEFAULTS,\n        IMPORT_STRINGS\n    )", "code_tokens": ["def", "reload_settings", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "# pragma: no cover", "# pylint: disable=W0603", "global", "swagger_settings", "if", "kwargs", "[", "'setting'", "]", "==", "'LOGIN_URL'", ":", "swagger_settings", ".", "LOGIN_URL", "=", "kwargs", "[", "'value'", "]", "if", "kwargs", "[", "'setting'", "]", "==", "'LOGOUT_URL'", ":", "swagger_settings", ".", "LOGOUT_URL", "=", "kwargs", "[", "'value'", "]", "if", "kwargs", "[", "'setting'", "]", "!=", "'SWAGGER_SETTINGS'", ":", "return", "swagger_settings", "=", "APISettings", "(", "kwargs", "[", "'value'", "]", ",", "DEFAULTS", ",", "IMPORT_STRINGS", ")"], "docstring": "Reloads settings during unit tests if override_settings decorator\n    is used. (Taken from DRF)", "docstring_tokens": ["Reloads", "settings", "during", "unit", "tests", "if", "override_settings", "decorator", "is", "used", ".", "(", "Taken", "from", "DRF", ")"], "sha": "102d22eaefb7898342ba2fb5af5618b9e3a32f1d", "url": "https://github.com/marcgibbons/django-rest-swagger/blob/102d22eaefb7898342ba2fb5af5618b9e3a32f1d/rest_framework_swagger/settings.py#L41-L60", "partition": "train"}
{"repo": "adafruit/Adafruit_Blinka", "path": "src/adafruit_blinka/microcontroller/generic_linux/i2c.py", "func_name": "I2C.scan", "original_string": "def scan(self):\n        \"\"\"Try to read a byte from each address, if you get an OSError it means the device isnt there\"\"\"\n        found = []\n        for addr in range(0,0x80):\n            try:\n                self._i2c_bus.read_byte(addr)\n            except OSError:\n                continue\n            found.append(addr)\n        return found", "language": "python", "code": "def scan(self):\n        \"\"\"Try to read a byte from each address, if you get an OSError it means the device isnt there\"\"\"\n        found = []\n        for addr in range(0,0x80):\n            try:\n                self._i2c_bus.read_byte(addr)\n            except OSError:\n                continue\n            found.append(addr)\n        return found", "code_tokens": ["def", "scan", "(", "self", ")", ":", "found", "=", "[", "]", "for", "addr", "in", "range", "(", "0", ",", "0x80", ")", ":", "try", ":", "self", ".", "_i2c_bus", ".", "read_byte", "(", "addr", ")", "except", "OSError", ":", "continue", "found", ".", "append", "(", "addr", ")", "return", "found"], "docstring": "Try to read a byte from each address, if you get an OSError it means the device isnt there", "docstring_tokens": ["Try", "to", "read", "a", "byte", "from", "each", "address", "if", "you", "get", "an", "OSError", "it", "means", "the", "device", "isnt", "there"], "sha": "b4a2b3bf7d8cc88477027b827bd0a8e9b19588ff", "url": "https://github.com/adafruit/Adafruit_Blinka/blob/b4a2b3bf7d8cc88477027b827bd0a8e9b19588ff/src/adafruit_blinka/microcontroller/generic_linux/i2c.py#L24-L33", "partition": "train"}
{"repo": "jupyter-widgets/ipyleaflet", "path": "ipyleaflet/xarray_ds.py", "func_name": "ds2json", "original_string": "def ds2json(ds, u_var, v_var, lat_dim='latitude', lon_dim='longitude', units=None):\n    \"\"\"\n    Assumes that the velocity components are given on a regular grid\n    (fixed spacing in latitude and longitude).\n\n    Parameters\n    ----------\n    u_var : str\n        Name of the U-component (zonal) variable.\n    v_var : str\n        Name of the V-component (meridional) variable.\n    lat_dim : str, optional\n        Name of the latitude dimension/coordinate\n        (default: 'latitude').\n    lon_dim : str, optional\n        Name of the longitude dimension/coordinate\n        (default: 'longitude').\n    units : str, optional\n        Velocity units (default: try getting units from the\n        'units' attributes of `u_var` and `v_var`).\n    \"\"\"\n    import numpy as np\n    ds = ds.copy()\n    for var_name in (u_var, v_var):\n        var_dims = ds[var_name].dims\n\n        if set(var_dims) != set([lat_dim, lon_dim]):\n            raise ValueError(\n                \"Invalid dimensions for variable '{}' in Dataset: \"\n                \"should include only {}, found {}.\"\n                .format(var_name, (lat_dim, lon_dim), var_dims)\n            )\n\n        # If dataset contains nans replace with 0\n        ds[var_name] = ds[var_name].fillna(0)\n\n    if units is None:\n        u_var_units = ds[u_var].attrs.get('units')\n        v_var_units = ds[v_var].attrs.get('units')\n\n        if u_var_units != v_var_units:\n            raise ValueError(\n                \"Different units found for U-component '{}' and \"\n                \"V-component '{}' variables: '{}' and '{}'\"\n                .format(u_var, v_var, u_var_units, v_var_units))\n\n        units = u_var_units\n\n    if units is None:\n        units = ''\n\n    # Data should be in gaussian grid format (latitudes descending)\n    if np.any(np.diff(ds[lat_dim].values) >= 0):\n        ds = ds.sel(**{lat_dim: slice(None, None, -1)})\n\n    # infer grid specifications (assume a rectangular grid)\n    lat = ds[lat_dim].values\n    lon = ds[lon_dim].values\n\n    lon_left = float(lon.min())\n    lon_right = float(lon.max())\n    lat_lower = float(lat.min())\n    lat_upper = float(lat.max())\n\n    dx = float((lon_right - lon_left) / (lon.size - 1))\n    dy = float((lat_upper - lat_lower) / (lat.size - 1))\n\n    nx = lon.size\n    ny = lat.size\n\n    u_v_spec = ([2, 3],\n                [\"Eastward current\", \"Northward current\"],\n                [u_var, v_var])\n\n    velocity_data = []\n\n    for p_number, p_name, var_name in zip(*u_v_spec):\n        velocity_data.append({\n            \"header\": {\n                \"parameterUnit\": units,\n                \"parameterNumber\": p_number,\n                \"dx\": dx, \"dy\": dy,\n                \"parameterNumberName\": p_name,\n                \"la1\": lat_upper,\n                \"la2\": lat_lower,\n                \"parameterCategory\": 2,\n                \"lo2\": lon_right,\n                \"nx\": nx,\n                \"ny\": ny,\n                \"refTime\": \"2017-02-01 23:00:00\",\n                \"lo1\": lon_left\n                },\n            \"data\": ds[var_name].values.flatten().tolist()\n        })\n\n    return velocity_data", "language": "python", "code": "def ds2json(ds, u_var, v_var, lat_dim='latitude', lon_dim='longitude', units=None):\n    \"\"\"\n    Assumes that the velocity components are given on a regular grid\n    (fixed spacing in latitude and longitude).\n\n    Parameters\n    ----------\n    u_var : str\n        Name of the U-component (zonal) variable.\n    v_var : str\n        Name of the V-component (meridional) variable.\n    lat_dim : str, optional\n        Name of the latitude dimension/coordinate\n        (default: 'latitude').\n    lon_dim : str, optional\n        Name of the longitude dimension/coordinate\n        (default: 'longitude').\n    units : str, optional\n        Velocity units (default: try getting units from the\n        'units' attributes of `u_var` and `v_var`).\n    \"\"\"\n    import numpy as np\n    ds = ds.copy()\n    for var_name in (u_var, v_var):\n        var_dims = ds[var_name].dims\n\n        if set(var_dims) != set([lat_dim, lon_dim]):\n            raise ValueError(\n                \"Invalid dimensions for variable '{}' in Dataset: \"\n                \"should include only {}, found {}.\"\n                .format(var_name, (lat_dim, lon_dim), var_dims)\n            )\n\n        # If dataset contains nans replace with 0\n        ds[var_name] = ds[var_name].fillna(0)\n\n    if units is None:\n        u_var_units = ds[u_var].attrs.get('units')\n        v_var_units = ds[v_var].attrs.get('units')\n\n        if u_var_units != v_var_units:\n            raise ValueError(\n                \"Different units found for U-component '{}' and \"\n                \"V-component '{}' variables: '{}' and '{}'\"\n                .format(u_var, v_var, u_var_units, v_var_units))\n\n        units = u_var_units\n\n    if units is None:\n        units = ''\n\n    # Data should be in gaussian grid format (latitudes descending)\n    if np.any(np.diff(ds[lat_dim].values) >= 0):\n        ds = ds.sel(**{lat_dim: slice(None, None, -1)})\n\n    # infer grid specifications (assume a rectangular grid)\n    lat = ds[lat_dim].values\n    lon = ds[lon_dim].values\n\n    lon_left = float(lon.min())\n    lon_right = float(lon.max())\n    lat_lower = float(lat.min())\n    lat_upper = float(lat.max())\n\n    dx = float((lon_right - lon_left) / (lon.size - 1))\n    dy = float((lat_upper - lat_lower) / (lat.size - 1))\n\n    nx = lon.size\n    ny = lat.size\n\n    u_v_spec = ([2, 3],\n                [\"Eastward current\", \"Northward current\"],\n                [u_var, v_var])\n\n    velocity_data = []\n\n    for p_number, p_name, var_name in zip(*u_v_spec):\n        velocity_data.append({\n            \"header\": {\n                \"parameterUnit\": units,\n                \"parameterNumber\": p_number,\n                \"dx\": dx, \"dy\": dy,\n                \"parameterNumberName\": p_name,\n                \"la1\": lat_upper,\n                \"la2\": lat_lower,\n                \"parameterCategory\": 2,\n                \"lo2\": lon_right,\n                \"nx\": nx,\n                \"ny\": ny,\n                \"refTime\": \"2017-02-01 23:00:00\",\n                \"lo1\": lon_left\n                },\n            \"data\": ds[var_name].values.flatten().tolist()\n        })\n\n    return velocity_data", "code_tokens": ["def", "ds2json", "(", "ds", ",", "u_var", ",", "v_var", ",", "lat_dim", "=", "'latitude'", ",", "lon_dim", "=", "'longitude'", ",", "units", "=", "None", ")", ":", "import", "numpy", "as", "np", "ds", "=", "ds", ".", "copy", "(", ")", "for", "var_name", "in", "(", "u_var", ",", "v_var", ")", ":", "var_dims", "=", "ds", "[", "var_name", "]", ".", "dims", "if", "set", "(", "var_dims", ")", "!=", "set", "(", "[", "lat_dim", ",", "lon_dim", "]", ")", ":", "raise", "ValueError", "(", "\"Invalid dimensions for variable '{}' in Dataset: \"", "\"should include only {}, found {}.\"", ".", "format", "(", "var_name", ",", "(", "lat_dim", ",", "lon_dim", ")", ",", "var_dims", ")", ")", "# If dataset contains nans replace with 0", "ds", "[", "var_name", "]", "=", "ds", "[", "var_name", "]", ".", "fillna", "(", "0", ")", "if", "units", "is", "None", ":", "u_var_units", "=", "ds", "[", "u_var", "]", ".", "attrs", ".", "get", "(", "'units'", ")", "v_var_units", "=", "ds", "[", "v_var", "]", ".", "attrs", ".", "get", "(", "'units'", ")", "if", "u_var_units", "!=", "v_var_units", ":", "raise", "ValueError", "(", "\"Different units found for U-component '{}' and \"", "\"V-component '{}' variables: '{}' and '{}'\"", ".", "format", "(", "u_var", ",", "v_var", ",", "u_var_units", ",", "v_var_units", ")", ")", "units", "=", "u_var_units", "if", "units", "is", "None", ":", "units", "=", "''", "# Data should be in gaussian grid format (latitudes descending)", "if", "np", ".", "any", "(", "np", ".", "diff", "(", "ds", "[", "lat_dim", "]", ".", "values", ")", ">=", "0", ")", ":", "ds", "=", "ds", ".", "sel", "(", "*", "*", "{", "lat_dim", ":", "slice", "(", "None", ",", "None", ",", "-", "1", ")", "}", ")", "# infer grid specifications (assume a rectangular grid)", "lat", "=", "ds", "[", "lat_dim", "]", ".", "values", "lon", "=", "ds", "[", "lon_dim", "]", ".", "values", "lon_left", "=", "float", "(", "lon", ".", "min", "(", ")", ")", "lon_right", "=", "float", "(", "lon", ".", "max", "(", ")", ")", "lat_lower", "=", "float", "(", "lat", ".", "min", "(", ")", ")", "lat_upper", "=", "float", "(", "lat", ".", "max", "(", ")", ")", "dx", "=", "float", "(", "(", "lon_right", "-", "lon_left", ")", "/", "(", "lon", ".", "size", "-", "1", ")", ")", "dy", "=", "float", "(", "(", "lat_upper", "-", "lat_lower", ")", "/", "(", "lat", ".", "size", "-", "1", ")", ")", "nx", "=", "lon", ".", "size", "ny", "=", "lat", ".", "size", "u_v_spec", "=", "(", "[", "2", ",", "3", "]", ",", "[", "\"Eastward current\"", ",", "\"Northward current\"", "]", ",", "[", "u_var", ",", "v_var", "]", ")", "velocity_data", "=", "[", "]", "for", "p_number", ",", "p_name", ",", "var_name", "in", "zip", "(", "*", "u_v_spec", ")", ":", "velocity_data", ".", "append", "(", "{", "\"header\"", ":", "{", "\"parameterUnit\"", ":", "units", ",", "\"parameterNumber\"", ":", "p_number", ",", "\"dx\"", ":", "dx", ",", "\"dy\"", ":", "dy", ",", "\"parameterNumberName\"", ":", "p_name", ",", "\"la1\"", ":", "lat_upper", ",", "\"la2\"", ":", "lat_lower", ",", "\"parameterCategory\"", ":", "2", ",", "\"lo2\"", ":", "lon_right", ",", "\"nx\"", ":", "nx", ",", "\"ny\"", ":", "ny", ",", "\"refTime\"", ":", "\"2017-02-01 23:00:00\"", ",", "\"lo1\"", ":", "lon_left", "}", ",", "\"data\"", ":", "ds", "[", "var_name", "]", ".", "values", ".", "flatten", "(", ")", ".", "tolist", "(", ")", "}", ")", "return", "velocity_data"], "docstring": "Assumes that the velocity components are given on a regular grid\n    (fixed spacing in latitude and longitude).\n\n    Parameters\n    ----------\n    u_var : str\n        Name of the U-component (zonal) variable.\n    v_var : str\n        Name of the V-component (meridional) variable.\n    lat_dim : str, optional\n        Name of the latitude dimension/coordinate\n        (default: 'latitude').\n    lon_dim : str, optional\n        Name of the longitude dimension/coordinate\n        (default: 'longitude').\n    units : str, optional\n        Velocity units (default: try getting units from the\n        'units' attributes of `u_var` and `v_var`).", "docstring_tokens": ["Assumes", "that", "the", "velocity", "components", "are", "given", "on", "a", "regular", "grid", "(", "fixed", "spacing", "in", "latitude", "and", "longitude", ")", "."], "sha": "74488d4699a5663fc28aabf94ebf08d956a30598", "url": "https://github.com/jupyter-widgets/ipyleaflet/blob/74488d4699a5663fc28aabf94ebf08d956a30598/ipyleaflet/xarray_ds.py#L11-L106", "partition": "train"}
{"repo": "mbedmicro/pyOCD", "path": "pyocd/utility/conversion.py", "func_name": "byte_list_to_u32le_list", "original_string": "def byte_list_to_u32le_list(data, pad=0x00):\n    \"\"\"! @brief Convert a list of bytes to a list of 32-bit integers (little endian)\n    \n    If the length of the data list is not a multiple of 4, then the pad value is used\n    for the additional required bytes.\n    \"\"\"\n    res = []\n    for i in range(len(data) // 4):\n        res.append(data[i * 4 + 0] |\n                   data[i * 4 + 1] << 8 |\n                   data[i * 4 + 2] << 16 |\n                   data[i * 4 + 3] << 24)\n    remainder = (len(data) % 4)\n    if remainder != 0:\n        padCount = 4 - remainder\n        res += byte_list_to_u32le_list(list(data[-remainder:]) + [pad] * padCount)\n    return res", "language": "python", "code": "def byte_list_to_u32le_list(data, pad=0x00):\n    \"\"\"! @brief Convert a list of bytes to a list of 32-bit integers (little endian)\n    \n    If the length of the data list is not a multiple of 4, then the pad value is used\n    for the additional required bytes.\n    \"\"\"\n    res = []\n    for i in range(len(data) // 4):\n        res.append(data[i * 4 + 0] |\n                   data[i * 4 + 1] << 8 |\n                   data[i * 4 + 2] << 16 |\n                   data[i * 4 + 3] << 24)\n    remainder = (len(data) % 4)\n    if remainder != 0:\n        padCount = 4 - remainder\n        res += byte_list_to_u32le_list(list(data[-remainder:]) + [pad] * padCount)\n    return res", "code_tokens": ["def", "byte_list_to_u32le_list", "(", "data", ",", "pad", "=", "0x00", ")", ":", "res", "=", "[", "]", "for", "i", "in", "range", "(", "len", "(", "data", ")", "//", "4", ")", ":", "res", ".", "append", "(", "data", "[", "i", "*", "4", "+", "0", "]", "|", "data", "[", "i", "*", "4", "+", "1", "]", "<<", "8", "|", "data", "[", "i", "*", "4", "+", "2", "]", "<<", "16", "|", "data", "[", "i", "*", "4", "+", "3", "]", "<<", "24", ")", "remainder", "=", "(", "len", "(", "data", ")", "%", "4", ")", "if", "remainder", "!=", "0", ":", "padCount", "=", "4", "-", "remainder", "res", "+=", "byte_list_to_u32le_list", "(", "list", "(", "data", "[", "-", "remainder", ":", "]", ")", "+", "[", "pad", "]", "*", "padCount", ")", "return", "res"], "docstring": "! @brief Convert a list of bytes to a list of 32-bit integers (little endian)\n    \n    If the length of the data list is not a multiple of 4, then the pad value is used\n    for the additional required bytes.", "docstring_tokens": ["!"], "sha": "41a174718a9739f3cbe785c2ba21cb7fd1310c6f", "url": "https://github.com/mbedmicro/pyOCD/blob/41a174718a9739f3cbe785c2ba21cb7fd1310c6f/pyocd/utility/conversion.py#L21-L37", "partition": "train"}
{"repo": "ofek/bit", "path": "bit/base32.py", "func_name": "encode", "original_string": "def encode(hrp, witver, witprog):\n    \"\"\"Encode a segwit address.\"\"\"\n    ret = bech32_encode(hrp, [witver] + convertbits(witprog, 8, 5))\n    if decode(ret) == (None, None):\n        return None\n    return ret", "language": "python", "code": "def encode(hrp, witver, witprog):\n    \"\"\"Encode a segwit address.\"\"\"\n    ret = bech32_encode(hrp, [witver] + convertbits(witprog, 8, 5))\n    if decode(ret) == (None, None):\n        return None\n    return ret", "code_tokens": ["def", "encode", "(", "hrp", ",", "witver", ",", "witprog", ")", ":", "ret", "=", "bech32_encode", "(", "hrp", ",", "[", "witver", "]", "+", "convertbits", "(", "witprog", ",", "8", ",", "5", ")", ")", "if", "decode", "(", "ret", ")", "==", "(", "None", ",", "None", ")", ":", "return", "None", "return", "ret"], "docstring": "Encode a segwit address.", "docstring_tokens": ["Encode", "a", "segwit", "address", "."], "sha": "20fc0e7047946c1f28f868008d99d659905c1af6", "url": "https://github.com/ofek/bit/blob/20fc0e7047946c1f28f868008d99d659905c1af6/bit/base32.py#L119-L124", "partition": "train"}
{"repo": "nok/sklearn-porter", "path": "sklearn_porter/estimator/classifier/GaussianNB/__init__.py", "func_name": "GaussianNB.export", "original_string": "def export(self, class_name, method_name, export_data=False,\n               export_dir='.', export_filename='data.json',\n               export_append_checksum=False, **kwargs):\n        \"\"\"\n        Port a trained estimator to the syntax of a chosen programming language.\n\n        Parameters\n        ----------\n        :param class_name : string\n            The name of the class in the returned result.\n        :param method_name : string\n            The name of the method in the returned result.\n        :param export_data : bool, default: False\n            Whether the model data should be saved or not.\n        :param export_dir : string, default: '.' (current directory)\n            The directory where the model data should be saved.\n        :param export_filename : string, default: 'data.json'\n            The filename of the exported model data.\n        :param export_append_checksum : bool, default: False\n            Whether to append the checksum to the filename or not.\n\n        Returns\n        -------\n        :return : string\n            The transpiled algorithm with the defined placeholders.\n        \"\"\"\n        # Arguments:\n        self.class_name = class_name\n        self.method_name = method_name\n\n        # Estimator:\n        est = self.estimator\n\n        self.n_features = len(est.sigma_[0])\n        self.n_classes = len(est.classes_)\n\n        temp_type = self.temp('type')\n        temp_arr = self.temp('arr')\n        temp_arr_ = self.temp('arr[]')\n        temp_arr__ = self.temp('arr[][]')\n\n        # Create class prior probabilities:\n        priors = [temp_type.format(self.repr(c)) for c in est.class_prior_]\n        priors = ', '.join(priors)\n        self.priors = temp_arr_.format(type='double', name='priors',\n                                       values=priors)\n\n        # Create sigmas:\n        sigmas = []\n        for sigma in est.sigma_:\n            tmp = [temp_type.format(self.repr(s)) for s in sigma]\n            tmp = temp_arr.format(', '.join(tmp))\n            sigmas.append(tmp)\n        sigmas = ', '.join(sigmas)\n        self.sigmas = temp_arr__.format(type='double', name='sigmas',\n                                        values=sigmas)\n\n        # Create thetas:\n        thetas = []\n        for theta in est.theta_:\n            tmp = [temp_type.format(self.repr(t)) for t in theta]\n            tmp = temp_arr.format(', '.join(tmp))\n            thetas.append(tmp)\n        thetas = ', '.join(thetas)\n        self.thetas = temp_arr__.format(type='double', name='thetas',\n                                        values=thetas)\n\n        if self.target_method == 'predict':\n            # Exported:\n            if export_data and os.path.isdir(export_dir):\n                self.export_data(export_dir, export_filename,\n                                 export_append_checksum)\n                return self.predict('exported')\n            # Separated:\n            return self.predict('separated')", "language": "python", "code": "def export(self, class_name, method_name, export_data=False,\n               export_dir='.', export_filename='data.json',\n               export_append_checksum=False, **kwargs):\n        \"\"\"\n        Port a trained estimator to the syntax of a chosen programming language.\n\n        Parameters\n        ----------\n        :param class_name : string\n            The name of the class in the returned result.\n        :param method_name : string\n            The name of the method in the returned result.\n        :param export_data : bool, default: False\n            Whether the model data should be saved or not.\n        :param export_dir : string, default: '.' (current directory)\n            The directory where the model data should be saved.\n        :param export_filename : string, default: 'data.json'\n            The filename of the exported model data.\n        :param export_append_checksum : bool, default: False\n            Whether to append the checksum to the filename or not.\n\n        Returns\n        -------\n        :return : string\n            The transpiled algorithm with the defined placeholders.\n        \"\"\"\n        # Arguments:\n        self.class_name = class_name\n        self.method_name = method_name\n\n        # Estimator:\n        est = self.estimator\n\n        self.n_features = len(est.sigma_[0])\n        self.n_classes = len(est.classes_)\n\n        temp_type = self.temp('type')\n        temp_arr = self.temp('arr')\n        temp_arr_ = self.temp('arr[]')\n        temp_arr__ = self.temp('arr[][]')\n\n        # Create class prior probabilities:\n        priors = [temp_type.format(self.repr(c)) for c in est.class_prior_]\n        priors = ', '.join(priors)\n        self.priors = temp_arr_.format(type='double', name='priors',\n                                       values=priors)\n\n        # Create sigmas:\n        sigmas = []\n        for sigma in est.sigma_:\n            tmp = [temp_type.format(self.repr(s)) for s in sigma]\n            tmp = temp_arr.format(', '.join(tmp))\n            sigmas.append(tmp)\n        sigmas = ', '.join(sigmas)\n        self.sigmas = temp_arr__.format(type='double', name='sigmas',\n                                        values=sigmas)\n\n        # Create thetas:\n        thetas = []\n        for theta in est.theta_:\n            tmp = [temp_type.format(self.repr(t)) for t in theta]\n            tmp = temp_arr.format(', '.join(tmp))\n            thetas.append(tmp)\n        thetas = ', '.join(thetas)\n        self.thetas = temp_arr__.format(type='double', name='thetas',\n                                        values=thetas)\n\n        if self.target_method == 'predict':\n            # Exported:\n            if export_data and os.path.isdir(export_dir):\n                self.export_data(export_dir, export_filename,\n                                 export_append_checksum)\n                return self.predict('exported')\n            # Separated:\n            return self.predict('separated')", "code_tokens": ["def", "export", "(", "self", ",", "class_name", ",", "method_name", ",", "export_data", "=", "False", ",", "export_dir", "=", "'.'", ",", "export_filename", "=", "'data.json'", ",", "export_append_checksum", "=", "False", ",", "*", "*", "kwargs", ")", ":", "# Arguments:", "self", ".", "class_name", "=", "class_name", "self", ".", "method_name", "=", "method_name", "# Estimator:", "est", "=", "self", ".", "estimator", "self", ".", "n_features", "=", "len", "(", "est", ".", "sigma_", "[", "0", "]", ")", "self", ".", "n_classes", "=", "len", "(", "est", ".", "classes_", ")", "temp_type", "=", "self", ".", "temp", "(", "'type'", ")", "temp_arr", "=", "self", ".", "temp", "(", "'arr'", ")", "temp_arr_", "=", "self", ".", "temp", "(", "'arr[]'", ")", "temp_arr__", "=", "self", ".", "temp", "(", "'arr[][]'", ")", "# Create class prior probabilities:", "priors", "=", "[", "temp_type", ".", "format", "(", "self", ".", "repr", "(", "c", ")", ")", "for", "c", "in", "est", ".", "class_prior_", "]", "priors", "=", "', '", ".", "join", "(", "priors", ")", "self", ".", "priors", "=", "temp_arr_", ".", "format", "(", "type", "=", "'double'", ",", "name", "=", "'priors'", ",", "values", "=", "priors", ")", "# Create sigmas:", "sigmas", "=", "[", "]", "for", "sigma", "in", "est", ".", "sigma_", ":", "tmp", "=", "[", "temp_type", ".", "format", "(", "self", ".", "repr", "(", "s", ")", ")", "for", "s", "in", "sigma", "]", "tmp", "=", "temp_arr", ".", "format", "(", "', '", ".", "join", "(", "tmp", ")", ")", "sigmas", ".", "append", "(", "tmp", ")", "sigmas", "=", "', '", ".", "join", "(", "sigmas", ")", "self", ".", "sigmas", "=", "temp_arr__", ".", "format", "(", "type", "=", "'double'", ",", "name", "=", "'sigmas'", ",", "values", "=", "sigmas", ")", "# Create thetas:", "thetas", "=", "[", "]", "for", "theta", "in", "est", ".", "theta_", ":", "tmp", "=", "[", "temp_type", ".", "format", "(", "self", ".", "repr", "(", "t", ")", ")", "for", "t", "in", "theta", "]", "tmp", "=", "temp_arr", ".", "format", "(", "', '", ".", "join", "(", "tmp", ")", ")", "thetas", ".", "append", "(", "tmp", ")", "thetas", "=", "', '", ".", "join", "(", "thetas", ")", "self", ".", "thetas", "=", "temp_arr__", ".", "format", "(", "type", "=", "'double'", ",", "name", "=", "'thetas'", ",", "values", "=", "thetas", ")", "if", "self", ".", "target_method", "==", "'predict'", ":", "# Exported:", "if", "export_data", "and", "os", ".", "path", ".", "isdir", "(", "export_dir", ")", ":", "self", ".", "export_data", "(", "export_dir", ",", "export_filename", ",", "export_append_checksum", ")", "return", "self", ".", "predict", "(", "'exported'", ")", "# Separated:", "return", "self", ".", "predict", "(", "'separated'", ")"], "docstring": "Port a trained estimator to the syntax of a chosen programming language.\n\n        Parameters\n        ----------\n        :param class_name : string\n            The name of the class in the returned result.\n        :param method_name : string\n            The name of the method in the returned result.\n        :param export_data : bool, default: False\n            Whether the model data should be saved or not.\n        :param export_dir : string, default: '.' (current directory)\n            The directory where the model data should be saved.\n        :param export_filename : string, default: 'data.json'\n            The filename of the exported model data.\n        :param export_append_checksum : bool, default: False\n            Whether to append the checksum to the filename or not.\n\n        Returns\n        -------\n        :return : string\n            The transpiled algorithm with the defined placeholders.", "docstring_tokens": ["Port", "a", "trained", "estimator", "to", "the", "syntax", "of", "a", "chosen", "programming", "language", "."], "sha": "04673f768310bde31f9747a68a5e070592441ef2", "url": "https://github.com/nok/sklearn-porter/blob/04673f768310bde31f9747a68a5e070592441ef2/sklearn_porter/estimator/classifier/GaussianNB/__init__.py#L61-L135", "partition": "train"}
{"repo": "nok/sklearn-porter", "path": "sklearn_porter/estimator/classifier/GaussianNB/__init__.py", "func_name": "GaussianNB.export_data", "original_string": "def export_data(self, directory, filename, with_md5_hash=False):\n        \"\"\"\n        Save model data in a JSON file.\n\n        Parameters\n        ----------\n        :param directory : string\n            The directory.\n        :param filename : string\n            The filename.\n        :param with_md5_hash : bool\n            Whether to append the checksum to the filename or not.\n        \"\"\"\n        model_data = {\n            'priors': self.estimator.class_prior_.tolist(),\n            'sigmas': self.estimator.sigma_.tolist(),\n            'thetas': self.estimator.theta_.tolist()\n        }\n        encoder.FLOAT_REPR = lambda o: self.repr(o)\n        json_data = dumps(model_data, sort_keys=True)\n        if with_md5_hash:\n            import hashlib\n            json_hash = hashlib.md5(json_data).hexdigest()\n            filename = filename.split('.json')[0] + '_' + json_hash + '.json'\n        path = os.path.join(directory, filename)\n        with open(path, 'w') as fp:\n            fp.write(json_data)", "language": "python", "code": "def export_data(self, directory, filename, with_md5_hash=False):\n        \"\"\"\n        Save model data in a JSON file.\n\n        Parameters\n        ----------\n        :param directory : string\n            The directory.\n        :param filename : string\n            The filename.\n        :param with_md5_hash : bool\n            Whether to append the checksum to the filename or not.\n        \"\"\"\n        model_data = {\n            'priors': self.estimator.class_prior_.tolist(),\n            'sigmas': self.estimator.sigma_.tolist(),\n            'thetas': self.estimator.theta_.tolist()\n        }\n        encoder.FLOAT_REPR = lambda o: self.repr(o)\n        json_data = dumps(model_data, sort_keys=True)\n        if with_md5_hash:\n            import hashlib\n            json_hash = hashlib.md5(json_data).hexdigest()\n            filename = filename.split('.json')[0] + '_' + json_hash + '.json'\n        path = os.path.join(directory, filename)\n        with open(path, 'w') as fp:\n            fp.write(json_data)", "code_tokens": ["def", "export_data", "(", "self", ",", "directory", ",", "filename", ",", "with_md5_hash", "=", "False", ")", ":", "model_data", "=", "{", "'priors'", ":", "self", ".", "estimator", ".", "class_prior_", ".", "tolist", "(", ")", ",", "'sigmas'", ":", "self", ".", "estimator", ".", "sigma_", ".", "tolist", "(", ")", ",", "'thetas'", ":", "self", ".", "estimator", ".", "theta_", ".", "tolist", "(", ")", "}", "encoder", ".", "FLOAT_REPR", "=", "lambda", "o", ":", "self", ".", "repr", "(", "o", ")", "json_data", "=", "dumps", "(", "model_data", ",", "sort_keys", "=", "True", ")", "if", "with_md5_hash", ":", "import", "hashlib", "json_hash", "=", "hashlib", ".", "md5", "(", "json_data", ")", ".", "hexdigest", "(", ")", "filename", "=", "filename", ".", "split", "(", "'.json'", ")", "[", "0", "]", "+", "'_'", "+", "json_hash", "+", "'.json'", "path", "=", "os", ".", "path", ".", "join", "(", "directory", ",", "filename", ")", "with", "open", "(", "path", ",", "'w'", ")", "as", "fp", ":", "fp", ".", "write", "(", "json_data", ")"], "docstring": "Save model data in a JSON file.\n\n        Parameters\n        ----------\n        :param directory : string\n            The directory.\n        :param filename : string\n            The filename.\n        :param with_md5_hash : bool\n            Whether to append the checksum to the filename or not.", "docstring_tokens": ["Save", "model", "data", "in", "a", "JSON", "file", "."], "sha": "04673f768310bde31f9747a68a5e070592441ef2", "url": "https://github.com/nok/sklearn-porter/blob/04673f768310bde31f9747a68a5e070592441ef2/sklearn_porter/estimator/classifier/GaussianNB/__init__.py#L160-L186", "partition": "train"}
{"repo": "daviddrysdale/python-phonenumbers", "path": "python/phonenumbers/phonenumberutil.py", "func_name": "region_code_for_number", "original_string": "def region_code_for_number(numobj):\n    \"\"\"Returns the region where a phone number is from.\n\n    This could be used for geocoding at the region level. Only guarantees\n    correct results for valid, full numbers (not short-codes, or invalid\n    numbers).\n\n    Arguments:\n    numobj -- The phone number object whose origin we want to know\n\n    Returns the region where the phone number is from, or None if no region\n    matches this calling code.\n\n    \"\"\"\n    country_code = numobj.country_code\n    regions = COUNTRY_CODE_TO_REGION_CODE.get(country_code, None)\n    if regions is None:\n        return None\n\n    if len(regions) == 1:\n        return regions[0]\n    else:\n        return _region_code_for_number_from_list(numobj, regions)", "language": "python", "code": "def region_code_for_number(numobj):\n    \"\"\"Returns the region where a phone number is from.\n\n    This could be used for geocoding at the region level. Only guarantees\n    correct results for valid, full numbers (not short-codes, or invalid\n    numbers).\n\n    Arguments:\n    numobj -- The phone number object whose origin we want to know\n\n    Returns the region where the phone number is from, or None if no region\n    matches this calling code.\n\n    \"\"\"\n    country_code = numobj.country_code\n    regions = COUNTRY_CODE_TO_REGION_CODE.get(country_code, None)\n    if regions is None:\n        return None\n\n    if len(regions) == 1:\n        return regions[0]\n    else:\n        return _region_code_for_number_from_list(numobj, regions)", "code_tokens": ["def", "region_code_for_number", "(", "numobj", ")", ":", "country_code", "=", "numobj", ".", "country_code", "regions", "=", "COUNTRY_CODE_TO_REGION_CODE", ".", "get", "(", "country_code", ",", "None", ")", "if", "regions", "is", "None", ":", "return", "None", "if", "len", "(", "regions", ")", "==", "1", ":", "return", "regions", "[", "0", "]", "else", ":", "return", "_region_code_for_number_from_list", "(", "numobj", ",", "regions", ")"], "docstring": "Returns the region where a phone number is from.\n\n    This could be used for geocoding at the region level. Only guarantees\n    correct results for valid, full numbers (not short-codes, or invalid\n    numbers).\n\n    Arguments:\n    numobj -- The phone number object whose origin we want to know\n\n    Returns the region where the phone number is from, or None if no region\n    matches this calling code.", "docstring_tokens": ["Returns", "the", "region", "where", "a", "phone", "number", "is", "from", "."], "sha": "9cc5bb4ab5e661e70789b4c64bf7a9383c7bdc20", "url": "https://github.com/daviddrysdale/python-phonenumbers/blob/9cc5bb4ab5e661e70789b4c64bf7a9383c7bdc20/python/phonenumbers/phonenumberutil.py#L2022-L2044", "partition": "train"}
{"repo": "Shopify/shopify_python_api", "path": "shopify/resources/gift_card.py", "func_name": "GiftCard.add_adjustment", "original_string": "def add_adjustment(self, adjustment):\n        \"\"\"\n        Create a new Gift Card Adjustment\n        \"\"\"\n        resource = self.post(\"adjustments\", adjustment.encode())\n        return GiftCardAdjustment(GiftCard.format.decode(resource.body))", "language": "python", "code": "def add_adjustment(self, adjustment):\n        \"\"\"\n        Create a new Gift Card Adjustment\n        \"\"\"\n        resource = self.post(\"adjustments\", adjustment.encode())\n        return GiftCardAdjustment(GiftCard.format.decode(resource.body))", "code_tokens": ["def", "add_adjustment", "(", "self", ",", "adjustment", ")", ":", "resource", "=", "self", ".", "post", "(", "\"adjustments\"", ",", "adjustment", ".", "encode", "(", ")", ")", "return", "GiftCardAdjustment", "(", "GiftCard", ".", "format", ".", "decode", "(", "resource", ".", "body", ")", ")"], "docstring": "Create a new Gift Card Adjustment", "docstring_tokens": ["Create", "a", "new", "Gift", "Card", "Adjustment"], "sha": "88d3ba332fb2cd331f87517a16f2c2d4296cee90", "url": "https://github.com/Shopify/shopify_python_api/blob/88d3ba332fb2cd331f87517a16f2c2d4296cee90/shopify/resources/gift_card.py#L26-L31", "partition": "train"}
{"repo": "aiven/pghoard", "path": "pghoard/rohmu/object_storage/local.py", "func_name": "atomic_create_file", "original_string": "def atomic_create_file(file_path):\n    \"\"\"Open a temporary file for writing, rename to final name when done\"\"\"\n    fd, tmp_file_path = tempfile.mkstemp(\n        prefix=os.path.basename(file_path), dir=os.path.dirname(file_path), suffix=\".metadata_tmp\"\n    )\n    try:\n        with os.fdopen(fd, \"w\") as out_file:\n            yield out_file\n\n        os.rename(tmp_file_path, file_path)\n    except Exception:  # pytest: disable=broad-except\n        with contextlib.suppress(Exception):\n            os.unlink(tmp_file_path)\n        raise", "language": "python", "code": "def atomic_create_file(file_path):\n    \"\"\"Open a temporary file for writing, rename to final name when done\"\"\"\n    fd, tmp_file_path = tempfile.mkstemp(\n        prefix=os.path.basename(file_path), dir=os.path.dirname(file_path), suffix=\".metadata_tmp\"\n    )\n    try:\n        with os.fdopen(fd, \"w\") as out_file:\n            yield out_file\n\n        os.rename(tmp_file_path, file_path)\n    except Exception:  # pytest: disable=broad-except\n        with contextlib.suppress(Exception):\n            os.unlink(tmp_file_path)\n        raise", "code_tokens": ["def", "atomic_create_file", "(", "file_path", ")", ":", "fd", ",", "tmp_file_path", "=", "tempfile", ".", "mkstemp", "(", "prefix", "=", "os", ".", "path", ".", "basename", "(", "file_path", ")", ",", "dir", "=", "os", ".", "path", ".", "dirname", "(", "file_path", ")", ",", "suffix", "=", "\".metadata_tmp\"", ")", "try", ":", "with", "os", ".", "fdopen", "(", "fd", ",", "\"w\"", ")", "as", "out_file", ":", "yield", "out_file", "os", ".", "rename", "(", "tmp_file_path", ",", "file_path", ")", "except", "Exception", ":", "# pytest: disable=broad-except", "with", "contextlib", ".", "suppress", "(", "Exception", ")", ":", "os", ".", "unlink", "(", "tmp_file_path", ")", "raise"], "docstring": "Open a temporary file for writing, rename to final name when done", "docstring_tokens": ["Open", "a", "temporary", "file", "for", "writing", "rename", "to", "final", "name", "when", "done"], "sha": "2994165d4ef3ff7a5669a2527346bcbfb5b3bd8a", "url": "https://github.com/aiven/pghoard/blob/2994165d4ef3ff7a5669a2527346bcbfb5b3bd8a/pghoard/rohmu/object_storage/local.py#L217-L230", "partition": "train"}
{"repo": "aiven/pghoard", "path": "pghoard/patchedtarfile.py", "func_name": "copyfileobj", "original_string": "def copyfileobj(src, dst, length=None, exception=OSError):\n    \"\"\"Copy length bytes from fileobj src to fileobj dst.\n       If length is None, copy the entire content.\n    \"\"\"\n    if length == 0:\n        return\n    if length is None:\n        shutil.copyfileobj(src, dst)\n        return\n\n    # BUFSIZE = 16 * 1024\n    blocks, remainder = divmod(length, BUFSIZE)\n    # for b in range(blocks):\n    for _ in range(blocks):\n        buf = src.read(BUFSIZE)\n        if len(buf) < BUFSIZE:\n            raise exception(\"unexpected end of data\")\n        dst.write(buf)\n\n    if remainder != 0:\n        buf = src.read(remainder)\n        if len(buf) < remainder:\n            raise exception(\"unexpected end of data\")\n        dst.write(buf)\n    return", "language": "python", "code": "def copyfileobj(src, dst, length=None, exception=OSError):\n    \"\"\"Copy length bytes from fileobj src to fileobj dst.\n       If length is None, copy the entire content.\n    \"\"\"\n    if length == 0:\n        return\n    if length is None:\n        shutil.copyfileobj(src, dst)\n        return\n\n    # BUFSIZE = 16 * 1024\n    blocks, remainder = divmod(length, BUFSIZE)\n    # for b in range(blocks):\n    for _ in range(blocks):\n        buf = src.read(BUFSIZE)\n        if len(buf) < BUFSIZE:\n            raise exception(\"unexpected end of data\")\n        dst.write(buf)\n\n    if remainder != 0:\n        buf = src.read(remainder)\n        if len(buf) < remainder:\n            raise exception(\"unexpected end of data\")\n        dst.write(buf)\n    return", "code_tokens": ["def", "copyfileobj", "(", "src", ",", "dst", ",", "length", "=", "None", ",", "exception", "=", "OSError", ")", ":", "if", "length", "==", "0", ":", "return", "if", "length", "is", "None", ":", "shutil", ".", "copyfileobj", "(", "src", ",", "dst", ")", "return", "# BUFSIZE = 16 * 1024", "blocks", ",", "remainder", "=", "divmod", "(", "length", ",", "BUFSIZE", ")", "# for b in range(blocks):", "for", "_", "in", "range", "(", "blocks", ")", ":", "buf", "=", "src", ".", "read", "(", "BUFSIZE", ")", "if", "len", "(", "buf", ")", "<", "BUFSIZE", ":", "raise", "exception", "(", "\"unexpected end of data\"", ")", "dst", ".", "write", "(", "buf", ")", "if", "remainder", "!=", "0", ":", "buf", "=", "src", ".", "read", "(", "remainder", ")", "if", "len", "(", "buf", ")", "<", "remainder", ":", "raise", "exception", "(", "\"unexpected end of data\"", ")", "dst", ".", "write", "(", "buf", ")", "return"], "docstring": "Copy length bytes from fileobj src to fileobj dst.\n       If length is None, copy the entire content.", "docstring_tokens": ["Copy", "length", "bytes", "from", "fileobj", "src", "to", "fileobj", "dst", ".", "If", "length", "is", "None", "copy", "the", "entire", "content", "."], "sha": "2994165d4ef3ff7a5669a2527346bcbfb5b3bd8a", "url": "https://github.com/aiven/pghoard/blob/2994165d4ef3ff7a5669a2527346bcbfb5b3bd8a/pghoard/patchedtarfile.py#L12-L36", "partition": "train"}
{"repo": "scikit-hep/uproot", "path": "uproot/rootio.py", "func_name": "TKey.get", "original_string": "def get(self, dismiss=True):\n        \"\"\"Extract the object this key points to.\n\n        Objects are not read or decompressed until this function is explicitly called.\n        \"\"\"\n\n        try:\n            return _classof(self._context, self._fClassName).read(self._source, self._cursor.copied(), self._context, self)\n        finally:\n            if dismiss:\n                self._source.dismiss()", "language": "python", "code": "def get(self, dismiss=True):\n        \"\"\"Extract the object this key points to.\n\n        Objects are not read or decompressed until this function is explicitly called.\n        \"\"\"\n\n        try:\n            return _classof(self._context, self._fClassName).read(self._source, self._cursor.copied(), self._context, self)\n        finally:\n            if dismiss:\n                self._source.dismiss()", "code_tokens": ["def", "get", "(", "self", ",", "dismiss", "=", "True", ")", ":", "try", ":", "return", "_classof", "(", "self", ".", "_context", ",", "self", ".", "_fClassName", ")", ".", "read", "(", "self", ".", "_source", ",", "self", ".", "_cursor", ".", "copied", "(", ")", ",", "self", ".", "_context", ",", "self", ")", "finally", ":", "if", "dismiss", ":", "self", ".", "_source", ".", "dismiss", "(", ")"], "docstring": "Extract the object this key points to.\n\n        Objects are not read or decompressed until this function is explicitly called.", "docstring_tokens": ["Extract", "the", "object", "this", "key", "points", "to", "."], "sha": "fc406827e36ed87cfb1062806e118f53fd3a3b0a", "url": "https://github.com/scikit-hep/uproot/blob/fc406827e36ed87cfb1062806e118f53fd3a3b0a/uproot/rootio.py#L883-L893", "partition": "train"}
{"repo": "zarr-developers/zarr", "path": "zarr/hierarchy.py", "func_name": "group", "original_string": "def group(store=None, overwrite=False, chunk_store=None,\n          cache_attrs=True, synchronizer=None, path=None):\n    \"\"\"Create a group.\n\n    Parameters\n    ----------\n    store : MutableMapping or string, optional\n        Store or path to directory in file system.\n    overwrite : bool, optional\n        If True, delete any pre-existing data in `store` at `path` before\n        creating the group.\n    chunk_store : MutableMapping, optional\n        Separate storage for chunks. If not provided, `store` will be used\n        for storage of both chunks and metadata.\n    cache_attrs : bool, optional\n        If True (default), user attributes will be cached for attribute read\n        operations. If False, user attributes are reloaded from the store prior\n        to all attribute read operations.\n    synchronizer : object, optional\n        Array synchronizer.\n    path : string, optional\n        Group path within store.\n\n    Returns\n    -------\n    g : zarr.hierarchy.Group\n\n    Examples\n    --------\n    Create a group in memory::\n\n        >>> import zarr\n        >>> g = zarr.group()\n        >>> g\n        <zarr.hierarchy.Group '/'>\n\n    Create a group with a different store::\n\n        >>> store = zarr.DirectoryStore('data/example.zarr')\n        >>> g = zarr.group(store=store, overwrite=True)\n        >>> g\n        <zarr.hierarchy.Group '/'>\n\n    \"\"\"\n\n    # handle polymorphic store arg\n    store = _normalize_store_arg(store)\n    path = normalize_storage_path(path)\n\n    # require group\n    if overwrite or not contains_group(store):\n        init_group(store, overwrite=overwrite, chunk_store=chunk_store,\n                   path=path)\n\n    return Group(store, read_only=False, chunk_store=chunk_store,\n                 cache_attrs=cache_attrs, synchronizer=synchronizer, path=path)", "language": "python", "code": "def group(store=None, overwrite=False, chunk_store=None,\n          cache_attrs=True, synchronizer=None, path=None):\n    \"\"\"Create a group.\n\n    Parameters\n    ----------\n    store : MutableMapping or string, optional\n        Store or path to directory in file system.\n    overwrite : bool, optional\n        If True, delete any pre-existing data in `store` at `path` before\n        creating the group.\n    chunk_store : MutableMapping, optional\n        Separate storage for chunks. If not provided, `store` will be used\n        for storage of both chunks and metadata.\n    cache_attrs : bool, optional\n        If True (default), user attributes will be cached for attribute read\n        operations. If False, user attributes are reloaded from the store prior\n        to all attribute read operations.\n    synchronizer : object, optional\n        Array synchronizer.\n    path : string, optional\n        Group path within store.\n\n    Returns\n    -------\n    g : zarr.hierarchy.Group\n\n    Examples\n    --------\n    Create a group in memory::\n\n        >>> import zarr\n        >>> g = zarr.group()\n        >>> g\n        <zarr.hierarchy.Group '/'>\n\n    Create a group with a different store::\n\n        >>> store = zarr.DirectoryStore('data/example.zarr')\n        >>> g = zarr.group(store=store, overwrite=True)\n        >>> g\n        <zarr.hierarchy.Group '/'>\n\n    \"\"\"\n\n    # handle polymorphic store arg\n    store = _normalize_store_arg(store)\n    path = normalize_storage_path(path)\n\n    # require group\n    if overwrite or not contains_group(store):\n        init_group(store, overwrite=overwrite, chunk_store=chunk_store,\n                   path=path)\n\n    return Group(store, read_only=False, chunk_store=chunk_store,\n                 cache_attrs=cache_attrs, synchronizer=synchronizer, path=path)", "code_tokens": ["def", "group", "(", "store", "=", "None", ",", "overwrite", "=", "False", ",", "chunk_store", "=", "None", ",", "cache_attrs", "=", "True", ",", "synchronizer", "=", "None", ",", "path", "=", "None", ")", ":", "# handle polymorphic store arg", "store", "=", "_normalize_store_arg", "(", "store", ")", "path", "=", "normalize_storage_path", "(", "path", ")", "# require group", "if", "overwrite", "or", "not", "contains_group", "(", "store", ")", ":", "init_group", "(", "store", ",", "overwrite", "=", "overwrite", ",", "chunk_store", "=", "chunk_store", ",", "path", "=", "path", ")", "return", "Group", "(", "store", ",", "read_only", "=", "False", ",", "chunk_store", "=", "chunk_store", ",", "cache_attrs", "=", "cache_attrs", ",", "synchronizer", "=", "synchronizer", ",", "path", "=", "path", ")"], "docstring": "Create a group.\n\n    Parameters\n    ----------\n    store : MutableMapping or string, optional\n        Store or path to directory in file system.\n    overwrite : bool, optional\n        If True, delete any pre-existing data in `store` at `path` before\n        creating the group.\n    chunk_store : MutableMapping, optional\n        Separate storage for chunks. If not provided, `store` will be used\n        for storage of both chunks and metadata.\n    cache_attrs : bool, optional\n        If True (default), user attributes will be cached for attribute read\n        operations. If False, user attributes are reloaded from the store prior\n        to all attribute read operations.\n    synchronizer : object, optional\n        Array synchronizer.\n    path : string, optional\n        Group path within store.\n\n    Returns\n    -------\n    g : zarr.hierarchy.Group\n\n    Examples\n    --------\n    Create a group in memory::\n\n        >>> import zarr\n        >>> g = zarr.group()\n        >>> g\n        <zarr.hierarchy.Group '/'>\n\n    Create a group with a different store::\n\n        >>> store = zarr.DirectoryStore('data/example.zarr')\n        >>> g = zarr.group(store=store, overwrite=True)\n        >>> g\n        <zarr.hierarchy.Group '/'>", "docstring_tokens": ["Create", "a", "group", "."], "sha": "fb8e6d5ea6bc26e451e5cf0eaaee36977556d5b5", "url": "https://github.com/zarr-developers/zarr/blob/fb8e6d5ea6bc26e451e5cf0eaaee36977556d5b5/zarr/hierarchy.py#L1002-L1057", "partition": "train"}
{"repo": "openstack/horizon", "path": "horizon/tabs/views.py", "func_name": "TabbedTableView.handle_table", "original_string": "def handle_table(self, table_dict):\n        \"\"\"Loads the table data based on a given table_dict and handles them.\n\n        For the given dict containing a ``DataTable`` and a ``TableTab``\n        instance, it loads the table data for that tab and calls the\n        table's :meth:`~horizon.tables.DataTable.maybe_handle` method.\n        The return value will be the result of ``maybe_handle``.\n        \"\"\"\n        table = table_dict['table']\n        tab = table_dict['tab']\n        tab.load_table_data()\n        table_name = table._meta.name\n        tab._tables[table_name]._meta.has_prev_data = self.has_prev_data(table)\n        tab._tables[table_name]._meta.has_more_data = self.has_more_data(table)\n        handled = tab._tables[table_name].maybe_handle()\n        return handled", "language": "python", "code": "def handle_table(self, table_dict):\n        \"\"\"Loads the table data based on a given table_dict and handles them.\n\n        For the given dict containing a ``DataTable`` and a ``TableTab``\n        instance, it loads the table data for that tab and calls the\n        table's :meth:`~horizon.tables.DataTable.maybe_handle` method.\n        The return value will be the result of ``maybe_handle``.\n        \"\"\"\n        table = table_dict['table']\n        tab = table_dict['tab']\n        tab.load_table_data()\n        table_name = table._meta.name\n        tab._tables[table_name]._meta.has_prev_data = self.has_prev_data(table)\n        tab._tables[table_name]._meta.has_more_data = self.has_more_data(table)\n        handled = tab._tables[table_name].maybe_handle()\n        return handled", "code_tokens": ["def", "handle_table", "(", "self", ",", "table_dict", ")", ":", "table", "=", "table_dict", "[", "'table'", "]", "tab", "=", "table_dict", "[", "'tab'", "]", "tab", ".", "load_table_data", "(", ")", "table_name", "=", "table", ".", "_meta", ".", "name", "tab", ".", "_tables", "[", "table_name", "]", ".", "_meta", ".", "has_prev_data", "=", "self", ".", "has_prev_data", "(", "table", ")", "tab", ".", "_tables", "[", "table_name", "]", ".", "_meta", ".", "has_more_data", "=", "self", ".", "has_more_data", "(", "table", ")", "handled", "=", "tab", ".", "_tables", "[", "table_name", "]", ".", "maybe_handle", "(", ")", "return", "handled"], "docstring": "Loads the table data based on a given table_dict and handles them.\n\n        For the given dict containing a ``DataTable`` and a ``TableTab``\n        instance, it loads the table data for that tab and calls the\n        table's :meth:`~horizon.tables.DataTable.maybe_handle` method.\n        The return value will be the result of ``maybe_handle``.", "docstring_tokens": ["Loads", "the", "table", "data", "based", "on", "a", "given", "table_dict", "and", "handles", "them", "."], "sha": "5601ea9477323e599d9b766fcac1f8be742935b2", "url": "https://github.com/openstack/horizon/blob/5601ea9477323e599d9b766fcac1f8be742935b2/horizon/tabs/views.py#L102-L117", "partition": "train"}
{"repo": "openstack/horizon", "path": "openstack_dashboard/context_processors.py", "func_name": "openstack", "original_string": "def openstack(request):\n    \"\"\"Context processor necessary for OpenStack Dashboard functionality.\n\n    The following variables are added to the request context:\n\n    ``authorized_tenants``\n        A list of tenant objects which the current user has access to.\n\n    ``regions``\n\n        A dictionary containing information about region support, the current\n        region, and available regions.\n    \"\"\"\n    context = {}\n\n    # Auth/Keystone context\n    context.setdefault('authorized_tenants', [])\n    if request.user.is_authenticated:\n        context['authorized_tenants'] = [\n            tenant for tenant in\n            request.user.authorized_tenants if tenant.enabled]\n\n    # Region context/support\n    available_regions = getattr(settings, 'AVAILABLE_REGIONS', [])\n    regions = {'support': len(available_regions) > 1,\n               'current': {'endpoint': request.session.get('region_endpoint'),\n                           'name': request.session.get('region_name')},\n               'available': [{'endpoint': region[0], 'name':region[1]} for\n                             region in available_regions]}\n\n    # K2K Federation Service Providers context/support\n    available_providers = request.session.get('keystone_providers', [])\n    if available_providers:\n        provider_id = request.session.get('keystone_provider_id', None)\n        provider_name = None\n        for provider in available_providers:\n            if provider['id'] == provider_id:\n                provider_name = provider.get('name')\n\n        keystone_providers = {\n            'support': len(available_providers) > 1,\n            'current': {\n                'name': provider_name,\n                'id': provider_id\n            },\n            'available': [\n                {'name': keystone_provider['name'],\n                 'id': keystone_provider['id']}\n                for keystone_provider in available_providers]\n        }\n    else:\n        keystone_providers = {'support': False}\n\n    context['keystone_providers'] = keystone_providers\n    context['regions'] = regions\n\n    # Adding webroot access\n    context['WEBROOT'] = getattr(settings, \"WEBROOT\", \"/\")\n\n    user_menu_links = getattr(settings, \"USER_MENU_LINKS\", [])\n\n    if not getattr(settings, \"SHOW_KEYSTONE_V2_RC\", False):\n        user_menu_links = [\n            link for link in user_menu_links\n            if link['url'] != 'horizon:project:api_access:openrcv2']\n\n    context['USER_MENU_LINKS'] = user_menu_links\n\n    # Adding profiler support flag\n    profiler_settings = getattr(settings, 'OPENSTACK_PROFILER', {})\n    profiler_enabled = profiler_settings.get('enabled', False)\n    context['profiler_enabled'] = profiler_enabled\n    if profiler_enabled and 'profile_page' in request.COOKIES:\n        index_view_id = request.META.get(profiler.ROOT_HEADER, '')\n        hmac_keys = profiler_settings.get('keys', [])\n        context['x_trace_info'] = profiler.update_trace_headers(\n            hmac_keys, parent_id=index_view_id)\n\n    context['JS_CATALOG'] = get_js_catalog(conf)\n\n    return context", "language": "python", "code": "def openstack(request):\n    \"\"\"Context processor necessary for OpenStack Dashboard functionality.\n\n    The following variables are added to the request context:\n\n    ``authorized_tenants``\n        A list of tenant objects which the current user has access to.\n\n    ``regions``\n\n        A dictionary containing information about region support, the current\n        region, and available regions.\n    \"\"\"\n    context = {}\n\n    # Auth/Keystone context\n    context.setdefault('authorized_tenants', [])\n    if request.user.is_authenticated:\n        context['authorized_tenants'] = [\n            tenant for tenant in\n            request.user.authorized_tenants if tenant.enabled]\n\n    # Region context/support\n    available_regions = getattr(settings, 'AVAILABLE_REGIONS', [])\n    regions = {'support': len(available_regions) > 1,\n               'current': {'endpoint': request.session.get('region_endpoint'),\n                           'name': request.session.get('region_name')},\n               'available': [{'endpoint': region[0], 'name':region[1]} for\n                             region in available_regions]}\n\n    # K2K Federation Service Providers context/support\n    available_providers = request.session.get('keystone_providers', [])\n    if available_providers:\n        provider_id = request.session.get('keystone_provider_id', None)\n        provider_name = None\n        for provider in available_providers:\n            if provider['id'] == provider_id:\n                provider_name = provider.get('name')\n\n        keystone_providers = {\n            'support': len(available_providers) > 1,\n            'current': {\n                'name': provider_name,\n                'id': provider_id\n            },\n            'available': [\n                {'name': keystone_provider['name'],\n                 'id': keystone_provider['id']}\n                for keystone_provider in available_providers]\n        }\n    else:\n        keystone_providers = {'support': False}\n\n    context['keystone_providers'] = keystone_providers\n    context['regions'] = regions\n\n    # Adding webroot access\n    context['WEBROOT'] = getattr(settings, \"WEBROOT\", \"/\")\n\n    user_menu_links = getattr(settings, \"USER_MENU_LINKS\", [])\n\n    if not getattr(settings, \"SHOW_KEYSTONE_V2_RC\", False):\n        user_menu_links = [\n            link for link in user_menu_links\n            if link['url'] != 'horizon:project:api_access:openrcv2']\n\n    context['USER_MENU_LINKS'] = user_menu_links\n\n    # Adding profiler support flag\n    profiler_settings = getattr(settings, 'OPENSTACK_PROFILER', {})\n    profiler_enabled = profiler_settings.get('enabled', False)\n    context['profiler_enabled'] = profiler_enabled\n    if profiler_enabled and 'profile_page' in request.COOKIES:\n        index_view_id = request.META.get(profiler.ROOT_HEADER, '')\n        hmac_keys = profiler_settings.get('keys', [])\n        context['x_trace_info'] = profiler.update_trace_headers(\n            hmac_keys, parent_id=index_view_id)\n\n    context['JS_CATALOG'] = get_js_catalog(conf)\n\n    return context", "code_tokens": ["def", "openstack", "(", "request", ")", ":", "context", "=", "{", "}", "# Auth/Keystone context", "context", ".", "setdefault", "(", "'authorized_tenants'", ",", "[", "]", ")", "if", "request", ".", "user", ".", "is_authenticated", ":", "context", "[", "'authorized_tenants'", "]", "=", "[", "tenant", "for", "tenant", "in", "request", ".", "user", ".", "authorized_tenants", "if", "tenant", ".", "enabled", "]", "# Region context/support", "available_regions", "=", "getattr", "(", "settings", ",", "'AVAILABLE_REGIONS'", ",", "[", "]", ")", "regions", "=", "{", "'support'", ":", "len", "(", "available_regions", ")", ">", "1", ",", "'current'", ":", "{", "'endpoint'", ":", "request", ".", "session", ".", "get", "(", "'region_endpoint'", ")", ",", "'name'", ":", "request", ".", "session", ".", "get", "(", "'region_name'", ")", "}", ",", "'available'", ":", "[", "{", "'endpoint'", ":", "region", "[", "0", "]", ",", "'name'", ":", "region", "[", "1", "]", "}", "for", "region", "in", "available_regions", "]", "}", "# K2K Federation Service Providers context/support", "available_providers", "=", "request", ".", "session", ".", "get", "(", "'keystone_providers'", ",", "[", "]", ")", "if", "available_providers", ":", "provider_id", "=", "request", ".", "session", ".", "get", "(", "'keystone_provider_id'", ",", "None", ")", "provider_name", "=", "None", "for", "provider", "in", "available_providers", ":", "if", "provider", "[", "'id'", "]", "==", "provider_id", ":", "provider_name", "=", "provider", ".", "get", "(", "'name'", ")", "keystone_providers", "=", "{", "'support'", ":", "len", "(", "available_providers", ")", ">", "1", ",", "'current'", ":", "{", "'name'", ":", "provider_name", ",", "'id'", ":", "provider_id", "}", ",", "'available'", ":", "[", "{", "'name'", ":", "keystone_provider", "[", "'name'", "]", ",", "'id'", ":", "keystone_provider", "[", "'id'", "]", "}", "for", "keystone_provider", "in", "available_providers", "]", "}", "else", ":", "keystone_providers", "=", "{", "'support'", ":", "False", "}", "context", "[", "'keystone_providers'", "]", "=", "keystone_providers", "context", "[", "'regions'", "]", "=", "regions", "# Adding webroot access", "context", "[", "'WEBROOT'", "]", "=", "getattr", "(", "settings", ",", "\"WEBROOT\"", ",", "\"/\"", ")", "user_menu_links", "=", "getattr", "(", "settings", ",", "\"USER_MENU_LINKS\"", ",", "[", "]", ")", "if", "not", "getattr", "(", "settings", ",", "\"SHOW_KEYSTONE_V2_RC\"", ",", "False", ")", ":", "user_menu_links", "=", "[", "link", "for", "link", "in", "user_menu_links", "if", "link", "[", "'url'", "]", "!=", "'horizon:project:api_access:openrcv2'", "]", "context", "[", "'USER_MENU_LINKS'", "]", "=", "user_menu_links", "# Adding profiler support flag", "profiler_settings", "=", "getattr", "(", "settings", ",", "'OPENSTACK_PROFILER'", ",", "{", "}", ")", "profiler_enabled", "=", "profiler_settings", ".", "get", "(", "'enabled'", ",", "False", ")", "context", "[", "'profiler_enabled'", "]", "=", "profiler_enabled", "if", "profiler_enabled", "and", "'profile_page'", "in", "request", ".", "COOKIES", ":", "index_view_id", "=", "request", ".", "META", ".", "get", "(", "profiler", ".", "ROOT_HEADER", ",", "''", ")", "hmac_keys", "=", "profiler_settings", ".", "get", "(", "'keys'", ",", "[", "]", ")", "context", "[", "'x_trace_info'", "]", "=", "profiler", ".", "update_trace_headers", "(", "hmac_keys", ",", "parent_id", "=", "index_view_id", ")", "context", "[", "'JS_CATALOG'", "]", "=", "get_js_catalog", "(", "conf", ")", "return", "context"], "docstring": "Context processor necessary for OpenStack Dashboard functionality.\n\n    The following variables are added to the request context:\n\n    ``authorized_tenants``\n        A list of tenant objects which the current user has access to.\n\n    ``regions``\n\n        A dictionary containing information about region support, the current\n        region, and available regions.", "docstring_tokens": ["Context", "processor", "necessary", "for", "OpenStack", "Dashboard", "functionality", "."], "sha": "5601ea9477323e599d9b766fcac1f8be742935b2", "url": "https://github.com/openstack/horizon/blob/5601ea9477323e599d9b766fcac1f8be742935b2/openstack_dashboard/context_processors.py#L30-L110", "partition": "train"}
{"repo": "onelogin/python3-saml", "path": "src/onelogin/saml2/metadata.py", "func_name": "OneLogin_Saml2_Metadata.add_x509_key_descriptors", "original_string": "def add_x509_key_descriptors(metadata, cert=None, add_encryption=True):\n        \"\"\"\n        Adds the x509 descriptors (sign/encryption) to the metadata\n        The same cert will be used for sign/encrypt\n\n        :param metadata: SAML Metadata XML\n        :type metadata: string\n\n        :param cert: x509 cert\n        :type cert: string\n\n        :param add_encryption: Determines if the KeyDescriptor[use=\"encryption\"] should be added.\n        :type add_encryption: boolean\n\n        :returns: Metadata with KeyDescriptors\n        :rtype: string\n        \"\"\"\n        if cert is None or cert == '':\n            return metadata\n        try:\n            root = OneLogin_Saml2_XML.to_etree(metadata)\n        except Exception as e:\n            raise Exception('Error parsing metadata. ' + str(e))\n\n        assert root.tag == '{%s}EntityDescriptor' % OneLogin_Saml2_Constants.NS_MD\n        try:\n            sp_sso_descriptor = next(root.iterfind('.//md:SPSSODescriptor', namespaces=OneLogin_Saml2_Constants.NSMAP))\n        except StopIteration:\n            raise Exception('Malformed metadata.')\n\n        if add_encryption:\n            OneLogin_Saml2_Metadata.__add_x509_key_descriptors(sp_sso_descriptor, cert, False)\n        OneLogin_Saml2_Metadata.__add_x509_key_descriptors(sp_sso_descriptor, cert, True)\n        return OneLogin_Saml2_XML.to_string(root)", "language": "python", "code": "def add_x509_key_descriptors(metadata, cert=None, add_encryption=True):\n        \"\"\"\n        Adds the x509 descriptors (sign/encryption) to the metadata\n        The same cert will be used for sign/encrypt\n\n        :param metadata: SAML Metadata XML\n        :type metadata: string\n\n        :param cert: x509 cert\n        :type cert: string\n\n        :param add_encryption: Determines if the KeyDescriptor[use=\"encryption\"] should be added.\n        :type add_encryption: boolean\n\n        :returns: Metadata with KeyDescriptors\n        :rtype: string\n        \"\"\"\n        if cert is None or cert == '':\n            return metadata\n        try:\n            root = OneLogin_Saml2_XML.to_etree(metadata)\n        except Exception as e:\n            raise Exception('Error parsing metadata. ' + str(e))\n\n        assert root.tag == '{%s}EntityDescriptor' % OneLogin_Saml2_Constants.NS_MD\n        try:\n            sp_sso_descriptor = next(root.iterfind('.//md:SPSSODescriptor', namespaces=OneLogin_Saml2_Constants.NSMAP))\n        except StopIteration:\n            raise Exception('Malformed metadata.')\n\n        if add_encryption:\n            OneLogin_Saml2_Metadata.__add_x509_key_descriptors(sp_sso_descriptor, cert, False)\n        OneLogin_Saml2_Metadata.__add_x509_key_descriptors(sp_sso_descriptor, cert, True)\n        return OneLogin_Saml2_XML.to_string(root)", "code_tokens": ["def", "add_x509_key_descriptors", "(", "metadata", ",", "cert", "=", "None", ",", "add_encryption", "=", "True", ")", ":", "if", "cert", "is", "None", "or", "cert", "==", "''", ":", "return", "metadata", "try", ":", "root", "=", "OneLogin_Saml2_XML", ".", "to_etree", "(", "metadata", ")", "except", "Exception", "as", "e", ":", "raise", "Exception", "(", "'Error parsing metadata. '", "+", "str", "(", "e", ")", ")", "assert", "root", ".", "tag", "==", "'{%s}EntityDescriptor'", "%", "OneLogin_Saml2_Constants", ".", "NS_MD", "try", ":", "sp_sso_descriptor", "=", "next", "(", "root", ".", "iterfind", "(", "'.//md:SPSSODescriptor'", ",", "namespaces", "=", "OneLogin_Saml2_Constants", ".", "NSMAP", ")", ")", "except", "StopIteration", ":", "raise", "Exception", "(", "'Malformed metadata.'", ")", "if", "add_encryption", ":", "OneLogin_Saml2_Metadata", ".", "__add_x509_key_descriptors", "(", "sp_sso_descriptor", ",", "cert", ",", "False", ")", "OneLogin_Saml2_Metadata", ".", "__add_x509_key_descriptors", "(", "sp_sso_descriptor", ",", "cert", ",", "True", ")", "return", "OneLogin_Saml2_XML", ".", "to_string", "(", "root", ")"], "docstring": "Adds the x509 descriptors (sign/encryption) to the metadata\n        The same cert will be used for sign/encrypt\n\n        :param metadata: SAML Metadata XML\n        :type metadata: string\n\n        :param cert: x509 cert\n        :type cert: string\n\n        :param add_encryption: Determines if the KeyDescriptor[use=\"encryption\"] should be added.\n        :type add_encryption: boolean\n\n        :returns: Metadata with KeyDescriptors\n        :rtype: string", "docstring_tokens": ["Adds", "the", "x509", "descriptors", "(", "sign", "/", "encryption", ")", "to", "the", "metadata", "The", "same", "cert", "will", "be", "used", "for", "sign", "/", "encrypt"], "sha": "064b7275fba1e5f39a9116ba1cdcc5d01fc34daa", "url": "https://github.com/onelogin/python3-saml/blob/064b7275fba1e5f39a9116ba1cdcc5d01fc34daa/src/onelogin/saml2/metadata.py#L232-L265", "partition": "train"}
{"repo": "automl/HpBandSter", "path": "hpbandster/optimizers/config_generators/bohb.py", "func_name": "BOHB.get_config", "original_string": "def get_config(self, budget):\n\t\t\"\"\"\n\t\t\tFunction to sample a new configuration\n\n\t\t\tThis function is called inside Hyperband to query a new configuration\n\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\tbudget: float\n\t\t\t\tthe budget for which this configuration is scheduled\n\n\t\t\treturns: config\n\t\t\t\tshould return a valid configuration\n\n\t\t\"\"\"\n\t\t\n\t\tself.logger.debug('start sampling a new configuration.')\n\t\t\n\n\t\tsample = None\n\t\tinfo_dict = {}\n\t\t\n\t\t# If no model is available, sample from prior\n\t\t# also mix in a fraction of random configs\n\t\tif len(self.kde_models.keys()) == 0 or np.random.rand() < self.random_fraction:\n\t\t\tsample =  self.configspace.sample_configuration()\n\t\t\tinfo_dict['model_based_pick'] = False\n\n\t\tbest = np.inf\n\t\tbest_vector = None\n\n\t\tif sample is None:\n\t\t\ttry:\n\t\t\t\t\n\t\t\t\t#sample from largest budget\n\t\t\t\tbudget = max(self.kde_models.keys())\n\n\t\t\t\tl = self.kde_models[budget]['good'].pdf\n\t\t\t\tg = self.kde_models[budget]['bad' ].pdf\n\t\t\t\n\t\t\t\tminimize_me = lambda x: max(1e-32, g(x))/max(l(x),1e-32)\n\t\t\t\t\n\t\t\t\tkde_good = self.kde_models[budget]['good']\n\t\t\t\tkde_bad = self.kde_models[budget]['bad']\n\n\t\t\t\tfor i in range(self.num_samples):\n\t\t\t\t\tidx = np.random.randint(0, len(kde_good.data))\n\t\t\t\t\tdatum = kde_good.data[idx]\n\t\t\t\t\tvector = []\n\t\t\t\t\t\n\t\t\t\t\tfor m,bw,t in zip(datum, kde_good.bw, self.vartypes):\n\t\t\t\t\t\t\n\t\t\t\t\t\tbw = max(bw, self.min_bandwidth)\n\t\t\t\t\t\tif t == 0:\n\t\t\t\t\t\t\tbw = self.bw_factor*bw\n\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\tvector.append(sps.truncnorm.rvs(-m/bw,(1-m)/bw, loc=m, scale=bw))\n\t\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\t\tself.logger.warning(\"Truncated Normal failed for:\\ndatum=%s\\nbandwidth=%s\\nfor entry with value %s\"%(datum, kde_good.bw, m))\n\t\t\t\t\t\t\t\tself.logger.warning(\"data in the KDE:\\n%s\"%kde_good.data)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif np.random.rand() < (1-bw):\n\t\t\t\t\t\t\t\tvector.append(int(m))\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tvector.append(np.random.randint(t))\n\t\t\t\t\tval = minimize_me(vector)\n\n\t\t\t\t\tif not np.isfinite(val):\n\t\t\t\t\t\tself.logger.warning('sampled vector: %s has EI value %s'%(vector, val))\n\t\t\t\t\t\tself.logger.warning(\"data in the KDEs:\\n%s\\n%s\"%(kde_good.data, kde_bad.data))\n\t\t\t\t\t\tself.logger.warning(\"bandwidth of the KDEs:\\n%s\\n%s\"%(kde_good.bw, kde_bad.bw))\n\t\t\t\t\t\tself.logger.warning(\"l(x) = %s\"%(l(vector)))\n\t\t\t\t\t\tself.logger.warning(\"g(x) = %s\"%(g(vector)))\n\n\t\t\t\t\t\t# right now, this happens because a KDE does not contain all values for a categorical parameter\n\t\t\t\t\t\t# this cannot be fixed with the statsmodels KDE, so for now, we are just going to evaluate this one\n\t\t\t\t\t\t# if the good_kde has a finite value, i.e. there is no config with that value in the bad kde, so it shouldn't be terrible.\n\t\t\t\t\t\tif np.isfinite(l(vector)):\n\t\t\t\t\t\t\tbest_vector = vector\n\t\t\t\t\t\t\tbreak\n\n\t\t\t\t\tif val < best:\n\t\t\t\t\t\tbest = val\n\t\t\t\t\t\tbest_vector = vector\n\n\t\t\t\tif best_vector is None:\n\t\t\t\t\tself.logger.debug(\"Sampling based optimization with %i samples failed -> using random configuration\"%self.num_samples)\n\t\t\t\t\tsample = self.configspace.sample_configuration().get_dictionary()\n\t\t\t\t\tinfo_dict['model_based_pick']  = False\n\t\t\t\telse:\n\t\t\t\t\tself.logger.debug('best_vector: {}, {}, {}, {}'.format(best_vector, best, l(best_vector), g(best_vector)))\n\t\t\t\t\tfor i, hp_value in enumerate(best_vector):\n\t\t\t\t\t\tif isinstance(\n\t\t\t\t\t\t\tself.configspace.get_hyperparameter(\n\t\t\t\t\t\t\t\tself.configspace.get_hyperparameter_by_idx(i)\n\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\tConfigSpace.hyperparameters.CategoricalHyperparameter\n\t\t\t\t\t\t):\n\t\t\t\t\t\t\tbest_vector[i] = int(np.rint(best_vector[i]))\n\t\t\t\t\tsample = ConfigSpace.Configuration(self.configspace, vector=best_vector).get_dictionary()\n\t\t\t\t\t\n\t\t\t\t\ttry:\n\t\t\t\t\t\tsample = ConfigSpace.util.deactivate_inactive_hyperparameters(\n\t\t\t\t\t\t\t\t\tconfiguration_space=self.configspace,\n\t\t\t\t\t\t\t\t\tconfiguration=sample\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\tinfo_dict['model_based_pick'] = True\n\n\t\t\t\t\texcept Exception as e:\n\t\t\t\t\t\tself.logger.warning((\"=\"*50 + \"\\n\")*3 +\\\n\t\t\t\t\t\t\t\t\"Error converting configuration:\\n%s\"%sample+\\\n\t\t\t\t\t\t\t\t\"\\n here is a traceback:\" +\\\n\t\t\t\t\t\t\t\ttraceback.format_exc())\n\t\t\t\t\t\traise(e)\n\n\t\t\texcept:\n\t\t\t\tself.logger.warning(\"Sampling based optimization with %i samples failed\\n %s \\nUsing random configuration\"%(self.num_samples, traceback.format_exc()))\n\t\t\t\tsample = self.configspace.sample_configuration()\n\t\t\t\tinfo_dict['model_based_pick']  = False\n\n\n\t\ttry:\n\t\t\tsample = ConfigSpace.util.deactivate_inactive_hyperparameters(\n\t\t\t\tconfiguration_space=self.configspace,\n\t\t\t\tconfiguration=sample.get_dictionary()\n\t\t\t).get_dictionary()\n\t\texcept Exception as e:\n\t\t\tself.logger.warning(\"Error (%s) converting configuration: %s -> \"\n\t\t\t\t\t\t\t\t\"using random configuration!\",\n\t\t\t\t\t\t\t\te,\n\t\t\t\t\t\t\t\tsample)\n\t\t\tsample = self.configspace.sample_configuration().get_dictionary()\n\t\tself.logger.debug('done sampling a new configuration.')\n\t\treturn sample, info_dict", "language": "python", "code": "def get_config(self, budget):\n\t\t\"\"\"\n\t\t\tFunction to sample a new configuration\n\n\t\t\tThis function is called inside Hyperband to query a new configuration\n\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\tbudget: float\n\t\t\t\tthe budget for which this configuration is scheduled\n\n\t\t\treturns: config\n\t\t\t\tshould return a valid configuration\n\n\t\t\"\"\"\n\t\t\n\t\tself.logger.debug('start sampling a new configuration.')\n\t\t\n\n\t\tsample = None\n\t\tinfo_dict = {}\n\t\t\n\t\t# If no model is available, sample from prior\n\t\t# also mix in a fraction of random configs\n\t\tif len(self.kde_models.keys()) == 0 or np.random.rand() < self.random_fraction:\n\t\t\tsample =  self.configspace.sample_configuration()\n\t\t\tinfo_dict['model_based_pick'] = False\n\n\t\tbest = np.inf\n\t\tbest_vector = None\n\n\t\tif sample is None:\n\t\t\ttry:\n\t\t\t\t\n\t\t\t\t#sample from largest budget\n\t\t\t\tbudget = max(self.kde_models.keys())\n\n\t\t\t\tl = self.kde_models[budget]['good'].pdf\n\t\t\t\tg = self.kde_models[budget]['bad' ].pdf\n\t\t\t\n\t\t\t\tminimize_me = lambda x: max(1e-32, g(x))/max(l(x),1e-32)\n\t\t\t\t\n\t\t\t\tkde_good = self.kde_models[budget]['good']\n\t\t\t\tkde_bad = self.kde_models[budget]['bad']\n\n\t\t\t\tfor i in range(self.num_samples):\n\t\t\t\t\tidx = np.random.randint(0, len(kde_good.data))\n\t\t\t\t\tdatum = kde_good.data[idx]\n\t\t\t\t\tvector = []\n\t\t\t\t\t\n\t\t\t\t\tfor m,bw,t in zip(datum, kde_good.bw, self.vartypes):\n\t\t\t\t\t\t\n\t\t\t\t\t\tbw = max(bw, self.min_bandwidth)\n\t\t\t\t\t\tif t == 0:\n\t\t\t\t\t\t\tbw = self.bw_factor*bw\n\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\tvector.append(sps.truncnorm.rvs(-m/bw,(1-m)/bw, loc=m, scale=bw))\n\t\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\t\tself.logger.warning(\"Truncated Normal failed for:\\ndatum=%s\\nbandwidth=%s\\nfor entry with value %s\"%(datum, kde_good.bw, m))\n\t\t\t\t\t\t\t\tself.logger.warning(\"data in the KDE:\\n%s\"%kde_good.data)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif np.random.rand() < (1-bw):\n\t\t\t\t\t\t\t\tvector.append(int(m))\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tvector.append(np.random.randint(t))\n\t\t\t\t\tval = minimize_me(vector)\n\n\t\t\t\t\tif not np.isfinite(val):\n\t\t\t\t\t\tself.logger.warning('sampled vector: %s has EI value %s'%(vector, val))\n\t\t\t\t\t\tself.logger.warning(\"data in the KDEs:\\n%s\\n%s\"%(kde_good.data, kde_bad.data))\n\t\t\t\t\t\tself.logger.warning(\"bandwidth of the KDEs:\\n%s\\n%s\"%(kde_good.bw, kde_bad.bw))\n\t\t\t\t\t\tself.logger.warning(\"l(x) = %s\"%(l(vector)))\n\t\t\t\t\t\tself.logger.warning(\"g(x) = %s\"%(g(vector)))\n\n\t\t\t\t\t\t# right now, this happens because a KDE does not contain all values for a categorical parameter\n\t\t\t\t\t\t# this cannot be fixed with the statsmodels KDE, so for now, we are just going to evaluate this one\n\t\t\t\t\t\t# if the good_kde has a finite value, i.e. there is no config with that value in the bad kde, so it shouldn't be terrible.\n\t\t\t\t\t\tif np.isfinite(l(vector)):\n\t\t\t\t\t\t\tbest_vector = vector\n\t\t\t\t\t\t\tbreak\n\n\t\t\t\t\tif val < best:\n\t\t\t\t\t\tbest = val\n\t\t\t\t\t\tbest_vector = vector\n\n\t\t\t\tif best_vector is None:\n\t\t\t\t\tself.logger.debug(\"Sampling based optimization with %i samples failed -> using random configuration\"%self.num_samples)\n\t\t\t\t\tsample = self.configspace.sample_configuration().get_dictionary()\n\t\t\t\t\tinfo_dict['model_based_pick']  = False\n\t\t\t\telse:\n\t\t\t\t\tself.logger.debug('best_vector: {}, {}, {}, {}'.format(best_vector, best, l(best_vector), g(best_vector)))\n\t\t\t\t\tfor i, hp_value in enumerate(best_vector):\n\t\t\t\t\t\tif isinstance(\n\t\t\t\t\t\t\tself.configspace.get_hyperparameter(\n\t\t\t\t\t\t\t\tself.configspace.get_hyperparameter_by_idx(i)\n\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\tConfigSpace.hyperparameters.CategoricalHyperparameter\n\t\t\t\t\t\t):\n\t\t\t\t\t\t\tbest_vector[i] = int(np.rint(best_vector[i]))\n\t\t\t\t\tsample = ConfigSpace.Configuration(self.configspace, vector=best_vector).get_dictionary()\n\t\t\t\t\t\n\t\t\t\t\ttry:\n\t\t\t\t\t\tsample = ConfigSpace.util.deactivate_inactive_hyperparameters(\n\t\t\t\t\t\t\t\t\tconfiguration_space=self.configspace,\n\t\t\t\t\t\t\t\t\tconfiguration=sample\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\tinfo_dict['model_based_pick'] = True\n\n\t\t\t\t\texcept Exception as e:\n\t\t\t\t\t\tself.logger.warning((\"=\"*50 + \"\\n\")*3 +\\\n\t\t\t\t\t\t\t\t\"Error converting configuration:\\n%s\"%sample+\\\n\t\t\t\t\t\t\t\t\"\\n here is a traceback:\" +\\\n\t\t\t\t\t\t\t\ttraceback.format_exc())\n\t\t\t\t\t\traise(e)\n\n\t\t\texcept:\n\t\t\t\tself.logger.warning(\"Sampling based optimization with %i samples failed\\n %s \\nUsing random configuration\"%(self.num_samples, traceback.format_exc()))\n\t\t\t\tsample = self.configspace.sample_configuration()\n\t\t\t\tinfo_dict['model_based_pick']  = False\n\n\n\t\ttry:\n\t\t\tsample = ConfigSpace.util.deactivate_inactive_hyperparameters(\n\t\t\t\tconfiguration_space=self.configspace,\n\t\t\t\tconfiguration=sample.get_dictionary()\n\t\t\t).get_dictionary()\n\t\texcept Exception as e:\n\t\t\tself.logger.warning(\"Error (%s) converting configuration: %s -> \"\n\t\t\t\t\t\t\t\t\"using random configuration!\",\n\t\t\t\t\t\t\t\te,\n\t\t\t\t\t\t\t\tsample)\n\t\t\tsample = self.configspace.sample_configuration().get_dictionary()\n\t\tself.logger.debug('done sampling a new configuration.')\n\t\treturn sample, info_dict", "code_tokens": ["def", "get_config", "(", "self", ",", "budget", ")", ":", "self", ".", "logger", ".", "debug", "(", "'start sampling a new configuration.'", ")", "sample", "=", "None", "info_dict", "=", "{", "}", "# If no model is available, sample from prior", "# also mix in a fraction of random configs", "if", "len", "(", "self", ".", "kde_models", ".", "keys", "(", ")", ")", "==", "0", "or", "np", ".", "random", ".", "rand", "(", ")", "<", "self", ".", "random_fraction", ":", "sample", "=", "self", ".", "configspace", ".", "sample_configuration", "(", ")", "info_dict", "[", "'model_based_pick'", "]", "=", "False", "best", "=", "np", ".", "inf", "best_vector", "=", "None", "if", "sample", "is", "None", ":", "try", ":", "#sample from largest budget", "budget", "=", "max", "(", "self", ".", "kde_models", ".", "keys", "(", ")", ")", "l", "=", "self", ".", "kde_models", "[", "budget", "]", "[", "'good'", "]", ".", "pdf", "g", "=", "self", ".", "kde_models", "[", "budget", "]", "[", "'bad'", "]", ".", "pdf", "minimize_me", "=", "lambda", "x", ":", "max", "(", "1e-32", ",", "g", "(", "x", ")", ")", "/", "max", "(", "l", "(", "x", ")", ",", "1e-32", ")", "kde_good", "=", "self", ".", "kde_models", "[", "budget", "]", "[", "'good'", "]", "kde_bad", "=", "self", ".", "kde_models", "[", "budget", "]", "[", "'bad'", "]", "for", "i", "in", "range", "(", "self", ".", "num_samples", ")", ":", "idx", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "len", "(", "kde_good", ".", "data", ")", ")", "datum", "=", "kde_good", ".", "data", "[", "idx", "]", "vector", "=", "[", "]", "for", "m", ",", "bw", ",", "t", "in", "zip", "(", "datum", ",", "kde_good", ".", "bw", ",", "self", ".", "vartypes", ")", ":", "bw", "=", "max", "(", "bw", ",", "self", ".", "min_bandwidth", ")", "if", "t", "==", "0", ":", "bw", "=", "self", ".", "bw_factor", "*", "bw", "try", ":", "vector", ".", "append", "(", "sps", ".", "truncnorm", ".", "rvs", "(", "-", "m", "/", "bw", ",", "(", "1", "-", "m", ")", "/", "bw", ",", "loc", "=", "m", ",", "scale", "=", "bw", ")", ")", "except", ":", "self", ".", "logger", ".", "warning", "(", "\"Truncated Normal failed for:\\ndatum=%s\\nbandwidth=%s\\nfor entry with value %s\"", "%", "(", "datum", ",", "kde_good", ".", "bw", ",", "m", ")", ")", "self", ".", "logger", ".", "warning", "(", "\"data in the KDE:\\n%s\"", "%", "kde_good", ".", "data", ")", "else", ":", "if", "np", ".", "random", ".", "rand", "(", ")", "<", "(", "1", "-", "bw", ")", ":", "vector", ".", "append", "(", "int", "(", "m", ")", ")", "else", ":", "vector", ".", "append", "(", "np", ".", "random", ".", "randint", "(", "t", ")", ")", "val", "=", "minimize_me", "(", "vector", ")", "if", "not", "np", ".", "isfinite", "(", "val", ")", ":", "self", ".", "logger", ".", "warning", "(", "'sampled vector: %s has EI value %s'", "%", "(", "vector", ",", "val", ")", ")", "self", ".", "logger", ".", "warning", "(", "\"data in the KDEs:\\n%s\\n%s\"", "%", "(", "kde_good", ".", "data", ",", "kde_bad", ".", "data", ")", ")", "self", ".", "logger", ".", "warning", "(", "\"bandwidth of the KDEs:\\n%s\\n%s\"", "%", "(", "kde_good", ".", "bw", ",", "kde_bad", ".", "bw", ")", ")", "self", ".", "logger", ".", "warning", "(", "\"l(x) = %s\"", "%", "(", "l", "(", "vector", ")", ")", ")", "self", ".", "logger", ".", "warning", "(", "\"g(x) = %s\"", "%", "(", "g", "(", "vector", ")", ")", ")", "# right now, this happens because a KDE does not contain all values for a categorical parameter", "# this cannot be fixed with the statsmodels KDE, so for now, we are just going to evaluate this one", "# if the good_kde has a finite value, i.e. there is no config with that value in the bad kde, so it shouldn't be terrible.", "if", "np", ".", "isfinite", "(", "l", "(", "vector", ")", ")", ":", "best_vector", "=", "vector", "break", "if", "val", "<", "best", ":", "best", "=", "val", "best_vector", "=", "vector", "if", "best_vector", "is", "None", ":", "self", ".", "logger", ".", "debug", "(", "\"Sampling based optimization with %i samples failed -> using random configuration\"", "%", "self", ".", "num_samples", ")", "sample", "=", "self", ".", "configspace", ".", "sample_configuration", "(", ")", ".", "get_dictionary", "(", ")", "info_dict", "[", "'model_based_pick'", "]", "=", "False", "else", ":", "self", ".", "logger", ".", "debug", "(", "'best_vector: {}, {}, {}, {}'", ".", "format", "(", "best_vector", ",", "best", ",", "l", "(", "best_vector", ")", ",", "g", "(", "best_vector", ")", ")", ")", "for", "i", ",", "hp_value", "in", "enumerate", "(", "best_vector", ")", ":", "if", "isinstance", "(", "self", ".", "configspace", ".", "get_hyperparameter", "(", "self", ".", "configspace", ".", "get_hyperparameter_by_idx", "(", "i", ")", ")", ",", "ConfigSpace", ".", "hyperparameters", ".", "CategoricalHyperparameter", ")", ":", "best_vector", "[", "i", "]", "=", "int", "(", "np", ".", "rint", "(", "best_vector", "[", "i", "]", ")", ")", "sample", "=", "ConfigSpace", ".", "Configuration", "(", "self", ".", "configspace", ",", "vector", "=", "best_vector", ")", ".", "get_dictionary", "(", ")", "try", ":", "sample", "=", "ConfigSpace", ".", "util", ".", "deactivate_inactive_hyperparameters", "(", "configuration_space", "=", "self", ".", "configspace", ",", "configuration", "=", "sample", ")", "info_dict", "[", "'model_based_pick'", "]", "=", "True", "except", "Exception", "as", "e", ":", "self", ".", "logger", ".", "warning", "(", "(", "\"=\"", "*", "50", "+", "\"\\n\"", ")", "*", "3", "+", "\"Error converting configuration:\\n%s\"", "%", "sample", "+", "\"\\n here is a traceback:\"", "+", "traceback", ".", "format_exc", "(", ")", ")", "raise", "(", "e", ")", "except", ":", "self", ".", "logger", ".", "warning", "(", "\"Sampling based optimization with %i samples failed\\n %s \\nUsing random configuration\"", "%", "(", "self", ".", "num_samples", ",", "traceback", ".", "format_exc", "(", ")", ")", ")", "sample", "=", "self", ".", "configspace", ".", "sample_configuration", "(", ")", "info_dict", "[", "'model_based_pick'", "]", "=", "False", "try", ":", "sample", "=", "ConfigSpace", ".", "util", ".", "deactivate_inactive_hyperparameters", "(", "configuration_space", "=", "self", ".", "configspace", ",", "configuration", "=", "sample", ".", "get_dictionary", "(", ")", ")", ".", "get_dictionary", "(", ")", "except", "Exception", "as", "e", ":", "self", ".", "logger", ".", "warning", "(", "\"Error (%s) converting configuration: %s -> \"", "\"using random configuration!\"", ",", "e", ",", "sample", ")", "sample", "=", "self", ".", "configspace", ".", "sample_configuration", "(", ")", ".", "get_dictionary", "(", ")", "self", ".", "logger", ".", "debug", "(", "'done sampling a new configuration.'", ")", "return", "sample", ",", "info_dict"], "docstring": "Function to sample a new configuration\n\n\t\t\tThis function is called inside Hyperband to query a new configuration\n\n\n\t\t\tParameters:\n\t\t\t-----------\n\t\t\tbudget: float\n\t\t\t\tthe budget for which this configuration is scheduled\n\n\t\t\treturns: config\n\t\t\t\tshould return a valid configuration", "docstring_tokens": ["Function", "to", "sample", "a", "new", "configuration"], "sha": "841db4b827f342e5eb7f725723ea6461ac52d45a", "url": "https://github.com/automl/HpBandSter/blob/841db4b827f342e5eb7f725723ea6461ac52d45a/hpbandster/optimizers/config_generators/bohb.py#L99-L234", "partition": "train"}
{"repo": "scikit-tda/kepler-mapper", "path": "kmapper/adapter.py", "func_name": "to_networkx", "original_string": "def to_networkx(graph):\n    \"\"\" Convert a Mapper 1-complex to a networkx graph.\n\n    Parameters\n    -----------\n\n    graph: dictionary, graph object returned from `kmapper.map`\n\n    Returns\n    --------\n\n    g: graph as networkx.Graph() object\n\n    \"\"\"\n\n    # import here so networkx is not always required.\n    import networkx as nx\n\n    nodes = graph[\"nodes\"].keys()\n    edges = [[start, end] for start, ends in graph[\"links\"].items() for end in ends]\n\n    g = nx.Graph()\n    g.add_nodes_from(nodes)\n    nx.set_node_attributes(g, dict(graph[\"nodes\"]), \"membership\")\n\n    g.add_edges_from(edges)\n\n    return g", "language": "python", "code": "def to_networkx(graph):\n    \"\"\" Convert a Mapper 1-complex to a networkx graph.\n\n    Parameters\n    -----------\n\n    graph: dictionary, graph object returned from `kmapper.map`\n\n    Returns\n    --------\n\n    g: graph as networkx.Graph() object\n\n    \"\"\"\n\n    # import here so networkx is not always required.\n    import networkx as nx\n\n    nodes = graph[\"nodes\"].keys()\n    edges = [[start, end] for start, ends in graph[\"links\"].items() for end in ends]\n\n    g = nx.Graph()\n    g.add_nodes_from(nodes)\n    nx.set_node_attributes(g, dict(graph[\"nodes\"]), \"membership\")\n\n    g.add_edges_from(edges)\n\n    return g", "code_tokens": ["def", "to_networkx", "(", "graph", ")", ":", "# import here so networkx is not always required.", "import", "networkx", "as", "nx", "nodes", "=", "graph", "[", "\"nodes\"", "]", ".", "keys", "(", ")", "edges", "=", "[", "[", "start", ",", "end", "]", "for", "start", ",", "ends", "in", "graph", "[", "\"links\"", "]", ".", "items", "(", ")", "for", "end", "in", "ends", "]", "g", "=", "nx", ".", "Graph", "(", ")", "g", ".", "add_nodes_from", "(", "nodes", ")", "nx", ".", "set_node_attributes", "(", "g", ",", "dict", "(", "graph", "[", "\"nodes\"", "]", ")", ",", "\"membership\"", ")", "g", ".", "add_edges_from", "(", "edges", ")", "return", "g"], "docstring": "Convert a Mapper 1-complex to a networkx graph.\n\n    Parameters\n    -----------\n\n    graph: dictionary, graph object returned from `kmapper.map`\n\n    Returns\n    --------\n\n    g: graph as networkx.Graph() object", "docstring_tokens": ["Convert", "a", "Mapper", "1", "-", "complex", "to", "a", "networkx", "graph", "."], "sha": "d4ed39f6392b0a134dd573d7d9c4aa65fbef3a7d", "url": "https://github.com/scikit-tda/kepler-mapper/blob/d4ed39f6392b0a134dd573d7d9c4aa65fbef3a7d/kmapper/adapter.py#L8-L35", "partition": "train"}
{"repo": "PyThaiNLP/pythainlp", "path": "pythainlp/spell/__init__.py", "func_name": "spell", "original_string": "def spell(word: str, engine: str = \"pn\") -> List[str]:\n    \"\"\"\n    :param str word: word to check spelling\n    :param str engine:\n        * pn - Peter Norvig's algorithm (default)\n    :return: list of words\n    \"\"\"\n\n    return DEFAULT_SPELL_CHECKER.spell(word)", "language": "python", "code": "def spell(word: str, engine: str = \"pn\") -> List[str]:\n    \"\"\"\n    :param str word: word to check spelling\n    :param str engine:\n        * pn - Peter Norvig's algorithm (default)\n    :return: list of words\n    \"\"\"\n\n    return DEFAULT_SPELL_CHECKER.spell(word)", "code_tokens": ["def", "spell", "(", "word", ":", "str", ",", "engine", ":", "str", "=", "\"pn\"", ")", "->", "List", "[", "str", "]", ":", "return", "DEFAULT_SPELL_CHECKER", ".", "spell", "(", "word", ")"], "docstring": ":param str word: word to check spelling\n    :param str engine:\n        * pn - Peter Norvig's algorithm (default)\n    :return: list of words", "docstring_tokens": [":", "param", "str", "word", ":", "word", "to", "check", "spelling", ":", "param", "str", "engine", ":", "*", "pn", "-", "Peter", "Norvig", "s", "algorithm", "(", "default", ")", ":", "return", ":", "list", "of", "words"], "sha": "e9a300b8a99dfd1a67a955e7c06f62e4afe0fbca", "url": "https://github.com/PyThaiNLP/pythainlp/blob/e9a300b8a99dfd1a67a955e7c06f62e4afe0fbca/pythainlp/spell/__init__.py#L13-L21", "partition": "train"}
{"repo": "splunk/splunk-sdk-python", "path": "splunklib/searchcommands/environment.py", "func_name": "configure_logging", "original_string": "def configure_logging(logger_name, filename=None):\n    \"\"\" Configure logging and return the named logger and the location of the logging configuration file loaded.\n\n    This function expects a Splunk app directory structure::\n\n        <app-root>\n            bin\n                ...\n            default\n                ...\n            local\n                ...\n\n    This function looks for a logging configuration file at each of these locations, loading the first, if any,\n    logging configuration file that it finds::\n\n        local/{name}.logging.conf\n        default/{name}.logging.conf\n        local/logging.conf\n        default/logging.conf\n\n    The current working directory is set to *<app-root>* before the logging configuration file is loaded. Hence, paths\n    in the logging configuration file are relative to *<app-root>*. The current directory is reset before return.\n\n    You may short circuit the search for a logging configuration file by providing an alternative file location in\n    `path`. Logging configuration files must be in `ConfigParser format`_.\n\n    #Arguments:\n\n    :param logger_name: Logger name\n    :type logger_name: bytes, unicode\n\n    :param filename: Location of an alternative logging configuration file or `None`.\n    :type filename: bytes, unicode or NoneType\n\n    :returns: The named logger and the location of the logging configuration file loaded.\n    :rtype: tuple\n\n    .. _ConfigParser format: https://docs.python.org/2/library/logging.config.html#configuration-file-format\n\n    \"\"\"\n    if filename is None:\n        if logger_name is None:\n            probing_paths = [path.join('local', 'logging.conf'), path.join('default', 'logging.conf')]\n        else:\n            probing_paths = [\n                path.join('local', logger_name + '.logging.conf'),\n                path.join('default', logger_name + '.logging.conf'),\n                path.join('local', 'logging.conf'),\n                path.join('default', 'logging.conf')]\n        for relative_path in probing_paths:\n            configuration_file = path.join(app_root, relative_path)\n            if path.exists(configuration_file):\n                filename = configuration_file\n                break\n    elif not path.isabs(filename):\n        found = False\n        for conf in 'local', 'default':\n            configuration_file = path.join(app_root, conf, filename)\n            if path.exists(configuration_file):\n                filename = configuration_file\n                found = True\n                break\n        if not found:\n            raise ValueError('Logging configuration file \"{}\" not found in local or default directory'.format(filename))\n    elif not path.exists(filename):\n        raise ValueError('Logging configuration file \"{}\" not found'.format(filename))\n\n    if filename is not None:\n        global _current_logging_configuration_file\n        filename = path.realpath(filename)\n\n        if filename != _current_logging_configuration_file:\n            working_directory = getcwd()\n            chdir(app_root)\n            try:\n                fileConfig(filename, {'SPLUNK_HOME': splunk_home})\n            finally:\n                chdir(working_directory)\n            _current_logging_configuration_file = filename\n\n    if len(root.handlers) == 0:\n        root.addHandler(StreamHandler())\n\n    return None if logger_name is None else getLogger(logger_name), filename", "language": "python", "code": "def configure_logging(logger_name, filename=None):\n    \"\"\" Configure logging and return the named logger and the location of the logging configuration file loaded.\n\n    This function expects a Splunk app directory structure::\n\n        <app-root>\n            bin\n                ...\n            default\n                ...\n            local\n                ...\n\n    This function looks for a logging configuration file at each of these locations, loading the first, if any,\n    logging configuration file that it finds::\n\n        local/{name}.logging.conf\n        default/{name}.logging.conf\n        local/logging.conf\n        default/logging.conf\n\n    The current working directory is set to *<app-root>* before the logging configuration file is loaded. Hence, paths\n    in the logging configuration file are relative to *<app-root>*. The current directory is reset before return.\n\n    You may short circuit the search for a logging configuration file by providing an alternative file location in\n    `path`. Logging configuration files must be in `ConfigParser format`_.\n\n    #Arguments:\n\n    :param logger_name: Logger name\n    :type logger_name: bytes, unicode\n\n    :param filename: Location of an alternative logging configuration file or `None`.\n    :type filename: bytes, unicode or NoneType\n\n    :returns: The named logger and the location of the logging configuration file loaded.\n    :rtype: tuple\n\n    .. _ConfigParser format: https://docs.python.org/2/library/logging.config.html#configuration-file-format\n\n    \"\"\"\n    if filename is None:\n        if logger_name is None:\n            probing_paths = [path.join('local', 'logging.conf'), path.join('default', 'logging.conf')]\n        else:\n            probing_paths = [\n                path.join('local', logger_name + '.logging.conf'),\n                path.join('default', logger_name + '.logging.conf'),\n                path.join('local', 'logging.conf'),\n                path.join('default', 'logging.conf')]\n        for relative_path in probing_paths:\n            configuration_file = path.join(app_root, relative_path)\n            if path.exists(configuration_file):\n                filename = configuration_file\n                break\n    elif not path.isabs(filename):\n        found = False\n        for conf in 'local', 'default':\n            configuration_file = path.join(app_root, conf, filename)\n            if path.exists(configuration_file):\n                filename = configuration_file\n                found = True\n                break\n        if not found:\n            raise ValueError('Logging configuration file \"{}\" not found in local or default directory'.format(filename))\n    elif not path.exists(filename):\n        raise ValueError('Logging configuration file \"{}\" not found'.format(filename))\n\n    if filename is not None:\n        global _current_logging_configuration_file\n        filename = path.realpath(filename)\n\n        if filename != _current_logging_configuration_file:\n            working_directory = getcwd()\n            chdir(app_root)\n            try:\n                fileConfig(filename, {'SPLUNK_HOME': splunk_home})\n            finally:\n                chdir(working_directory)\n            _current_logging_configuration_file = filename\n\n    if len(root.handlers) == 0:\n        root.addHandler(StreamHandler())\n\n    return None if logger_name is None else getLogger(logger_name), filename", "code_tokens": ["def", "configure_logging", "(", "logger_name", ",", "filename", "=", "None", ")", ":", "if", "filename", "is", "None", ":", "if", "logger_name", "is", "None", ":", "probing_paths", "=", "[", "path", ".", "join", "(", "'local'", ",", "'logging.conf'", ")", ",", "path", ".", "join", "(", "'default'", ",", "'logging.conf'", ")", "]", "else", ":", "probing_paths", "=", "[", "path", ".", "join", "(", "'local'", ",", "logger_name", "+", "'.logging.conf'", ")", ",", "path", ".", "join", "(", "'default'", ",", "logger_name", "+", "'.logging.conf'", ")", ",", "path", ".", "join", "(", "'local'", ",", "'logging.conf'", ")", ",", "path", ".", "join", "(", "'default'", ",", "'logging.conf'", ")", "]", "for", "relative_path", "in", "probing_paths", ":", "configuration_file", "=", "path", ".", "join", "(", "app_root", ",", "relative_path", ")", "if", "path", ".", "exists", "(", "configuration_file", ")", ":", "filename", "=", "configuration_file", "break", "elif", "not", "path", ".", "isabs", "(", "filename", ")", ":", "found", "=", "False", "for", "conf", "in", "'local'", ",", "'default'", ":", "configuration_file", "=", "path", ".", "join", "(", "app_root", ",", "conf", ",", "filename", ")", "if", "path", ".", "exists", "(", "configuration_file", ")", ":", "filename", "=", "configuration_file", "found", "=", "True", "break", "if", "not", "found", ":", "raise", "ValueError", "(", "'Logging configuration file \"{}\" not found in local or default directory'", ".", "format", "(", "filename", ")", ")", "elif", "not", "path", ".", "exists", "(", "filename", ")", ":", "raise", "ValueError", "(", "'Logging configuration file \"{}\" not found'", ".", "format", "(", "filename", ")", ")", "if", "filename", "is", "not", "None", ":", "global", "_current_logging_configuration_file", "filename", "=", "path", ".", "realpath", "(", "filename", ")", "if", "filename", "!=", "_current_logging_configuration_file", ":", "working_directory", "=", "getcwd", "(", ")", "chdir", "(", "app_root", ")", "try", ":", "fileConfig", "(", "filename", ",", "{", "'SPLUNK_HOME'", ":", "splunk_home", "}", ")", "finally", ":", "chdir", "(", "working_directory", ")", "_current_logging_configuration_file", "=", "filename", "if", "len", "(", "root", ".", "handlers", ")", "==", "0", ":", "root", ".", "addHandler", "(", "StreamHandler", "(", ")", ")", "return", "None", "if", "logger_name", "is", "None", "else", "getLogger", "(", "logger_name", ")", ",", "filename"], "docstring": "Configure logging and return the named logger and the location of the logging configuration file loaded.\n\n    This function expects a Splunk app directory structure::\n\n        <app-root>\n            bin\n                ...\n            default\n                ...\n            local\n                ...\n\n    This function looks for a logging configuration file at each of these locations, loading the first, if any,\n    logging configuration file that it finds::\n\n        local/{name}.logging.conf\n        default/{name}.logging.conf\n        local/logging.conf\n        default/logging.conf\n\n    The current working directory is set to *<app-root>* before the logging configuration file is loaded. Hence, paths\n    in the logging configuration file are relative to *<app-root>*. The current directory is reset before return.\n\n    You may short circuit the search for a logging configuration file by providing an alternative file location in\n    `path`. Logging configuration files must be in `ConfigParser format`_.\n\n    #Arguments:\n\n    :param logger_name: Logger name\n    :type logger_name: bytes, unicode\n\n    :param filename: Location of an alternative logging configuration file or `None`.\n    :type filename: bytes, unicode or NoneType\n\n    :returns: The named logger and the location of the logging configuration file loaded.\n    :rtype: tuple\n\n    .. _ConfigParser format: https://docs.python.org/2/library/logging.config.html#configuration-file-format", "docstring_tokens": ["Configure", "logging", "and", "return", "the", "named", "logger", "and", "the", "location", "of", "the", "logging", "configuration", "file", "loaded", "."], "sha": "a245a4eeb93b3621730418008e31715912bcdcd8", "url": "https://github.com/splunk/splunk-sdk-python/blob/a245a4eeb93b3621730418008e31715912bcdcd8/splunklib/searchcommands/environment.py#L27-L111", "partition": "train"}
{"repo": "kennethreitz/legit", "path": "legit/scm.py", "func_name": "SCMRepo.git_exec", "original_string": "def git_exec(self, command, **kwargs):\n        \"\"\"Execute git commands\"\"\"\n        from .cli import verbose_echo\n\n        command.insert(0, self.git)\n        if kwargs.pop('no_verbose', False):  # used when git output isn't helpful to user\n            verbose = False\n        else:\n            verbose = self.verbose\n        verbose_echo(' '.join(command), verbose, self.fake)\n\n        if not self.fake:\n            result = self.repo.git.execute(command, **kwargs)\n        else:\n            if 'with_extended_output' in kwargs:\n                result = (0, '', '')\n            else:\n                result = ''\n        return result", "language": "python", "code": "def git_exec(self, command, **kwargs):\n        \"\"\"Execute git commands\"\"\"\n        from .cli import verbose_echo\n\n        command.insert(0, self.git)\n        if kwargs.pop('no_verbose', False):  # used when git output isn't helpful to user\n            verbose = False\n        else:\n            verbose = self.verbose\n        verbose_echo(' '.join(command), verbose, self.fake)\n\n        if not self.fake:\n            result = self.repo.git.execute(command, **kwargs)\n        else:\n            if 'with_extended_output' in kwargs:\n                result = (0, '', '')\n            else:\n                result = ''\n        return result", "code_tokens": ["def", "git_exec", "(", "self", ",", "command", ",", "*", "*", "kwargs", ")", ":", "from", ".", "cli", "import", "verbose_echo", "command", ".", "insert", "(", "0", ",", "self", ".", "git", ")", "if", "kwargs", ".", "pop", "(", "'no_verbose'", ",", "False", ")", ":", "# used when git output isn't helpful to user", "verbose", "=", "False", "else", ":", "verbose", "=", "self", ".", "verbose", "verbose_echo", "(", "' '", ".", "join", "(", "command", ")", ",", "verbose", ",", "self", ".", "fake", ")", "if", "not", "self", ".", "fake", ":", "result", "=", "self", ".", "repo", ".", "git", ".", "execute", "(", "command", ",", "*", "*", "kwargs", ")", "else", ":", "if", "'with_extended_output'", "in", "kwargs", ":", "result", "=", "(", "0", ",", "''", ",", "''", ")", "else", ":", "result", "=", "''", "return", "result"], "docstring": "Execute git commands", "docstring_tokens": ["Execute", "git", "commands"], "sha": "699802c5be665bd358456a940953b5c1d8672754", "url": "https://github.com/kennethreitz/legit/blob/699802c5be665bd358456a940953b5c1d8672754/legit/scm.py#L47-L65", "partition": "train"}
{"repo": "nschloe/meshio", "path": "meshio/abaqus_io.py", "func_name": "get_param_map", "original_string": "def get_param_map(word, required_keys=None):\n    \"\"\"\n    get the optional arguments on a line\n\n    Example\n    -------\n    >>> iline = 0\n    >>> word = 'elset,instance=dummy2,generate'\n    >>> params = get_param_map(iline, word, required_keys=['instance'])\n    params = {\n        'elset' : None,\n        'instance' : 'dummy2,\n        'generate' : None,\n    }\n    \"\"\"\n    if required_keys is None:\n        required_keys = []\n    words = word.split(\",\")\n    param_map = {}\n    for wordi in words:\n        if \"=\" not in wordi:\n            key = wordi.strip()\n            value = None\n        else:\n            sword = wordi.split(\"=\")\n            assert len(sword) == 2, sword\n            key = sword[0].strip()\n            value = sword[1].strip()\n        param_map[key] = value\n\n    msg = \"\"\n    for key in required_keys:\n        if key not in param_map:\n            msg += \"%r not found in %r\\n\" % (key, word)\n    if msg:\n        raise RuntimeError(msg)\n    return param_map", "language": "python", "code": "def get_param_map(word, required_keys=None):\n    \"\"\"\n    get the optional arguments on a line\n\n    Example\n    -------\n    >>> iline = 0\n    >>> word = 'elset,instance=dummy2,generate'\n    >>> params = get_param_map(iline, word, required_keys=['instance'])\n    params = {\n        'elset' : None,\n        'instance' : 'dummy2,\n        'generate' : None,\n    }\n    \"\"\"\n    if required_keys is None:\n        required_keys = []\n    words = word.split(\",\")\n    param_map = {}\n    for wordi in words:\n        if \"=\" not in wordi:\n            key = wordi.strip()\n            value = None\n        else:\n            sword = wordi.split(\"=\")\n            assert len(sword) == 2, sword\n            key = sword[0].strip()\n            value = sword[1].strip()\n        param_map[key] = value\n\n    msg = \"\"\n    for key in required_keys:\n        if key not in param_map:\n            msg += \"%r not found in %r\\n\" % (key, word)\n    if msg:\n        raise RuntimeError(msg)\n    return param_map", "code_tokens": ["def", "get_param_map", "(", "word", ",", "required_keys", "=", "None", ")", ":", "if", "required_keys", "is", "None", ":", "required_keys", "=", "[", "]", "words", "=", "word", ".", "split", "(", "\",\"", ")", "param_map", "=", "{", "}", "for", "wordi", "in", "words", ":", "if", "\"=\"", "not", "in", "wordi", ":", "key", "=", "wordi", ".", "strip", "(", ")", "value", "=", "None", "else", ":", "sword", "=", "wordi", ".", "split", "(", "\"=\"", ")", "assert", "len", "(", "sword", ")", "==", "2", ",", "sword", "key", "=", "sword", "[", "0", "]", ".", "strip", "(", ")", "value", "=", "sword", "[", "1", "]", ".", "strip", "(", ")", "param_map", "[", "key", "]", "=", "value", "msg", "=", "\"\"", "for", "key", "in", "required_keys", ":", "if", "key", "not", "in", "param_map", ":", "msg", "+=", "\"%r not found in %r\\n\"", "%", "(", "key", ",", "word", ")", "if", "msg", ":", "raise", "RuntimeError", "(", "msg", ")", "return", "param_map"], "docstring": "get the optional arguments on a line\n\n    Example\n    -------\n    >>> iline = 0\n    >>> word = 'elset,instance=dummy2,generate'\n    >>> params = get_param_map(iline, word, required_keys=['instance'])\n    params = {\n        'elset' : None,\n        'instance' : 'dummy2,\n        'generate' : None,\n    }", "docstring_tokens": ["get", "the", "optional", "arguments", "on", "a", "line"], "sha": "cb78285962b573fb46a4f3f54276206d922bdbcb", "url": "https://github.com/nschloe/meshio/blob/cb78285962b573fb46a4f3f54276206d922bdbcb/meshio/abaqus_io.py#L193-L229", "partition": "train"}
{"repo": "ly0/baidupcsapi", "path": "baidupcsapi/api.py", "func_name": "check_login", "original_string": "def check_login(func):\n    \"\"\"\u68c0\u67e5\u7528\u6237\u767b\u5f55\u72b6\u6001\n    :param func: \u9700\u8981\u88ab\u68c0\u67e5\u7684\u51fd\u6570\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        ret = func(*args, **kwargs)\n        if type(ret) == requests.Response:\n            # \u68c0\u6d4b\u7ed3\u679c\u662f\u5426\u4e3aJSON\n            if ret.content[0]!=b'{' and ret.content[0]!=b'[':\n                return ret\n            try:\n                foo = json.loads(ret.content.decode('utf-8'))\n                if 'errno' in foo and foo['errno'] == -6:\n                    logging.debug(\n                            'Offline, deleting cookies file then relogin.')\n                    path = '.{0}.cookies'.format(args[0].username)\n                    if os.path.exists(path):\n                        os.remove(path)\n                    args[0]._initiate()\n            except:\n                raise LoginFailed('User unsigned in.')\n        return ret\n\n    return wrapper", "language": "python", "code": "def check_login(func):\n    \"\"\"\u68c0\u67e5\u7528\u6237\u767b\u5f55\u72b6\u6001\n    :param func: \u9700\u8981\u88ab\u68c0\u67e5\u7684\u51fd\u6570\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        ret = func(*args, **kwargs)\n        if type(ret) == requests.Response:\n            # \u68c0\u6d4b\u7ed3\u679c\u662f\u5426\u4e3aJSON\n            if ret.content[0]!=b'{' and ret.content[0]!=b'[':\n                return ret\n            try:\n                foo = json.loads(ret.content.decode('utf-8'))\n                if 'errno' in foo and foo['errno'] == -6:\n                    logging.debug(\n                            'Offline, deleting cookies file then relogin.')\n                    path = '.{0}.cookies'.format(args[0].username)\n                    if os.path.exists(path):\n                        os.remove(path)\n                    args[0]._initiate()\n            except:\n                raise LoginFailed('User unsigned in.')\n        return ret\n\n    return wrapper", "code_tokens": ["def", "check_login", "(", "func", ")", ":", "@", "wraps", "(", "func", ")", "def", "wrapper", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "ret", "=", "func", "(", "*", "args", ",", "*", "*", "kwargs", ")", "if", "type", "(", "ret", ")", "==", "requests", ".", "Response", ":", "# \u68c0\u6d4b\u7ed3\u679c\u662f\u5426\u4e3aJSON", "if", "ret", ".", "content", "[", "0", "]", "!=", "b'{'", "and", "ret", ".", "content", "[", "0", "]", "!=", "b'['", ":", "return", "ret", "try", ":", "foo", "=", "json", ".", "loads", "(", "ret", ".", "content", ".", "decode", "(", "'utf-8'", ")", ")", "if", "'errno'", "in", "foo", "and", "foo", "[", "'errno'", "]", "==", "-", "6", ":", "logging", ".", "debug", "(", "'Offline, deleting cookies file then relogin.'", ")", "path", "=", "'.{0}.cookies'", ".", "format", "(", "args", "[", "0", "]", ".", "username", ")", "if", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "os", ".", "remove", "(", "path", ")", "args", "[", "0", "]", ".", "_initiate", "(", ")", "except", ":", "raise", "LoginFailed", "(", "'User unsigned in.'", ")", "return", "ret", "return", "wrapper"], "docstring": "\u68c0\u67e5\u7528\u6237\u767b\u5f55\u72b6\u6001\n    :param func: \u9700\u8981\u88ab\u68c0\u67e5\u7684\u51fd\u6570", "docstring_tokens": ["\u68c0\u67e5\u7528\u6237\u767b\u5f55\u72b6\u6001", ":", "param", "func", ":", "\u9700\u8981\u88ab\u68c0\u67e5\u7684\u51fd\u6570"], "sha": "6f6feeef0767a75b3b968924727460eb09242d76", "url": "https://github.com/ly0/baidupcsapi/blob/6f6feeef0767a75b3b968924727460eb09242d76/baidupcsapi/api.py#L119-L144", "partition": "train"}
{"repo": "spulec/moto", "path": "moto/batch/models.py", "func_name": "BatchBackend.update_job_queue", "original_string": "def update_job_queue(self, queue_name, priority, state, compute_env_order):\n        \"\"\"\n        Update a job queue\n\n        :param queue_name: Queue name\n        :type queue_name: str\n        :param priority: Queue priority\n        :type priority: int\n        :param state: Queue state\n        :type state: string\n        :param compute_env_order: Compute environment list\n        :type compute_env_order: list of dict\n        :return: Tuple of Name, ARN\n        :rtype: tuple of str\n        \"\"\"\n        if queue_name is None:\n            raise ClientException('jobQueueName must be provided')\n\n        job_queue = self.get_job_queue(queue_name)\n        if job_queue is None:\n            raise ClientException('Job queue {0} does not exist'.format(queue_name))\n\n        if state is not None:\n            if state not in ('ENABLED', 'DISABLED'):\n                raise ClientException('state {0} must be one of ENABLED | DISABLED'.format(state))\n\n            job_queue.state = state\n\n        if compute_env_order is not None:\n            if len(compute_env_order) == 0:\n                raise ClientException('At least 1 compute environment must be provided')\n            try:\n                # orders and extracts computeEnvironment names\n                ordered_compute_environments = [item['computeEnvironment'] for item in sorted(compute_env_order, key=lambda x: x['order'])]\n                env_objects = []\n                # Check each ARN exists, then make a list of compute env's\n                for arn in ordered_compute_environments:\n                    env = self.get_compute_environment_by_arn(arn)\n                    if env is None:\n                        raise ClientException('Compute environment {0} does not exist'.format(arn))\n                    env_objects.append(env)\n            except Exception:\n                raise ClientException('computeEnvironmentOrder is malformed')\n\n            job_queue.env_order_json = compute_env_order\n            job_queue.environments = env_objects\n\n        if priority is not None:\n            job_queue.priority = priority\n\n        return queue_name, job_queue.arn", "language": "python", "code": "def update_job_queue(self, queue_name, priority, state, compute_env_order):\n        \"\"\"\n        Update a job queue\n\n        :param queue_name: Queue name\n        :type queue_name: str\n        :param priority: Queue priority\n        :type priority: int\n        :param state: Queue state\n        :type state: string\n        :param compute_env_order: Compute environment list\n        :type compute_env_order: list of dict\n        :return: Tuple of Name, ARN\n        :rtype: tuple of str\n        \"\"\"\n        if queue_name is None:\n            raise ClientException('jobQueueName must be provided')\n\n        job_queue = self.get_job_queue(queue_name)\n        if job_queue is None:\n            raise ClientException('Job queue {0} does not exist'.format(queue_name))\n\n        if state is not None:\n            if state not in ('ENABLED', 'DISABLED'):\n                raise ClientException('state {0} must be one of ENABLED | DISABLED'.format(state))\n\n            job_queue.state = state\n\n        if compute_env_order is not None:\n            if len(compute_env_order) == 0:\n                raise ClientException('At least 1 compute environment must be provided')\n            try:\n                # orders and extracts computeEnvironment names\n                ordered_compute_environments = [item['computeEnvironment'] for item in sorted(compute_env_order, key=lambda x: x['order'])]\n                env_objects = []\n                # Check each ARN exists, then make a list of compute env's\n                for arn in ordered_compute_environments:\n                    env = self.get_compute_environment_by_arn(arn)\n                    if env is None:\n                        raise ClientException('Compute environment {0} does not exist'.format(arn))\n                    env_objects.append(env)\n            except Exception:\n                raise ClientException('computeEnvironmentOrder is malformed')\n\n            job_queue.env_order_json = compute_env_order\n            job_queue.environments = env_objects\n\n        if priority is not None:\n            job_queue.priority = priority\n\n        return queue_name, job_queue.arn", "code_tokens": ["def", "update_job_queue", "(", "self", ",", "queue_name", ",", "priority", ",", "state", ",", "compute_env_order", ")", ":", "if", "queue_name", "is", "None", ":", "raise", "ClientException", "(", "'jobQueueName must be provided'", ")", "job_queue", "=", "self", ".", "get_job_queue", "(", "queue_name", ")", "if", "job_queue", "is", "None", ":", "raise", "ClientException", "(", "'Job queue {0} does not exist'", ".", "format", "(", "queue_name", ")", ")", "if", "state", "is", "not", "None", ":", "if", "state", "not", "in", "(", "'ENABLED'", ",", "'DISABLED'", ")", ":", "raise", "ClientException", "(", "'state {0} must be one of ENABLED | DISABLED'", ".", "format", "(", "state", ")", ")", "job_queue", ".", "state", "=", "state", "if", "compute_env_order", "is", "not", "None", ":", "if", "len", "(", "compute_env_order", ")", "==", "0", ":", "raise", "ClientException", "(", "'At least 1 compute environment must be provided'", ")", "try", ":", "# orders and extracts computeEnvironment names", "ordered_compute_environments", "=", "[", "item", "[", "'computeEnvironment'", "]", "for", "item", "in", "sorted", "(", "compute_env_order", ",", "key", "=", "lambda", "x", ":", "x", "[", "'order'", "]", ")", "]", "env_objects", "=", "[", "]", "# Check each ARN exists, then make a list of compute env's", "for", "arn", "in", "ordered_compute_environments", ":", "env", "=", "self", ".", "get_compute_environment_by_arn", "(", "arn", ")", "if", "env", "is", "None", ":", "raise", "ClientException", "(", "'Compute environment {0} does not exist'", ".", "format", "(", "arn", ")", ")", "env_objects", ".", "append", "(", "env", ")", "except", "Exception", ":", "raise", "ClientException", "(", "'computeEnvironmentOrder is malformed'", ")", "job_queue", ".", "env_order_json", "=", "compute_env_order", "job_queue", ".", "environments", "=", "env_objects", "if", "priority", "is", "not", "None", ":", "job_queue", ".", "priority", "=", "priority", "return", "queue_name", ",", "job_queue", ".", "arn"], "docstring": "Update a job queue\n\n        :param queue_name: Queue name\n        :type queue_name: str\n        :param priority: Queue priority\n        :type priority: int\n        :param state: Queue state\n        :type state: string\n        :param compute_env_order: Compute environment list\n        :type compute_env_order: list of dict\n        :return: Tuple of Name, ARN\n        :rtype: tuple of str", "docstring_tokens": ["Update", "a", "job", "queue"], "sha": "4a286c4bc288933bb023396e2784a6fdbb966bc9", "url": "https://github.com/spulec/moto/blob/4a286c4bc288933bb023396e2784a6fdbb966bc9/moto/batch/models.py#L874-L924", "partition": "train"}
{"repo": "chrismattmann/tika-python", "path": "tika/parser.py", "func_name": "from_file", "original_string": "def from_file(filename, serverEndpoint=ServerEndpoint, xmlContent=False, headers=None, config_path=None):\n    '''\n    Parses a file for metadata and content\n    :param filename: path to file which needs to be parsed\n    :param serverEndpoint: Server endpoint url\n    :param xmlContent: Whether or not XML content be requested.\n                    Default is 'False', which results in text content.\n    :param headers: Request headers to be sent to the tika reset server, should\n                    be a dictionary. This is optional\n    :return: dictionary having 'metadata' and 'content' keys.\n            'content' has a str value and metadata has a dict type value.\n    '''\n    if not xmlContent:\n        jsonOutput = parse1('all', filename, serverEndpoint, headers=headers, config_path=config_path)\n    else:\n        jsonOutput = parse1('all', filename, serverEndpoint, services={'meta': '/meta', 'text': '/tika', 'all': '/rmeta/xml'},\n                            headers=headers, config_path=config_path)\n    return _parse(jsonOutput)", "language": "python", "code": "def from_file(filename, serverEndpoint=ServerEndpoint, xmlContent=False, headers=None, config_path=None):\n    '''\n    Parses a file for metadata and content\n    :param filename: path to file which needs to be parsed\n    :param serverEndpoint: Server endpoint url\n    :param xmlContent: Whether or not XML content be requested.\n                    Default is 'False', which results in text content.\n    :param headers: Request headers to be sent to the tika reset server, should\n                    be a dictionary. This is optional\n    :return: dictionary having 'metadata' and 'content' keys.\n            'content' has a str value and metadata has a dict type value.\n    '''\n    if not xmlContent:\n        jsonOutput = parse1('all', filename, serverEndpoint, headers=headers, config_path=config_path)\n    else:\n        jsonOutput = parse1('all', filename, serverEndpoint, services={'meta': '/meta', 'text': '/tika', 'all': '/rmeta/xml'},\n                            headers=headers, config_path=config_path)\n    return _parse(jsonOutput)", "code_tokens": ["def", "from_file", "(", "filename", ",", "serverEndpoint", "=", "ServerEndpoint", ",", "xmlContent", "=", "False", ",", "headers", "=", "None", ",", "config_path", "=", "None", ")", ":", "if", "not", "xmlContent", ":", "jsonOutput", "=", "parse1", "(", "'all'", ",", "filename", ",", "serverEndpoint", ",", "headers", "=", "headers", ",", "config_path", "=", "config_path", ")", "else", ":", "jsonOutput", "=", "parse1", "(", "'all'", ",", "filename", ",", "serverEndpoint", ",", "services", "=", "{", "'meta'", ":", "'/meta'", ",", "'text'", ":", "'/tika'", ",", "'all'", ":", "'/rmeta/xml'", "}", ",", "headers", "=", "headers", ",", "config_path", "=", "config_path", ")", "return", "_parse", "(", "jsonOutput", ")"], "docstring": "Parses a file for metadata and content\n    :param filename: path to file which needs to be parsed\n    :param serverEndpoint: Server endpoint url\n    :param xmlContent: Whether or not XML content be requested.\n                    Default is 'False', which results in text content.\n    :param headers: Request headers to be sent to the tika reset server, should\n                    be a dictionary. This is optional\n    :return: dictionary having 'metadata' and 'content' keys.\n            'content' has a str value and metadata has a dict type value.", "docstring_tokens": ["Parses", "a", "file", "for", "metadata", "and", "content", ":", "param", "filename", ":", "path", "to", "file", "which", "needs", "to", "be", "parsed", ":", "param", "serverEndpoint", ":", "Server", "endpoint", "url", ":", "param", "xmlContent", ":", "Whether", "or", "not", "XML", "content", "be", "requested", ".", "Default", "is", "False", "which", "results", "in", "text", "content", ".", ":", "param", "headers", ":", "Request", "headers", "to", "be", "sent", "to", "the", "tika", "reset", "server", "should", "be", "a", "dictionary", ".", "This", "is", "optional", ":", "return", ":", "dictionary", "having", "metadata", "and", "content", "keys", ".", "content", "has", "a", "str", "value", "and", "metadata", "has", "a", "dict", "type", "value", "."], "sha": "ffd3879ac3eaa9142c0fb6557cc1dc52d458a75a", "url": "https://github.com/chrismattmann/tika-python/blob/ffd3879ac3eaa9142c0fb6557cc1dc52d458a75a/tika/parser.py#L23-L40", "partition": "train"}
{"repo": "Guake/guake", "path": "guake/terminal.py", "func_name": "GuakeTerminal.delete_shell", "original_string": "def delete_shell(self, pid):\n        \"\"\"This function will kill the shell on a tab, trying to send\n        a sigterm and if it doesn't work, a sigkill. Between these two\n        signals, we have a timeout of 3 seconds, so is recommended to\n        call this in another thread. This doesn't change any thing in\n        UI, so you can use python's start_new_thread.\n        \"\"\"\n        try:\n            os.kill(pid, signal.SIGHUP)\n        except OSError:\n            pass\n        num_tries = 30\n\n        while num_tries > 0:\n            try:\n                # Try to wait for the pid to be closed. If it does not\n                # exist anymore, an OSError is raised and we can\n                # safely ignore it.\n                if os.waitpid(pid, os.WNOHANG)[0] != 0:\n                    break\n            except OSError:\n                break\n            sleep(0.1)\n            num_tries -= 1\n\n        if num_tries == 0:\n            try:\n                os.kill(pid, signal.SIGKILL)\n                os.waitpid(pid, 0)\n            except OSError:\n                # if this part of code was reached, means that SIGTERM\n                # did the work and SIGKILL wasnt needed.\n                pass", "language": "python", "code": "def delete_shell(self, pid):\n        \"\"\"This function will kill the shell on a tab, trying to send\n        a sigterm and if it doesn't work, a sigkill. Between these two\n        signals, we have a timeout of 3 seconds, so is recommended to\n        call this in another thread. This doesn't change any thing in\n        UI, so you can use python's start_new_thread.\n        \"\"\"\n        try:\n            os.kill(pid, signal.SIGHUP)\n        except OSError:\n            pass\n        num_tries = 30\n\n        while num_tries > 0:\n            try:\n                # Try to wait for the pid to be closed. If it does not\n                # exist anymore, an OSError is raised and we can\n                # safely ignore it.\n                if os.waitpid(pid, os.WNOHANG)[0] != 0:\n                    break\n            except OSError:\n                break\n            sleep(0.1)\n            num_tries -= 1\n\n        if num_tries == 0:\n            try:\n                os.kill(pid, signal.SIGKILL)\n                os.waitpid(pid, 0)\n            except OSError:\n                # if this part of code was reached, means that SIGTERM\n                # did the work and SIGKILL wasnt needed.\n                pass", "code_tokens": ["def", "delete_shell", "(", "self", ",", "pid", ")", ":", "try", ":", "os", ".", "kill", "(", "pid", ",", "signal", ".", "SIGHUP", ")", "except", "OSError", ":", "pass", "num_tries", "=", "30", "while", "num_tries", ">", "0", ":", "try", ":", "# Try to wait for the pid to be closed. If it does not", "# exist anymore, an OSError is raised and we can", "# safely ignore it.", "if", "os", ".", "waitpid", "(", "pid", ",", "os", ".", "WNOHANG", ")", "[", "0", "]", "!=", "0", ":", "break", "except", "OSError", ":", "break", "sleep", "(", "0.1", ")", "num_tries", "-=", "1", "if", "num_tries", "==", "0", ":", "try", ":", "os", ".", "kill", "(", "pid", ",", "signal", ".", "SIGKILL", ")", "os", ".", "waitpid", "(", "pid", ",", "0", ")", "except", "OSError", ":", "# if this part of code was reached, means that SIGTERM", "# did the work and SIGKILL wasnt needed.", "pass"], "docstring": "This function will kill the shell on a tab, trying to send\n        a sigterm and if it doesn't work, a sigkill. Between these two\n        signals, we have a timeout of 3 seconds, so is recommended to\n        call this in another thread. This doesn't change any thing in\n        UI, so you can use python's start_new_thread.", "docstring_tokens": ["This", "function", "will", "kill", "the", "shell", "on", "a", "tab", "trying", "to", "send", "a", "sigterm", "and", "if", "it", "doesn", "t", "work", "a", "sigkill", ".", "Between", "these", "two", "signals", "we", "have", "a", "timeout", "of", "3", "seconds", "so", "is", "recommended", "to", "call", "this", "in", "another", "thread", ".", "This", "doesn", "t", "change", "any", "thing", "in", "UI", "so", "you", "can", "use", "python", "s", "start_new_thread", "."], "sha": "4153ef38f9044cbed6494075fce80acd5809df2b", "url": "https://github.com/Guake/guake/blob/4153ef38f9044cbed6494075fce80acd5809df2b/guake/terminal.py#L463-L495", "partition": "train"}
{"repo": "graphql-python/graphql-core", "path": "graphql/utils/is_valid_value.py", "func_name": "is_valid_value", "original_string": "def is_valid_value(value, type):\n    # type: (Any, Any) -> List\n    \"\"\"Given a type and any value, return True if that value is valid.\"\"\"\n    if isinstance(type, GraphQLNonNull):\n        of_type = type.of_type\n        if value is None:\n            return [u'Expected \"{}\", found null.'.format(type)]\n\n        return is_valid_value(value, of_type)\n\n    if value is None:\n        return _empty_list\n\n    if isinstance(type, GraphQLList):\n        item_type = type.of_type\n        if not isinstance(value, string_types) and isinstance(value, Iterable):\n            errors = []\n            for i, item in enumerate(value):\n                item_errors = is_valid_value(item, item_type)\n                for error in item_errors:\n                    errors.append(u\"In element #{}: {}\".format(i, error))\n\n            return errors\n\n        else:\n            return is_valid_value(value, item_type)\n\n    if isinstance(type, GraphQLInputObjectType):\n        if not isinstance(value, Mapping):\n            return [u'Expected \"{}\", found not an object.'.format(type)]\n\n        fields = type.fields\n        errors = []\n\n        for provided_field in sorted(value.keys()):\n            if provided_field not in fields:\n                errors.append(u'In field \"{}\": Unknown field.'.format(provided_field))\n\n        for field_name, field in fields.items():\n            subfield_errors = is_valid_value(value.get(field_name), field.type)\n            errors.extend(\n                u'In field \"{}\": {}'.format(field_name, e) for e in subfield_errors\n            )\n\n        return errors\n\n    assert isinstance(type, (GraphQLScalarType, GraphQLEnumType)), \"Must be input type\"\n\n    # Scalar/Enum input checks to ensure the type can parse the value to\n    # a non-null value.\n    parse_result = type.parse_value(value)\n    if parse_result is None:\n        return [u'Expected type \"{}\", found {}.'.format(type, json.dumps(value))]\n\n    return _empty_list", "language": "python", "code": "def is_valid_value(value, type):\n    # type: (Any, Any) -> List\n    \"\"\"Given a type and any value, return True if that value is valid.\"\"\"\n    if isinstance(type, GraphQLNonNull):\n        of_type = type.of_type\n        if value is None:\n            return [u'Expected \"{}\", found null.'.format(type)]\n\n        return is_valid_value(value, of_type)\n\n    if value is None:\n        return _empty_list\n\n    if isinstance(type, GraphQLList):\n        item_type = type.of_type\n        if not isinstance(value, string_types) and isinstance(value, Iterable):\n            errors = []\n            for i, item in enumerate(value):\n                item_errors = is_valid_value(item, item_type)\n                for error in item_errors:\n                    errors.append(u\"In element #{}: {}\".format(i, error))\n\n            return errors\n\n        else:\n            return is_valid_value(value, item_type)\n\n    if isinstance(type, GraphQLInputObjectType):\n        if not isinstance(value, Mapping):\n            return [u'Expected \"{}\", found not an object.'.format(type)]\n\n        fields = type.fields\n        errors = []\n\n        for provided_field in sorted(value.keys()):\n            if provided_field not in fields:\n                errors.append(u'In field \"{}\": Unknown field.'.format(provided_field))\n\n        for field_name, field in fields.items():\n            subfield_errors = is_valid_value(value.get(field_name), field.type)\n            errors.extend(\n                u'In field \"{}\": {}'.format(field_name, e) for e in subfield_errors\n            )\n\n        return errors\n\n    assert isinstance(type, (GraphQLScalarType, GraphQLEnumType)), \"Must be input type\"\n\n    # Scalar/Enum input checks to ensure the type can parse the value to\n    # a non-null value.\n    parse_result = type.parse_value(value)\n    if parse_result is None:\n        return [u'Expected type \"{}\", found {}.'.format(type, json.dumps(value))]\n\n    return _empty_list", "code_tokens": ["def", "is_valid_value", "(", "value", ",", "type", ")", ":", "# type: (Any, Any) -> List", "if", "isinstance", "(", "type", ",", "GraphQLNonNull", ")", ":", "of_type", "=", "type", ".", "of_type", "if", "value", "is", "None", ":", "return", "[", "u'Expected \"{}\", found null.'", ".", "format", "(", "type", ")", "]", "return", "is_valid_value", "(", "value", ",", "of_type", ")", "if", "value", "is", "None", ":", "return", "_empty_list", "if", "isinstance", "(", "type", ",", "GraphQLList", ")", ":", "item_type", "=", "type", ".", "of_type", "if", "not", "isinstance", "(", "value", ",", "string_types", ")", "and", "isinstance", "(", "value", ",", "Iterable", ")", ":", "errors", "=", "[", "]", "for", "i", ",", "item", "in", "enumerate", "(", "value", ")", ":", "item_errors", "=", "is_valid_value", "(", "item", ",", "item_type", ")", "for", "error", "in", "item_errors", ":", "errors", ".", "append", "(", "u\"In element #{}: {}\"", ".", "format", "(", "i", ",", "error", ")", ")", "return", "errors", "else", ":", "return", "is_valid_value", "(", "value", ",", "item_type", ")", "if", "isinstance", "(", "type", ",", "GraphQLInputObjectType", ")", ":", "if", "not", "isinstance", "(", "value", ",", "Mapping", ")", ":", "return", "[", "u'Expected \"{}\", found not an object.'", ".", "format", "(", "type", ")", "]", "fields", "=", "type", ".", "fields", "errors", "=", "[", "]", "for", "provided_field", "in", "sorted", "(", "value", ".", "keys", "(", ")", ")", ":", "if", "provided_field", "not", "in", "fields", ":", "errors", ".", "append", "(", "u'In field \"{}\": Unknown field.'", ".", "format", "(", "provided_field", ")", ")", "for", "field_name", ",", "field", "in", "fields", ".", "items", "(", ")", ":", "subfield_errors", "=", "is_valid_value", "(", "value", ".", "get", "(", "field_name", ")", ",", "field", ".", "type", ")", "errors", ".", "extend", "(", "u'In field \"{}\": {}'", ".", "format", "(", "field_name", ",", "e", ")", "for", "e", "in", "subfield_errors", ")", "return", "errors", "assert", "isinstance", "(", "type", ",", "(", "GraphQLScalarType", ",", "GraphQLEnumType", ")", ")", ",", "\"Must be input type\"", "# Scalar/Enum input checks to ensure the type can parse the value to", "# a non-null value.", "parse_result", "=", "type", ".", "parse_value", "(", "value", ")", "if", "parse_result", "is", "None", ":", "return", "[", "u'Expected type \"{}\", found {}.'", ".", "format", "(", "type", ",", "json", ".", "dumps", "(", "value", ")", ")", "]", "return", "_empty_list"], "docstring": "Given a type and any value, return True if that value is valid.", "docstring_tokens": ["Given", "a", "type", "and", "any", "value", "return", "True", "if", "that", "value", "is", "valid", "."], "sha": "d8e9d3abe7c209eb2f51cf001402783bfd480596", "url": "https://github.com/graphql-python/graphql-core/blob/d8e9d3abe7c209eb2f51cf001402783bfd480596/graphql/utils/is_valid_value.py#L28-L82", "partition": "train"}
{"repo": "graphql-python/graphql-core", "path": "graphql/backend/cache.py", "func_name": "get_unique_schema_id", "original_string": "def get_unique_schema_id(schema):\n    # type: (GraphQLSchema) -> str\n    \"\"\"Get a unique id given a GraphQLSchema\"\"\"\n    assert isinstance(schema, GraphQLSchema), (\n        \"Must receive a GraphQLSchema as schema. Received {}\"\n    ).format(repr(schema))\n\n    if schema not in _cached_schemas:\n        _cached_schemas[schema] = sha1(str(schema).encode(\"utf-8\")).hexdigest()\n    return _cached_schemas[schema]", "language": "python", "code": "def get_unique_schema_id(schema):\n    # type: (GraphQLSchema) -> str\n    \"\"\"Get a unique id given a GraphQLSchema\"\"\"\n    assert isinstance(schema, GraphQLSchema), (\n        \"Must receive a GraphQLSchema as schema. Received {}\"\n    ).format(repr(schema))\n\n    if schema not in _cached_schemas:\n        _cached_schemas[schema] = sha1(str(schema).encode(\"utf-8\")).hexdigest()\n    return _cached_schemas[schema]", "code_tokens": ["def", "get_unique_schema_id", "(", "schema", ")", ":", "# type: (GraphQLSchema) -> str", "assert", "isinstance", "(", "schema", ",", "GraphQLSchema", ")", ",", "(", "\"Must receive a GraphQLSchema as schema. Received {}\"", ")", ".", "format", "(", "repr", "(", "schema", ")", ")", "if", "schema", "not", "in", "_cached_schemas", ":", "_cached_schemas", "[", "schema", "]", "=", "sha1", "(", "str", "(", "schema", ")", ".", "encode", "(", "\"utf-8\"", ")", ")", ".", "hexdigest", "(", ")", "return", "_cached_schemas", "[", "schema", "]"], "docstring": "Get a unique id given a GraphQLSchema", "docstring_tokens": ["Get", "a", "unique", "id", "given", "a", "GraphQLSchema"], "sha": "d8e9d3abe7c209eb2f51cf001402783bfd480596", "url": "https://github.com/graphql-python/graphql-core/blob/d8e9d3abe7c209eb2f51cf001402783bfd480596/graphql/backend/cache.py#L18-L27", "partition": "train"}
{"repo": "kieferk/dfply", "path": "dfply/select.py", "func_name": "select_if", "original_string": "def select_if(df, fun):\n    \"\"\"Selects columns where fun(ction) is true\n    Args:\n        fun: a function that will be applied to columns\n    \"\"\"\n\n    def _filter_f(col):\n        try:\n            return fun(df[col])\n        except:\n            return False\n\n    cols = list(filter(_filter_f, df.columns))\n    return df[cols]", "language": "python", "code": "def select_if(df, fun):\n    \"\"\"Selects columns where fun(ction) is true\n    Args:\n        fun: a function that will be applied to columns\n    \"\"\"\n\n    def _filter_f(col):\n        try:\n            return fun(df[col])\n        except:\n            return False\n\n    cols = list(filter(_filter_f, df.columns))\n    return df[cols]", "code_tokens": ["def", "select_if", "(", "df", ",", "fun", ")", ":", "def", "_filter_f", "(", "col", ")", ":", "try", ":", "return", "fun", "(", "df", "[", "col", "]", ")", "except", ":", "return", "False", "cols", "=", "list", "(", "filter", "(", "_filter_f", ",", "df", ".", "columns", ")", ")", "return", "df", "[", "cols", "]"], "docstring": "Selects columns where fun(ction) is true\n    Args:\n        fun: a function that will be applied to columns", "docstring_tokens": ["Selects", "columns", "where", "fun", "(", "ction", ")", "is", "true", "Args", ":", "fun", ":", "a", "function", "that", "will", "be", "applied", "to", "columns"], "sha": "6a858f066602735a90f8b6b85106bc39ceadc282", "url": "https://github.com/kieferk/dfply/blob/6a858f066602735a90f8b6b85106bc39ceadc282/dfply/select.py#L79-L92", "partition": "train"}
{"repo": "kieferk/dfply", "path": "dfply/select.py", "func_name": "drop_if", "original_string": "def drop_if(df, fun):\n    \"\"\"Drops columns where fun(ction) is true\n    Args:\n        fun: a function that will be applied to columns\n    \"\"\"\n\n    def _filter_f(col):\n        try:\n            return fun(df[col])\n        except:\n            return False\n\n    cols = list(filter(_filter_f, df.columns))\n    return df.drop(cols, axis=1)", "language": "python", "code": "def drop_if(df, fun):\n    \"\"\"Drops columns where fun(ction) is true\n    Args:\n        fun: a function that will be applied to columns\n    \"\"\"\n\n    def _filter_f(col):\n        try:\n            return fun(df[col])\n        except:\n            return False\n\n    cols = list(filter(_filter_f, df.columns))\n    return df.drop(cols, axis=1)", "code_tokens": ["def", "drop_if", "(", "df", ",", "fun", ")", ":", "def", "_filter_f", "(", "col", ")", ":", "try", ":", "return", "fun", "(", "df", "[", "col", "]", ")", "except", ":", "return", "False", "cols", "=", "list", "(", "filter", "(", "_filter_f", ",", "df", ".", "columns", ")", ")", "return", "df", ".", "drop", "(", "cols", ",", "axis", "=", "1", ")"], "docstring": "Drops columns where fun(ction) is true\n    Args:\n        fun: a function that will be applied to columns", "docstring_tokens": ["Drops", "columns", "where", "fun", "(", "ction", ")", "is", "true", "Args", ":", "fun", ":", "a", "function", "that", "will", "be", "applied", "to", "columns"], "sha": "6a858f066602735a90f8b6b85106bc39ceadc282", "url": "https://github.com/kieferk/dfply/blob/6a858f066602735a90f8b6b85106bc39ceadc282/dfply/select.py#L96-L109", "partition": "train"}
{"repo": "lyst/lightfm", "path": "lightfm/lightfm.py", "func_name": "LightFM.set_params", "original_string": "def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        Returns\n        -------\n\n        self\n        \"\"\"\n\n        valid_params = self.get_params()\n\n        for key, value in params.items():\n            if key not in valid_params:\n                raise ValueError(\n                    \"Invalid parameter %s for estimator %s. \"\n                    \"Check the list of available parameters \"\n                    \"with `estimator.get_params().keys()`.\"\n                    % (key, self.__class__.__name__)\n                )\n\n            setattr(self, key, value)\n\n        return self", "language": "python", "code": "def set_params(self, **params):\n        \"\"\"\n        Set the parameters of this estimator.\n\n        Returns\n        -------\n\n        self\n        \"\"\"\n\n        valid_params = self.get_params()\n\n        for key, value in params.items():\n            if key not in valid_params:\n                raise ValueError(\n                    \"Invalid parameter %s for estimator %s. \"\n                    \"Check the list of available parameters \"\n                    \"with `estimator.get_params().keys()`.\"\n                    % (key, self.__class__.__name__)\n                )\n\n            setattr(self, key, value)\n\n        return self", "code_tokens": ["def", "set_params", "(", "self", ",", "*", "*", "params", ")", ":", "valid_params", "=", "self", ".", "get_params", "(", ")", "for", "key", ",", "value", "in", "params", ".", "items", "(", ")", ":", "if", "key", "not", "in", "valid_params", ":", "raise", "ValueError", "(", "\"Invalid parameter %s for estimator %s. \"", "\"Check the list of available parameters \"", "\"with `estimator.get_params().keys()`.\"", "%", "(", "key", ",", "self", ".", "__class__", ".", "__name__", ")", ")", "setattr", "(", "self", ",", "key", ",", "value", ")", "return", "self"], "docstring": "Set the parameters of this estimator.\n\n        Returns\n        -------\n\n        self", "docstring_tokens": ["Set", "the", "parameters", "of", "this", "estimator", "."], "sha": "87b942f87759b8336f9066a25e4762ae7d95455e", "url": "https://github.com/lyst/lightfm/blob/87b942f87759b8336f9066a25e4762ae7d95455e/lightfm/lightfm.py#L1040-L1063", "partition": "train"}
{"repo": "quantopian/empyrical", "path": "empyrical/stats.py", "func_name": "_adjust_returns", "original_string": "def _adjust_returns(returns, adjustment_factor):\n    \"\"\"\n    Returns the returns series adjusted by adjustment_factor. Optimizes for the\n    case of adjustment_factor being 0 by returning returns itself, not a copy!\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n    adjustment_factor : pd.Series or np.ndarray or float or int\n\n    Returns\n    -------\n    adjusted_returns : array-like\n    \"\"\"\n    if isinstance(adjustment_factor, (float, int)) and adjustment_factor == 0:\n        return returns\n    return returns - adjustment_factor", "language": "python", "code": "def _adjust_returns(returns, adjustment_factor):\n    \"\"\"\n    Returns the returns series adjusted by adjustment_factor. Optimizes for the\n    case of adjustment_factor being 0 by returning returns itself, not a copy!\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n    adjustment_factor : pd.Series or np.ndarray or float or int\n\n    Returns\n    -------\n    adjusted_returns : array-like\n    \"\"\"\n    if isinstance(adjustment_factor, (float, int)) and adjustment_factor == 0:\n        return returns\n    return returns - adjustment_factor", "code_tokens": ["def", "_adjust_returns", "(", "returns", ",", "adjustment_factor", ")", ":", "if", "isinstance", "(", "adjustment_factor", ",", "(", "float", ",", "int", ")", ")", "and", "adjustment_factor", "==", "0", ":", "return", "returns", "return", "returns", "-", "adjustment_factor"], "docstring": "Returns the returns series adjusted by adjustment_factor. Optimizes for the\n    case of adjustment_factor being 0 by returning returns itself, not a copy!\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n    adjustment_factor : pd.Series or np.ndarray or float or int\n\n    Returns\n    -------\n    adjusted_returns : array-like", "docstring_tokens": ["Returns", "the", "returns", "series", "adjusted", "by", "adjustment_factor", ".", "Optimizes", "for", "the", "case", "of", "adjustment_factor", "being", "0", "by", "returning", "returns", "itself", "not", "a", "copy!"], "sha": "badbdca75f5b293f28b5e947974894de041d6868", "url": "https://github.com/quantopian/empyrical/blob/badbdca75f5b293f28b5e947974894de041d6868/empyrical/stats.py#L127-L143", "partition": "train"}
{"repo": "django-extensions/django-extensions", "path": "django_extensions/management/modelviz.py", "func_name": "ModelGraph.use_model", "original_string": "def use_model(self, model_name):\n        \"\"\"\n        Decide whether to use a model, based on the model name and the lists of\n        models to exclude and include.\n        \"\"\"\n        # Check against exclude list.\n        if self.exclude_models:\n            for model_pattern in self.exclude_models:\n                model_pattern = '^%s$' % model_pattern.replace('*', '.*')\n                if re.search(model_pattern, model_name):\n                    return False\n        # Check against exclude list.\n        elif self.include_models:\n            for model_pattern in self.include_models:\n                model_pattern = '^%s$' % model_pattern.replace('*', '.*')\n                if re.search(model_pattern, model_name):\n                    return True\n        # Return `True` if `include_models` is falsey, otherwise return `False`.\n        return not self.include_models", "language": "python", "code": "def use_model(self, model_name):\n        \"\"\"\n        Decide whether to use a model, based on the model name and the lists of\n        models to exclude and include.\n        \"\"\"\n        # Check against exclude list.\n        if self.exclude_models:\n            for model_pattern in self.exclude_models:\n                model_pattern = '^%s$' % model_pattern.replace('*', '.*')\n                if re.search(model_pattern, model_name):\n                    return False\n        # Check against exclude list.\n        elif self.include_models:\n            for model_pattern in self.include_models:\n                model_pattern = '^%s$' % model_pattern.replace('*', '.*')\n                if re.search(model_pattern, model_name):\n                    return True\n        # Return `True` if `include_models` is falsey, otherwise return `False`.\n        return not self.include_models", "code_tokens": ["def", "use_model", "(", "self", ",", "model_name", ")", ":", "# Check against exclude list.", "if", "self", ".", "exclude_models", ":", "for", "model_pattern", "in", "self", ".", "exclude_models", ":", "model_pattern", "=", "'^%s$'", "%", "model_pattern", ".", "replace", "(", "'*'", ",", "'.*'", ")", "if", "re", ".", "search", "(", "model_pattern", ",", "model_name", ")", ":", "return", "False", "# Check against exclude list.", "elif", "self", ".", "include_models", ":", "for", "model_pattern", "in", "self", ".", "include_models", ":", "model_pattern", "=", "'^%s$'", "%", "model_pattern", ".", "replace", "(", "'*'", ",", "'.*'", ")", "if", "re", ".", "search", "(", "model_pattern", ",", "model_name", ")", ":", "return", "True", "# Return `True` if `include_models` is falsey, otherwise return `False`.", "return", "not", "self", ".", "include_models"], "docstring": "Decide whether to use a model, based on the model name and the lists of\n        models to exclude and include.", "docstring_tokens": ["Decide", "whether", "to", "use", "a", "model", "based", "on", "the", "model", "name", "and", "the", "lists", "of", "models", "to", "exclude", "and", "include", "."], "sha": "7e0bef97ea6cb7f9eea5e2528e3a985a83a7b9b8", "url": "https://github.com/django-extensions/django-extensions/blob/7e0bef97ea6cb7f9eea5e2528e3a985a83a7b9b8/django_extensions/management/modelviz.py#L359-L377", "partition": "train"}
{"repo": "google/transitfeed", "path": "transitfeed/shapelib.py", "func_name": "GetClosestPoint", "original_string": "def GetClosestPoint(x, a, b):\n  \"\"\"\n  Returns the point on the great circle segment ab closest to x.\n  \"\"\"\n  assert(x.IsUnitLength())\n  assert(a.IsUnitLength())\n  assert(b.IsUnitLength())\n\n  a_cross_b = a.RobustCrossProd(b)\n  # project to the great circle going through a and b\n  p = x.Minus(\n      a_cross_b.Times(\n      x.DotProd(a_cross_b) / a_cross_b.Norm2()))\n\n  # if p lies between a and b, return it\n  if SimpleCCW(a_cross_b, a, p) and SimpleCCW(p, b, a_cross_b):\n    return p.Normalize()\n\n  # otherwise return the closer of a or b\n  if x.Minus(a).Norm2() <= x.Minus(b).Norm2():\n    return a\n  else:\n    return b", "language": "python", "code": "def GetClosestPoint(x, a, b):\n  \"\"\"\n  Returns the point on the great circle segment ab closest to x.\n  \"\"\"\n  assert(x.IsUnitLength())\n  assert(a.IsUnitLength())\n  assert(b.IsUnitLength())\n\n  a_cross_b = a.RobustCrossProd(b)\n  # project to the great circle going through a and b\n  p = x.Minus(\n      a_cross_b.Times(\n      x.DotProd(a_cross_b) / a_cross_b.Norm2()))\n\n  # if p lies between a and b, return it\n  if SimpleCCW(a_cross_b, a, p) and SimpleCCW(p, b, a_cross_b):\n    return p.Normalize()\n\n  # otherwise return the closer of a or b\n  if x.Minus(a).Norm2() <= x.Minus(b).Norm2():\n    return a\n  else:\n    return b", "code_tokens": ["def", "GetClosestPoint", "(", "x", ",", "a", ",", "b", ")", ":", "assert", "(", "x", ".", "IsUnitLength", "(", ")", ")", "assert", "(", "a", ".", "IsUnitLength", "(", ")", ")", "assert", "(", "b", ".", "IsUnitLength", "(", ")", ")", "a_cross_b", "=", "a", ".", "RobustCrossProd", "(", "b", ")", "# project to the great circle going through a and b", "p", "=", "x", ".", "Minus", "(", "a_cross_b", ".", "Times", "(", "x", ".", "DotProd", "(", "a_cross_b", ")", "/", "a_cross_b", ".", "Norm2", "(", ")", ")", ")", "# if p lies between a and b, return it", "if", "SimpleCCW", "(", "a_cross_b", ",", "a", ",", "p", ")", "and", "SimpleCCW", "(", "p", ",", "b", ",", "a_cross_b", ")", ":", "return", "p", ".", "Normalize", "(", ")", "# otherwise return the closer of a or b", "if", "x", ".", "Minus", "(", "a", ")", ".", "Norm2", "(", ")", "<=", "x", ".", "Minus", "(", "b", ")", ".", "Norm2", "(", ")", ":", "return", "a", "else", ":", "return", "b"], "docstring": "Returns the point on the great circle segment ab closest to x.", "docstring_tokens": ["Returns", "the", "point", "on", "the", "great", "circle", "segment", "ab", "closest", "to", "x", "."], "sha": "eb2991a3747ba541b2cb66502b305b6304a1f85f", "url": "https://github.com/google/transitfeed/blob/eb2991a3747ba541b2cb66502b305b6304a1f85f/transitfeed/shapelib.py#L221-L243", "partition": "train"}
{"repo": "google/transitfeed", "path": "transitfeed/shapelib.py", "func_name": "Point.Norm2", "original_string": "def Norm2(self):\n    \"\"\"\n    Returns the L_2 (Euclidean) norm of self.\n    \"\"\"\n    sum = self.x * self.x + self.y * self.y + self.z * self.z\n    return math.sqrt(float(sum))", "language": "python", "code": "def Norm2(self):\n    \"\"\"\n    Returns the L_2 (Euclidean) norm of self.\n    \"\"\"\n    sum = self.x * self.x + self.y * self.y + self.z * self.z\n    return math.sqrt(float(sum))", "code_tokens": ["def", "Norm2", "(", "self", ")", ":", "sum", "=", "self", ".", "x", "*", "self", ".", "x", "+", "self", ".", "y", "*", "self", ".", "y", "+", "self", ".", "z", "*", "self", ".", "z", "return", "math", ".", "sqrt", "(", "float", "(", "sum", ")", ")"], "docstring": "Returns the L_2 (Euclidean) norm of self.", "docstring_tokens": ["Returns", "the", "L_2", "(", "Euclidean", ")", "norm", "of", "self", "."], "sha": "eb2991a3747ba541b2cb66502b305b6304a1f85f", "url": "https://github.com/google/transitfeed/blob/eb2991a3747ba541b2cb66502b305b6304a1f85f/transitfeed/shapelib.py#L69-L74", "partition": "train"}
{"repo": "equinor/segyio", "path": "python/segyio/segy.py", "func_name": "SegyFile.interpret", "original_string": "def interpret(self, ilines, xlines, offsets=None, sorting=TraceSortingFormat.INLINE_SORTING):\n\n        \"\"\" (Re-)interpret structure on top of a file\n\n        (Re-)interpret the structure of the file given the new sorting, ilines,\n        xlines and offset indices. Note that file itself is not changed in any\n        way, it is only segyio's interpretation of the file that changes. It's\n        a way of telling segyio that a file is laid out in a particular way,\n        even though the header fields say otherwise.\n\n        `interpret` expect that the ilines-, xlines- and offsets-indices are\n        unique. It also expect the dimensions of ilines, xlines and offset to\n        match the tracecount.\n\n        Parameters\n        ----------\n        f : SegyFile\n        ilines : array_like\n            ilines indices in new structure\n        xlines : array_like\n            xlines indices in new structure\n        offsets : array_like\n            offset indices in new structure\n        sorting : int, string or TraceSortingFormat\n            Sorting in new structure\n\n        Notes\n        -----\n\n        .. versionadded:: 1.8\n\n        Examples\n        --------\n        (Re)interpret the structure of the file:\n\n        >>> ilines = [10, 11, 12, 13]\n        >>> xlines = [20, 21, 22, 23, 24]\n        >>> with segyio.open(file, ignore_geometry=True) as f:\n        ... f.interpret(ilines, xlines)\n        \"\"\"\n\n        valid_sortings = {\n            1           : TraceSortingFormat.CROSSLINE_SORTING,\n            2           : TraceSortingFormat.INLINE_SORTING,\n            'iline'     : TraceSortingFormat.INLINE_SORTING,\n            'inline'    : TraceSortingFormat.INLINE_SORTING,\n            'xl'        : TraceSortingFormat.CROSSLINE_SORTING,\n            'crossline' : TraceSortingFormat.CROSSLINE_SORTING,\n            TraceSortingFormat.INLINE_SORTING    : TraceSortingFormat.INLINE_SORTING,\n            TraceSortingFormat.CROSSLINE_SORTING : TraceSortingFormat.CROSSLINE_SORTING,\n        }\n\n        if sorting not in valid_sortings:\n            error = \"Invalid sorting\"\n            solution = \"valid sorting options are: {}\".format(valid_sortings.keys())\n            raise ValueError('{}, {}'.format(error, solution))\n\n        if offsets is None:\n            offsets = np.arange(1)\n\n        ilines  = np.copy(np.asarray(ilines,  dtype=np.intc))\n        xlines  = np.copy(np.asarray(xlines,  dtype=np.intc))\n        offsets = np.copy(np.asarray(offsets, dtype=np.intc))\n\n        if np.unique(ilines).size != ilines.size:\n            error = \"Inlines inconsistent\"\n            solution = \"expect all inlines to be unique\"\n            raise ValueError(\"{}, {}\".format(error, solution))\n\n        if np.unique(xlines).size != xlines.size:\n            error = \"Crosslines inconsistent\"\n            solution = \"expect all crosslines to be unique\"\n            raise ValueError(\"{}, {}\".format(error, solution))\n\n        if np.unique(offsets).size != offsets.size:\n            error = \"Offsets inconsistent\"\n            solution = \"expect all offsets to be unique\"\n            raise ValueError(\"{}, {}\".format(error, solution))\n\n        if ilines.size * xlines.size * offsets.size != self.tracecount:\n            error = (\"Invalid dimensions, ilines ({}) * xlines ({}) * offsets \"\n                     \"({}) should match the number of traces ({})\").format(ilines.size,\n                                                                           xlines.size,\n                                                                           offsets.size,\n                                                                           self.tracecount)\n            raise ValueError(error)\n\n        from . import _segyio\n\n        line_metrics = _segyio.line_metrics(sorting,\n                                            self.tracecount,\n                                            ilines.size,\n                                            xlines.size,\n                                            offsets.size)\n\n        self._iline_length = line_metrics['iline_length']\n        self._iline_stride = line_metrics['iline_stride']\n\n        self._xline_length = line_metrics['xline_length']\n        self._xline_stride = line_metrics['xline_stride']\n\n        self._sorting = sorting\n        self._offsets = offsets\n        self._ilines = ilines\n        self._xlines = xlines\n\n        return self", "language": "python", "code": "def interpret(self, ilines, xlines, offsets=None, sorting=TraceSortingFormat.INLINE_SORTING):\n\n        \"\"\" (Re-)interpret structure on top of a file\n\n        (Re-)interpret the structure of the file given the new sorting, ilines,\n        xlines and offset indices. Note that file itself is not changed in any\n        way, it is only segyio's interpretation of the file that changes. It's\n        a way of telling segyio that a file is laid out in a particular way,\n        even though the header fields say otherwise.\n\n        `interpret` expect that the ilines-, xlines- and offsets-indices are\n        unique. It also expect the dimensions of ilines, xlines and offset to\n        match the tracecount.\n\n        Parameters\n        ----------\n        f : SegyFile\n        ilines : array_like\n            ilines indices in new structure\n        xlines : array_like\n            xlines indices in new structure\n        offsets : array_like\n            offset indices in new structure\n        sorting : int, string or TraceSortingFormat\n            Sorting in new structure\n\n        Notes\n        -----\n\n        .. versionadded:: 1.8\n\n        Examples\n        --------\n        (Re)interpret the structure of the file:\n\n        >>> ilines = [10, 11, 12, 13]\n        >>> xlines = [20, 21, 22, 23, 24]\n        >>> with segyio.open(file, ignore_geometry=True) as f:\n        ... f.interpret(ilines, xlines)\n        \"\"\"\n\n        valid_sortings = {\n            1           : TraceSortingFormat.CROSSLINE_SORTING,\n            2           : TraceSortingFormat.INLINE_SORTING,\n            'iline'     : TraceSortingFormat.INLINE_SORTING,\n            'inline'    : TraceSortingFormat.INLINE_SORTING,\n            'xl'        : TraceSortingFormat.CROSSLINE_SORTING,\n            'crossline' : TraceSortingFormat.CROSSLINE_SORTING,\n            TraceSortingFormat.INLINE_SORTING    : TraceSortingFormat.INLINE_SORTING,\n            TraceSortingFormat.CROSSLINE_SORTING : TraceSortingFormat.CROSSLINE_SORTING,\n        }\n\n        if sorting not in valid_sortings:\n            error = \"Invalid sorting\"\n            solution = \"valid sorting options are: {}\".format(valid_sortings.keys())\n            raise ValueError('{}, {}'.format(error, solution))\n\n        if offsets is None:\n            offsets = np.arange(1)\n\n        ilines  = np.copy(np.asarray(ilines,  dtype=np.intc))\n        xlines  = np.copy(np.asarray(xlines,  dtype=np.intc))\n        offsets = np.copy(np.asarray(offsets, dtype=np.intc))\n\n        if np.unique(ilines).size != ilines.size:\n            error = \"Inlines inconsistent\"\n            solution = \"expect all inlines to be unique\"\n            raise ValueError(\"{}, {}\".format(error, solution))\n\n        if np.unique(xlines).size != xlines.size:\n            error = \"Crosslines inconsistent\"\n            solution = \"expect all crosslines to be unique\"\n            raise ValueError(\"{}, {}\".format(error, solution))\n\n        if np.unique(offsets).size != offsets.size:\n            error = \"Offsets inconsistent\"\n            solution = \"expect all offsets to be unique\"\n            raise ValueError(\"{}, {}\".format(error, solution))\n\n        if ilines.size * xlines.size * offsets.size != self.tracecount:\n            error = (\"Invalid dimensions, ilines ({}) * xlines ({}) * offsets \"\n                     \"({}) should match the number of traces ({})\").format(ilines.size,\n                                                                           xlines.size,\n                                                                           offsets.size,\n                                                                           self.tracecount)\n            raise ValueError(error)\n\n        from . import _segyio\n\n        line_metrics = _segyio.line_metrics(sorting,\n                                            self.tracecount,\n                                            ilines.size,\n                                            xlines.size,\n                                            offsets.size)\n\n        self._iline_length = line_metrics['iline_length']\n        self._iline_stride = line_metrics['iline_stride']\n\n        self._xline_length = line_metrics['xline_length']\n        self._xline_stride = line_metrics['xline_stride']\n\n        self._sorting = sorting\n        self._offsets = offsets\n        self._ilines = ilines\n        self._xlines = xlines\n\n        return self", "code_tokens": ["def", "interpret", "(", "self", ",", "ilines", ",", "xlines", ",", "offsets", "=", "None", ",", "sorting", "=", "TraceSortingFormat", ".", "INLINE_SORTING", ")", ":", "valid_sortings", "=", "{", "1", ":", "TraceSortingFormat", ".", "CROSSLINE_SORTING", ",", "2", ":", "TraceSortingFormat", ".", "INLINE_SORTING", ",", "'iline'", ":", "TraceSortingFormat", ".", "INLINE_SORTING", ",", "'inline'", ":", "TraceSortingFormat", ".", "INLINE_SORTING", ",", "'xl'", ":", "TraceSortingFormat", ".", "CROSSLINE_SORTING", ",", "'crossline'", ":", "TraceSortingFormat", ".", "CROSSLINE_SORTING", ",", "TraceSortingFormat", ".", "INLINE_SORTING", ":", "TraceSortingFormat", ".", "INLINE_SORTING", ",", "TraceSortingFormat", ".", "CROSSLINE_SORTING", ":", "TraceSortingFormat", ".", "CROSSLINE_SORTING", ",", "}", "if", "sorting", "not", "in", "valid_sortings", ":", "error", "=", "\"Invalid sorting\"", "solution", "=", "\"valid sorting options are: {}\"", ".", "format", "(", "valid_sortings", ".", "keys", "(", ")", ")", "raise", "ValueError", "(", "'{}, {}'", ".", "format", "(", "error", ",", "solution", ")", ")", "if", "offsets", "is", "None", ":", "offsets", "=", "np", ".", "arange", "(", "1", ")", "ilines", "=", "np", ".", "copy", "(", "np", ".", "asarray", "(", "ilines", ",", "dtype", "=", "np", ".", "intc", ")", ")", "xlines", "=", "np", ".", "copy", "(", "np", ".", "asarray", "(", "xlines", ",", "dtype", "=", "np", ".", "intc", ")", ")", "offsets", "=", "np", ".", "copy", "(", "np", ".", "asarray", "(", "offsets", ",", "dtype", "=", "np", ".", "intc", ")", ")", "if", "np", ".", "unique", "(", "ilines", ")", ".", "size", "!=", "ilines", ".", "size", ":", "error", "=", "\"Inlines inconsistent\"", "solution", "=", "\"expect all inlines to be unique\"", "raise", "ValueError", "(", "\"{}, {}\"", ".", "format", "(", "error", ",", "solution", ")", ")", "if", "np", ".", "unique", "(", "xlines", ")", ".", "size", "!=", "xlines", ".", "size", ":", "error", "=", "\"Crosslines inconsistent\"", "solution", "=", "\"expect all crosslines to be unique\"", "raise", "ValueError", "(", "\"{}, {}\"", ".", "format", "(", "error", ",", "solution", ")", ")", "if", "np", ".", "unique", "(", "offsets", ")", ".", "size", "!=", "offsets", ".", "size", ":", "error", "=", "\"Offsets inconsistent\"", "solution", "=", "\"expect all offsets to be unique\"", "raise", "ValueError", "(", "\"{}, {}\"", ".", "format", "(", "error", ",", "solution", ")", ")", "if", "ilines", ".", "size", "*", "xlines", ".", "size", "*", "offsets", ".", "size", "!=", "self", ".", "tracecount", ":", "error", "=", "(", "\"Invalid dimensions, ilines ({}) * xlines ({}) * offsets \"", "\"({}) should match the number of traces ({})\"", ")", ".", "format", "(", "ilines", ".", "size", ",", "xlines", ".", "size", ",", "offsets", ".", "size", ",", "self", ".", "tracecount", ")", "raise", "ValueError", "(", "error", ")", "from", ".", "import", "_segyio", "line_metrics", "=", "_segyio", ".", "line_metrics", "(", "sorting", ",", "self", ".", "tracecount", ",", "ilines", ".", "size", ",", "xlines", ".", "size", ",", "offsets", ".", "size", ")", "self", ".", "_iline_length", "=", "line_metrics", "[", "'iline_length'", "]", "self", ".", "_iline_stride", "=", "line_metrics", "[", "'iline_stride'", "]", "self", ".", "_xline_length", "=", "line_metrics", "[", "'xline_length'", "]", "self", ".", "_xline_stride", "=", "line_metrics", "[", "'xline_stride'", "]", "self", ".", "_sorting", "=", "sorting", "self", ".", "_offsets", "=", "offsets", "self", ".", "_ilines", "=", "ilines", "self", ".", "_xlines", "=", "xlines", "return", "self"], "docstring": "(Re-)interpret structure on top of a file\n\n        (Re-)interpret the structure of the file given the new sorting, ilines,\n        xlines and offset indices. Note that file itself is not changed in any\n        way, it is only segyio's interpretation of the file that changes. It's\n        a way of telling segyio that a file is laid out in a particular way,\n        even though the header fields say otherwise.\n\n        `interpret` expect that the ilines-, xlines- and offsets-indices are\n        unique. It also expect the dimensions of ilines, xlines and offset to\n        match the tracecount.\n\n        Parameters\n        ----------\n        f : SegyFile\n        ilines : array_like\n            ilines indices in new structure\n        xlines : array_like\n            xlines indices in new structure\n        offsets : array_like\n            offset indices in new structure\n        sorting : int, string or TraceSortingFormat\n            Sorting in new structure\n\n        Notes\n        -----\n\n        .. versionadded:: 1.8\n\n        Examples\n        --------\n        (Re)interpret the structure of the file:\n\n        >>> ilines = [10, 11, 12, 13]\n        >>> xlines = [20, 21, 22, 23, 24]\n        >>> with segyio.open(file, ignore_geometry=True) as f:\n        ... f.interpret(ilines, xlines)", "docstring_tokens": ["(", "Re", "-", ")", "interpret", "structure", "on", "top", "of", "a", "file"], "sha": "58fd449947ccd330b9af0699d6b8710550d34e8e", "url": "https://github.com/equinor/segyio/blob/58fd449947ccd330b9af0699d6b8710550d34e8e/python/segyio/segy.py#L859-L965", "partition": "train"}
{"repo": "pymc-devs/pymc", "path": "pymc/graph.py", "func_name": "moral_graph", "original_string": "def moral_graph(model, format='raw', prog='dot', path=None, name=None):\n    \"\"\"\n    moral_graph(model,format='raw', prog='dot', path=None)\n\n    Draws the moral graph for this model and writes it to path with filename name.\n    Returns the pydot 'dot' object for further user manipulation.\n\n    GraphViz and PyDot must be installed to use this function.\n\n    :Parameters:\n      model : PyMC Model instance\n      format : string\n        'ps', 'ps2', 'hpgl', 'pcl', 'mif', 'pic', 'gd', 'gd2', 'gif', 'jpg',\n        'jpeg', 'png', 'wbmp', 'ismap', 'imap', 'cmap', 'cmapx', 'vrml', 'vtx', 'mp',\n        'fig', 'svg', 'svgz', 'dia', 'dot', 'canon', 'plain', 'plain-ext', 'xdot'\n      prog : string\n        'dot', 'neato'\n      path : string\n        If model.__name__ is defined and path is None, the output file is\n        ./'name'.'format'.\n\n    :Note:\n      format='raw' outputs a GraphViz dot file.\n    \"\"\"\n    if not pydot_imported:\n        raise ImportError(\n            'PyDot must be installed to use the moral_graph function.\\n PyDot is available from http://dkbza.org/pydot.html')\n\n    model.moral_dot_object = pydot.Dot()\n\n    # Data are filled ellipses\n    for datum in model.observed_stochastics:\n        model.moral_dot_object.add_node(\n            pydot.Node(name=datum.__name__,\n                       style='filled'))\n\n    # Stochastics are open ellipses\n    for s in model.stochastics:\n        model.moral_dot_object.add_node(pydot.Node(name=s.__name__))\n\n    gone_already = set()\n    for s in model.stochastics | model.observed_stochastics:\n        gone_already.add(s)\n        for other_s in s.moral_neighbors:\n            if not other_s in gone_already:\n                model.moral_dot_object.add_edge(\n                    pydot.Edge(src=other_s.__name__,\n                               dst=s.__name__,\n                               arrowhead='none'))\n\n    # Draw the graph\n    ext = format\n    if format == 'raw':\n        ext = 'dot'\n    if name is None:\n        name = model.__name__\n    name = name + '.' + ext\n    if not path is None:\n        model.moral_dot_object.write(\n            path=os.path.join(path,\n                              name),\n            format=format,\n            prog=prog)\n    else:\n        model.moral_dot_object.write(\n            path='./' +\n            name,\n            format=format,\n            prog=prog)\n\n    return model.moral_dot_object", "language": "python", "code": "def moral_graph(model, format='raw', prog='dot', path=None, name=None):\n    \"\"\"\n    moral_graph(model,format='raw', prog='dot', path=None)\n\n    Draws the moral graph for this model and writes it to path with filename name.\n    Returns the pydot 'dot' object for further user manipulation.\n\n    GraphViz and PyDot must be installed to use this function.\n\n    :Parameters:\n      model : PyMC Model instance\n      format : string\n        'ps', 'ps2', 'hpgl', 'pcl', 'mif', 'pic', 'gd', 'gd2', 'gif', 'jpg',\n        'jpeg', 'png', 'wbmp', 'ismap', 'imap', 'cmap', 'cmapx', 'vrml', 'vtx', 'mp',\n        'fig', 'svg', 'svgz', 'dia', 'dot', 'canon', 'plain', 'plain-ext', 'xdot'\n      prog : string\n        'dot', 'neato'\n      path : string\n        If model.__name__ is defined and path is None, the output file is\n        ./'name'.'format'.\n\n    :Note:\n      format='raw' outputs a GraphViz dot file.\n    \"\"\"\n    if not pydot_imported:\n        raise ImportError(\n            'PyDot must be installed to use the moral_graph function.\\n PyDot is available from http://dkbza.org/pydot.html')\n\n    model.moral_dot_object = pydot.Dot()\n\n    # Data are filled ellipses\n    for datum in model.observed_stochastics:\n        model.moral_dot_object.add_node(\n            pydot.Node(name=datum.__name__,\n                       style='filled'))\n\n    # Stochastics are open ellipses\n    for s in model.stochastics:\n        model.moral_dot_object.add_node(pydot.Node(name=s.__name__))\n\n    gone_already = set()\n    for s in model.stochastics | model.observed_stochastics:\n        gone_already.add(s)\n        for other_s in s.moral_neighbors:\n            if not other_s in gone_already:\n                model.moral_dot_object.add_edge(\n                    pydot.Edge(src=other_s.__name__,\n                               dst=s.__name__,\n                               arrowhead='none'))\n\n    # Draw the graph\n    ext = format\n    if format == 'raw':\n        ext = 'dot'\n    if name is None:\n        name = model.__name__\n    name = name + '.' + ext\n    if not path is None:\n        model.moral_dot_object.write(\n            path=os.path.join(path,\n                              name),\n            format=format,\n            prog=prog)\n    else:\n        model.moral_dot_object.write(\n            path='./' +\n            name,\n            format=format,\n            prog=prog)\n\n    return model.moral_dot_object", "code_tokens": ["def", "moral_graph", "(", "model", ",", "format", "=", "'raw'", ",", "prog", "=", "'dot'", ",", "path", "=", "None", ",", "name", "=", "None", ")", ":", "if", "not", "pydot_imported", ":", "raise", "ImportError", "(", "'PyDot must be installed to use the moral_graph function.\\n PyDot is available from http://dkbza.org/pydot.html'", ")", "model", ".", "moral_dot_object", "=", "pydot", ".", "Dot", "(", ")", "# Data are filled ellipses", "for", "datum", "in", "model", ".", "observed_stochastics", ":", "model", ".", "moral_dot_object", ".", "add_node", "(", "pydot", ".", "Node", "(", "name", "=", "datum", ".", "__name__", ",", "style", "=", "'filled'", ")", ")", "# Stochastics are open ellipses", "for", "s", "in", "model", ".", "stochastics", ":", "model", ".", "moral_dot_object", ".", "add_node", "(", "pydot", ".", "Node", "(", "name", "=", "s", ".", "__name__", ")", ")", "gone_already", "=", "set", "(", ")", "for", "s", "in", "model", ".", "stochastics", "|", "model", ".", "observed_stochastics", ":", "gone_already", ".", "add", "(", "s", ")", "for", "other_s", "in", "s", ".", "moral_neighbors", ":", "if", "not", "other_s", "in", "gone_already", ":", "model", ".", "moral_dot_object", ".", "add_edge", "(", "pydot", ".", "Edge", "(", "src", "=", "other_s", ".", "__name__", ",", "dst", "=", "s", ".", "__name__", ",", "arrowhead", "=", "'none'", ")", ")", "# Draw the graph", "ext", "=", "format", "if", "format", "==", "'raw'", ":", "ext", "=", "'dot'", "if", "name", "is", "None", ":", "name", "=", "model", ".", "__name__", "name", "=", "name", "+", "'.'", "+", "ext", "if", "not", "path", "is", "None", ":", "model", ".", "moral_dot_object", ".", "write", "(", "path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "name", ")", ",", "format", "=", "format", ",", "prog", "=", "prog", ")", "else", ":", "model", ".", "moral_dot_object", ".", "write", "(", "path", "=", "'./'", "+", "name", ",", "format", "=", "format", ",", "prog", "=", "prog", ")", "return", "model", ".", "moral_dot_object"], "docstring": "moral_graph(model,format='raw', prog='dot', path=None)\n\n    Draws the moral graph for this model and writes it to path with filename name.\n    Returns the pydot 'dot' object for further user manipulation.\n\n    GraphViz and PyDot must be installed to use this function.\n\n    :Parameters:\n      model : PyMC Model instance\n      format : string\n        'ps', 'ps2', 'hpgl', 'pcl', 'mif', 'pic', 'gd', 'gd2', 'gif', 'jpg',\n        'jpeg', 'png', 'wbmp', 'ismap', 'imap', 'cmap', 'cmapx', 'vrml', 'vtx', 'mp',\n        'fig', 'svg', 'svgz', 'dia', 'dot', 'canon', 'plain', 'plain-ext', 'xdot'\n      prog : string\n        'dot', 'neato'\n      path : string\n        If model.__name__ is defined and path is None, the output file is\n        ./'name'.'format'.\n\n    :Note:\n      format='raw' outputs a GraphViz dot file.", "docstring_tokens": ["moral_graph", "(", "model", "format", "=", "raw", "prog", "=", "dot", "path", "=", "None", ")"], "sha": "c6e530210bff4c0d7189b35b2c971bc53f93f7cd", "url": "https://github.com/pymc-devs/pymc/blob/c6e530210bff4c0d7189b35b2c971bc53f93f7cd/pymc/graph.py#L13-L83", "partition": "train"}
{"repo": "jaegertracing/jaeger-client-python", "path": "jaeger_client/thrift_gen/zipkincore/ZipkinCollector.py", "func_name": "Client.submitZipkinBatch", "original_string": "def submitZipkinBatch(self, spans):\n    \"\"\"\n    Parameters:\n     - spans\n    \"\"\"\n    self._seqid += 1\n    future = self._reqs[self._seqid] = concurrent.Future()\n    self.send_submitZipkinBatch(spans)\n    return future", "language": "python", "code": "def submitZipkinBatch(self, spans):\n    \"\"\"\n    Parameters:\n     - spans\n    \"\"\"\n    self._seqid += 1\n    future = self._reqs[self._seqid] = concurrent.Future()\n    self.send_submitZipkinBatch(spans)\n    return future", "code_tokens": ["def", "submitZipkinBatch", "(", "self", ",", "spans", ")", ":", "self", ".", "_seqid", "+=", "1", "future", "=", "self", ".", "_reqs", "[", "self", ".", "_seqid", "]", "=", "concurrent", ".", "Future", "(", ")", "self", ".", "send_submitZipkinBatch", "(", "spans", ")", "return", "future"], "docstring": "Parameters:\n     - spans", "docstring_tokens": ["Parameters", ":", "-", "spans"], "sha": "06face094757c645a6d81f0e073c001931a22a05", "url": "https://github.com/jaegertracing/jaeger-client-python/blob/06face094757c645a6d81f0e073c001931a22a05/jaeger_client/thrift_gen/zipkincore/ZipkinCollector.py#L70-L78", "partition": "train"}
{"repo": "RedHatInsights/insights-core", "path": "insights/combiners/lvm.py", "func_name": "get_shared_data", "original_string": "def get_shared_data(component):\n    \"\"\"\n    Returns the actual list of component data based on how data is\n    stored in component, either from the `data` attribute or from the\n    `data['content']` attribute.\n\n    Returns:\n        list: List of component data.\n    \"\"\"\n    if component:\n        return (copy.deepcopy(component.data)\n                if 'content' not in component.data\n                else copy.deepcopy(component.data['content']))\n    else:\n        return []", "language": "python", "code": "def get_shared_data(component):\n    \"\"\"\n    Returns the actual list of component data based on how data is\n    stored in component, either from the `data` attribute or from the\n    `data['content']` attribute.\n\n    Returns:\n        list: List of component data.\n    \"\"\"\n    if component:\n        return (copy.deepcopy(component.data)\n                if 'content' not in component.data\n                else copy.deepcopy(component.data['content']))\n    else:\n        return []", "code_tokens": ["def", "get_shared_data", "(", "component", ")", ":", "if", "component", ":", "return", "(", "copy", ".", "deepcopy", "(", "component", ".", "data", ")", "if", "'content'", "not", "in", "component", ".", "data", "else", "copy", ".", "deepcopy", "(", "component", ".", "data", "[", "'content'", "]", ")", ")", "else", ":", "return", "[", "]"], "docstring": "Returns the actual list of component data based on how data is\n    stored in component, either from the `data` attribute or from the\n    `data['content']` attribute.\n\n    Returns:\n        list: List of component data.", "docstring_tokens": ["Returns", "the", "actual", "list", "of", "component", "data", "based", "on", "how", "data", "is", "stored", "in", "component", "either", "from", "the", "data", "attribute", "or", "from", "the", "data", "[", "content", "]", "attribute", "."], "sha": "b57cbf8ed7c089672426ede0441e0a4f789ef4a1", "url": "https://github.com/RedHatInsights/insights-core/blob/b57cbf8ed7c089672426ede0441e0a4f789ef4a1/insights/combiners/lvm.py#L82-L96", "partition": "train"}
{"repo": "RedHatInsights/insights-core", "path": "insights/combiners/lvm.py", "func_name": "merge_lvm_data", "original_string": "def merge_lvm_data(primary, secondary, name_key):\n    \"\"\"\n    Returns a dictionary containing the set of data from primary and secondary\n    where values in primary will always be returned if present, and values in\n    secondary will only be returned if not present in primary, or if the value\n    in primary is `None`.\n\n    Sample input Data::\n\n        primary = [\n            {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'name_key': 'xyz'},\n            {'a': None, 'b': 12, 'c': 13, 'd': 14, 'name_key': 'qrs'},\n            {'a': None, 'b': 12, 'c': 13, 'd': 14, 'name_key': 'def'},\n        ]\n        secondary = [\n            {'a': 31, 'e': 33, 'name_key': 'xyz'},\n            {'a': 11, 'e': 23, 'name_key': 'qrs'},\n            {'a': 1, 'e': 3, 'name_key': 'ghi'},\n        ]\n\n    Returns:\n        dict: Dictionary of key value pairs from obj1 and obj2::\n\n            {\n                'xyz': {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 33, 'name_key': 'xyz'},\n                'qrs': {'a': 11, 'b': 12, 'c': 13, d: 14, e: 23, 'name_key': 'qrs'},\n                'def': {'a': None, 'b': 12, 'c': 13, 'd': 14, 'name_key': 'def'},\n                'ghi': {'a': 1, 'e': 3, 'name_key': 'ghi'}\n            }\n\n    \"\"\"\n    pri_data = to_name_key_dict(primary, name_key)\n    # Prime results with secondary data, to be updated with primary data\n    combined_data = to_name_key_dict(secondary, name_key)\n    for name in pri_data:\n        if name not in combined_data:\n            # Data only in primary\n            combined_data[name] = pri_data[name]\n        else:\n            # Data in both primary and secondary, pick primary if better or no secondary\n            combined_data[name].update(dict(\n                (k, v) for k, v in pri_data[name].items()\n                if v is not None or k not in combined_data[name]\n            ))\n\n    return set_defaults(combined_data)", "language": "python", "code": "def merge_lvm_data(primary, secondary, name_key):\n    \"\"\"\n    Returns a dictionary containing the set of data from primary and secondary\n    where values in primary will always be returned if present, and values in\n    secondary will only be returned if not present in primary, or if the value\n    in primary is `None`.\n\n    Sample input Data::\n\n        primary = [\n            {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'name_key': 'xyz'},\n            {'a': None, 'b': 12, 'c': 13, 'd': 14, 'name_key': 'qrs'},\n            {'a': None, 'b': 12, 'c': 13, 'd': 14, 'name_key': 'def'},\n        ]\n        secondary = [\n            {'a': 31, 'e': 33, 'name_key': 'xyz'},\n            {'a': 11, 'e': 23, 'name_key': 'qrs'},\n            {'a': 1, 'e': 3, 'name_key': 'ghi'},\n        ]\n\n    Returns:\n        dict: Dictionary of key value pairs from obj1 and obj2::\n\n            {\n                'xyz': {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 33, 'name_key': 'xyz'},\n                'qrs': {'a': 11, 'b': 12, 'c': 13, d: 14, e: 23, 'name_key': 'qrs'},\n                'def': {'a': None, 'b': 12, 'c': 13, 'd': 14, 'name_key': 'def'},\n                'ghi': {'a': 1, 'e': 3, 'name_key': 'ghi'}\n            }\n\n    \"\"\"\n    pri_data = to_name_key_dict(primary, name_key)\n    # Prime results with secondary data, to be updated with primary data\n    combined_data = to_name_key_dict(secondary, name_key)\n    for name in pri_data:\n        if name not in combined_data:\n            # Data only in primary\n            combined_data[name] = pri_data[name]\n        else:\n            # Data in both primary and secondary, pick primary if better or no secondary\n            combined_data[name].update(dict(\n                (k, v) for k, v in pri_data[name].items()\n                if v is not None or k not in combined_data[name]\n            ))\n\n    return set_defaults(combined_data)", "code_tokens": ["def", "merge_lvm_data", "(", "primary", ",", "secondary", ",", "name_key", ")", ":", "pri_data", "=", "to_name_key_dict", "(", "primary", ",", "name_key", ")", "# Prime results with secondary data, to be updated with primary data", "combined_data", "=", "to_name_key_dict", "(", "secondary", ",", "name_key", ")", "for", "name", "in", "pri_data", ":", "if", "name", "not", "in", "combined_data", ":", "# Data only in primary", "combined_data", "[", "name", "]", "=", "pri_data", "[", "name", "]", "else", ":", "# Data in both primary and secondary, pick primary if better or no secondary", "combined_data", "[", "name", "]", ".", "update", "(", "dict", "(", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "pri_data", "[", "name", "]", ".", "items", "(", ")", "if", "v", "is", "not", "None", "or", "k", "not", "in", "combined_data", "[", "name", "]", ")", ")", "return", "set_defaults", "(", "combined_data", ")"], "docstring": "Returns a dictionary containing the set of data from primary and secondary\n    where values in primary will always be returned if present, and values in\n    secondary will only be returned if not present in primary, or if the value\n    in primary is `None`.\n\n    Sample input Data::\n\n        primary = [\n            {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'name_key': 'xyz'},\n            {'a': None, 'b': 12, 'c': 13, 'd': 14, 'name_key': 'qrs'},\n            {'a': None, 'b': 12, 'c': 13, 'd': 14, 'name_key': 'def'},\n        ]\n        secondary = [\n            {'a': 31, 'e': 33, 'name_key': 'xyz'},\n            {'a': 11, 'e': 23, 'name_key': 'qrs'},\n            {'a': 1, 'e': 3, 'name_key': 'ghi'},\n        ]\n\n    Returns:\n        dict: Dictionary of key value pairs from obj1 and obj2::\n\n            {\n                'xyz': {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 33, 'name_key': 'xyz'},\n                'qrs': {'a': 11, 'b': 12, 'c': 13, d: 14, e: 23, 'name_key': 'qrs'},\n                'def': {'a': None, 'b': 12, 'c': 13, 'd': 14, 'name_key': 'def'},\n                'ghi': {'a': 1, 'e': 3, 'name_key': 'ghi'}\n            }", "docstring_tokens": ["Returns", "a", "dictionary", "containing", "the", "set", "of", "data", "from", "primary", "and", "secondary", "where", "values", "in", "primary", "will", "always", "be", "returned", "if", "present", "and", "values", "in", "secondary", "will", "only", "be", "returned", "if", "not", "present", "in", "primary", "or", "if", "the", "value", "in", "primary", "is", "None", "."], "sha": "b57cbf8ed7c089672426ede0441e0a4f789ef4a1", "url": "https://github.com/RedHatInsights/insights-core/blob/b57cbf8ed7c089672426ede0441e0a4f789ef4a1/insights/combiners/lvm.py#L112-L157", "partition": "train"}
{"repo": "GNS3/gns3-server", "path": "gns3server/compute/qemu/__init__.py", "func_name": "Qemu.create_disk", "original_string": "def create_disk(self, qemu_img, path, options):\n        \"\"\"\n        Create a qemu disk with qemu-img\n\n        :param qemu_img: qemu-img binary path\n        :param path: Image path\n        :param options: Disk image creation options\n        \"\"\"\n\n        try:\n            img_format = options.pop(\"format\")\n            img_size = options.pop(\"size\")\n\n            if not os.path.isabs(path):\n                directory = self.get_images_directory()\n                os.makedirs(directory, exist_ok=True)\n                path = os.path.join(directory, os.path.basename(path))\n\n            try:\n                if os.path.exists(path):\n                    raise QemuError(\"Could not create disk image {} already exist\".format(path))\n            except UnicodeEncodeError:\n                raise QemuError(\"Could not create disk image {}, \"\n                                \"path contains characters not supported by filesystem\".format(path))\n\n            command = [qemu_img, \"create\", \"-f\", img_format]\n            for option in sorted(options.keys()):\n                command.extend([\"-o\", \"{}={}\".format(option, options[option])])\n            command.append(path)\n            command.append(\"{}M\".format(img_size))\n\n            process = yield from asyncio.create_subprocess_exec(*command)\n            yield from process.wait()\n        except (OSError, subprocess.SubprocessError) as e:\n            raise QemuError(\"Could not create disk image {}:{}\".format(path, e))", "language": "python", "code": "def create_disk(self, qemu_img, path, options):\n        \"\"\"\n        Create a qemu disk with qemu-img\n\n        :param qemu_img: qemu-img binary path\n        :param path: Image path\n        :param options: Disk image creation options\n        \"\"\"\n\n        try:\n            img_format = options.pop(\"format\")\n            img_size = options.pop(\"size\")\n\n            if not os.path.isabs(path):\n                directory = self.get_images_directory()\n                os.makedirs(directory, exist_ok=True)\n                path = os.path.join(directory, os.path.basename(path))\n\n            try:\n                if os.path.exists(path):\n                    raise QemuError(\"Could not create disk image {} already exist\".format(path))\n            except UnicodeEncodeError:\n                raise QemuError(\"Could not create disk image {}, \"\n                                \"path contains characters not supported by filesystem\".format(path))\n\n            command = [qemu_img, \"create\", \"-f\", img_format]\n            for option in sorted(options.keys()):\n                command.extend([\"-o\", \"{}={}\".format(option, options[option])])\n            command.append(path)\n            command.append(\"{}M\".format(img_size))\n\n            process = yield from asyncio.create_subprocess_exec(*command)\n            yield from process.wait()\n        except (OSError, subprocess.SubprocessError) as e:\n            raise QemuError(\"Could not create disk image {}:{}\".format(path, e))", "code_tokens": ["def", "create_disk", "(", "self", ",", "qemu_img", ",", "path", ",", "options", ")", ":", "try", ":", "img_format", "=", "options", ".", "pop", "(", "\"format\"", ")", "img_size", "=", "options", ".", "pop", "(", "\"size\"", ")", "if", "not", "os", ".", "path", ".", "isabs", "(", "path", ")", ":", "directory", "=", "self", ".", "get_images_directory", "(", ")", "os", ".", "makedirs", "(", "directory", ",", "exist_ok", "=", "True", ")", "path", "=", "os", ".", "path", ".", "join", "(", "directory", ",", "os", ".", "path", ".", "basename", "(", "path", ")", ")", "try", ":", "if", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "raise", "QemuError", "(", "\"Could not create disk image {} already exist\"", ".", "format", "(", "path", ")", ")", "except", "UnicodeEncodeError", ":", "raise", "QemuError", "(", "\"Could not create disk image {}, \"", "\"path contains characters not supported by filesystem\"", ".", "format", "(", "path", ")", ")", "command", "=", "[", "qemu_img", ",", "\"create\"", ",", "\"-f\"", ",", "img_format", "]", "for", "option", "in", "sorted", "(", "options", ".", "keys", "(", ")", ")", ":", "command", ".", "extend", "(", "[", "\"-o\"", ",", "\"{}={}\"", ".", "format", "(", "option", ",", "options", "[", "option", "]", ")", "]", ")", "command", ".", "append", "(", "path", ")", "command", ".", "append", "(", "\"{}M\"", ".", "format", "(", "img_size", ")", ")", "process", "=", "yield", "from", "asyncio", ".", "create_subprocess_exec", "(", "*", "command", ")", "yield", "from", "process", ".", "wait", "(", ")", "except", "(", "OSError", ",", "subprocess", ".", "SubprocessError", ")", "as", "e", ":", "raise", "QemuError", "(", "\"Could not create disk image {}:{}\"", ".", "format", "(", "path", ",", "e", ")", ")"], "docstring": "Create a qemu disk with qemu-img\n\n        :param qemu_img: qemu-img binary path\n        :param path: Image path\n        :param options: Disk image creation options", "docstring_tokens": ["Create", "a", "qemu", "disk", "with", "qemu", "-", "img"], "sha": "a221678448fb5d24e977ef562f81d56aacc89ab1", "url": "https://github.com/GNS3/gns3-server/blob/a221678448fb5d24e977ef562f81d56aacc89ab1/gns3server/compute/qemu/__init__.py#L233-L267", "partition": "train"}
{"repo": "jmcarpenter2/swifter", "path": "swifter/swifter.py", "func_name": "_SwifterObject.set_npartitions", "original_string": "def set_npartitions(self, npartitions=None):\n        \"\"\"\n        Set the number of partitions to use for dask\n        \"\"\"\n        if npartitions is None:\n            self._npartitions = cpu_count() * 2\n        else:\n            self._npartitions = npartitions\n        return self", "language": "python", "code": "def set_npartitions(self, npartitions=None):\n        \"\"\"\n        Set the number of partitions to use for dask\n        \"\"\"\n        if npartitions is None:\n            self._npartitions = cpu_count() * 2\n        else:\n            self._npartitions = npartitions\n        return self", "code_tokens": ["def", "set_npartitions", "(", "self", ",", "npartitions", "=", "None", ")", ":", "if", "npartitions", "is", "None", ":", "self", ".", "_npartitions", "=", "cpu_count", "(", ")", "*", "2", "else", ":", "self", ".", "_npartitions", "=", "npartitions", "return", "self"], "docstring": "Set the number of partitions to use for dask", "docstring_tokens": ["Set", "the", "number", "of", "partitions", "to", "use", "for", "dask"], "sha": "ed5fc3235b43f981fa58ac9bc982c8209d4e3df3", "url": "https://github.com/jmcarpenter2/swifter/blob/ed5fc3235b43f981fa58ac9bc982c8209d4e3df3/swifter/swifter.py#L39-L47", "partition": "train"}
{"repo": "jmcarpenter2/swifter", "path": "swifter/swifter.py", "func_name": "_SwifterObject.rolling", "original_string": "def rolling(self, window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None):\n        \"\"\"\n        Create a swifter rolling object\n        \"\"\"\n        kwds = {\n            \"window\": window,\n            \"min_periods\": min_periods,\n            \"center\": center,\n            \"win_type\": win_type,\n            \"on\": on,\n            \"axis\": axis,\n            \"closed\": closed,\n        }\n        return Rolling(self._obj, self._npartitions, self._dask_threshold, self._scheduler, self._progress_bar, **kwds)", "language": "python", "code": "def rolling(self, window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None):\n        \"\"\"\n        Create a swifter rolling object\n        \"\"\"\n        kwds = {\n            \"window\": window,\n            \"min_periods\": min_periods,\n            \"center\": center,\n            \"win_type\": win_type,\n            \"on\": on,\n            \"axis\": axis,\n            \"closed\": closed,\n        }\n        return Rolling(self._obj, self._npartitions, self._dask_threshold, self._scheduler, self._progress_bar, **kwds)", "code_tokens": ["def", "rolling", "(", "self", ",", "window", ",", "min_periods", "=", "None", ",", "center", "=", "False", ",", "win_type", "=", "None", ",", "on", "=", "None", ",", "axis", "=", "0", ",", "closed", "=", "None", ")", ":", "kwds", "=", "{", "\"window\"", ":", "window", ",", "\"min_periods\"", ":", "min_periods", ",", "\"center\"", ":", "center", ",", "\"win_type\"", ":", "win_type", ",", "\"on\"", ":", "on", ",", "\"axis\"", ":", "axis", ",", "\"closed\"", ":", "closed", ",", "}", "return", "Rolling", "(", "self", ".", "_obj", ",", "self", ".", "_npartitions", ",", "self", ".", "_dask_threshold", ",", "self", ".", "_scheduler", ",", "self", ".", "_progress_bar", ",", "*", "*", "kwds", ")"], "docstring": "Create a swifter rolling object", "docstring_tokens": ["Create", "a", "swifter", "rolling", "object"], "sha": "ed5fc3235b43f981fa58ac9bc982c8209d4e3df3", "url": "https://github.com/jmcarpenter2/swifter/blob/ed5fc3235b43f981fa58ac9bc982c8209d4e3df3/swifter/swifter.py#L78-L91", "partition": "train"}
{"repo": "jmcarpenter2/swifter", "path": "swifter/swifter.py", "func_name": "Transformation.apply", "original_string": "def apply(self, func, *args, **kwds):\n        \"\"\"\n        Apply the function to the transformed swifter object\n        \"\"\"\n        # estimate time to pandas apply\n        wrapped = self._wrapped_apply(func, *args, **kwds)\n        n_repeats = 3\n        timed = timeit.timeit(wrapped, number=n_repeats)\n        samp_proc_est = timed / n_repeats\n        est_apply_duration = samp_proc_est / self._SAMP_SIZE * self._nrows\n\n        # if pandas apply takes too long, use dask\n        if est_apply_duration > self._dask_threshold:\n            return self._dask_apply(func, *args, **kwds)\n        else:  # use pandas\n            if self._progress_bar:\n                tqdm.pandas(desc=\"Pandas Apply\")\n                return self._obj_pd.progress_apply(func, *args, **kwds)\n            else:\n                return self._obj_pd.apply(func, *args, **kwds)", "language": "python", "code": "def apply(self, func, *args, **kwds):\n        \"\"\"\n        Apply the function to the transformed swifter object\n        \"\"\"\n        # estimate time to pandas apply\n        wrapped = self._wrapped_apply(func, *args, **kwds)\n        n_repeats = 3\n        timed = timeit.timeit(wrapped, number=n_repeats)\n        samp_proc_est = timed / n_repeats\n        est_apply_duration = samp_proc_est / self._SAMP_SIZE * self._nrows\n\n        # if pandas apply takes too long, use dask\n        if est_apply_duration > self._dask_threshold:\n            return self._dask_apply(func, *args, **kwds)\n        else:  # use pandas\n            if self._progress_bar:\n                tqdm.pandas(desc=\"Pandas Apply\")\n                return self._obj_pd.progress_apply(func, *args, **kwds)\n            else:\n                return self._obj_pd.apply(func, *args, **kwds)", "code_tokens": ["def", "apply", "(", "self", ",", "func", ",", "*", "args", ",", "*", "*", "kwds", ")", ":", "# estimate time to pandas apply", "wrapped", "=", "self", ".", "_wrapped_apply", "(", "func", ",", "*", "args", ",", "*", "*", "kwds", ")", "n_repeats", "=", "3", "timed", "=", "timeit", ".", "timeit", "(", "wrapped", ",", "number", "=", "n_repeats", ")", "samp_proc_est", "=", "timed", "/", "n_repeats", "est_apply_duration", "=", "samp_proc_est", "/", "self", ".", "_SAMP_SIZE", "*", "self", ".", "_nrows", "# if pandas apply takes too long, use dask", "if", "est_apply_duration", ">", "self", ".", "_dask_threshold", ":", "return", "self", ".", "_dask_apply", "(", "func", ",", "*", "args", ",", "*", "*", "kwds", ")", "else", ":", "# use pandas", "if", "self", ".", "_progress_bar", ":", "tqdm", ".", "pandas", "(", "desc", "=", "\"Pandas Apply\"", ")", "return", "self", ".", "_obj_pd", ".", "progress_apply", "(", "func", ",", "*", "args", ",", "*", "*", "kwds", ")", "else", ":", "return", "self", ".", "_obj_pd", ".", "apply", "(", "func", ",", "*", "args", ",", "*", "*", "kwds", ")"], "docstring": "Apply the function to the transformed swifter object", "docstring_tokens": ["Apply", "the", "function", "to", "the", "transformed", "swifter", "object"], "sha": "ed5fc3235b43f981fa58ac9bc982c8209d4e3df3", "url": "https://github.com/jmcarpenter2/swifter/blob/ed5fc3235b43f981fa58ac9bc982c8209d4e3df3/swifter/swifter.py#L338-L357", "partition": "train"}
{"repo": "summernote/django-summernote", "path": "django_summernote/utils.py", "func_name": "using_config", "original_string": "def using_config(_func=None):\n    \"\"\"\n    This allows a function to use Summernote configuration\n    as a global variable, temporarily.\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def inner_dec(*args, **kwargs):\n            g = func.__globals__\n            var_name = 'config'\n            sentinel = object()\n\n            oldvalue = g.get(var_name, sentinel)\n            g[var_name] = apps.get_app_config('django_summernote').config\n\n            try:\n                res = func(*args, **kwargs)\n            finally:\n                if oldvalue is sentinel:\n                    del g[var_name]\n                else:\n                    g[var_name] = oldvalue\n\n            return res\n        return inner_dec\n\n    if _func is None:\n        return decorator\n    else:\n        return decorator(_func)", "language": "python", "code": "def using_config(_func=None):\n    \"\"\"\n    This allows a function to use Summernote configuration\n    as a global variable, temporarily.\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def inner_dec(*args, **kwargs):\n            g = func.__globals__\n            var_name = 'config'\n            sentinel = object()\n\n            oldvalue = g.get(var_name, sentinel)\n            g[var_name] = apps.get_app_config('django_summernote').config\n\n            try:\n                res = func(*args, **kwargs)\n            finally:\n                if oldvalue is sentinel:\n                    del g[var_name]\n                else:\n                    g[var_name] = oldvalue\n\n            return res\n        return inner_dec\n\n    if _func is None:\n        return decorator\n    else:\n        return decorator(_func)", "code_tokens": ["def", "using_config", "(", "_func", "=", "None", ")", ":", "def", "decorator", "(", "func", ")", ":", "@", "wraps", "(", "func", ")", "def", "inner_dec", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "g", "=", "func", ".", "__globals__", "var_name", "=", "'config'", "sentinel", "=", "object", "(", ")", "oldvalue", "=", "g", ".", "get", "(", "var_name", ",", "sentinel", ")", "g", "[", "var_name", "]", "=", "apps", ".", "get_app_config", "(", "'django_summernote'", ")", ".", "config", "try", ":", "res", "=", "func", "(", "*", "args", ",", "*", "*", "kwargs", ")", "finally", ":", "if", "oldvalue", "is", "sentinel", ":", "del", "g", "[", "var_name", "]", "else", ":", "g", "[", "var_name", "]", "=", "oldvalue", "return", "res", "return", "inner_dec", "if", "_func", "is", "None", ":", "return", "decorator", "else", ":", "return", "decorator", "(", "_func", ")"], "docstring": "This allows a function to use Summernote configuration\n    as a global variable, temporarily.", "docstring_tokens": ["This", "allows", "a", "function", "to", "use", "Summernote", "configuration", "as", "a", "global", "variable", "temporarily", "."], "sha": "bc7fbbf065d88a909fe3e1533c84110e0dd132bc", "url": "https://github.com/summernote/django-summernote/blob/bc7fbbf065d88a909fe3e1533c84110e0dd132bc/django_summernote/utils.py#L117-L146", "partition": "train"}
{"repo": "summernote/django-summernote", "path": "django_summernote/utils.py", "func_name": "uploaded_filepath", "original_string": "def uploaded_filepath(instance, filename):\n    \"\"\"\n    Returns default filepath for uploaded files.\n    \"\"\"\n    ext = filename.split('.')[-1]\n    filename = \"%s.%s\" % (uuid.uuid4(), ext)\n    today = datetime.now().strftime('%Y-%m-%d')\n    return os.path.join('django-summernote', today, filename)", "language": "python", "code": "def uploaded_filepath(instance, filename):\n    \"\"\"\n    Returns default filepath for uploaded files.\n    \"\"\"\n    ext = filename.split('.')[-1]\n    filename = \"%s.%s\" % (uuid.uuid4(), ext)\n    today = datetime.now().strftime('%Y-%m-%d')\n    return os.path.join('django-summernote', today, filename)", "code_tokens": ["def", "uploaded_filepath", "(", "instance", ",", "filename", ")", ":", "ext", "=", "filename", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", "filename", "=", "\"%s.%s\"", "%", "(", "uuid", ".", "uuid4", "(", ")", ",", "ext", ")", "today", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d'", ")", "return", "os", ".", "path", ".", "join", "(", "'django-summernote'", ",", "today", ",", "filename", ")"], "docstring": "Returns default filepath for uploaded files.", "docstring_tokens": ["Returns", "default", "filepath", "for", "uploaded", "files", "."], "sha": "bc7fbbf065d88a909fe3e1533c84110e0dd132bc", "url": "https://github.com/summernote/django-summernote/blob/bc7fbbf065d88a909fe3e1533c84110e0dd132bc/django_summernote/utils.py#L149-L156", "partition": "train"}
{"repo": "summernote/django-summernote", "path": "django_summernote/utils.py", "func_name": "get_attachment_model", "original_string": "def get_attachment_model():\n    \"\"\"\n    Returns the Attachment model that is active in this project.\n    \"\"\"\n\n    try:\n        from .models import AbstractAttachment\n        klass = apps.get_model(config[\"attachment_model\"])\n        if not issubclass(klass, AbstractAttachment):\n            raise ImproperlyConfigured(\n                \"SUMMERNOTE_CONFIG['attachment_model'] refers to model '%s' that is not \"\n                \"inherited from 'django_summernote.models.AbstractAttachment'\" % config[\"attachment_model\"]\n            )\n        return klass\n    except ValueError:\n        raise ImproperlyConfigured(\"SUMMERNOTE_CONFIG['attachment_model'] must be of the form 'app_label.model_name'\")\n    except LookupError:\n        raise ImproperlyConfigured(\n            \"SUMMERNOTE_CONFIG['attachment_model'] refers to model '%s' that has not been installed\" % config[\"attachment_model\"]\n        )", "language": "python", "code": "def get_attachment_model():\n    \"\"\"\n    Returns the Attachment model that is active in this project.\n    \"\"\"\n\n    try:\n        from .models import AbstractAttachment\n        klass = apps.get_model(config[\"attachment_model\"])\n        if not issubclass(klass, AbstractAttachment):\n            raise ImproperlyConfigured(\n                \"SUMMERNOTE_CONFIG['attachment_model'] refers to model '%s' that is not \"\n                \"inherited from 'django_summernote.models.AbstractAttachment'\" % config[\"attachment_model\"]\n            )\n        return klass\n    except ValueError:\n        raise ImproperlyConfigured(\"SUMMERNOTE_CONFIG['attachment_model'] must be of the form 'app_label.model_name'\")\n    except LookupError:\n        raise ImproperlyConfigured(\n            \"SUMMERNOTE_CONFIG['attachment_model'] refers to model '%s' that has not been installed\" % config[\"attachment_model\"]\n        )", "code_tokens": ["def", "get_attachment_model", "(", ")", ":", "try", ":", "from", ".", "models", "import", "AbstractAttachment", "klass", "=", "apps", ".", "get_model", "(", "config", "[", "\"attachment_model\"", "]", ")", "if", "not", "issubclass", "(", "klass", ",", "AbstractAttachment", ")", ":", "raise", "ImproperlyConfigured", "(", "\"SUMMERNOTE_CONFIG['attachment_model'] refers to model '%s' that is not \"", "\"inherited from 'django_summernote.models.AbstractAttachment'\"", "%", "config", "[", "\"attachment_model\"", "]", ")", "return", "klass", "except", "ValueError", ":", "raise", "ImproperlyConfigured", "(", "\"SUMMERNOTE_CONFIG['attachment_model'] must be of the form 'app_label.model_name'\"", ")", "except", "LookupError", ":", "raise", "ImproperlyConfigured", "(", "\"SUMMERNOTE_CONFIG['attachment_model'] refers to model '%s' that has not been installed\"", "%", "config", "[", "\"attachment_model\"", "]", ")"], "docstring": "Returns the Attachment model that is active in this project.", "docstring_tokens": ["Returns", "the", "Attachment", "model", "that", "is", "active", "in", "this", "project", "."], "sha": "bc7fbbf065d88a909fe3e1533c84110e0dd132bc", "url": "https://github.com/summernote/django-summernote/blob/bc7fbbf065d88a909fe3e1533c84110e0dd132bc/django_summernote/utils.py#L180-L199", "partition": "train"}
{"repo": "summernote/django-summernote", "path": "django_summernote/apps.py", "func_name": "DjangoSummernoteConfig._copy_old_configs", "original_string": "def _copy_old_configs(self, user, default):\n        \"\"\"\n        NOTE: Will be deprecated from 0.9\n        Copying old-style settings for backword-compatibility\n        \"\"\"\n        DEPRECATED_SUMMERNOTE_CONFIGS = (\n            'width',\n            'height',\n            'lang',\n            'toolbar',\n        )\n        for key in DEPRECATED_SUMMERNOTE_CONFIGS:\n            if user.get(key):\n                self.config['summernote'][key] = user.get(key)\n            if not self.config['summernote'].get(key):\n                self.config['summernote'][key] = default['summernote'].get(key)", "language": "python", "code": "def _copy_old_configs(self, user, default):\n        \"\"\"\n        NOTE: Will be deprecated from 0.9\n        Copying old-style settings for backword-compatibility\n        \"\"\"\n        DEPRECATED_SUMMERNOTE_CONFIGS = (\n            'width',\n            'height',\n            'lang',\n            'toolbar',\n        )\n        for key in DEPRECATED_SUMMERNOTE_CONFIGS:\n            if user.get(key):\n                self.config['summernote'][key] = user.get(key)\n            if not self.config['summernote'].get(key):\n                self.config['summernote'][key] = default['summernote'].get(key)", "code_tokens": ["def", "_copy_old_configs", "(", "self", ",", "user", ",", "default", ")", ":", "DEPRECATED_SUMMERNOTE_CONFIGS", "=", "(", "'width'", ",", "'height'", ",", "'lang'", ",", "'toolbar'", ",", ")", "for", "key", "in", "DEPRECATED_SUMMERNOTE_CONFIGS", ":", "if", "user", ".", "get", "(", "key", ")", ":", "self", ".", "config", "[", "'summernote'", "]", "[", "key", "]", "=", "user", ".", "get", "(", "key", ")", "if", "not", "self", ".", "config", "[", "'summernote'", "]", ".", "get", "(", "key", ")", ":", "self", ".", "config", "[", "'summernote'", "]", "[", "key", "]", "=", "default", "[", "'summernote'", "]", ".", "get", "(", "key", ")"], "docstring": "NOTE: Will be deprecated from 0.9\n        Copying old-style settings for backword-compatibility", "docstring_tokens": ["NOTE", ":", "Will", "be", "deprecated", "from", "0", ".", "9", "Copying", "old", "-", "style", "settings", "for", "backword", "-", "compatibility"], "sha": "bc7fbbf065d88a909fe3e1533c84110e0dd132bc", "url": "https://github.com/summernote/django-summernote/blob/bc7fbbf065d88a909fe3e1533c84110e0dd132bc/django_summernote/apps.py#L93-L108", "partition": "train"}
{"repo": "PyMySQL/mysqlclient-python", "path": "MySQLdb/connections.py", "func_name": "numeric_part", "original_string": "def numeric_part(s):\n    \"\"\"Returns the leading numeric part of a string.\n\n    >>> numeric_part(\"20-alpha\")\n    20\n    >>> numeric_part(\"foo\")\n    >>> numeric_part(\"16b\")\n    16\n    \"\"\"\n\n    m = re_numeric_part.match(s)\n    if m:\n        return int(m.group(1))\n    return None", "language": "python", "code": "def numeric_part(s):\n    \"\"\"Returns the leading numeric part of a string.\n\n    >>> numeric_part(\"20-alpha\")\n    20\n    >>> numeric_part(\"foo\")\n    >>> numeric_part(\"16b\")\n    16\n    \"\"\"\n\n    m = re_numeric_part.match(s)\n    if m:\n        return int(m.group(1))\n    return None", "code_tokens": ["def", "numeric_part", "(", "s", ")", ":", "m", "=", "re_numeric_part", ".", "match", "(", "s", ")", "if", "m", ":", "return", "int", "(", "m", ".", "group", "(", "1", ")", ")", "return", "None"], "docstring": "Returns the leading numeric part of a string.\n\n    >>> numeric_part(\"20-alpha\")\n    20\n    >>> numeric_part(\"foo\")\n    >>> numeric_part(\"16b\")\n    16", "docstring_tokens": ["Returns", "the", "leading", "numeric", "part", "of", "a", "string", "."], "sha": "b66971ee36be96b772ae7fdec79ccc1611376f3c", "url": "https://github.com/PyMySQL/mysqlclient-python/blob/b66971ee36be96b772ae7fdec79ccc1611376f3c/MySQLdb/connections.py#L21-L34", "partition": "train"}
{"repo": "MagicStack/asyncpg", "path": "asyncpg/utils.py", "func_name": "_mogrify", "original_string": "async def _mogrify(conn, query, args):\n    \"\"\"Safely inline arguments to query text.\"\"\"\n    # Introspect the target query for argument types and\n    # build a list of safely-quoted fully-qualified type names.\n    ps = await conn.prepare(query)\n    paramtypes = []\n    for t in ps.get_parameters():\n        if t.name.endswith('[]'):\n            pname = '_' + t.name[:-2]\n        else:\n            pname = t.name\n\n        paramtypes.append('{}.{}'.format(\n            _quote_ident(t.schema), _quote_ident(pname)))\n    del ps\n\n    # Use Postgres to convert arguments to text representation\n    # by casting each value to text.\n    cols = ['quote_literal(${}::{}::text)'.format(i, t)\n            for i, t in enumerate(paramtypes, start=1)]\n\n    textified = await conn.fetchrow(\n        'SELECT {cols}'.format(cols=', '.join(cols)), *args)\n\n    # Finally, replace $n references with text values.\n    return re.sub(\n        r'\\$(\\d+)\\b', lambda m: textified[int(m.group(1)) - 1], query)", "language": "python", "code": "async def _mogrify(conn, query, args):\n    \"\"\"Safely inline arguments to query text.\"\"\"\n    # Introspect the target query for argument types and\n    # build a list of safely-quoted fully-qualified type names.\n    ps = await conn.prepare(query)\n    paramtypes = []\n    for t in ps.get_parameters():\n        if t.name.endswith('[]'):\n            pname = '_' + t.name[:-2]\n        else:\n            pname = t.name\n\n        paramtypes.append('{}.{}'.format(\n            _quote_ident(t.schema), _quote_ident(pname)))\n    del ps\n\n    # Use Postgres to convert arguments to text representation\n    # by casting each value to text.\n    cols = ['quote_literal(${}::{}::text)'.format(i, t)\n            for i, t in enumerate(paramtypes, start=1)]\n\n    textified = await conn.fetchrow(\n        'SELECT {cols}'.format(cols=', '.join(cols)), *args)\n\n    # Finally, replace $n references with text values.\n    return re.sub(\n        r'\\$(\\d+)\\b', lambda m: textified[int(m.group(1)) - 1], query)", "code_tokens": ["async", "def", "_mogrify", "(", "conn", ",", "query", ",", "args", ")", ":", "# Introspect the target query for argument types and", "# build a list of safely-quoted fully-qualified type names.", "ps", "=", "await", "conn", ".", "prepare", "(", "query", ")", "paramtypes", "=", "[", "]", "for", "t", "in", "ps", ".", "get_parameters", "(", ")", ":", "if", "t", ".", "name", ".", "endswith", "(", "'[]'", ")", ":", "pname", "=", "'_'", "+", "t", ".", "name", "[", ":", "-", "2", "]", "else", ":", "pname", "=", "t", ".", "name", "paramtypes", ".", "append", "(", "'{}.{}'", ".", "format", "(", "_quote_ident", "(", "t", ".", "schema", ")", ",", "_quote_ident", "(", "pname", ")", ")", ")", "del", "ps", "# Use Postgres to convert arguments to text representation", "# by casting each value to text.", "cols", "=", "[", "'quote_literal(${}::{}::text)'", ".", "format", "(", "i", ",", "t", ")", "for", "i", ",", "t", "in", "enumerate", "(", "paramtypes", ",", "start", "=", "1", ")", "]", "textified", "=", "await", "conn", ".", "fetchrow", "(", "'SELECT {cols}'", ".", "format", "(", "cols", "=", "', '", ".", "join", "(", "cols", ")", ")", ",", "*", "args", ")", "# Finally, replace $n references with text values.", "return", "re", ".", "sub", "(", "r'\\$(\\d+)\\b'", ",", "lambda", "m", ":", "textified", "[", "int", "(", "m", ".", "group", "(", "1", ")", ")", "-", "1", "]", ",", "query", ")"], "docstring": "Safely inline arguments to query text.", "docstring_tokens": ["Safely", "inline", "arguments", "to", "query", "text", "."], "sha": "92c2d81256a1efd8cab12c0118d74ccd1c18131b", "url": "https://github.com/MagicStack/asyncpg/blob/92c2d81256a1efd8cab12c0118d74ccd1c18131b/asyncpg/utils.py#L19-L45", "partition": "train"}
{"repo": "jrfonseca/gprof2dot", "path": "gprof2dot.py", "func_name": "main", "original_string": "def main():\n    \"\"\"Main program.\"\"\"\n\n    global totalMethod\n\n    formatNames = list(formats.keys())\n    formatNames.sort()\n\n    themeNames = list(themes.keys())\n    themeNames.sort()\n\n    labelNames = list(labels.keys())\n    labelNames.sort()\n\n    optparser = optparse.OptionParser(\n        usage=\"\\n\\t%prog [options] [file] ...\")\n    optparser.add_option(\n        '-o', '--output', metavar='FILE',\n        type=\"string\", dest=\"output\",\n        help=\"output filename [stdout]\")\n    optparser.add_option(\n        '-n', '--node-thres', metavar='PERCENTAGE',\n        type=\"float\", dest=\"node_thres\", default=0.5,\n        help=\"eliminate nodes below this threshold [default: %default]\")\n    optparser.add_option(\n        '-e', '--edge-thres', metavar='PERCENTAGE',\n        type=\"float\", dest=\"edge_thres\", default=0.1,\n        help=\"eliminate edges below this threshold [default: %default]\")\n    optparser.add_option(\n        '-f', '--format',\n        type=\"choice\", choices=formatNames,\n        dest=\"format\", default=\"prof\",\n        help=\"profile format: %s [default: %%default]\" % naturalJoin(formatNames))\n    optparser.add_option(\n        '--total',\n        type=\"choice\", choices=('callratios', 'callstacks'),\n        dest=\"totalMethod\", default=totalMethod,\n        help=\"preferred method of calculating total time: callratios or callstacks (currently affects only perf format) [default: %default]\")\n    optparser.add_option(\n        '-c', '--colormap',\n        type=\"choice\", choices=themeNames,\n        dest=\"theme\", default=\"color\",\n        help=\"color map: %s [default: %%default]\" % naturalJoin(themeNames))\n    optparser.add_option(\n        '-s', '--strip',\n        action=\"store_true\",\n        dest=\"strip\", default=False,\n        help=\"strip function parameters, template parameters, and const modifiers from demangled C++ function names\")\n    optparser.add_option(\n        '--color-nodes-by-selftime',\n        action=\"store_true\",\n        dest=\"color_nodes_by_selftime\", default=False,\n        help=\"color nodes by self time, rather than by total time (sum of self and descendants)\")\n    optparser.add_option(\n        '--colour-nodes-by-selftime',\n        action=\"store_true\",\n        dest=\"color_nodes_by_selftime\",\n        help=optparse.SUPPRESS_HELP)\n    optparser.add_option(\n        '-w', '--wrap',\n        action=\"store_true\",\n        dest=\"wrap\", default=False,\n        help=\"wrap function names\")\n    optparser.add_option(\n        '--show-samples',\n        action=\"store_true\",\n        dest=\"show_samples\", default=False,\n        help=\"show function samples\")\n    optparser.add_option(\n        '--node-label', metavar='MEASURE',\n        type='choice', choices=labelNames,\n        action='append',\n        dest='node_labels',\n        help=\"measurements to on show the node (can be specified multiple times): %s [default: %s]\" % (\n            naturalJoin(labelNames), ', '.join(defaultLabelNames)))\n    # add option to create subtree or show paths\n    optparser.add_option(\n        '-z', '--root',\n        type=\"string\",\n        dest=\"root\", default=\"\",\n        help=\"prune call graph to show only descendants of specified root function\")\n    optparser.add_option(\n        '-l', '--leaf',\n        type=\"string\",\n        dest=\"leaf\", default=\"\",\n        help=\"prune call graph to show only ancestors of specified leaf function\")\n    optparser.add_option(\n        '--depth',\n        type=\"int\",\n        dest=\"depth\", default=-1,\n        help=\"prune call graph to show only descendants or ancestors until specified depth\")\n    # add a new option to control skew of the colorization curve\n    optparser.add_option(\n        '--skew',\n        type=\"float\", dest=\"theme_skew\", default=1.0,\n        help=\"skew the colorization curve.  Values < 1.0 give more variety to lower percentages.  Values > 1.0 give less variety to lower percentages\")\n    # add option for filtering by file path\n    optparser.add_option(\n        '-p', '--path', action=\"append\",\n        type=\"string\", dest=\"filter_paths\",\n        help=\"Filter all modules not in a specified path\")\n    (options, args) = optparser.parse_args(sys.argv[1:])\n\n    if len(args) > 1 and options.format != 'pstats':\n        optparser.error('incorrect number of arguments')\n\n    try:\n        theme = themes[options.theme]\n    except KeyError:\n        optparser.error('invalid colormap \\'%s\\'' % options.theme)\n\n    # set skew on the theme now that it has been picked.\n    if options.theme_skew:\n        theme.skew = options.theme_skew\n\n    totalMethod = options.totalMethod\n\n    try:\n        Format = formats[options.format]\n    except KeyError:\n        optparser.error('invalid format \\'%s\\'' % options.format)\n\n    if Format.stdinInput:\n        if not args:\n            fp = sys.stdin\n        elif PYTHON_3:\n            fp = open(args[0], 'rt', encoding='UTF-8')\n        else:\n            fp = open(args[0], 'rt')\n        parser = Format(fp)\n    elif Format.multipleInput:\n        if not args:\n            optparser.error('at least a file must be specified for %s input' % options.format)\n        parser = Format(*args)\n    else:\n        if len(args) != 1:\n            optparser.error('exactly one file must be specified for %s input' % options.format)\n        parser = Format(args[0])\n\n    profile = parser.parse()\n\n    if options.output is None:\n        if PYTHON_3:\n            output = open(sys.stdout.fileno(), mode='wt', encoding='UTF-8', closefd=False)\n        else:\n            output = sys.stdout\n    else:\n        if PYTHON_3:\n            output = open(options.output, 'wt', encoding='UTF-8')\n        else:\n            output = open(options.output, 'wt')\n\n    dot = DotWriter(output)\n    dot.strip = options.strip\n    dot.wrap = options.wrap\n\n    labelNames = options.node_labels or defaultLabelNames\n    dot.show_function_events = [labels[l] for l in labelNames]\n    if options.show_samples:\n        dot.show_function_events.append(SAMPLES)\n\n    profile = profile\n    profile.prune(options.node_thres/100.0, options.edge_thres/100.0, options.filter_paths, options.color_nodes_by_selftime)\n\n    if options.root:\n        rootIds = profile.getFunctionIds(options.root)\n        if not rootIds:\n            sys.stderr.write('root node ' + options.root + ' not found (might already be pruned : try -e0 -n0 flags)\\n')\n            sys.exit(1)\n        profile.prune_root(rootIds, options.depth)\n    if options.leaf:\n        leafIds = profile.getFunctionIds(options.leaf)\n        if not leafIds:\n            sys.stderr.write('leaf node ' + options.leaf + ' not found (maybe already pruned : try -e0 -n0 flags)\\n')\n            sys.exit(1)\n        profile.prune_leaf(leafIds, options.depth)\n\n    dot.graph(profile, theme)", "language": "python", "code": "def main():\n    \"\"\"Main program.\"\"\"\n\n    global totalMethod\n\n    formatNames = list(formats.keys())\n    formatNames.sort()\n\n    themeNames = list(themes.keys())\n    themeNames.sort()\n\n    labelNames = list(labels.keys())\n    labelNames.sort()\n\n    optparser = optparse.OptionParser(\n        usage=\"\\n\\t%prog [options] [file] ...\")\n    optparser.add_option(\n        '-o', '--output', metavar='FILE',\n        type=\"string\", dest=\"output\",\n        help=\"output filename [stdout]\")\n    optparser.add_option(\n        '-n', '--node-thres', metavar='PERCENTAGE',\n        type=\"float\", dest=\"node_thres\", default=0.5,\n        help=\"eliminate nodes below this threshold [default: %default]\")\n    optparser.add_option(\n        '-e', '--edge-thres', metavar='PERCENTAGE',\n        type=\"float\", dest=\"edge_thres\", default=0.1,\n        help=\"eliminate edges below this threshold [default: %default]\")\n    optparser.add_option(\n        '-f', '--format',\n        type=\"choice\", choices=formatNames,\n        dest=\"format\", default=\"prof\",\n        help=\"profile format: %s [default: %%default]\" % naturalJoin(formatNames))\n    optparser.add_option(\n        '--total',\n        type=\"choice\", choices=('callratios', 'callstacks'),\n        dest=\"totalMethod\", default=totalMethod,\n        help=\"preferred method of calculating total time: callratios or callstacks (currently affects only perf format) [default: %default]\")\n    optparser.add_option(\n        '-c', '--colormap',\n        type=\"choice\", choices=themeNames,\n        dest=\"theme\", default=\"color\",\n        help=\"color map: %s [default: %%default]\" % naturalJoin(themeNames))\n    optparser.add_option(\n        '-s', '--strip',\n        action=\"store_true\",\n        dest=\"strip\", default=False,\n        help=\"strip function parameters, template parameters, and const modifiers from demangled C++ function names\")\n    optparser.add_option(\n        '--color-nodes-by-selftime',\n        action=\"store_true\",\n        dest=\"color_nodes_by_selftime\", default=False,\n        help=\"color nodes by self time, rather than by total time (sum of self and descendants)\")\n    optparser.add_option(\n        '--colour-nodes-by-selftime',\n        action=\"store_true\",\n        dest=\"color_nodes_by_selftime\",\n        help=optparse.SUPPRESS_HELP)\n    optparser.add_option(\n        '-w', '--wrap',\n        action=\"store_true\",\n        dest=\"wrap\", default=False,\n        help=\"wrap function names\")\n    optparser.add_option(\n        '--show-samples',\n        action=\"store_true\",\n        dest=\"show_samples\", default=False,\n        help=\"show function samples\")\n    optparser.add_option(\n        '--node-label', metavar='MEASURE',\n        type='choice', choices=labelNames,\n        action='append',\n        dest='node_labels',\n        help=\"measurements to on show the node (can be specified multiple times): %s [default: %s]\" % (\n            naturalJoin(labelNames), ', '.join(defaultLabelNames)))\n    # add option to create subtree or show paths\n    optparser.add_option(\n        '-z', '--root',\n        type=\"string\",\n        dest=\"root\", default=\"\",\n        help=\"prune call graph to show only descendants of specified root function\")\n    optparser.add_option(\n        '-l', '--leaf',\n        type=\"string\",\n        dest=\"leaf\", default=\"\",\n        help=\"prune call graph to show only ancestors of specified leaf function\")\n    optparser.add_option(\n        '--depth',\n        type=\"int\",\n        dest=\"depth\", default=-1,\n        help=\"prune call graph to show only descendants or ancestors until specified depth\")\n    # add a new option to control skew of the colorization curve\n    optparser.add_option(\n        '--skew',\n        type=\"float\", dest=\"theme_skew\", default=1.0,\n        help=\"skew the colorization curve.  Values < 1.0 give more variety to lower percentages.  Values > 1.0 give less variety to lower percentages\")\n    # add option for filtering by file path\n    optparser.add_option(\n        '-p', '--path', action=\"append\",\n        type=\"string\", dest=\"filter_paths\",\n        help=\"Filter all modules not in a specified path\")\n    (options, args) = optparser.parse_args(sys.argv[1:])\n\n    if len(args) > 1 and options.format != 'pstats':\n        optparser.error('incorrect number of arguments')\n\n    try:\n        theme = themes[options.theme]\n    except KeyError:\n        optparser.error('invalid colormap \\'%s\\'' % options.theme)\n\n    # set skew on the theme now that it has been picked.\n    if options.theme_skew:\n        theme.skew = options.theme_skew\n\n    totalMethod = options.totalMethod\n\n    try:\n        Format = formats[options.format]\n    except KeyError:\n        optparser.error('invalid format \\'%s\\'' % options.format)\n\n    if Format.stdinInput:\n        if not args:\n            fp = sys.stdin\n        elif PYTHON_3:\n            fp = open(args[0], 'rt', encoding='UTF-8')\n        else:\n            fp = open(args[0], 'rt')\n        parser = Format(fp)\n    elif Format.multipleInput:\n        if not args:\n            optparser.error('at least a file must be specified for %s input' % options.format)\n        parser = Format(*args)\n    else:\n        if len(args) != 1:\n            optparser.error('exactly one file must be specified for %s input' % options.format)\n        parser = Format(args[0])\n\n    profile = parser.parse()\n\n    if options.output is None:\n        if PYTHON_3:\n            output = open(sys.stdout.fileno(), mode='wt', encoding='UTF-8', closefd=False)\n        else:\n            output = sys.stdout\n    else:\n        if PYTHON_3:\n            output = open(options.output, 'wt', encoding='UTF-8')\n        else:\n            output = open(options.output, 'wt')\n\n    dot = DotWriter(output)\n    dot.strip = options.strip\n    dot.wrap = options.wrap\n\n    labelNames = options.node_labels or defaultLabelNames\n    dot.show_function_events = [labels[l] for l in labelNames]\n    if options.show_samples:\n        dot.show_function_events.append(SAMPLES)\n\n    profile = profile\n    profile.prune(options.node_thres/100.0, options.edge_thres/100.0, options.filter_paths, options.color_nodes_by_selftime)\n\n    if options.root:\n        rootIds = profile.getFunctionIds(options.root)\n        if not rootIds:\n            sys.stderr.write('root node ' + options.root + ' not found (might already be pruned : try -e0 -n0 flags)\\n')\n            sys.exit(1)\n        profile.prune_root(rootIds, options.depth)\n    if options.leaf:\n        leafIds = profile.getFunctionIds(options.leaf)\n        if not leafIds:\n            sys.stderr.write('leaf node ' + options.leaf + ' not found (maybe already pruned : try -e0 -n0 flags)\\n')\n            sys.exit(1)\n        profile.prune_leaf(leafIds, options.depth)\n\n    dot.graph(profile, theme)", "code_tokens": ["def", "main", "(", ")", ":", "global", "totalMethod", "formatNames", "=", "list", "(", "formats", ".", "keys", "(", ")", ")", "formatNames", ".", "sort", "(", ")", "themeNames", "=", "list", "(", "themes", ".", "keys", "(", ")", ")", "themeNames", ".", "sort", "(", ")", "labelNames", "=", "list", "(", "labels", ".", "keys", "(", ")", ")", "labelNames", ".", "sort", "(", ")", "optparser", "=", "optparse", ".", "OptionParser", "(", "usage", "=", "\"\\n\\t%prog [options] [file] ...\"", ")", "optparser", ".", "add_option", "(", "'-o'", ",", "'--output'", ",", "metavar", "=", "'FILE'", ",", "type", "=", "\"string\"", ",", "dest", "=", "\"output\"", ",", "help", "=", "\"output filename [stdout]\"", ")", "optparser", ".", "add_option", "(", "'-n'", ",", "'--node-thres'", ",", "metavar", "=", "'PERCENTAGE'", ",", "type", "=", "\"float\"", ",", "dest", "=", "\"node_thres\"", ",", "default", "=", "0.5", ",", "help", "=", "\"eliminate nodes below this threshold [default: %default]\"", ")", "optparser", ".", "add_option", "(", "'-e'", ",", "'--edge-thres'", ",", "metavar", "=", "'PERCENTAGE'", ",", "type", "=", "\"float\"", ",", "dest", "=", "\"edge_thres\"", ",", "default", "=", "0.1", ",", "help", "=", "\"eliminate edges below this threshold [default: %default]\"", ")", "optparser", ".", "add_option", "(", "'-f'", ",", "'--format'", ",", "type", "=", "\"choice\"", ",", "choices", "=", "formatNames", ",", "dest", "=", "\"format\"", ",", "default", "=", "\"prof\"", ",", "help", "=", "\"profile format: %s [default: %%default]\"", "%", "naturalJoin", "(", "formatNames", ")", ")", "optparser", ".", "add_option", "(", "'--total'", ",", "type", "=", "\"choice\"", ",", "choices", "=", "(", "'callratios'", ",", "'callstacks'", ")", ",", "dest", "=", "\"totalMethod\"", ",", "default", "=", "totalMethod", ",", "help", "=", "\"preferred method of calculating total time: callratios or callstacks (currently affects only perf format) [default: %default]\"", ")", "optparser", ".", "add_option", "(", "'-c'", ",", "'--colormap'", ",", "type", "=", "\"choice\"", ",", "choices", "=", "themeNames", ",", "dest", "=", "\"theme\"", ",", "default", "=", "\"color\"", ",", "help", "=", "\"color map: %s [default: %%default]\"", "%", "naturalJoin", "(", "themeNames", ")", ")", "optparser", ".", "add_option", "(", "'-s'", ",", "'--strip'", ",", "action", "=", "\"store_true\"", ",", "dest", "=", "\"strip\"", ",", "default", "=", "False", ",", "help", "=", "\"strip function parameters, template parameters, and const modifiers from demangled C++ function names\"", ")", "optparser", ".", "add_option", "(", "'--color-nodes-by-selftime'", ",", "action", "=", "\"store_true\"", ",", "dest", "=", "\"color_nodes_by_selftime\"", ",", "default", "=", "False", ",", "help", "=", "\"color nodes by self time, rather than by total time (sum of self and descendants)\"", ")", "optparser", ".", "add_option", "(", "'--colour-nodes-by-selftime'", ",", "action", "=", "\"store_true\"", ",", "dest", "=", "\"color_nodes_by_selftime\"", ",", "help", "=", "optparse", ".", "SUPPRESS_HELP", ")", "optparser", ".", "add_option", "(", "'-w'", ",", "'--wrap'", ",", "action", "=", "\"store_true\"", ",", "dest", "=", "\"wrap\"", ",", "default", "=", "False", ",", "help", "=", "\"wrap function names\"", ")", "optparser", ".", "add_option", "(", "'--show-samples'", ",", "action", "=", "\"store_true\"", ",", "dest", "=", "\"show_samples\"", ",", "default", "=", "False", ",", "help", "=", "\"show function samples\"", ")", "optparser", ".", "add_option", "(", "'--node-label'", ",", "metavar", "=", "'MEASURE'", ",", "type", "=", "'choice'", ",", "choices", "=", "labelNames", ",", "action", "=", "'append'", ",", "dest", "=", "'node_labels'", ",", "help", "=", "\"measurements to on show the node (can be specified multiple times): %s [default: %s]\"", "%", "(", "naturalJoin", "(", "labelNames", ")", ",", "', '", ".", "join", "(", "defaultLabelNames", ")", ")", ")", "# add option to create subtree or show paths", "optparser", ".", "add_option", "(", "'-z'", ",", "'--root'", ",", "type", "=", "\"string\"", ",", "dest", "=", "\"root\"", ",", "default", "=", "\"\"", ",", "help", "=", "\"prune call graph to show only descendants of specified root function\"", ")", "optparser", ".", "add_option", "(", "'-l'", ",", "'--leaf'", ",", "type", "=", "\"string\"", ",", "dest", "=", "\"leaf\"", ",", "default", "=", "\"\"", ",", "help", "=", "\"prune call graph to show only ancestors of specified leaf function\"", ")", "optparser", ".", "add_option", "(", "'--depth'", ",", "type", "=", "\"int\"", ",", "dest", "=", "\"depth\"", ",", "default", "=", "-", "1", ",", "help", "=", "\"prune call graph to show only descendants or ancestors until specified depth\"", ")", "# add a new option to control skew of the colorization curve", "optparser", ".", "add_option", "(", "'--skew'", ",", "type", "=", "\"float\"", ",", "dest", "=", "\"theme_skew\"", ",", "default", "=", "1.0", ",", "help", "=", "\"skew the colorization curve.  Values < 1.0 give more variety to lower percentages.  Values > 1.0 give less variety to lower percentages\"", ")", "# add option for filtering by file path", "optparser", ".", "add_option", "(", "'-p'", ",", "'--path'", ",", "action", "=", "\"append\"", ",", "type", "=", "\"string\"", ",", "dest", "=", "\"filter_paths\"", ",", "help", "=", "\"Filter all modules not in a specified path\"", ")", "(", "options", ",", "args", ")", "=", "optparser", ".", "parse_args", "(", "sys", ".", "argv", "[", "1", ":", "]", ")", "if", "len", "(", "args", ")", ">", "1", "and", "options", ".", "format", "!=", "'pstats'", ":", "optparser", ".", "error", "(", "'incorrect number of arguments'", ")", "try", ":", "theme", "=", "themes", "[", "options", ".", "theme", "]", "except", "KeyError", ":", "optparser", ".", "error", "(", "'invalid colormap \\'%s\\''", "%", "options", ".", "theme", ")", "# set skew on the theme now that it has been picked.", "if", "options", ".", "theme_skew", ":", "theme", ".", "skew", "=", "options", ".", "theme_skew", "totalMethod", "=", "options", ".", "totalMethod", "try", ":", "Format", "=", "formats", "[", "options", ".", "format", "]", "except", "KeyError", ":", "optparser", ".", "error", "(", "'invalid format \\'%s\\''", "%", "options", ".", "format", ")", "if", "Format", ".", "stdinInput", ":", "if", "not", "args", ":", "fp", "=", "sys", ".", "stdin", "elif", "PYTHON_3", ":", "fp", "=", "open", "(", "args", "[", "0", "]", ",", "'rt'", ",", "encoding", "=", "'UTF-8'", ")", "else", ":", "fp", "=", "open", "(", "args", "[", "0", "]", ",", "'rt'", ")", "parser", "=", "Format", "(", "fp", ")", "elif", "Format", ".", "multipleInput", ":", "if", "not", "args", ":", "optparser", ".", "error", "(", "'at least a file must be specified for %s input'", "%", "options", ".", "format", ")", "parser", "=", "Format", "(", "*", "args", ")", "else", ":", "if", "len", "(", "args", ")", "!=", "1", ":", "optparser", ".", "error", "(", "'exactly one file must be specified for %s input'", "%", "options", ".", "format", ")", "parser", "=", "Format", "(", "args", "[", "0", "]", ")", "profile", "=", "parser", ".", "parse", "(", ")", "if", "options", ".", "output", "is", "None", ":", "if", "PYTHON_3", ":", "output", "=", "open", "(", "sys", ".", "stdout", ".", "fileno", "(", ")", ",", "mode", "=", "'wt'", ",", "encoding", "=", "'UTF-8'", ",", "closefd", "=", "False", ")", "else", ":", "output", "=", "sys", ".", "stdout", "else", ":", "if", "PYTHON_3", ":", "output", "=", "open", "(", "options", ".", "output", ",", "'wt'", ",", "encoding", "=", "'UTF-8'", ")", "else", ":", "output", "=", "open", "(", "options", ".", "output", ",", "'wt'", ")", "dot", "=", "DotWriter", "(", "output", ")", "dot", ".", "strip", "=", "options", ".", "strip", "dot", ".", "wrap", "=", "options", ".", "wrap", "labelNames", "=", "options", ".", "node_labels", "or", "defaultLabelNames", "dot", ".", "show_function_events", "=", "[", "labels", "[", "l", "]", "for", "l", "in", "labelNames", "]", "if", "options", ".", "show_samples", ":", "dot", ".", "show_function_events", ".", "append", "(", "SAMPLES", ")", "profile", "=", "profile", "profile", ".", "prune", "(", "options", ".", "node_thres", "/", "100.0", ",", "options", ".", "edge_thres", "/", "100.0", ",", "options", ".", "filter_paths", ",", "options", ".", "color_nodes_by_selftime", ")", "if", "options", ".", "root", ":", "rootIds", "=", "profile", ".", "getFunctionIds", "(", "options", ".", "root", ")", "if", "not", "rootIds", ":", "sys", ".", "stderr", ".", "write", "(", "'root node '", "+", "options", ".", "root", "+", "' not found (might already be pruned : try -e0 -n0 flags)\\n'", ")", "sys", ".", "exit", "(", "1", ")", "profile", ".", "prune_root", "(", "rootIds", ",", "options", ".", "depth", ")", "if", "options", ".", "leaf", ":", "leafIds", "=", "profile", ".", "getFunctionIds", "(", "options", ".", "leaf", ")", "if", "not", "leafIds", ":", "sys", ".", "stderr", ".", "write", "(", "'leaf node '", "+", "options", ".", "leaf", "+", "' not found (maybe already pruned : try -e0 -n0 flags)\\n'", ")", "sys", ".", "exit", "(", "1", ")", "profile", ".", "prune_leaf", "(", "leafIds", ",", "options", ".", "depth", ")", "dot", ".", "graph", "(", "profile", ",", "theme", ")"], "docstring": "Main program.", "docstring_tokens": ["Main", "program", "."], "sha": "0500e89f001e555f5eaa32e70793b4875f2f70db", "url": "https://github.com/jrfonseca/gprof2dot/blob/0500e89f001e555f5eaa32e70793b4875f2f70db/gprof2dot.py#L3173-L3350", "partition": "train"}
{"repo": "google/openhtf", "path": "openhtf/util/console_output.py", "func_name": "action_result_context", "original_string": "def action_result_context(action_text,\n                          width=60,\n                          status_width=8,\n                          succeed_text='OK',\n                          fail_text='FAIL',\n                          unknown_text='????',\n                          file=sys.stdout,\n                          logger=_LOG):\n  \"\"\"A contextmanager that prints actions and results to the CLI.\n\n  When entering the context, the action will be printed, and when the context\n  is exited, the result will be printed. The object yielded by the context is\n  used to mark the action as a success or failure, and a raise from inside the\n  context will also result in the action being marked fail. If the result is\n  left unset, then indicative text (\"????\") will be printed as the result.\n\n  Args:\n    action_text: Text to be displayed that describes the action being taken.\n    width: Total width for each line of output.\n    status_width: Width of the just the status message portion of each line.\n    succeed_text: Status message displayed when the action succeeds.\n    fail_text: Status message displayed when the action fails.\n    unknown_text: Status message displayed when the result is left unset.\n    file: Specific file object to write to write CLI output to.\n    logger: A logger to use, or None to disable logging.\n\n  Example usage:\n    with action_result_context('Doing an action that will succeed...') as act:\n      time.sleep(2)\n      act.succeed()\n\n    with action_result_context('Doing an action with unset result...') as act:\n      time.sleep(2)\n\n    with action_result_context('Doing an action that will fail...') as act:\n      time.sleep(2)\n      act.fail()\n\n    with action_result_context('Doing an action that will raise...') as act:\n      time.sleep(2)\n      import textwrap\n      raise RuntimeError(textwrap.dedent('''\\\n          Uh oh, looks like there was a raise in the mix.\n\n          If you see this message, it means you are running the console_output\n          module directly rather than using it as a library. Things to try:\n\n            * Not running it as a module.\n            * Running it as a module and enjoying the preview text.\n            * Getting another coffee.'''))\n\n  Example output:\n    Doing an action that will succeed...                [  OK  ]\n    Doing an action with unset result...                [ ???? ]\n    Doing an action that will fail...                   [ FAIL ]\n    Doing an action that will raise...                  [ FAIL ]\n    ...\n  \"\"\"\n  if logger:\n    logger.debug('Action - %s', action_text)\n  if not CLI_QUIET:\n    file.write(''.join((action_text, '\\r')))\n    file.flush()\n    spacing = (width - status_width - _printed_len(action_text)) * ' '\n\n  result = ActionResult()\n  try:\n    yield result\n  except Exception as err:\n    if logger:\n      logger.debug('Result - %s [ %s ]', action_text, fail_text)\n    if not CLI_QUIET:\n      file.write(''.join((action_text, spacing)))\n      bracket_print(fail_text, width=status_width, color=colorama.Fore.RED,\n                    file=file)\n    if not isinstance(err, ActionFailedError):\n      raise\n    return\n\n  result_text = succeed_text if result.success else unknown_text\n  result_color = colorama.Fore.GREEN if result.success else colorama.Fore.YELLOW\n  if logger:\n    logger.debug('Result - %s [ %s ]', action_text, result_text)\n  if not CLI_QUIET:\n    file.write(''.join((action_text, spacing)))\n    bracket_print(result_text, width=status_width, color=result_color,\n                  file=file)", "language": "python", "code": "def action_result_context(action_text,\n                          width=60,\n                          status_width=8,\n                          succeed_text='OK',\n                          fail_text='FAIL',\n                          unknown_text='????',\n                          file=sys.stdout,\n                          logger=_LOG):\n  \"\"\"A contextmanager that prints actions and results to the CLI.\n\n  When entering the context, the action will be printed, and when the context\n  is exited, the result will be printed. The object yielded by the context is\n  used to mark the action as a success or failure, and a raise from inside the\n  context will also result in the action being marked fail. If the result is\n  left unset, then indicative text (\"????\") will be printed as the result.\n\n  Args:\n    action_text: Text to be displayed that describes the action being taken.\n    width: Total width for each line of output.\n    status_width: Width of the just the status message portion of each line.\n    succeed_text: Status message displayed when the action succeeds.\n    fail_text: Status message displayed when the action fails.\n    unknown_text: Status message displayed when the result is left unset.\n    file: Specific file object to write to write CLI output to.\n    logger: A logger to use, or None to disable logging.\n\n  Example usage:\n    with action_result_context('Doing an action that will succeed...') as act:\n      time.sleep(2)\n      act.succeed()\n\n    with action_result_context('Doing an action with unset result...') as act:\n      time.sleep(2)\n\n    with action_result_context('Doing an action that will fail...') as act:\n      time.sleep(2)\n      act.fail()\n\n    with action_result_context('Doing an action that will raise...') as act:\n      time.sleep(2)\n      import textwrap\n      raise RuntimeError(textwrap.dedent('''\\\n          Uh oh, looks like there was a raise in the mix.\n\n          If you see this message, it means you are running the console_output\n          module directly rather than using it as a library. Things to try:\n\n            * Not running it as a module.\n            * Running it as a module and enjoying the preview text.\n            * Getting another coffee.'''))\n\n  Example output:\n    Doing an action that will succeed...                [  OK  ]\n    Doing an action with unset result...                [ ???? ]\n    Doing an action that will fail...                   [ FAIL ]\n    Doing an action that will raise...                  [ FAIL ]\n    ...\n  \"\"\"\n  if logger:\n    logger.debug('Action - %s', action_text)\n  if not CLI_QUIET:\n    file.write(''.join((action_text, '\\r')))\n    file.flush()\n    spacing = (width - status_width - _printed_len(action_text)) * ' '\n\n  result = ActionResult()\n  try:\n    yield result\n  except Exception as err:\n    if logger:\n      logger.debug('Result - %s [ %s ]', action_text, fail_text)\n    if not CLI_QUIET:\n      file.write(''.join((action_text, spacing)))\n      bracket_print(fail_text, width=status_width, color=colorama.Fore.RED,\n                    file=file)\n    if not isinstance(err, ActionFailedError):\n      raise\n    return\n\n  result_text = succeed_text if result.success else unknown_text\n  result_color = colorama.Fore.GREEN if result.success else colorama.Fore.YELLOW\n  if logger:\n    logger.debug('Result - %s [ %s ]', action_text, result_text)\n  if not CLI_QUIET:\n    file.write(''.join((action_text, spacing)))\n    bracket_print(result_text, width=status_width, color=result_color,\n                  file=file)", "code_tokens": ["def", "action_result_context", "(", "action_text", ",", "width", "=", "60", ",", "status_width", "=", "8", ",", "succeed_text", "=", "'OK'", ",", "fail_text", "=", "'FAIL'", ",", "unknown_text", "=", "'????'", ",", "file", "=", "sys", ".", "stdout", ",", "logger", "=", "_LOG", ")", ":", "if", "logger", ":", "logger", ".", "debug", "(", "'Action - %s'", ",", "action_text", ")", "if", "not", "CLI_QUIET", ":", "file", ".", "write", "(", "''", ".", "join", "(", "(", "action_text", ",", "'\\r'", ")", ")", ")", "file", ".", "flush", "(", ")", "spacing", "=", "(", "width", "-", "status_width", "-", "_printed_len", "(", "action_text", ")", ")", "*", "' '", "result", "=", "ActionResult", "(", ")", "try", ":", "yield", "result", "except", "Exception", "as", "err", ":", "if", "logger", ":", "logger", ".", "debug", "(", "'Result - %s [ %s ]'", ",", "action_text", ",", "fail_text", ")", "if", "not", "CLI_QUIET", ":", "file", ".", "write", "(", "''", ".", "join", "(", "(", "action_text", ",", "spacing", ")", ")", ")", "bracket_print", "(", "fail_text", ",", "width", "=", "status_width", ",", "color", "=", "colorama", ".", "Fore", ".", "RED", ",", "file", "=", "file", ")", "if", "not", "isinstance", "(", "err", ",", "ActionFailedError", ")", ":", "raise", "return", "result_text", "=", "succeed_text", "if", "result", ".", "success", "else", "unknown_text", "result_color", "=", "colorama", ".", "Fore", ".", "GREEN", "if", "result", ".", "success", "else", "colorama", ".", "Fore", ".", "YELLOW", "if", "logger", ":", "logger", ".", "debug", "(", "'Result - %s [ %s ]'", ",", "action_text", ",", "result_text", ")", "if", "not", "CLI_QUIET", ":", "file", ".", "write", "(", "''", ".", "join", "(", "(", "action_text", ",", "spacing", ")", ")", ")", "bracket_print", "(", "result_text", ",", "width", "=", "status_width", ",", "color", "=", "result_color", ",", "file", "=", "file", ")"], "docstring": "A contextmanager that prints actions and results to the CLI.\n\n  When entering the context, the action will be printed, and when the context\n  is exited, the result will be printed. The object yielded by the context is\n  used to mark the action as a success or failure, and a raise from inside the\n  context will also result in the action being marked fail. If the result is\n  left unset, then indicative text (\"????\") will be printed as the result.\n\n  Args:\n    action_text: Text to be displayed that describes the action being taken.\n    width: Total width for each line of output.\n    status_width: Width of the just the status message portion of each line.\n    succeed_text: Status message displayed when the action succeeds.\n    fail_text: Status message displayed when the action fails.\n    unknown_text: Status message displayed when the result is left unset.\n    file: Specific file object to write to write CLI output to.\n    logger: A logger to use, or None to disable logging.\n\n  Example usage:\n    with action_result_context('Doing an action that will succeed...') as act:\n      time.sleep(2)\n      act.succeed()\n\n    with action_result_context('Doing an action with unset result...') as act:\n      time.sleep(2)\n\n    with action_result_context('Doing an action that will fail...') as act:\n      time.sleep(2)\n      act.fail()\n\n    with action_result_context('Doing an action that will raise...') as act:\n      time.sleep(2)\n      import textwrap\n      raise RuntimeError(textwrap.dedent('''\\\n          Uh oh, looks like there was a raise in the mix.\n\n          If you see this message, it means you are running the console_output\n          module directly rather than using it as a library. Things to try:\n\n            * Not running it as a module.\n            * Running it as a module and enjoying the preview text.\n            * Getting another coffee.'''))\n\n  Example output:\n    Doing an action that will succeed...                [  OK  ]\n    Doing an action with unset result...                [ ???? ]\n    Doing an action that will fail...                   [ FAIL ]\n    Doing an action that will raise...                  [ FAIL ]\n    ...", "docstring_tokens": ["A", "contextmanager", "that", "prints", "actions", "and", "results", "to", "the", "CLI", "."], "sha": "655e85df7134db7bdf8f8fdd6ff9a6bf932e7b09", "url": "https://github.com/google/openhtf/blob/655e85df7134db7bdf8f8fdd6ff9a6bf932e7b09/openhtf/util/console_output.py#L204-L290", "partition": "train"}
{"repo": "google/openhtf", "path": "openhtf/util/exceptions.py", "func_name": "reraise", "original_string": "def reraise(exc_type, message=None, *args, **kwargs):  # pylint: disable=invalid-name\n  \"\"\"reraises an exception for exception translation.\n\n  This is primarily used for when you immediately reraise an exception that is\n  thrown in a library, so that your client will not have to depend on various\n  exceptions defined in the library implementation that is being abstracted. The\n  advantage of this helper function is somewhat preserve traceback information\n  although it is polluted by the reraise frame.\n\n  Example Code:\n    def A():\n      raise Exception('Whoops')\n    def main():\n      try:\n        A()\n      except Exception as e:\n        exceptions.reraise(ValueError)\n    main()\n\n  Traceback (most recent call last):\n    File \"exception.py\", line 53, in <module>\n      main()\n    File \"exception.py\", line 49, in main\n      reraise(ValueError)\n    File \"exception.py\", line 47, in main\n      A()\n    File \"exception.py\", line 42, in A\n      raise Exception('Whoops')\n  ValueError: line 49\n\n  When this code is run, the additional stack frames for calling A() and raising\n  within A() are printed out in exception, whereas a bare exception translation\n  would lose this information. As long as you ignore the reraise stack frame,\n  the stack trace is okay looking.\n\n  Generally this can be fixed by hacking on CPython to allow modification of\n  traceback objects ala\n  https://github.com/mitsuhiko/jinja2/blob/master/jinja2/debug.py, but this is\n  fixed in Python 3 anyways and that method is the definition of hackery.\n\n  Args:\n    exc_type: (Exception) Exception class to create.\n    message: (str) Optional message to place in exception instance. Usually not\n      needed as the original exception probably has a message that will be\n      printed out in the modified stacktrace.\n    *args: Args to pass to exception constructor.\n    **kwargs: Kwargs to pass to exception constructor.\n  \"\"\"\n  last_lineno = inspect.currentframe().f_back.f_lineno\n  line_msg = 'line %s: ' % last_lineno\n  if message:\n    line_msg += str(message)\n  raise exc_type(line_msg, *args, **kwargs).raise_with_traceback(sys.exc_info()[2])", "language": "python", "code": "def reraise(exc_type, message=None, *args, **kwargs):  # pylint: disable=invalid-name\n  \"\"\"reraises an exception for exception translation.\n\n  This is primarily used for when you immediately reraise an exception that is\n  thrown in a library, so that your client will not have to depend on various\n  exceptions defined in the library implementation that is being abstracted. The\n  advantage of this helper function is somewhat preserve traceback information\n  although it is polluted by the reraise frame.\n\n  Example Code:\n    def A():\n      raise Exception('Whoops')\n    def main():\n      try:\n        A()\n      except Exception as e:\n        exceptions.reraise(ValueError)\n    main()\n\n  Traceback (most recent call last):\n    File \"exception.py\", line 53, in <module>\n      main()\n    File \"exception.py\", line 49, in main\n      reraise(ValueError)\n    File \"exception.py\", line 47, in main\n      A()\n    File \"exception.py\", line 42, in A\n      raise Exception('Whoops')\n  ValueError: line 49\n\n  When this code is run, the additional stack frames for calling A() and raising\n  within A() are printed out in exception, whereas a bare exception translation\n  would lose this information. As long as you ignore the reraise stack frame,\n  the stack trace is okay looking.\n\n  Generally this can be fixed by hacking on CPython to allow modification of\n  traceback objects ala\n  https://github.com/mitsuhiko/jinja2/blob/master/jinja2/debug.py, but this is\n  fixed in Python 3 anyways and that method is the definition of hackery.\n\n  Args:\n    exc_type: (Exception) Exception class to create.\n    message: (str) Optional message to place in exception instance. Usually not\n      needed as the original exception probably has a message that will be\n      printed out in the modified stacktrace.\n    *args: Args to pass to exception constructor.\n    **kwargs: Kwargs to pass to exception constructor.\n  \"\"\"\n  last_lineno = inspect.currentframe().f_back.f_lineno\n  line_msg = 'line %s: ' % last_lineno\n  if message:\n    line_msg += str(message)\n  raise exc_type(line_msg, *args, **kwargs).raise_with_traceback(sys.exc_info()[2])", "code_tokens": ["def", "reraise", "(", "exc_type", ",", "message", "=", "None", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "# pylint: disable=invalid-name", "last_lineno", "=", "inspect", ".", "currentframe", "(", ")", ".", "f_back", ".", "f_lineno", "line_msg", "=", "'line %s: '", "%", "last_lineno", "if", "message", ":", "line_msg", "+=", "str", "(", "message", ")", "raise", "exc_type", "(", "line_msg", ",", "*", "args", ",", "*", "*", "kwargs", ")", ".", "raise_with_traceback", "(", "sys", ".", "exc_info", "(", ")", "[", "2", "]", ")"], "docstring": "reraises an exception for exception translation.\n\n  This is primarily used for when you immediately reraise an exception that is\n  thrown in a library, so that your client will not have to depend on various\n  exceptions defined in the library implementation that is being abstracted. The\n  advantage of this helper function is somewhat preserve traceback information\n  although it is polluted by the reraise frame.\n\n  Example Code:\n    def A():\n      raise Exception('Whoops')\n    def main():\n      try:\n        A()\n      except Exception as e:\n        exceptions.reraise(ValueError)\n    main()\n\n  Traceback (most recent call last):\n    File \"exception.py\", line 53, in <module>\n      main()\n    File \"exception.py\", line 49, in main\n      reraise(ValueError)\n    File \"exception.py\", line 47, in main\n      A()\n    File \"exception.py\", line 42, in A\n      raise Exception('Whoops')\n  ValueError: line 49\n\n  When this code is run, the additional stack frames for calling A() and raising\n  within A() are printed out in exception, whereas a bare exception translation\n  would lose this information. As long as you ignore the reraise stack frame,\n  the stack trace is okay looking.\n\n  Generally this can be fixed by hacking on CPython to allow modification of\n  traceback objects ala\n  https://github.com/mitsuhiko/jinja2/blob/master/jinja2/debug.py, but this is\n  fixed in Python 3 anyways and that method is the definition of hackery.\n\n  Args:\n    exc_type: (Exception) Exception class to create.\n    message: (str) Optional message to place in exception instance. Usually not\n      needed as the original exception probably has a message that will be\n      printed out in the modified stacktrace.\n    *args: Args to pass to exception constructor.\n    **kwargs: Kwargs to pass to exception constructor.", "docstring_tokens": ["reraises", "an", "exception", "for", "exception", "translation", "."], "sha": "655e85df7134db7bdf8f8fdd6ff9a6bf932e7b09", "url": "https://github.com/google/openhtf/blob/655e85df7134db7bdf8f8fdd6ff9a6bf932e7b09/openhtf/util/exceptions.py#L22-L74", "partition": "train"}
{"repo": "rflamary/POT", "path": "ot/plot.py", "func_name": "plot1D_mat", "original_string": "def plot1D_mat(a, b, M, title=''):\n    \"\"\" Plot matrix M  with the source and target 1D distribution\n\n    Creates a subplot with the source distribution a on the left and\n    target distribution b on the tot. The matrix M is shown in between.\n\n\n    Parameters\n    ----------\n    a : np.array, shape (na,)\n        Source distribution\n    b : np.array, shape (nb,)\n        Target distribution\n    M : np.array, shape (na,nb)\n        Matrix to plot\n    \"\"\"\n    na, nb = M.shape\n\n    gs = gridspec.GridSpec(3, 3)\n\n    xa = np.arange(na)\n    xb = np.arange(nb)\n\n    ax1 = pl.subplot(gs[0, 1:])\n    pl.plot(xb, b, 'r', label='Target distribution')\n    pl.yticks(())\n    pl.title(title)\n\n    ax2 = pl.subplot(gs[1:, 0])\n    pl.plot(a, xa, 'b', label='Source distribution')\n    pl.gca().invert_xaxis()\n    pl.gca().invert_yaxis()\n    pl.xticks(())\n\n    pl.subplot(gs[1:, 1:], sharex=ax1, sharey=ax2)\n    pl.imshow(M, interpolation='nearest')\n    pl.axis('off')\n\n    pl.xlim((0, nb))\n    pl.tight_layout()\n    pl.subplots_adjust(wspace=0., hspace=0.2)", "language": "python", "code": "def plot1D_mat(a, b, M, title=''):\n    \"\"\" Plot matrix M  with the source and target 1D distribution\n\n    Creates a subplot with the source distribution a on the left and\n    target distribution b on the tot. The matrix M is shown in between.\n\n\n    Parameters\n    ----------\n    a : np.array, shape (na,)\n        Source distribution\n    b : np.array, shape (nb,)\n        Target distribution\n    M : np.array, shape (na,nb)\n        Matrix to plot\n    \"\"\"\n    na, nb = M.shape\n\n    gs = gridspec.GridSpec(3, 3)\n\n    xa = np.arange(na)\n    xb = np.arange(nb)\n\n    ax1 = pl.subplot(gs[0, 1:])\n    pl.plot(xb, b, 'r', label='Target distribution')\n    pl.yticks(())\n    pl.title(title)\n\n    ax2 = pl.subplot(gs[1:, 0])\n    pl.plot(a, xa, 'b', label='Source distribution')\n    pl.gca().invert_xaxis()\n    pl.gca().invert_yaxis()\n    pl.xticks(())\n\n    pl.subplot(gs[1:, 1:], sharex=ax1, sharey=ax2)\n    pl.imshow(M, interpolation='nearest')\n    pl.axis('off')\n\n    pl.xlim((0, nb))\n    pl.tight_layout()\n    pl.subplots_adjust(wspace=0., hspace=0.2)", "code_tokens": ["def", "plot1D_mat", "(", "a", ",", "b", ",", "M", ",", "title", "=", "''", ")", ":", "na", ",", "nb", "=", "M", ".", "shape", "gs", "=", "gridspec", ".", "GridSpec", "(", "3", ",", "3", ")", "xa", "=", "np", ".", "arange", "(", "na", ")", "xb", "=", "np", ".", "arange", "(", "nb", ")", "ax1", "=", "pl", ".", "subplot", "(", "gs", "[", "0", ",", "1", ":", "]", ")", "pl", ".", "plot", "(", "xb", ",", "b", ",", "'r'", ",", "label", "=", "'Target distribution'", ")", "pl", ".", "yticks", "(", "(", ")", ")", "pl", ".", "title", "(", "title", ")", "ax2", "=", "pl", ".", "subplot", "(", "gs", "[", "1", ":", ",", "0", "]", ")", "pl", ".", "plot", "(", "a", ",", "xa", ",", "'b'", ",", "label", "=", "'Source distribution'", ")", "pl", ".", "gca", "(", ")", ".", "invert_xaxis", "(", ")", "pl", ".", "gca", "(", ")", ".", "invert_yaxis", "(", ")", "pl", ".", "xticks", "(", "(", ")", ")", "pl", ".", "subplot", "(", "gs", "[", "1", ":", ",", "1", ":", "]", ",", "sharex", "=", "ax1", ",", "sharey", "=", "ax2", ")", "pl", ".", "imshow", "(", "M", ",", "interpolation", "=", "'nearest'", ")", "pl", ".", "axis", "(", "'off'", ")", "pl", ".", "xlim", "(", "(", "0", ",", "nb", ")", ")", "pl", ".", "tight_layout", "(", ")", "pl", ".", "subplots_adjust", "(", "wspace", "=", "0.", ",", "hspace", "=", "0.2", ")"], "docstring": "Plot matrix M  with the source and target 1D distribution\n\n    Creates a subplot with the source distribution a on the left and\n    target distribution b on the tot. The matrix M is shown in between.\n\n\n    Parameters\n    ----------\n    a : np.array, shape (na,)\n        Source distribution\n    b : np.array, shape (nb,)\n        Target distribution\n    M : np.array, shape (na,nb)\n        Matrix to plot", "docstring_tokens": ["Plot", "matrix", "M", "with", "the", "source", "and", "target", "1D", "distribution"], "sha": "c5108efc7b6702e1af3928bef1032e6b37734d1c", "url": "https://github.com/rflamary/POT/blob/c5108efc7b6702e1af3928bef1032e6b37734d1c/ot/plot.py#L14-L54", "partition": "train"}
{"repo": "rflamary/POT", "path": "ot/plot.py", "func_name": "plot2D_samples_mat", "original_string": "def plot2D_samples_mat(xs, xt, G, thr=1e-8, **kwargs):\n    \"\"\" Plot matrix M  in 2D with  lines using alpha values\n\n    Plot lines between source and target 2D samples with a color\n    proportional to the value of the matrix G between samples.\n\n\n    Parameters\n    ----------\n    xs : ndarray, shape (ns,2)\n        Source samples positions\n    b : ndarray, shape (nt,2)\n        Target samples positions\n    G : ndarray, shape (na,nb)\n        OT matrix\n    thr : float, optional\n        threshold above which the line is drawn\n    **kwargs : dict\n        paameters given to the plot functions (default color is black if\n        nothing given)\n    \"\"\"\n    if ('color' not in kwargs) and ('c' not in kwargs):\n        kwargs['color'] = 'k'\n    mx = G.max()\n    for i in range(xs.shape[0]):\n        for j in range(xt.shape[0]):\n            if G[i, j] / mx > thr:\n                pl.plot([xs[i, 0], xt[j, 0]], [xs[i, 1], xt[j, 1]],\n                        alpha=G[i, j] / mx, **kwargs)", "language": "python", "code": "def plot2D_samples_mat(xs, xt, G, thr=1e-8, **kwargs):\n    \"\"\" Plot matrix M  in 2D with  lines using alpha values\n\n    Plot lines between source and target 2D samples with a color\n    proportional to the value of the matrix G between samples.\n\n\n    Parameters\n    ----------\n    xs : ndarray, shape (ns,2)\n        Source samples positions\n    b : ndarray, shape (nt,2)\n        Target samples positions\n    G : ndarray, shape (na,nb)\n        OT matrix\n    thr : float, optional\n        threshold above which the line is drawn\n    **kwargs : dict\n        paameters given to the plot functions (default color is black if\n        nothing given)\n    \"\"\"\n    if ('color' not in kwargs) and ('c' not in kwargs):\n        kwargs['color'] = 'k'\n    mx = G.max()\n    for i in range(xs.shape[0]):\n        for j in range(xt.shape[0]):\n            if G[i, j] / mx > thr:\n                pl.plot([xs[i, 0], xt[j, 0]], [xs[i, 1], xt[j, 1]],\n                        alpha=G[i, j] / mx, **kwargs)", "code_tokens": ["def", "plot2D_samples_mat", "(", "xs", ",", "xt", ",", "G", ",", "thr", "=", "1e-8", ",", "*", "*", "kwargs", ")", ":", "if", "(", "'color'", "not", "in", "kwargs", ")", "and", "(", "'c'", "not", "in", "kwargs", ")", ":", "kwargs", "[", "'color'", "]", "=", "'k'", "mx", "=", "G", ".", "max", "(", ")", "for", "i", "in", "range", "(", "xs", ".", "shape", "[", "0", "]", ")", ":", "for", "j", "in", "range", "(", "xt", ".", "shape", "[", "0", "]", ")", ":", "if", "G", "[", "i", ",", "j", "]", "/", "mx", ">", "thr", ":", "pl", ".", "plot", "(", "[", "xs", "[", "i", ",", "0", "]", ",", "xt", "[", "j", ",", "0", "]", "]", ",", "[", "xs", "[", "i", ",", "1", "]", ",", "xt", "[", "j", ",", "1", "]", "]", ",", "alpha", "=", "G", "[", "i", ",", "j", "]", "/", "mx", ",", "*", "*", "kwargs", ")"], "docstring": "Plot matrix M  in 2D with  lines using alpha values\n\n    Plot lines between source and target 2D samples with a color\n    proportional to the value of the matrix G between samples.\n\n\n    Parameters\n    ----------\n    xs : ndarray, shape (ns,2)\n        Source samples positions\n    b : ndarray, shape (nt,2)\n        Target samples positions\n    G : ndarray, shape (na,nb)\n        OT matrix\n    thr : float, optional\n        threshold above which the line is drawn\n    **kwargs : dict\n        paameters given to the plot functions (default color is black if\n        nothing given)", "docstring_tokens": ["Plot", "matrix", "M", "in", "2D", "with", "lines", "using", "alpha", "values"], "sha": "c5108efc7b6702e1af3928bef1032e6b37734d1c", "url": "https://github.com/rflamary/POT/blob/c5108efc7b6702e1af3928bef1032e6b37734d1c/ot/plot.py#L57-L85", "partition": "train"}
{"repo": "stitchfix/pyxley", "path": "pyxley/charts/datamaps/datamaps.py", "func_name": "DatamapUSA.to_json", "original_string": "def to_json(df, state_index, color_index, fills):\n        \"\"\"Transforms dataframe to json response\"\"\"\n        records = {}\n        for i, row in df.iterrows():\n\n            records[row[state_index]] = {\n                \"fillKey\": row[color_index]\n            }\n\n        return {\n            \"data\": records,\n            \"fills\": fills\n        }", "language": "python", "code": "def to_json(df, state_index, color_index, fills):\n        \"\"\"Transforms dataframe to json response\"\"\"\n        records = {}\n        for i, row in df.iterrows():\n\n            records[row[state_index]] = {\n                \"fillKey\": row[color_index]\n            }\n\n        return {\n            \"data\": records,\n            \"fills\": fills\n        }", "code_tokens": ["def", "to_json", "(", "df", ",", "state_index", ",", "color_index", ",", "fills", ")", ":", "records", "=", "{", "}", "for", "i", ",", "row", "in", "df", ".", "iterrows", "(", ")", ":", "records", "[", "row", "[", "state_index", "]", "]", "=", "{", "\"fillKey\"", ":", "row", "[", "color_index", "]", "}", "return", "{", "\"data\"", ":", "records", ",", "\"fills\"", ":", "fills", "}"], "docstring": "Transforms dataframe to json response", "docstring_tokens": ["Transforms", "dataframe", "to", "json", "response"], "sha": "2dab00022d977d986169cd8a629b3a2f91be893f", "url": "https://github.com/stitchfix/pyxley/blob/2dab00022d977d986169cd8a629b3a2f91be893f/pyxley/charts/datamaps/datamaps.py#L123-L135", "partition": "train"}
{"repo": "aws/aws-iot-device-sdk-python", "path": "AWSIoTPythonSDK/core/shadow/deviceShadow.py", "func_name": "deviceShadow.shadowGet", "original_string": "def shadowGet(self, srcCallback, srcTimeout):\n        \"\"\"\n        **Description**\n\n        Retrieve the device shadow JSON document from AWS IoT by publishing an empty JSON document to the \n        corresponding shadow topics. Shadow response topics will be subscribed to receive responses from \n        AWS IoT regarding the result of the get operation. Retrieved shadow JSON document will be available \n        in the registered callback. If no response is received within the provided timeout, a timeout \n        notification will be passed into the registered callback.\n\n        **Syntax**\n\n        .. code:: python\n\n          # Retrieve the shadow JSON document from AWS IoT, with a timeout set to 5 seconds\n          BotShadow.shadowGet(customCallback, 5)\n\n        **Parameters**\n\n        *srcCallback* - Function to be called when the response for this shadow request comes back. Should \n        be in form :code:`customCallback(payload, responseStatus, token)`, where :code:`payload` is the \n        JSON document returned, :code:`responseStatus` indicates whether the request has been accepted, \n        rejected or is a delta message, :code:`token` is the token used for tracing in this request.\n\n        *srcTimeout* - Timeout to determine whether the request is invalid. When a request gets timeout, \n        a timeout notification will be generated and put into the registered callback to notify users.\n\n        **Returns**\n\n        The token used for tracing in this shadow request.\n\n        \"\"\"\n        with self._dataStructureLock:\n            # Update callback data structure\n            self._shadowSubscribeCallbackTable[\"get\"] = srcCallback\n            # Update number of pending feedback\n            self._shadowSubscribeStatusTable[\"get\"] += 1\n            # clientToken\n            currentToken = self._tokenHandler.getNextToken()\n            self._tokenPool[currentToken] = Timer(srcTimeout, self._timerHandler, [\"get\", currentToken])\n            self._basicJSONParserHandler.setString(\"{}\")\n            self._basicJSONParserHandler.validateJSON()\n            self._basicJSONParserHandler.setAttributeValue(\"clientToken\", currentToken)\n            currentPayload = self._basicJSONParserHandler.regenerateString()\n        # Two subscriptions\n        if not self._isPersistentSubscribe or not self._isGetSubscribed:\n            self._shadowManagerHandler.basicShadowSubscribe(self._shadowName, \"get\", self.generalCallback)\n            self._isGetSubscribed = True\n            self._logger.info(\"Subscribed to get accepted/rejected topics for deviceShadow: \" + self._shadowName)\n        # One publish\n        self._shadowManagerHandler.basicShadowPublish(self._shadowName, \"get\", currentPayload)\n        # Start the timer\n        self._tokenPool[currentToken].start()\n        return currentToken", "language": "python", "code": "def shadowGet(self, srcCallback, srcTimeout):\n        \"\"\"\n        **Description**\n\n        Retrieve the device shadow JSON document from AWS IoT by publishing an empty JSON document to the \n        corresponding shadow topics. Shadow response topics will be subscribed to receive responses from \n        AWS IoT regarding the result of the get operation. Retrieved shadow JSON document will be available \n        in the registered callback. If no response is received within the provided timeout, a timeout \n        notification will be passed into the registered callback.\n\n        **Syntax**\n\n        .. code:: python\n\n          # Retrieve the shadow JSON document from AWS IoT, with a timeout set to 5 seconds\n          BotShadow.shadowGet(customCallback, 5)\n\n        **Parameters**\n\n        *srcCallback* - Function to be called when the response for this shadow request comes back. Should \n        be in form :code:`customCallback(payload, responseStatus, token)`, where :code:`payload` is the \n        JSON document returned, :code:`responseStatus` indicates whether the request has been accepted, \n        rejected or is a delta message, :code:`token` is the token used for tracing in this request.\n\n        *srcTimeout* - Timeout to determine whether the request is invalid. When a request gets timeout, \n        a timeout notification will be generated and put into the registered callback to notify users.\n\n        **Returns**\n\n        The token used for tracing in this shadow request.\n\n        \"\"\"\n        with self._dataStructureLock:\n            # Update callback data structure\n            self._shadowSubscribeCallbackTable[\"get\"] = srcCallback\n            # Update number of pending feedback\n            self._shadowSubscribeStatusTable[\"get\"] += 1\n            # clientToken\n            currentToken = self._tokenHandler.getNextToken()\n            self._tokenPool[currentToken] = Timer(srcTimeout, self._timerHandler, [\"get\", currentToken])\n            self._basicJSONParserHandler.setString(\"{}\")\n            self._basicJSONParserHandler.validateJSON()\n            self._basicJSONParserHandler.setAttributeValue(\"clientToken\", currentToken)\n            currentPayload = self._basicJSONParserHandler.regenerateString()\n        # Two subscriptions\n        if not self._isPersistentSubscribe or not self._isGetSubscribed:\n            self._shadowManagerHandler.basicShadowSubscribe(self._shadowName, \"get\", self.generalCallback)\n            self._isGetSubscribed = True\n            self._logger.info(\"Subscribed to get accepted/rejected topics for deviceShadow: \" + self._shadowName)\n        # One publish\n        self._shadowManagerHandler.basicShadowPublish(self._shadowName, \"get\", currentPayload)\n        # Start the timer\n        self._tokenPool[currentToken].start()\n        return currentToken", "code_tokens": ["def", "shadowGet", "(", "self", ",", "srcCallback", ",", "srcTimeout", ")", ":", "with", "self", ".", "_dataStructureLock", ":", "# Update callback data structure", "self", ".", "_shadowSubscribeCallbackTable", "[", "\"get\"", "]", "=", "srcCallback", "# Update number of pending feedback", "self", ".", "_shadowSubscribeStatusTable", "[", "\"get\"", "]", "+=", "1", "# clientToken", "currentToken", "=", "self", ".", "_tokenHandler", ".", "getNextToken", "(", ")", "self", ".", "_tokenPool", "[", "currentToken", "]", "=", "Timer", "(", "srcTimeout", ",", "self", ".", "_timerHandler", ",", "[", "\"get\"", ",", "currentToken", "]", ")", "self", ".", "_basicJSONParserHandler", ".", "setString", "(", "\"{}\"", ")", "self", ".", "_basicJSONParserHandler", ".", "validateJSON", "(", ")", "self", ".", "_basicJSONParserHandler", ".", "setAttributeValue", "(", "\"clientToken\"", ",", "currentToken", ")", "currentPayload", "=", "self", ".", "_basicJSONParserHandler", ".", "regenerateString", "(", ")", "# Two subscriptions", "if", "not", "self", ".", "_isPersistentSubscribe", "or", "not", "self", ".", "_isGetSubscribed", ":", "self", ".", "_shadowManagerHandler", ".", "basicShadowSubscribe", "(", "self", ".", "_shadowName", ",", "\"get\"", ",", "self", ".", "generalCallback", ")", "self", ".", "_isGetSubscribed", "=", "True", "self", ".", "_logger", ".", "info", "(", "\"Subscribed to get accepted/rejected topics for deviceShadow: \"", "+", "self", ".", "_shadowName", ")", "# One publish", "self", ".", "_shadowManagerHandler", ".", "basicShadowPublish", "(", "self", ".", "_shadowName", ",", "\"get\"", ",", "currentPayload", ")", "# Start the timer", "self", ".", "_tokenPool", "[", "currentToken", "]", ".", "start", "(", ")", "return", "currentToken"], "docstring": "**Description**\n\n        Retrieve the device shadow JSON document from AWS IoT by publishing an empty JSON document to the \n        corresponding shadow topics. Shadow response topics will be subscribed to receive responses from \n        AWS IoT regarding the result of the get operation. Retrieved shadow JSON document will be available \n        in the registered callback. If no response is received within the provided timeout, a timeout \n        notification will be passed into the registered callback.\n\n        **Syntax**\n\n        .. code:: python\n\n          # Retrieve the shadow JSON document from AWS IoT, with a timeout set to 5 seconds\n          BotShadow.shadowGet(customCallback, 5)\n\n        **Parameters**\n\n        *srcCallback* - Function to be called when the response for this shadow request comes back. Should \n        be in form :code:`customCallback(payload, responseStatus, token)`, where :code:`payload` is the \n        JSON document returned, :code:`responseStatus` indicates whether the request has been accepted, \n        rejected or is a delta message, :code:`token` is the token used for tracing in this request.\n\n        *srcTimeout* - Timeout to determine whether the request is invalid. When a request gets timeout, \n        a timeout notification will be generated and put into the registered callback to notify users.\n\n        **Returns**\n\n        The token used for tracing in this shadow request.", "docstring_tokens": ["**", "Description", "**"], "sha": "f0aa2ce34b21dd2e44f4fb7e1d058656aaf2fc62", "url": "https://github.com/aws/aws-iot-device-sdk-python/blob/f0aa2ce34b21dd2e44f4fb7e1d058656aaf2fc62/AWSIoTPythonSDK/core/shadow/deviceShadow.py#L197-L250", "partition": "train"}
{"repo": "onelogin/python-saml", "path": "src/onelogin/saml2/logout_request.py", "func_name": "OneLogin_Saml2_Logout_Request.is_valid", "original_string": "def is_valid(self, request_data, raise_exceptions=False):\n        \"\"\"\n        Checks if the Logout Request received is valid\n        :param request_data: Request Data\n        :type request_data: dict\n        :param raise_exceptions: Whether to return false on failure or raise an exception\n        :type raise_exceptions: Boolean\n        :return: If the Logout Request is or not valid\n        :rtype: boolean\n        \"\"\"\n        self.__error = None\n        lowercase_urlencoding = False\n        try:\n            dom = fromstring(self.__logout_request, forbid_dtd=True)\n\n            idp_data = self.__settings.get_idp_data()\n            idp_entity_id = idp_data['entityId']\n\n            if 'get_data' in request_data.keys():\n                get_data = request_data['get_data']\n            else:\n                get_data = {}\n\n            if 'lowercase_urlencoding' in request_data.keys():\n                lowercase_urlencoding = request_data['lowercase_urlencoding']\n\n            if self.__settings.is_strict():\n                res = OneLogin_Saml2_Utils.validate_xml(dom, 'saml-schema-protocol-2.0.xsd', self.__settings.is_debug_active())\n                if not isinstance(res, Document):\n                    raise OneLogin_Saml2_ValidationError(\n                        'Invalid SAML Logout Request. Not match the saml-schema-protocol-2.0.xsd',\n                        OneLogin_Saml2_ValidationError.INVALID_XML_FORMAT\n                    )\n\n                security = self.__settings.get_security_data()\n\n                current_url = OneLogin_Saml2_Utils.get_self_url_no_query(request_data)\n\n                # Check NotOnOrAfter\n                if dom.get('NotOnOrAfter', None):\n                    na = OneLogin_Saml2_Utils.parse_SAML_to_time(dom.get('NotOnOrAfter'))\n                    if na <= OneLogin_Saml2_Utils.now():\n                        raise OneLogin_Saml2_ValidationError(\n                            'Could not validate timestamp: expired. Check system clock.',\n                            OneLogin_Saml2_ValidationError.RESPONSE_EXPIRED\n                        )\n\n                # Check destination\n                if dom.get('Destination', None):\n                    destination = dom.get('Destination')\n                    if destination != '':\n                        if current_url not in destination:\n                            raise Exception(\n                                'The LogoutRequest was received at '\n                                '%(currentURL)s instead of %(destination)s' %\n                                {\n                                    'currentURL': current_url,\n                                    'destination': destination,\n                                },\n                                OneLogin_Saml2_ValidationError.WRONG_DESTINATION\n                            )\n\n                # Check issuer\n                issuer = OneLogin_Saml2_Logout_Request.get_issuer(dom)\n                if issuer is not None and issuer != idp_entity_id:\n                    raise OneLogin_Saml2_ValidationError(\n                        'Invalid issuer in the Logout Request (expected %(idpEntityId)s, got %(issuer)s)' %\n                        {\n                            'idpEntityId': idp_entity_id,\n                            'issuer': issuer\n                        },\n                        OneLogin_Saml2_ValidationError.WRONG_ISSUER\n                    )\n\n                if security['wantMessagesSigned']:\n                    if 'Signature' not in get_data:\n                        raise OneLogin_Saml2_ValidationError(\n                            'The Message of the Logout Request is not signed and the SP require it',\n                            OneLogin_Saml2_ValidationError.NO_SIGNED_MESSAGE\n                        )\n\n            if 'Signature' in get_data:\n                if 'SigAlg' not in get_data:\n                    sign_alg = OneLogin_Saml2_Constants.RSA_SHA1\n                else:\n                    sign_alg = get_data['SigAlg']\n\n                signed_query = 'SAMLRequest=%s' % OneLogin_Saml2_Utils.get_encoded_parameter(get_data, 'SAMLRequest', lowercase_urlencoding=lowercase_urlencoding)\n                if 'RelayState' in get_data:\n                    signed_query = '%s&RelayState=%s' % (signed_query, OneLogin_Saml2_Utils.get_encoded_parameter(get_data, 'RelayState', lowercase_urlencoding=lowercase_urlencoding))\n                signed_query = '%s&SigAlg=%s' % (signed_query, OneLogin_Saml2_Utils.get_encoded_parameter(get_data, 'SigAlg', OneLogin_Saml2_Constants.RSA_SHA1, lowercase_urlencoding=lowercase_urlencoding))\n\n                exists_x509cert = 'x509cert' in idp_data and idp_data['x509cert']\n                exists_multix509sign = 'x509certMulti' in idp_data and \\\n                    'signing' in idp_data['x509certMulti'] and \\\n                    idp_data['x509certMulti']['signing']\n\n                if not (exists_x509cert or exists_multix509sign):\n                    raise OneLogin_Saml2_Error(\n                        'In order to validate the sign on the Logout Request, the x509cert of the IdP is required',\n                        OneLogin_Saml2_Error.CERT_NOT_FOUND\n                    )\n                if exists_multix509sign:\n                    for cert in idp_data['x509certMulti']['signing']:\n                        if OneLogin_Saml2_Utils.validate_binary_sign(signed_query, b64decode(get_data['Signature']), cert, sign_alg):\n                            return True\n                    raise OneLogin_Saml2_ValidationError(\n                        'Signature validation failed. Logout Request rejected',\n                        OneLogin_Saml2_ValidationError.INVALID_SIGNATURE\n                    )\n                else:\n                    cert = idp_data['x509cert']\n\n                    if not OneLogin_Saml2_Utils.validate_binary_sign(signed_query, b64decode(get_data['Signature']), cert, sign_alg):\n                        raise OneLogin_Saml2_ValidationError(\n                            'Signature validation failed. Logout Request rejected',\n                            OneLogin_Saml2_ValidationError.INVALID_SIGNATURE\n                        )\n            return True\n        except Exception as err:\n            # pylint: disable=R0801sign_alg\n            self.__error = err.__str__()\n            debug = self.__settings.is_debug_active()\n            if debug:\n                print(err.__str__())\n            if raise_exceptions:\n                raise err\n            return False", "language": "python", "code": "def is_valid(self, request_data, raise_exceptions=False):\n        \"\"\"\n        Checks if the Logout Request received is valid\n        :param request_data: Request Data\n        :type request_data: dict\n        :param raise_exceptions: Whether to return false on failure or raise an exception\n        :type raise_exceptions: Boolean\n        :return: If the Logout Request is or not valid\n        :rtype: boolean\n        \"\"\"\n        self.__error = None\n        lowercase_urlencoding = False\n        try:\n            dom = fromstring(self.__logout_request, forbid_dtd=True)\n\n            idp_data = self.__settings.get_idp_data()\n            idp_entity_id = idp_data['entityId']\n\n            if 'get_data' in request_data.keys():\n                get_data = request_data['get_data']\n            else:\n                get_data = {}\n\n            if 'lowercase_urlencoding' in request_data.keys():\n                lowercase_urlencoding = request_data['lowercase_urlencoding']\n\n            if self.__settings.is_strict():\n                res = OneLogin_Saml2_Utils.validate_xml(dom, 'saml-schema-protocol-2.0.xsd', self.__settings.is_debug_active())\n                if not isinstance(res, Document):\n                    raise OneLogin_Saml2_ValidationError(\n                        'Invalid SAML Logout Request. Not match the saml-schema-protocol-2.0.xsd',\n                        OneLogin_Saml2_ValidationError.INVALID_XML_FORMAT\n                    )\n\n                security = self.__settings.get_security_data()\n\n                current_url = OneLogin_Saml2_Utils.get_self_url_no_query(request_data)\n\n                # Check NotOnOrAfter\n                if dom.get('NotOnOrAfter', None):\n                    na = OneLogin_Saml2_Utils.parse_SAML_to_time(dom.get('NotOnOrAfter'))\n                    if na <= OneLogin_Saml2_Utils.now():\n                        raise OneLogin_Saml2_ValidationError(\n                            'Could not validate timestamp: expired. Check system clock.',\n                            OneLogin_Saml2_ValidationError.RESPONSE_EXPIRED\n                        )\n\n                # Check destination\n                if dom.get('Destination', None):\n                    destination = dom.get('Destination')\n                    if destination != '':\n                        if current_url not in destination:\n                            raise Exception(\n                                'The LogoutRequest was received at '\n                                '%(currentURL)s instead of %(destination)s' %\n                                {\n                                    'currentURL': current_url,\n                                    'destination': destination,\n                                },\n                                OneLogin_Saml2_ValidationError.WRONG_DESTINATION\n                            )\n\n                # Check issuer\n                issuer = OneLogin_Saml2_Logout_Request.get_issuer(dom)\n                if issuer is not None and issuer != idp_entity_id:\n                    raise OneLogin_Saml2_ValidationError(\n                        'Invalid issuer in the Logout Request (expected %(idpEntityId)s, got %(issuer)s)' %\n                        {\n                            'idpEntityId': idp_entity_id,\n                            'issuer': issuer\n                        },\n                        OneLogin_Saml2_ValidationError.WRONG_ISSUER\n                    )\n\n                if security['wantMessagesSigned']:\n                    if 'Signature' not in get_data:\n                        raise OneLogin_Saml2_ValidationError(\n                            'The Message of the Logout Request is not signed and the SP require it',\n                            OneLogin_Saml2_ValidationError.NO_SIGNED_MESSAGE\n                        )\n\n            if 'Signature' in get_data:\n                if 'SigAlg' not in get_data:\n                    sign_alg = OneLogin_Saml2_Constants.RSA_SHA1\n                else:\n                    sign_alg = get_data['SigAlg']\n\n                signed_query = 'SAMLRequest=%s' % OneLogin_Saml2_Utils.get_encoded_parameter(get_data, 'SAMLRequest', lowercase_urlencoding=lowercase_urlencoding)\n                if 'RelayState' in get_data:\n                    signed_query = '%s&RelayState=%s' % (signed_query, OneLogin_Saml2_Utils.get_encoded_parameter(get_data, 'RelayState', lowercase_urlencoding=lowercase_urlencoding))\n                signed_query = '%s&SigAlg=%s' % (signed_query, OneLogin_Saml2_Utils.get_encoded_parameter(get_data, 'SigAlg', OneLogin_Saml2_Constants.RSA_SHA1, lowercase_urlencoding=lowercase_urlencoding))\n\n                exists_x509cert = 'x509cert' in idp_data and idp_data['x509cert']\n                exists_multix509sign = 'x509certMulti' in idp_data and \\\n                    'signing' in idp_data['x509certMulti'] and \\\n                    idp_data['x509certMulti']['signing']\n\n                if not (exists_x509cert or exists_multix509sign):\n                    raise OneLogin_Saml2_Error(\n                        'In order to validate the sign on the Logout Request, the x509cert of the IdP is required',\n                        OneLogin_Saml2_Error.CERT_NOT_FOUND\n                    )\n                if exists_multix509sign:\n                    for cert in idp_data['x509certMulti']['signing']:\n                        if OneLogin_Saml2_Utils.validate_binary_sign(signed_query, b64decode(get_data['Signature']), cert, sign_alg):\n                            return True\n                    raise OneLogin_Saml2_ValidationError(\n                        'Signature validation failed. Logout Request rejected',\n                        OneLogin_Saml2_ValidationError.INVALID_SIGNATURE\n                    )\n                else:\n                    cert = idp_data['x509cert']\n\n                    if not OneLogin_Saml2_Utils.validate_binary_sign(signed_query, b64decode(get_data['Signature']), cert, sign_alg):\n                        raise OneLogin_Saml2_ValidationError(\n                            'Signature validation failed. Logout Request rejected',\n                            OneLogin_Saml2_ValidationError.INVALID_SIGNATURE\n                        )\n            return True\n        except Exception as err:\n            # pylint: disable=R0801sign_alg\n            self.__error = err.__str__()\n            debug = self.__settings.is_debug_active()\n            if debug:\n                print(err.__str__())\n            if raise_exceptions:\n                raise err\n            return False", "code_tokens": ["def", "is_valid", "(", "self", ",", "request_data", ",", "raise_exceptions", "=", "False", ")", ":", "self", ".", "__error", "=", "None", "lowercase_urlencoding", "=", "False", "try", ":", "dom", "=", "fromstring", "(", "self", ".", "__logout_request", ",", "forbid_dtd", "=", "True", ")", "idp_data", "=", "self", ".", "__settings", ".", "get_idp_data", "(", ")", "idp_entity_id", "=", "idp_data", "[", "'entityId'", "]", "if", "'get_data'", "in", "request_data", ".", "keys", "(", ")", ":", "get_data", "=", "request_data", "[", "'get_data'", "]", "else", ":", "get_data", "=", "{", "}", "if", "'lowercase_urlencoding'", "in", "request_data", ".", "keys", "(", ")", ":", "lowercase_urlencoding", "=", "request_data", "[", "'lowercase_urlencoding'", "]", "if", "self", ".", "__settings", ".", "is_strict", "(", ")", ":", "res", "=", "OneLogin_Saml2_Utils", ".", "validate_xml", "(", "dom", ",", "'saml-schema-protocol-2.0.xsd'", ",", "self", ".", "__settings", ".", "is_debug_active", "(", ")", ")", "if", "not", "isinstance", "(", "res", ",", "Document", ")", ":", "raise", "OneLogin_Saml2_ValidationError", "(", "'Invalid SAML Logout Request. Not match the saml-schema-protocol-2.0.xsd'", ",", "OneLogin_Saml2_ValidationError", ".", "INVALID_XML_FORMAT", ")", "security", "=", "self", ".", "__settings", ".", "get_security_data", "(", ")", "current_url", "=", "OneLogin_Saml2_Utils", ".", "get_self_url_no_query", "(", "request_data", ")", "# Check NotOnOrAfter", "if", "dom", ".", "get", "(", "'NotOnOrAfter'", ",", "None", ")", ":", "na", "=", "OneLogin_Saml2_Utils", ".", "parse_SAML_to_time", "(", "dom", ".", "get", "(", "'NotOnOrAfter'", ")", ")", "if", "na", "<=", "OneLogin_Saml2_Utils", ".", "now", "(", ")", ":", "raise", "OneLogin_Saml2_ValidationError", "(", "'Could not validate timestamp: expired. Check system clock.'", ",", "OneLogin_Saml2_ValidationError", ".", "RESPONSE_EXPIRED", ")", "# Check destination", "if", "dom", ".", "get", "(", "'Destination'", ",", "None", ")", ":", "destination", "=", "dom", ".", "get", "(", "'Destination'", ")", "if", "destination", "!=", "''", ":", "if", "current_url", "not", "in", "destination", ":", "raise", "Exception", "(", "'The LogoutRequest was received at '", "'%(currentURL)s instead of %(destination)s'", "%", "{", "'currentURL'", ":", "current_url", ",", "'destination'", ":", "destination", ",", "}", ",", "OneLogin_Saml2_ValidationError", ".", "WRONG_DESTINATION", ")", "# Check issuer", "issuer", "=", "OneLogin_Saml2_Logout_Request", ".", "get_issuer", "(", "dom", ")", "if", "issuer", "is", "not", "None", "and", "issuer", "!=", "idp_entity_id", ":", "raise", "OneLogin_Saml2_ValidationError", "(", "'Invalid issuer in the Logout Request (expected %(idpEntityId)s, got %(issuer)s)'", "%", "{", "'idpEntityId'", ":", "idp_entity_id", ",", "'issuer'", ":", "issuer", "}", ",", "OneLogin_Saml2_ValidationError", ".", "WRONG_ISSUER", ")", "if", "security", "[", "'wantMessagesSigned'", "]", ":", "if", "'Signature'", "not", "in", "get_data", ":", "raise", "OneLogin_Saml2_ValidationError", "(", "'The Message of the Logout Request is not signed and the SP require it'", ",", "OneLogin_Saml2_ValidationError", ".", "NO_SIGNED_MESSAGE", ")", "if", "'Signature'", "in", "get_data", ":", "if", "'SigAlg'", "not", "in", "get_data", ":", "sign_alg", "=", "OneLogin_Saml2_Constants", ".", "RSA_SHA1", "else", ":", "sign_alg", "=", "get_data", "[", "'SigAlg'", "]", "signed_query", "=", "'SAMLRequest=%s'", "%", "OneLogin_Saml2_Utils", ".", "get_encoded_parameter", "(", "get_data", ",", "'SAMLRequest'", ",", "lowercase_urlencoding", "=", "lowercase_urlencoding", ")", "if", "'RelayState'", "in", "get_data", ":", "signed_query", "=", "'%s&RelayState=%s'", "%", "(", "signed_query", ",", "OneLogin_Saml2_Utils", ".", "get_encoded_parameter", "(", "get_data", ",", "'RelayState'", ",", "lowercase_urlencoding", "=", "lowercase_urlencoding", ")", ")", "signed_query", "=", "'%s&SigAlg=%s'", "%", "(", "signed_query", ",", "OneLogin_Saml2_Utils", ".", "get_encoded_parameter", "(", "get_data", ",", "'SigAlg'", ",", "OneLogin_Saml2_Constants", ".", "RSA_SHA1", ",", "lowercase_urlencoding", "=", "lowercase_urlencoding", ")", ")", "exists_x509cert", "=", "'x509cert'", "in", "idp_data", "and", "idp_data", "[", "'x509cert'", "]", "exists_multix509sign", "=", "'x509certMulti'", "in", "idp_data", "and", "'signing'", "in", "idp_data", "[", "'x509certMulti'", "]", "and", "idp_data", "[", "'x509certMulti'", "]", "[", "'signing'", "]", "if", "not", "(", "exists_x509cert", "or", "exists_multix509sign", ")", ":", "raise", "OneLogin_Saml2_Error", "(", "'In order to validate the sign on the Logout Request, the x509cert of the IdP is required'", ",", "OneLogin_Saml2_Error", ".", "CERT_NOT_FOUND", ")", "if", "exists_multix509sign", ":", "for", "cert", "in", "idp_data", "[", "'x509certMulti'", "]", "[", "'signing'", "]", ":", "if", "OneLogin_Saml2_Utils", ".", "validate_binary_sign", "(", "signed_query", ",", "b64decode", "(", "get_data", "[", "'Signature'", "]", ")", ",", "cert", ",", "sign_alg", ")", ":", "return", "True", "raise", "OneLogin_Saml2_ValidationError", "(", "'Signature validation failed. Logout Request rejected'", ",", "OneLogin_Saml2_ValidationError", ".", "INVALID_SIGNATURE", ")", "else", ":", "cert", "=", "idp_data", "[", "'x509cert'", "]", "if", "not", "OneLogin_Saml2_Utils", ".", "validate_binary_sign", "(", "signed_query", ",", "b64decode", "(", "get_data", "[", "'Signature'", "]", ")", ",", "cert", ",", "sign_alg", ")", ":", "raise", "OneLogin_Saml2_ValidationError", "(", "'Signature validation failed. Logout Request rejected'", ",", "OneLogin_Saml2_ValidationError", ".", "INVALID_SIGNATURE", ")", "return", "True", "except", "Exception", "as", "err", ":", "# pylint: disable=R0801sign_alg", "self", ".", "__error", "=", "err", ".", "__str__", "(", ")", "debug", "=", "self", ".", "__settings", ".", "is_debug_active", "(", ")", "if", "debug", ":", "print", "(", "err", ".", "__str__", "(", ")", ")", "if", "raise_exceptions", ":", "raise", "err", "return", "False"], "docstring": "Checks if the Logout Request received is valid\n        :param request_data: Request Data\n        :type request_data: dict\n        :param raise_exceptions: Whether to return false on failure or raise an exception\n        :type raise_exceptions: Boolean\n        :return: If the Logout Request is or not valid\n        :rtype: boolean", "docstring_tokens": ["Checks", "if", "the", "Logout", "Request", "received", "is", "valid", ":", "param", "request_data", ":", "Request", "Data", ":", "type", "request_data", ":", "dict", ":", "param", "raise_exceptions", ":", "Whether", "to", "return", "false", "on", "failure", "or", "raise", "an", "exception", ":", "type", "raise_exceptions", ":", "Boolean", ":", "return", ":", "If", "the", "Logout", "Request", "is", "or", "not", "valid", ":", "rtype", ":", "boolean"], "sha": "9fe7a72da5b4caa1529c1640b52d2649447ce49b", "url": "https://github.com/onelogin/python-saml/blob/9fe7a72da5b4caa1529c1640b52d2649447ce49b/src/onelogin/saml2/logout_request.py#L304-L431", "partition": "train"}
{"repo": "sryza/spark-timeseries", "path": "python/sparkts/models/EWMA.py", "func_name": "fit_model", "original_string": "def fit_model(ts, sc=None):\n    \"\"\"\n    Fits an EWMA model to a time series. Uses the first point in the time series as a starting\n    value. Uses sum squared error as an objective function to optimize to find smoothing parameter\n    The model for EWMA is recursively defined as S_t = (1 - a) * X_t + a * S_{t-1}, where\n    a is the smoothing parameter, X is the original series, and S is the smoothed series\n    Note that the optimization is performed as unbounded optimization, although in its formal\n    definition the smoothing parameter is <= 1, which corresponds to an inequality bounded\n    optimization. Given this, the resulting smoothing parameter should always be sanity checked\n    https://en.wikipedia.org/wiki/Exponential_smoothing\n    \n    Parameters\n    ----------\n    ts:\n        the time series to which we want to fit an EWMA model as a Numpy array\n        \n    Returns an EWMA model\n    \"\"\"\n    assert sc != None, \"Missing SparkContext\"\n\n    jvm = sc._jvm\n    jmodel = jvm.com.cloudera.sparkts.models.EWMA.fitModel(_py2java(sc, Vectors.dense(ts)))\n    return EWMAModel(jmodel=jmodel, sc=sc)", "language": "python", "code": "def fit_model(ts, sc=None):\n    \"\"\"\n    Fits an EWMA model to a time series. Uses the first point in the time series as a starting\n    value. Uses sum squared error as an objective function to optimize to find smoothing parameter\n    The model for EWMA is recursively defined as S_t = (1 - a) * X_t + a * S_{t-1}, where\n    a is the smoothing parameter, X is the original series, and S is the smoothed series\n    Note that the optimization is performed as unbounded optimization, although in its formal\n    definition the smoothing parameter is <= 1, which corresponds to an inequality bounded\n    optimization. Given this, the resulting smoothing parameter should always be sanity checked\n    https://en.wikipedia.org/wiki/Exponential_smoothing\n    \n    Parameters\n    ----------\n    ts:\n        the time series to which we want to fit an EWMA model as a Numpy array\n        \n    Returns an EWMA model\n    \"\"\"\n    assert sc != None, \"Missing SparkContext\"\n\n    jvm = sc._jvm\n    jmodel = jvm.com.cloudera.sparkts.models.EWMA.fitModel(_py2java(sc, Vectors.dense(ts)))\n    return EWMAModel(jmodel=jmodel, sc=sc)", "code_tokens": ["def", "fit_model", "(", "ts", ",", "sc", "=", "None", ")", ":", "assert", "sc", "!=", "None", ",", "\"Missing SparkContext\"", "jvm", "=", "sc", ".", "_jvm", "jmodel", "=", "jvm", ".", "com", ".", "cloudera", ".", "sparkts", ".", "models", ".", "EWMA", ".", "fitModel", "(", "_py2java", "(", "sc", ",", "Vectors", ".", "dense", "(", "ts", ")", ")", ")", "return", "EWMAModel", "(", "jmodel", "=", "jmodel", ",", "sc", "=", "sc", ")"], "docstring": "Fits an EWMA model to a time series. Uses the first point in the time series as a starting\n    value. Uses sum squared error as an objective function to optimize to find smoothing parameter\n    The model for EWMA is recursively defined as S_t = (1 - a) * X_t + a * S_{t-1}, where\n    a is the smoothing parameter, X is the original series, and S is the smoothed series\n    Note that the optimization is performed as unbounded optimization, although in its formal\n    definition the smoothing parameter is <= 1, which corresponds to an inequality bounded\n    optimization. Given this, the resulting smoothing parameter should always be sanity checked\n    https://en.wikipedia.org/wiki/Exponential_smoothing\n    \n    Parameters\n    ----------\n    ts:\n        the time series to which we want to fit an EWMA model as a Numpy array\n        \n    Returns an EWMA model", "docstring_tokens": ["Fits", "an", "EWMA", "model", "to", "a", "time", "series", ".", "Uses", "the", "first", "point", "in", "the", "time", "series", "as", "a", "starting", "value", ".", "Uses", "sum", "squared", "error", "as", "an", "objective", "function", "to", "optimize", "to", "find", "smoothing", "parameter", "The", "model", "for", "EWMA", "is", "recursively", "defined", "as", "S_t", "=", "(", "1", "-", "a", ")", "*", "X_t", "+", "a", "*", "S_", "{", "t", "-", "1", "}", "where", "a", "is", "the", "smoothing", "parameter", "X", "is", "the", "original", "series", "and", "S", "is", "the", "smoothed", "series", "Note", "that", "the", "optimization", "is", "performed", "as", "unbounded", "optimization", "although", "in", "its", "formal", "definition", "the", "smoothing", "parameter", "is", "<", "=", "1", "which", "corresponds", "to", "an", "inequality", "bounded", "optimization", ".", "Given", "this", "the", "resulting", "smoothing", "parameter", "should", "always", "be", "sanity", "checked", "https", ":", "//", "en", ".", "wikipedia", ".", "org", "/", "wiki", "/", "Exponential_smoothing", "Parameters", "----------", "ts", ":", "the", "time", "series", "to", "which", "we", "want", "to", "fit", "an", "EWMA", "model", "as", "a", "Numpy", "array", "Returns", "an", "EWMA", "model"], "sha": "280aa887dc08ab114411245268f230fdabb76eec", "url": "https://github.com/sryza/spark-timeseries/blob/280aa887dc08ab114411245268f230fdabb76eec/python/sparkts/models/EWMA.py#L13-L35", "partition": "train"}
{"repo": "google/textfsm", "path": "textfsm/terminal.py", "func_name": "Pager._GetCh", "original_string": "def _GetCh(self):\n    \"\"\"Read a single character from the user.\n\n    Returns:\n      A string, the character read.\n    \"\"\"\n    fd = self._tty.fileno()\n    old = termios.tcgetattr(fd)\n    try:\n      tty.setraw(fd)\n      ch = self._tty.read(1)\n      # Also support arrow key shortcuts (escape + 2 chars)\n      if ord(ch) == 27:\n        ch += self._tty.read(2)\n    finally:\n      termios.tcsetattr(fd, termios.TCSADRAIN, old)\n    return ch", "language": "python", "code": "def _GetCh(self):\n    \"\"\"Read a single character from the user.\n\n    Returns:\n      A string, the character read.\n    \"\"\"\n    fd = self._tty.fileno()\n    old = termios.tcgetattr(fd)\n    try:\n      tty.setraw(fd)\n      ch = self._tty.read(1)\n      # Also support arrow key shortcuts (escape + 2 chars)\n      if ord(ch) == 27:\n        ch += self._tty.read(2)\n    finally:\n      termios.tcsetattr(fd, termios.TCSADRAIN, old)\n    return ch", "code_tokens": ["def", "_GetCh", "(", "self", ")", ":", "fd", "=", "self", ".", "_tty", ".", "fileno", "(", ")", "old", "=", "termios", ".", "tcgetattr", "(", "fd", ")", "try", ":", "tty", ".", "setraw", "(", "fd", ")", "ch", "=", "self", ".", "_tty", ".", "read", "(", "1", ")", "# Also support arrow key shortcuts (escape + 2 chars)", "if", "ord", "(", "ch", ")", "==", "27", ":", "ch", "+=", "self", ".", "_tty", ".", "read", "(", "2", ")", "finally", ":", "termios", ".", "tcsetattr", "(", "fd", ",", "termios", ".", "TCSADRAIN", ",", "old", ")", "return", "ch"], "docstring": "Read a single character from the user.\n\n    Returns:\n      A string, the character read.", "docstring_tokens": ["Read", "a", "single", "character", "from", "the", "user", "."], "sha": "63a2aaece33e07947aa80963dca99b893964633b", "url": "https://github.com/google/textfsm/blob/63a2aaece33e07947aa80963dca99b893964633b/textfsm/terminal.py#L428-L444", "partition": "train"}
{"repo": "skyfielders/python-skyfield", "path": "skyfield/relativity.py", "func_name": "add_deflection", "original_string": "def add_deflection(position, observer, ephemeris, t,\n                   include_earth_deflection, count=3):\n    \"\"\"Update `position` for how solar system masses will deflect its light.\n\n    Given the ICRS `position` [x,y,z] of an object (au) that is being\n    viewed from the `observer` also expressed as [x,y,z], and given an\n    ephemeris that can be used to determine solar system body positions,\n    and given the time `t` and Boolean `apply_earth` indicating whether\n    to worry about the effect of Earth's mass, and a `count` of how many\n    major solar system bodies to worry about, this function updates\n    `position` in-place to show how the masses in the solar system will\n    deflect its image.\n\n    \"\"\"\n    # Compute light-time to observed object.\n\n    tlt = length_of(position) / C_AUDAY\n\n    # Cycle through gravitating bodies.\n\n    jd_tdb = t.tdb\n    ts = t.ts\n    for name in deflectors[:count]:\n        try:\n            deflector = ephemeris[name]\n        except KeyError:\n            deflector = ephemeris[name + ' barycenter']\n\n        # Get position of gravitating body wrt ss barycenter at time 't_tdb'.\n\n        bposition = deflector.at(ts.tdb(jd=jd_tdb)).position.au  # TODO\n\n        # Get position of gravitating body wrt observer at time 'jd_tdb'.\n\n        gpv = bposition - observer\n\n        # Compute light-time from point on incoming light ray that is closest\n        # to gravitating body.\n\n        dlt = light_time_difference(position, gpv)\n\n        # Get position of gravitating body wrt ss barycenter at time when\n        # incoming photons were closest to it.\n\n        tclose = jd_tdb\n\n        # if dlt > 0.0:\n        #     tclose = jd - dlt\n\n        tclose = where(dlt > 0.0, jd_tdb - dlt, tclose)\n        tclose = where(tlt < dlt, jd_tdb - tlt, tclose)\n\n        # if tlt < dlt:\n        #     tclose = jd - tlt\n\n        bposition = deflector.at(ts.tdb(jd=tclose)).position.au  # TODO\n        rmass = rmasses[name]\n        _add_deflection(position, observer, bposition, rmass)\n\n    # If observer is not at geocenter, add in deflection due to Earth.\n\n    if include_earth_deflection.any():\n        deflector = ephemeris['earth']\n        bposition = deflector.at(ts.tdb(jd=tclose)).position.au  # TODO\n        rmass = rmasses['earth']\n        # TODO: Make the following code less messy, maybe by having\n        # _add_deflection() return a new vector instead of modifying the\n        # old one in-place.\n        deflected_position = position.copy()\n        _add_deflection(deflected_position, observer, bposition, rmass)\n        if include_earth_deflection.shape:\n            position[:,include_earth_deflection] = (\n                deflected_position[:,include_earth_deflection])\n        else:\n            position[:] = deflected_position[:]", "language": "python", "code": "def add_deflection(position, observer, ephemeris, t,\n                   include_earth_deflection, count=3):\n    \"\"\"Update `position` for how solar system masses will deflect its light.\n\n    Given the ICRS `position` [x,y,z] of an object (au) that is being\n    viewed from the `observer` also expressed as [x,y,z], and given an\n    ephemeris that can be used to determine solar system body positions,\n    and given the time `t` and Boolean `apply_earth` indicating whether\n    to worry about the effect of Earth's mass, and a `count` of how many\n    major solar system bodies to worry about, this function updates\n    `position` in-place to show how the masses in the solar system will\n    deflect its image.\n\n    \"\"\"\n    # Compute light-time to observed object.\n\n    tlt = length_of(position) / C_AUDAY\n\n    # Cycle through gravitating bodies.\n\n    jd_tdb = t.tdb\n    ts = t.ts\n    for name in deflectors[:count]:\n        try:\n            deflector = ephemeris[name]\n        except KeyError:\n            deflector = ephemeris[name + ' barycenter']\n\n        # Get position of gravitating body wrt ss barycenter at time 't_tdb'.\n\n        bposition = deflector.at(ts.tdb(jd=jd_tdb)).position.au  # TODO\n\n        # Get position of gravitating body wrt observer at time 'jd_tdb'.\n\n        gpv = bposition - observer\n\n        # Compute light-time from point on incoming light ray that is closest\n        # to gravitating body.\n\n        dlt = light_time_difference(position, gpv)\n\n        # Get position of gravitating body wrt ss barycenter at time when\n        # incoming photons were closest to it.\n\n        tclose = jd_tdb\n\n        # if dlt > 0.0:\n        #     tclose = jd - dlt\n\n        tclose = where(dlt > 0.0, jd_tdb - dlt, tclose)\n        tclose = where(tlt < dlt, jd_tdb - tlt, tclose)\n\n        # if tlt < dlt:\n        #     tclose = jd - tlt\n\n        bposition = deflector.at(ts.tdb(jd=tclose)).position.au  # TODO\n        rmass = rmasses[name]\n        _add_deflection(position, observer, bposition, rmass)\n\n    # If observer is not at geocenter, add in deflection due to Earth.\n\n    if include_earth_deflection.any():\n        deflector = ephemeris['earth']\n        bposition = deflector.at(ts.tdb(jd=tclose)).position.au  # TODO\n        rmass = rmasses['earth']\n        # TODO: Make the following code less messy, maybe by having\n        # _add_deflection() return a new vector instead of modifying the\n        # old one in-place.\n        deflected_position = position.copy()\n        _add_deflection(deflected_position, observer, bposition, rmass)\n        if include_earth_deflection.shape:\n            position[:,include_earth_deflection] = (\n                deflected_position[:,include_earth_deflection])\n        else:\n            position[:] = deflected_position[:]", "code_tokens": ["def", "add_deflection", "(", "position", ",", "observer", ",", "ephemeris", ",", "t", ",", "include_earth_deflection", ",", "count", "=", "3", ")", ":", "# Compute light-time to observed object.", "tlt", "=", "length_of", "(", "position", ")", "/", "C_AUDAY", "# Cycle through gravitating bodies.", "jd_tdb", "=", "t", ".", "tdb", "ts", "=", "t", ".", "ts", "for", "name", "in", "deflectors", "[", ":", "count", "]", ":", "try", ":", "deflector", "=", "ephemeris", "[", "name", "]", "except", "KeyError", ":", "deflector", "=", "ephemeris", "[", "name", "+", "' barycenter'", "]", "# Get position of gravitating body wrt ss barycenter at time 't_tdb'.", "bposition", "=", "deflector", ".", "at", "(", "ts", ".", "tdb", "(", "jd", "=", "jd_tdb", ")", ")", ".", "position", ".", "au", "# TODO", "# Get position of gravitating body wrt observer at time 'jd_tdb'.", "gpv", "=", "bposition", "-", "observer", "# Compute light-time from point on incoming light ray that is closest", "# to gravitating body.", "dlt", "=", "light_time_difference", "(", "position", ",", "gpv", ")", "# Get position of gravitating body wrt ss barycenter at time when", "# incoming photons were closest to it.", "tclose", "=", "jd_tdb", "# if dlt > 0.0:", "#     tclose = jd - dlt", "tclose", "=", "where", "(", "dlt", ">", "0.0", ",", "jd_tdb", "-", "dlt", ",", "tclose", ")", "tclose", "=", "where", "(", "tlt", "<", "dlt", ",", "jd_tdb", "-", "tlt", ",", "tclose", ")", "# if tlt < dlt:", "#     tclose = jd - tlt", "bposition", "=", "deflector", ".", "at", "(", "ts", ".", "tdb", "(", "jd", "=", "tclose", ")", ")", ".", "position", ".", "au", "# TODO", "rmass", "=", "rmasses", "[", "name", "]", "_add_deflection", "(", "position", ",", "observer", ",", "bposition", ",", "rmass", ")", "# If observer is not at geocenter, add in deflection due to Earth.", "if", "include_earth_deflection", ".", "any", "(", ")", ":", "deflector", "=", "ephemeris", "[", "'earth'", "]", "bposition", "=", "deflector", ".", "at", "(", "ts", ".", "tdb", "(", "jd", "=", "tclose", ")", ")", ".", "position", ".", "au", "# TODO", "rmass", "=", "rmasses", "[", "'earth'", "]", "# TODO: Make the following code less messy, maybe by having", "# _add_deflection() return a new vector instead of modifying the", "# old one in-place.", "deflected_position", "=", "position", ".", "copy", "(", ")", "_add_deflection", "(", "deflected_position", ",", "observer", ",", "bposition", ",", "rmass", ")", "if", "include_earth_deflection", ".", "shape", ":", "position", "[", ":", ",", "include_earth_deflection", "]", "=", "(", "deflected_position", "[", ":", ",", "include_earth_deflection", "]", ")", "else", ":", "position", "[", ":", "]", "=", "deflected_position", "[", ":", "]"], "docstring": "Update `position` for how solar system masses will deflect its light.\n\n    Given the ICRS `position` [x,y,z] of an object (au) that is being\n    viewed from the `observer` also expressed as [x,y,z], and given an\n    ephemeris that can be used to determine solar system body positions,\n    and given the time `t` and Boolean `apply_earth` indicating whether\n    to worry about the effect of Earth's mass, and a `count` of how many\n    major solar system bodies to worry about, this function updates\n    `position` in-place to show how the masses in the solar system will\n    deflect its image.", "docstring_tokens": ["Update", "position", "for", "how", "solar", "system", "masses", "will", "deflect", "its", "light", "."], "sha": "51d9e042e06457f6b1f2415296d50a38cb3a300f", "url": "https://github.com/skyfielders/python-skyfield/blob/51d9e042e06457f6b1f2415296d50a38cb3a300f/skyfield/relativity.py#L23-L97", "partition": "train"}
{"repo": "KristianOellegaard/django-health-check", "path": "health_check/views.py", "func_name": "MediaType.parse_header", "original_string": "def parse_header(cls, value='*/*'):\n        \"\"\"Parse HTTP accept header and return instances sorted by weight.\"\"\"\n        yield from sorted((\n            cls.from_string(token.strip())\n            for token in value.split(',')\n            if token.strip()\n        ), reverse=True)", "language": "python", "code": "def parse_header(cls, value='*/*'):\n        \"\"\"Parse HTTP accept header and return instances sorted by weight.\"\"\"\n        yield from sorted((\n            cls.from_string(token.strip())\n            for token in value.split(',')\n            if token.strip()\n        ), reverse=True)", "code_tokens": ["def", "parse_header", "(", "cls", ",", "value", "=", "'*/*'", ")", ":", "yield", "from", "sorted", "(", "(", "cls", ".", "from_string", "(", "token", ".", "strip", "(", ")", ")", "for", "token", "in", "value", ".", "split", "(", "','", ")", "if", "token", ".", "strip", "(", ")", ")", ",", "reverse", "=", "True", ")"], "docstring": "Parse HTTP accept header and return instances sorted by weight.", "docstring_tokens": ["Parse", "HTTP", "accept", "header", "and", "return", "instances", "sorted", "by", "weight", "."], "sha": "575f811b7224dba0ef5f113791ca6aab20711041", "url": "https://github.com/KristianOellegaard/django-health-check/blob/575f811b7224dba0ef5f113791ca6aab20711041/health_check/views.py#L39-L45", "partition": "train"}
{"repo": "spulec/freezegun", "path": "freezegun/api.py", "func_name": "convert_to_timezone_naive", "original_string": "def convert_to_timezone_naive(time_to_freeze):\n    \"\"\"\n    Converts a potentially timezone-aware datetime to be a naive UTC datetime\n    \"\"\"\n    if time_to_freeze.tzinfo:\n        time_to_freeze -= time_to_freeze.utcoffset()\n        time_to_freeze = time_to_freeze.replace(tzinfo=None)\n    return time_to_freeze", "language": "python", "code": "def convert_to_timezone_naive(time_to_freeze):\n    \"\"\"\n    Converts a potentially timezone-aware datetime to be a naive UTC datetime\n    \"\"\"\n    if time_to_freeze.tzinfo:\n        time_to_freeze -= time_to_freeze.utcoffset()\n        time_to_freeze = time_to_freeze.replace(tzinfo=None)\n    return time_to_freeze", "code_tokens": ["def", "convert_to_timezone_naive", "(", "time_to_freeze", ")", ":", "if", "time_to_freeze", ".", "tzinfo", ":", "time_to_freeze", "-=", "time_to_freeze", ".", "utcoffset", "(", ")", "time_to_freeze", "=", "time_to_freeze", ".", "replace", "(", "tzinfo", "=", "None", ")", "return", "time_to_freeze"], "docstring": "Converts a potentially timezone-aware datetime to be a naive UTC datetime", "docstring_tokens": ["Converts", "a", "potentially", "timezone", "-", "aware", "datetime", "to", "be", "a", "naive", "UTC", "datetime"], "sha": "9347d133f33f675c87bb0569d70d9d95abef737f", "url": "https://github.com/spulec/freezegun/blob/9347d133f33f675c87bb0569d70d9d95abef737f/freezegun/api.py#L364-L371", "partition": "train"}
{"repo": "spulec/freezegun", "path": "freezegun/api.py", "func_name": "_parse_time_to_freeze", "original_string": "def _parse_time_to_freeze(time_to_freeze_str):\n    \"\"\"Parses all the possible inputs for freeze_time\n    :returns: a naive ``datetime.datetime`` object\n    \"\"\"\n    if time_to_freeze_str is None:\n        time_to_freeze_str = datetime.datetime.utcnow()\n\n    if isinstance(time_to_freeze_str, datetime.datetime):\n        time_to_freeze = time_to_freeze_str\n    elif isinstance(time_to_freeze_str, datetime.date):\n        time_to_freeze = datetime.datetime.combine(time_to_freeze_str, datetime.time())\n    elif isinstance(time_to_freeze_str, datetime.timedelta):\n        time_to_freeze = datetime.datetime.utcnow() + time_to_freeze_str\n    else:\n        time_to_freeze = parser.parse(time_to_freeze_str)\n\n    return convert_to_timezone_naive(time_to_freeze)", "language": "python", "code": "def _parse_time_to_freeze(time_to_freeze_str):\n    \"\"\"Parses all the possible inputs for freeze_time\n    :returns: a naive ``datetime.datetime`` object\n    \"\"\"\n    if time_to_freeze_str is None:\n        time_to_freeze_str = datetime.datetime.utcnow()\n\n    if isinstance(time_to_freeze_str, datetime.datetime):\n        time_to_freeze = time_to_freeze_str\n    elif isinstance(time_to_freeze_str, datetime.date):\n        time_to_freeze = datetime.datetime.combine(time_to_freeze_str, datetime.time())\n    elif isinstance(time_to_freeze_str, datetime.timedelta):\n        time_to_freeze = datetime.datetime.utcnow() + time_to_freeze_str\n    else:\n        time_to_freeze = parser.parse(time_to_freeze_str)\n\n    return convert_to_timezone_naive(time_to_freeze)", "code_tokens": ["def", "_parse_time_to_freeze", "(", "time_to_freeze_str", ")", ":", "if", "time_to_freeze_str", "is", "None", ":", "time_to_freeze_str", "=", "datetime", ".", "datetime", ".", "utcnow", "(", ")", "if", "isinstance", "(", "time_to_freeze_str", ",", "datetime", ".", "datetime", ")", ":", "time_to_freeze", "=", "time_to_freeze_str", "elif", "isinstance", "(", "time_to_freeze_str", ",", "datetime", ".", "date", ")", ":", "time_to_freeze", "=", "datetime", ".", "datetime", ".", "combine", "(", "time_to_freeze_str", ",", "datetime", ".", "time", "(", ")", ")", "elif", "isinstance", "(", "time_to_freeze_str", ",", "datetime", ".", "timedelta", ")", ":", "time_to_freeze", "=", "datetime", ".", "datetime", ".", "utcnow", "(", ")", "+", "time_to_freeze_str", "else", ":", "time_to_freeze", "=", "parser", ".", "parse", "(", "time_to_freeze_str", ")", "return", "convert_to_timezone_naive", "(", "time_to_freeze", ")"], "docstring": "Parses all the possible inputs for freeze_time\n    :returns: a naive ``datetime.datetime`` object", "docstring_tokens": ["Parses", "all", "the", "possible", "inputs", "for", "freeze_time", ":", "returns", ":", "a", "naive", "datetime", ".", "datetime", "object"], "sha": "9347d133f33f675c87bb0569d70d9d95abef737f", "url": "https://github.com/spulec/freezegun/blob/9347d133f33f675c87bb0569d70d9d95abef737f/freezegun/api.py#L397-L413", "partition": "train"}
{"repo": "spulec/freezegun", "path": "freezegun/api.py", "func_name": "FrozenDateTimeFactory.move_to", "original_string": "def move_to(self, target_datetime):\n        \"\"\"Moves frozen date to the given ``target_datetime``\"\"\"\n        target_datetime = _parse_time_to_freeze(target_datetime)\n        delta = target_datetime - self.time_to_freeze\n        self.tick(delta=delta)", "language": "python", "code": "def move_to(self, target_datetime):\n        \"\"\"Moves frozen date to the given ``target_datetime``\"\"\"\n        target_datetime = _parse_time_to_freeze(target_datetime)\n        delta = target_datetime - self.time_to_freeze\n        self.tick(delta=delta)", "code_tokens": ["def", "move_to", "(", "self", ",", "target_datetime", ")", ":", "target_datetime", "=", "_parse_time_to_freeze", "(", "target_datetime", ")", "delta", "=", "target_datetime", "-", "self", ".", "time_to_freeze", "self", ".", "tick", "(", "delta", "=", "delta", ")"], "docstring": "Moves frozen date to the given ``target_datetime``", "docstring_tokens": ["Moves", "frozen", "date", "to", "the", "given", "target_datetime"], "sha": "9347d133f33f675c87bb0569d70d9d95abef737f", "url": "https://github.com/spulec/freezegun/blob/9347d133f33f675c87bb0569d70d9d95abef737f/freezegun/api.py#L448-L452", "partition": "train"}
{"repo": "mbj4668/pyang", "path": "pyang/plugins/jsonxsl.py", "func_name": "JsonXslPlugin.emit", "original_string": "def emit(self, ctx, modules, fd):\n        \"\"\"Main control function.\n\n        Set up the top-level parts of the stylesheet, then process\n        recursively all nodes in all data trees, and finally emit the\n        serialized stylesheet.\n        \"\"\"\n        for (epos, etag, eargs) in ctx.errors:\n            if error.is_error(error.err_level(etag)):\n                raise error.EmitError(\"JSONXSL plugin needs a valid module\")\n        self.real_prefix = unique_prefixes(ctx)\n        self.top_names = []\n        for m in modules:\n            self.top_names.extend([c.arg for c in m.i_children if\n                                   c.keyword not in (\"rpc\", \"notification\")])\n        tree = ET.ElementTree(ss)\n        ET.SubElement(ss, \"output\", method=\"text\")\n        xsltdir = os.environ.get(\"PYANG_XSLT_DIR\",\n                                 \"/usr/local/share/yang/xslt\")\n        ET.SubElement(ss, \"include\", href=xsltdir + \"/jsonxsl-templates.xsl\")\n        ET.SubElement(ss, \"strip-space\", elements=\"*\")\n        nsmap = ET.SubElement(ss, \"template\", name=\"nsuri-to-module\")\n        ET.SubElement(nsmap, \"param\", name=\"uri\")\n        choo = ET.SubElement(nsmap, \"choose\")\n        for module in self.real_prefix.keys():\n            ns_uri = module.search_one(\"namespace\").arg\n            ss.attrib[\"xmlns:\" + self.real_prefix[module]] = ns_uri\n            when = ET.SubElement(choo, \"when\", test=\"$uri='\" + ns_uri + \"'\")\n            self.xsl_text(module.i_modulename, when)\n            self.process_module(module)\n        if sys.version > \"3\":\n            tree.write(fd, encoding=\"unicode\", xml_declaration=True)\n        elif sys.version > \"2.7\":\n            tree.write(fd, encoding=\"UTF-8\", xml_declaration=True)\n        else:\n            tree.write(fd, encoding=\"UTF-8\")", "language": "python", "code": "def emit(self, ctx, modules, fd):\n        \"\"\"Main control function.\n\n        Set up the top-level parts of the stylesheet, then process\n        recursively all nodes in all data trees, and finally emit the\n        serialized stylesheet.\n        \"\"\"\n        for (epos, etag, eargs) in ctx.errors:\n            if error.is_error(error.err_level(etag)):\n                raise error.EmitError(\"JSONXSL plugin needs a valid module\")\n        self.real_prefix = unique_prefixes(ctx)\n        self.top_names = []\n        for m in modules:\n            self.top_names.extend([c.arg for c in m.i_children if\n                                   c.keyword not in (\"rpc\", \"notification\")])\n        tree = ET.ElementTree(ss)\n        ET.SubElement(ss, \"output\", method=\"text\")\n        xsltdir = os.environ.get(\"PYANG_XSLT_DIR\",\n                                 \"/usr/local/share/yang/xslt\")\n        ET.SubElement(ss, \"include\", href=xsltdir + \"/jsonxsl-templates.xsl\")\n        ET.SubElement(ss, \"strip-space\", elements=\"*\")\n        nsmap = ET.SubElement(ss, \"template\", name=\"nsuri-to-module\")\n        ET.SubElement(nsmap, \"param\", name=\"uri\")\n        choo = ET.SubElement(nsmap, \"choose\")\n        for module in self.real_prefix.keys():\n            ns_uri = module.search_one(\"namespace\").arg\n            ss.attrib[\"xmlns:\" + self.real_prefix[module]] = ns_uri\n            when = ET.SubElement(choo, \"when\", test=\"$uri='\" + ns_uri + \"'\")\n            self.xsl_text(module.i_modulename, when)\n            self.process_module(module)\n        if sys.version > \"3\":\n            tree.write(fd, encoding=\"unicode\", xml_declaration=True)\n        elif sys.version > \"2.7\":\n            tree.write(fd, encoding=\"UTF-8\", xml_declaration=True)\n        else:\n            tree.write(fd, encoding=\"UTF-8\")", "code_tokens": ["def", "emit", "(", "self", ",", "ctx", ",", "modules", ",", "fd", ")", ":", "for", "(", "epos", ",", "etag", ",", "eargs", ")", "in", "ctx", ".", "errors", ":", "if", "error", ".", "is_error", "(", "error", ".", "err_level", "(", "etag", ")", ")", ":", "raise", "error", ".", "EmitError", "(", "\"JSONXSL plugin needs a valid module\"", ")", "self", ".", "real_prefix", "=", "unique_prefixes", "(", "ctx", ")", "self", ".", "top_names", "=", "[", "]", "for", "m", "in", "modules", ":", "self", ".", "top_names", ".", "extend", "(", "[", "c", ".", "arg", "for", "c", "in", "m", ".", "i_children", "if", "c", ".", "keyword", "not", "in", "(", "\"rpc\"", ",", "\"notification\"", ")", "]", ")", "tree", "=", "ET", ".", "ElementTree", "(", "ss", ")", "ET", ".", "SubElement", "(", "ss", ",", "\"output\"", ",", "method", "=", "\"text\"", ")", "xsltdir", "=", "os", ".", "environ", ".", "get", "(", "\"PYANG_XSLT_DIR\"", ",", "\"/usr/local/share/yang/xslt\"", ")", "ET", ".", "SubElement", "(", "ss", ",", "\"include\"", ",", "href", "=", "xsltdir", "+", "\"/jsonxsl-templates.xsl\"", ")", "ET", ".", "SubElement", "(", "ss", ",", "\"strip-space\"", ",", "elements", "=", "\"*\"", ")", "nsmap", "=", "ET", ".", "SubElement", "(", "ss", ",", "\"template\"", ",", "name", "=", "\"nsuri-to-module\"", ")", "ET", ".", "SubElement", "(", "nsmap", ",", "\"param\"", ",", "name", "=", "\"uri\"", ")", "choo", "=", "ET", ".", "SubElement", "(", "nsmap", ",", "\"choose\"", ")", "for", "module", "in", "self", ".", "real_prefix", ".", "keys", "(", ")", ":", "ns_uri", "=", "module", ".", "search_one", "(", "\"namespace\"", ")", ".", "arg", "ss", ".", "attrib", "[", "\"xmlns:\"", "+", "self", ".", "real_prefix", "[", "module", "]", "]", "=", "ns_uri", "when", "=", "ET", ".", "SubElement", "(", "choo", ",", "\"when\"", ",", "test", "=", "\"$uri='\"", "+", "ns_uri", "+", "\"'\"", ")", "self", ".", "xsl_text", "(", "module", ".", "i_modulename", ",", "when", ")", "self", ".", "process_module", "(", "module", ")", "if", "sys", ".", "version", ">", "\"3\"", ":", "tree", ".", "write", "(", "fd", ",", "encoding", "=", "\"unicode\"", ",", "xml_declaration", "=", "True", ")", "elif", "sys", ".", "version", ">", "\"2.7\"", ":", "tree", ".", "write", "(", "fd", ",", "encoding", "=", "\"UTF-8\"", ",", "xml_declaration", "=", "True", ")", "else", ":", "tree", ".", "write", "(", "fd", ",", "encoding", "=", "\"UTF-8\"", ")"], "docstring": "Main control function.\n\n        Set up the top-level parts of the stylesheet, then process\n        recursively all nodes in all data trees, and finally emit the\n        serialized stylesheet.", "docstring_tokens": ["Main", "control", "function", "."], "sha": "f2a5cc3142162e5b9ee4e18d154568d939ff63dd", "url": "https://github.com/mbj4668/pyang/blob/f2a5cc3142162e5b9ee4e18d154568d939ff63dd/pyang/plugins/jsonxsl.py#L64-L99", "partition": "train"}
{"repo": "quantmind/pulsar", "path": "pulsar/apps/greenio/utils.py", "func_name": "run_in_greenlet", "original_string": "def run_in_greenlet(callable):\n    \"\"\"Decorator to run a ``callable`` on a new greenlet.\n\n    A ``callable`` decorated with this decorator returns a coroutine\n    \"\"\"\n    @wraps(callable)\n    async def _(*args, **kwargs):\n        green = greenlet(callable)\n        # switch to the new greenlet\n        result = green.switch(*args, **kwargs)\n        # back to the parent\n        while isawaitable(result):\n            # keep on switching back to the greenlet if we get an awaitable\n            try:\n                result = green.switch((await result))\n            except Exception:\n                exc_info = sys.exc_info()\n                result = green.throw(*exc_info)\n\n        return green.switch(result)\n\n    return _", "language": "python", "code": "def run_in_greenlet(callable):\n    \"\"\"Decorator to run a ``callable`` on a new greenlet.\n\n    A ``callable`` decorated with this decorator returns a coroutine\n    \"\"\"\n    @wraps(callable)\n    async def _(*args, **kwargs):\n        green = greenlet(callable)\n        # switch to the new greenlet\n        result = green.switch(*args, **kwargs)\n        # back to the parent\n        while isawaitable(result):\n            # keep on switching back to the greenlet if we get an awaitable\n            try:\n                result = green.switch((await result))\n            except Exception:\n                exc_info = sys.exc_info()\n                result = green.throw(*exc_info)\n\n        return green.switch(result)\n\n    return _", "code_tokens": ["def", "run_in_greenlet", "(", "callable", ")", ":", "@", "wraps", "(", "callable", ")", "async", "def", "_", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "green", "=", "greenlet", "(", "callable", ")", "# switch to the new greenlet", "result", "=", "green", ".", "switch", "(", "*", "args", ",", "*", "*", "kwargs", ")", "# back to the parent", "while", "isawaitable", "(", "result", ")", ":", "# keep on switching back to the greenlet if we get an awaitable", "try", ":", "result", "=", "green", ".", "switch", "(", "(", "await", "result", ")", ")", "except", "Exception", ":", "exc_info", "=", "sys", ".", "exc_info", "(", ")", "result", "=", "green", ".", "throw", "(", "*", "exc_info", ")", "return", "green", ".", "switch", "(", "result", ")", "return", "_"], "docstring": "Decorator to run a ``callable`` on a new greenlet.\n\n    A ``callable`` decorated with this decorator returns a coroutine", "docstring_tokens": ["Decorator", "to", "run", "a", "callable", "on", "a", "new", "greenlet", "."], "sha": "fee44e871954aa6ca36d00bb5a3739abfdb89b26", "url": "https://github.com/quantmind/pulsar/blob/fee44e871954aa6ca36d00bb5a3739abfdb89b26/pulsar/apps/greenio/utils.py#L27-L48", "partition": "train"}
{"repo": "litaotao/IPython-Dashboard", "path": "dashboard/server/utils.py", "func_name": "build_response", "original_string": "def build_response(content, code=200):\n    \"\"\"Build response, add headers\"\"\"\n    response = make_response( jsonify(content), content['code'] )\n    response.headers['Access-Control-Allow-Origin'] = '*'\n    response.headers['Access-Control-Allow-Headers'] = \\\n            'Origin, X-Requested-With, Content-Type, Accept, Authorization'\n    return response", "language": "python", "code": "def build_response(content, code=200):\n    \"\"\"Build response, add headers\"\"\"\n    response = make_response( jsonify(content), content['code'] )\n    response.headers['Access-Control-Allow-Origin'] = '*'\n    response.headers['Access-Control-Allow-Headers'] = \\\n            'Origin, X-Requested-With, Content-Type, Accept, Authorization'\n    return response", "code_tokens": ["def", "build_response", "(", "content", ",", "code", "=", "200", ")", ":", "response", "=", "make_response", "(", "jsonify", "(", "content", ")", ",", "content", "[", "'code'", "]", ")", "response", ".", "headers", "[", "'Access-Control-Allow-Origin'", "]", "=", "'*'", "response", ".", "headers", "[", "'Access-Control-Allow-Headers'", "]", "=", "'Origin, X-Requested-With, Content-Type, Accept, Authorization'", "return", "response"], "docstring": "Build response, add headers", "docstring_tokens": ["Build", "response", "add", "headers"], "sha": "b28a6b447c86bcec562e554efe96c64660ddf7a2", "url": "https://github.com/litaotao/IPython-Dashboard/blob/b28a6b447c86bcec562e554efe96c64660ddf7a2/dashboard/server/utils.py#L18-L24", "partition": "train"}
{"repo": "litaotao/IPython-Dashboard", "path": "dashboard/server/resources/sql.py", "func_name": "SqlData.post", "original_string": "def post(self):\n        '''return executed sql result to client.\n\n        post data format:\n\n            {\"options\": ['all', 'last', 'first', 'format'], \"sql_raw\": \"raw sql ...\"}\n\n        Returns:\n            sql result.\n        '''\n        ## format sql\n\n        data = request.get_json()\n        options, sql_raw = data.get('options'), data.get('sql_raw')\n\n        if options == 'format':\n            sql_formmated = sqlparse.format(sql_raw, keyword_case='upper', reindent=True)\n            return build_response(dict(data=sql_formmated, code=200))\n\n        elif options in ('all', 'selected'):\n            conn = SQL(config.sql_host, config.sql_port, config.sql_user,\n                       config.sql_pwd, config.sql_db)\n\n            result = conn.run(sql_raw)\n            return build_response(dict(data=result, code=200))\n        else:\n\n            pass\n\n\n\n\n\n\n\n        pass", "language": "python", "code": "def post(self):\n        '''return executed sql result to client.\n\n        post data format:\n\n            {\"options\": ['all', 'last', 'first', 'format'], \"sql_raw\": \"raw sql ...\"}\n\n        Returns:\n            sql result.\n        '''\n        ## format sql\n\n        data = request.get_json()\n        options, sql_raw = data.get('options'), data.get('sql_raw')\n\n        if options == 'format':\n            sql_formmated = sqlparse.format(sql_raw, keyword_case='upper', reindent=True)\n            return build_response(dict(data=sql_formmated, code=200))\n\n        elif options in ('all', 'selected'):\n            conn = SQL(config.sql_host, config.sql_port, config.sql_user,\n                       config.sql_pwd, config.sql_db)\n\n            result = conn.run(sql_raw)\n            return build_response(dict(data=result, code=200))\n        else:\n\n            pass\n\n\n\n\n\n\n\n        pass", "code_tokens": ["def", "post", "(", "self", ")", ":", "## format sql", "data", "=", "request", ".", "get_json", "(", ")", "options", ",", "sql_raw", "=", "data", ".", "get", "(", "'options'", ")", ",", "data", ".", "get", "(", "'sql_raw'", ")", "if", "options", "==", "'format'", ":", "sql_formmated", "=", "sqlparse", ".", "format", "(", "sql_raw", ",", "keyword_case", "=", "'upper'", ",", "reindent", "=", "True", ")", "return", "build_response", "(", "dict", "(", "data", "=", "sql_formmated", ",", "code", "=", "200", ")", ")", "elif", "options", "in", "(", "'all'", ",", "'selected'", ")", ":", "conn", "=", "SQL", "(", "config", ".", "sql_host", ",", "config", ".", "sql_port", ",", "config", ".", "sql_user", ",", "config", ".", "sql_pwd", ",", "config", ".", "sql_db", ")", "result", "=", "conn", ".", "run", "(", "sql_raw", ")", "return", "build_response", "(", "dict", "(", "data", "=", "result", ",", "code", "=", "200", ")", ")", "else", ":", "pass", "pass"], "docstring": "return executed sql result to client.\n\n        post data format:\n\n            {\"options\": ['all', 'last', 'first', 'format'], \"sql_raw\": \"raw sql ...\"}\n\n        Returns:\n            sql result.", "docstring_tokens": ["return", "executed", "sql", "result", "to", "client", "."], "sha": "b28a6b447c86bcec562e554efe96c64660ddf7a2", "url": "https://github.com/litaotao/IPython-Dashboard/blob/b28a6b447c86bcec562e554efe96c64660ddf7a2/dashboard/server/resources/sql.py#L40-L75", "partition": "train"}
{"repo": "adafruit/Adafruit_CircuitPython_NeoPixel", "path": "neopixel.py", "func_name": "NeoPixel.show", "original_string": "def show(self):\n        \"\"\"Shows the new colors on the pixels themselves if they haven't already\n        been autowritten.\n\n        The colors may or may not be showing after this function returns because\n        it may be done asynchronously.\"\"\"\n        if self.brightness > 0.99:\n            neopixel_write(self.pin, self.buf)\n        else:\n            neopixel_write(self.pin, bytearray([int(i * self.brightness) for i in self.buf]))", "language": "python", "code": "def show(self):\n        \"\"\"Shows the new colors on the pixels themselves if they haven't already\n        been autowritten.\n\n        The colors may or may not be showing after this function returns because\n        it may be done asynchronously.\"\"\"\n        if self.brightness > 0.99:\n            neopixel_write(self.pin, self.buf)\n        else:\n            neopixel_write(self.pin, bytearray([int(i * self.brightness) for i in self.buf]))", "code_tokens": ["def", "show", "(", "self", ")", ":", "if", "self", ".", "brightness", ">", "0.99", ":", "neopixel_write", "(", "self", ".", "pin", ",", "self", ".", "buf", ")", "else", ":", "neopixel_write", "(", "self", ".", "pin", ",", "bytearray", "(", "[", "int", "(", "i", "*", "self", ".", "brightness", ")", "for", "i", "in", "self", ".", "buf", "]", ")", ")"], "docstring": "Shows the new colors on the pixels themselves if they haven't already\n        been autowritten.\n\n        The colors may or may not be showing after this function returns because\n        it may be done asynchronously.", "docstring_tokens": ["Shows", "the", "new", "colors", "on", "the", "pixels", "themselves", "if", "they", "haven", "t", "already", "been", "autowritten", "."], "sha": "c0ed34813a608b64ed044826553918ddbad12f0c", "url": "https://github.com/adafruit/Adafruit_CircuitPython_NeoPixel/blob/c0ed34813a608b64ed044826553918ddbad12f0c/neopixel.py#L223-L232", "partition": "train"}
{"repo": "dask/dask-kubernetes", "path": "dask_kubernetes/objects.py", "func_name": "_set_k8s_attribute", "original_string": "def _set_k8s_attribute(obj, attribute, value):\n    \"\"\"\n    Set a specific value on a kubernetes object's attribute\n\n    obj\n        an object from Kubernetes Python API client\n    attribute\n        Should be a Kubernetes API style attribute (with camelCase)\n    value\n        Can be anything (string, list, dict, k8s objects) that can be\n        accepted by the k8s python client\n    \"\"\"\n    current_value = None\n    attribute_name = None\n    # All k8s python client objects have an 'attribute_map' property\n    # which has as keys python style attribute names (api_client)\n    # and as values the kubernetes JSON API style attribute names\n    # (apiClient). We want to allow users to use the JSON API style attribute\n    # names only.\n    for python_attribute, json_attribute in obj.attribute_map.items():\n        if json_attribute == attribute:\n            attribute_name = python_attribute\n            break\n    else:\n        raise ValueError('Attribute must be one of {}'.format(obj.attribute_map.values()))\n\n    if hasattr(obj, attribute_name):\n        current_value = getattr(obj, attribute_name)\n\n    if current_value is not None:\n        # This will ensure that current_value is something JSONable,\n        # so a dict, list, or scalar\n        current_value = SERIALIZATION_API_CLIENT.sanitize_for_serialization(\n            current_value\n        )\n\n    if isinstance(current_value, dict):\n        # Deep merge our dictionaries!\n        setattr(obj, attribute_name, merge_dictionaries(current_value, value))\n    elif isinstance(current_value, list):\n        # Just append lists\n        setattr(obj, attribute_name, current_value + value)\n    else:\n        # Replace everything else\n        setattr(obj, attribute_name, value)", "language": "python", "code": "def _set_k8s_attribute(obj, attribute, value):\n    \"\"\"\n    Set a specific value on a kubernetes object's attribute\n\n    obj\n        an object from Kubernetes Python API client\n    attribute\n        Should be a Kubernetes API style attribute (with camelCase)\n    value\n        Can be anything (string, list, dict, k8s objects) that can be\n        accepted by the k8s python client\n    \"\"\"\n    current_value = None\n    attribute_name = None\n    # All k8s python client objects have an 'attribute_map' property\n    # which has as keys python style attribute names (api_client)\n    # and as values the kubernetes JSON API style attribute names\n    # (apiClient). We want to allow users to use the JSON API style attribute\n    # names only.\n    for python_attribute, json_attribute in obj.attribute_map.items():\n        if json_attribute == attribute:\n            attribute_name = python_attribute\n            break\n    else:\n        raise ValueError('Attribute must be one of {}'.format(obj.attribute_map.values()))\n\n    if hasattr(obj, attribute_name):\n        current_value = getattr(obj, attribute_name)\n\n    if current_value is not None:\n        # This will ensure that current_value is something JSONable,\n        # so a dict, list, or scalar\n        current_value = SERIALIZATION_API_CLIENT.sanitize_for_serialization(\n            current_value\n        )\n\n    if isinstance(current_value, dict):\n        # Deep merge our dictionaries!\n        setattr(obj, attribute_name, merge_dictionaries(current_value, value))\n    elif isinstance(current_value, list):\n        # Just append lists\n        setattr(obj, attribute_name, current_value + value)\n    else:\n        # Replace everything else\n        setattr(obj, attribute_name, value)", "code_tokens": ["def", "_set_k8s_attribute", "(", "obj", ",", "attribute", ",", "value", ")", ":", "current_value", "=", "None", "attribute_name", "=", "None", "# All k8s python client objects have an 'attribute_map' property", "# which has as keys python style attribute names (api_client)", "# and as values the kubernetes JSON API style attribute names", "# (apiClient). We want to allow users to use the JSON API style attribute", "# names only.", "for", "python_attribute", ",", "json_attribute", "in", "obj", ".", "attribute_map", ".", "items", "(", ")", ":", "if", "json_attribute", "==", "attribute", ":", "attribute_name", "=", "python_attribute", "break", "else", ":", "raise", "ValueError", "(", "'Attribute must be one of {}'", ".", "format", "(", "obj", ".", "attribute_map", ".", "values", "(", ")", ")", ")", "if", "hasattr", "(", "obj", ",", "attribute_name", ")", ":", "current_value", "=", "getattr", "(", "obj", ",", "attribute_name", ")", "if", "current_value", "is", "not", "None", ":", "# This will ensure that current_value is something JSONable,", "# so a dict, list, or scalar", "current_value", "=", "SERIALIZATION_API_CLIENT", ".", "sanitize_for_serialization", "(", "current_value", ")", "if", "isinstance", "(", "current_value", ",", "dict", ")", ":", "# Deep merge our dictionaries!", "setattr", "(", "obj", ",", "attribute_name", ",", "merge_dictionaries", "(", "current_value", ",", "value", ")", ")", "elif", "isinstance", "(", "current_value", ",", "list", ")", ":", "# Just append lists", "setattr", "(", "obj", ",", "attribute_name", ",", "current_value", "+", "value", ")", "else", ":", "# Replace everything else", "setattr", "(", "obj", ",", "attribute_name", ",", "value", ")"], "docstring": "Set a specific value on a kubernetes object's attribute\n\n    obj\n        an object from Kubernetes Python API client\n    attribute\n        Should be a Kubernetes API style attribute (with camelCase)\n    value\n        Can be anything (string, list, dict, k8s objects) that can be\n        accepted by the k8s python client", "docstring_tokens": ["Set", "a", "specific", "value", "on", "a", "kubernetes", "object", "s", "attribute"], "sha": "8a4883ecd902460b446bb1f43ed97efe398a135e", "url": "https://github.com/dask/dask-kubernetes/blob/8a4883ecd902460b446bb1f43ed97efe398a135e/dask_kubernetes/objects.py#L20-L64", "partition": "train"}
{"repo": "viniciuschiele/flask-apscheduler", "path": "flask_apscheduler/api.py", "func_name": "resume_job", "original_string": "def resume_job(job_id):\n    \"\"\"Resumes a job.\"\"\"\n\n    try:\n        current_app.apscheduler.resume_job(job_id)\n        job = current_app.apscheduler.get_job(job_id)\n        return jsonify(job)\n    except JobLookupError:\n        return jsonify(dict(error_message='Job %s not found' % job_id), status=404)\n    except Exception as e:\n        return jsonify(dict(error_message=str(e)), status=500)", "language": "python", "code": "def resume_job(job_id):\n    \"\"\"Resumes a job.\"\"\"\n\n    try:\n        current_app.apscheduler.resume_job(job_id)\n        job = current_app.apscheduler.get_job(job_id)\n        return jsonify(job)\n    except JobLookupError:\n        return jsonify(dict(error_message='Job %s not found' % job_id), status=404)\n    except Exception as e:\n        return jsonify(dict(error_message=str(e)), status=500)", "code_tokens": ["def", "resume_job", "(", "job_id", ")", ":", "try", ":", "current_app", ".", "apscheduler", ".", "resume_job", "(", "job_id", ")", "job", "=", "current_app", ".", "apscheduler", ".", "get_job", "(", "job_id", ")", "return", "jsonify", "(", "job", ")", "except", "JobLookupError", ":", "return", "jsonify", "(", "dict", "(", "error_message", "=", "'Job %s not found'", "%", "job_id", ")", ",", "status", "=", "404", ")", "except", "Exception", "as", "e", ":", "return", "jsonify", "(", "dict", "(", "error_message", "=", "str", "(", "e", ")", ")", ",", "status", "=", "500", ")"], "docstring": "Resumes a job.", "docstring_tokens": ["Resumes", "a", "job", "."], "sha": "cc52c39e1948c4e8de5da0d01db45f1779f61997", "url": "https://github.com/viniciuschiele/flask-apscheduler/blob/cc52c39e1948c4e8de5da0d01db45f1779f61997/flask_apscheduler/api.py#L113-L123", "partition": "train"}
{"repo": "pgjones/quart", "path": "quart/cli.py", "func_name": "QuartGroup.get_command", "original_string": "def get_command(self, ctx: click.Context, name: str) -> click.Command:\n        \"\"\"Return the relevant command given the context and name.\n\n        .. warning::\n\n            This differs substaintially from Flask in that it allows\n            for the inbuilt commands to be overridden.\n        \"\"\"\n        info = ctx.ensure_object(ScriptInfo)\n        command = None\n        try:\n            command = info.load_app().cli.get_command(ctx, name)\n        except NoAppException:\n            pass\n        if command is None:\n            command = super().get_command(ctx, name)\n        return command", "language": "python", "code": "def get_command(self, ctx: click.Context, name: str) -> click.Command:\n        \"\"\"Return the relevant command given the context and name.\n\n        .. warning::\n\n            This differs substaintially from Flask in that it allows\n            for the inbuilt commands to be overridden.\n        \"\"\"\n        info = ctx.ensure_object(ScriptInfo)\n        command = None\n        try:\n            command = info.load_app().cli.get_command(ctx, name)\n        except NoAppException:\n            pass\n        if command is None:\n            command = super().get_command(ctx, name)\n        return command", "code_tokens": ["def", "get_command", "(", "self", ",", "ctx", ":", "click", ".", "Context", ",", "name", ":", "str", ")", "->", "click", ".", "Command", ":", "info", "=", "ctx", ".", "ensure_object", "(", "ScriptInfo", ")", "command", "=", "None", "try", ":", "command", "=", "info", ".", "load_app", "(", ")", ".", "cli", ".", "get_command", "(", "ctx", ",", "name", ")", "except", "NoAppException", ":", "pass", "if", "command", "is", "None", ":", "command", "=", "super", "(", ")", ".", "get_command", "(", "ctx", ",", "name", ")", "return", "command"], "docstring": "Return the relevant command given the context and name.\n\n        .. warning::\n\n            This differs substaintially from Flask in that it allows\n            for the inbuilt commands to be overridden.", "docstring_tokens": ["Return", "the", "relevant", "command", "given", "the", "context", "and", "name", "."], "sha": "7cb2d3bd98e8746025764f2b933abc12041fa175", "url": "https://github.com/pgjones/quart/blob/7cb2d3bd98e8746025764f2b933abc12041fa175/quart/cli.py#L155-L171", "partition": "train"}
{"repo": "pgjones/quart", "path": "quart/templating.py", "func_name": "render_template", "original_string": "async def render_template(template_name_or_list: Union[str, List[str]], **context: Any) -> str:\n    \"\"\"Render the template with the context given.\n\n    Arguments:\n        template_name_or_list: Template name to render of a list of\n            possible template names.\n        context: The variables to pass to the template.\n    \"\"\"\n    await current_app.update_template_context(context)\n    template = current_app.jinja_env.get_or_select_template(template_name_or_list)\n    return await _render(template, context)", "language": "python", "code": "async def render_template(template_name_or_list: Union[str, List[str]], **context: Any) -> str:\n    \"\"\"Render the template with the context given.\n\n    Arguments:\n        template_name_or_list: Template name to render of a list of\n            possible template names.\n        context: The variables to pass to the template.\n    \"\"\"\n    await current_app.update_template_context(context)\n    template = current_app.jinja_env.get_or_select_template(template_name_or_list)\n    return await _render(template, context)", "code_tokens": ["async", "def", "render_template", "(", "template_name_or_list", ":", "Union", "[", "str", ",", "List", "[", "str", "]", "]", ",", "*", "*", "context", ":", "Any", ")", "->", "str", ":", "await", "current_app", ".", "update_template_context", "(", "context", ")", "template", "=", "current_app", ".", "jinja_env", ".", "get_or_select_template", "(", "template_name_or_list", ")", "return", "await", "_render", "(", "template", ",", "context", ")"], "docstring": "Render the template with the context given.\n\n    Arguments:\n        template_name_or_list: Template name to render of a list of\n            possible template names.\n        context: The variables to pass to the template.", "docstring_tokens": ["Render", "the", "template", "with", "the", "context", "given", "."], "sha": "7cb2d3bd98e8746025764f2b933abc12041fa175", "url": "https://github.com/pgjones/quart/blob/7cb2d3bd98e8746025764f2b933abc12041fa175/quart/templating.py#L79-L89", "partition": "train"}
{"repo": "getsentry/raven-python", "path": "raven/contrib/django/models.py", "func_name": "install_middleware", "original_string": "def install_middleware(middleware_name, lookup_names=None):\n    \"\"\"\n    Install specified middleware\n    \"\"\"\n    if lookup_names is None:\n        lookup_names = (middleware_name,)\n    # default settings.MIDDLEWARE is None\n    middleware_attr = 'MIDDLEWARE' if getattr(settings,\n                                              'MIDDLEWARE',\n                                              None) is not None \\\n        else 'MIDDLEWARE_CLASSES'\n    # make sure to get an empty tuple when attr is None\n    middleware = getattr(settings, middleware_attr, ()) or ()\n    if set(lookup_names).isdisjoint(set(middleware)):\n        setattr(settings,\n                middleware_attr,\n                type(middleware)((middleware_name,)) + middleware)", "language": "python", "code": "def install_middleware(middleware_name, lookup_names=None):\n    \"\"\"\n    Install specified middleware\n    \"\"\"\n    if lookup_names is None:\n        lookup_names = (middleware_name,)\n    # default settings.MIDDLEWARE is None\n    middleware_attr = 'MIDDLEWARE' if getattr(settings,\n                                              'MIDDLEWARE',\n                                              None) is not None \\\n        else 'MIDDLEWARE_CLASSES'\n    # make sure to get an empty tuple when attr is None\n    middleware = getattr(settings, middleware_attr, ()) or ()\n    if set(lookup_names).isdisjoint(set(middleware)):\n        setattr(settings,\n                middleware_attr,\n                type(middleware)((middleware_name,)) + middleware)", "code_tokens": ["def", "install_middleware", "(", "middleware_name", ",", "lookup_names", "=", "None", ")", ":", "if", "lookup_names", "is", "None", ":", "lookup_names", "=", "(", "middleware_name", ",", ")", "# default settings.MIDDLEWARE is None", "middleware_attr", "=", "'MIDDLEWARE'", "if", "getattr", "(", "settings", ",", "'MIDDLEWARE'", ",", "None", ")", "is", "not", "None", "else", "'MIDDLEWARE_CLASSES'", "# make sure to get an empty tuple when attr is None", "middleware", "=", "getattr", "(", "settings", ",", "middleware_attr", ",", "(", ")", ")", "or", "(", ")", "if", "set", "(", "lookup_names", ")", ".", "isdisjoint", "(", "set", "(", "middleware", ")", ")", ":", "setattr", "(", "settings", ",", "middleware_attr", ",", "type", "(", "middleware", ")", "(", "(", "middleware_name", ",", ")", ")", "+", "middleware", ")"], "docstring": "Install specified middleware", "docstring_tokens": ["Install", "specified", "middleware"], "sha": "d891c20f0f930153f508e9d698d9de42e910face", "url": "https://github.com/getsentry/raven-python/blob/d891c20f0f930153f508e9d698d9de42e910face/raven/contrib/django/models.py#L222-L238", "partition": "train"}
{"repo": "sebp/scikit-survival", "path": "sksurv/meta/base.py", "func_name": "_fit_and_score", "original_string": "def _fit_and_score(est, x, y, scorer, train_index, test_index, parameters, fit_params, predict_params):\n    \"\"\"Train survival model on given data and return its score on test data\"\"\"\n    X_train, y_train = _safe_split(est, x, y, train_index)\n    train_params = fit_params.copy()\n\n    # Training\n    est.set_params(**parameters)\n    est.fit(X_train, y_train, **train_params)\n\n    # Testing\n    test_predict_params = predict_params.copy()\n    X_test, y_test = _safe_split(est, x, y, test_index, train_index)\n\n    score = scorer(est, X_test, y_test, **test_predict_params)\n    if not isinstance(score, numbers.Number):\n        raise ValueError(\"scoring must return a number, got %s (%s) instead.\"\n                         % (str(score), type(score)))\n\n    return score", "language": "python", "code": "def _fit_and_score(est, x, y, scorer, train_index, test_index, parameters, fit_params, predict_params):\n    \"\"\"Train survival model on given data and return its score on test data\"\"\"\n    X_train, y_train = _safe_split(est, x, y, train_index)\n    train_params = fit_params.copy()\n\n    # Training\n    est.set_params(**parameters)\n    est.fit(X_train, y_train, **train_params)\n\n    # Testing\n    test_predict_params = predict_params.copy()\n    X_test, y_test = _safe_split(est, x, y, test_index, train_index)\n\n    score = scorer(est, X_test, y_test, **test_predict_params)\n    if not isinstance(score, numbers.Number):\n        raise ValueError(\"scoring must return a number, got %s (%s) instead.\"\n                         % (str(score), type(score)))\n\n    return score", "code_tokens": ["def", "_fit_and_score", "(", "est", ",", "x", ",", "y", ",", "scorer", ",", "train_index", ",", "test_index", ",", "parameters", ",", "fit_params", ",", "predict_params", ")", ":", "X_train", ",", "y_train", "=", "_safe_split", "(", "est", ",", "x", ",", "y", ",", "train_index", ")", "train_params", "=", "fit_params", ".", "copy", "(", ")", "# Training", "est", ".", "set_params", "(", "*", "*", "parameters", ")", "est", ".", "fit", "(", "X_train", ",", "y_train", ",", "*", "*", "train_params", ")", "# Testing", "test_predict_params", "=", "predict_params", ".", "copy", "(", ")", "X_test", ",", "y_test", "=", "_safe_split", "(", "est", ",", "x", ",", "y", ",", "test_index", ",", "train_index", ")", "score", "=", "scorer", "(", "est", ",", "X_test", ",", "y_test", ",", "*", "*", "test_predict_params", ")", "if", "not", "isinstance", "(", "score", ",", "numbers", ".", "Number", ")", ":", "raise", "ValueError", "(", "\"scoring must return a number, got %s (%s) instead.\"", "%", "(", "str", "(", "score", ")", ",", "type", "(", "score", ")", ")", ")", "return", "score"], "docstring": "Train survival model on given data and return its score on test data", "docstring_tokens": ["Train", "survival", "model", "on", "given", "data", "and", "return", "its", "score", "on", "test", "data"], "sha": "cfc99fd20454cdd6f4f20fe331b39f2191ccaabc", "url": "https://github.com/sebp/scikit-survival/blob/cfc99fd20454cdd6f4f20fe331b39f2191ccaabc/sksurv/meta/base.py#L17-L35", "partition": "train"}
{"repo": "apragacz/django-rest-registration", "path": "rest_registration/api/views/register_email.py", "func_name": "register_email", "original_string": "def register_email(request):\n    '''\n    Register new email.\n    '''\n    user = request.user\n\n    serializer = RegisterEmailSerializer(data=request.data)\n    serializer.is_valid(raise_exception=True)\n\n    email = serializer.validated_data['email']\n\n    template_config = (\n        registration_settings.REGISTER_EMAIL_VERIFICATION_EMAIL_TEMPLATES)\n    if registration_settings.REGISTER_EMAIL_VERIFICATION_ENABLED:\n        signer = RegisterEmailSigner({\n            'user_id': user.pk,\n            'email': email,\n        }, request=request)\n        send_verification_notification(\n            user, signer, template_config, email=email)\n    else:\n        email_field = get_user_setting('EMAIL_FIELD')\n        setattr(user, email_field, email)\n        user.save()\n\n    return get_ok_response('Register email link email sent')", "language": "python", "code": "def register_email(request):\n    '''\n    Register new email.\n    '''\n    user = request.user\n\n    serializer = RegisterEmailSerializer(data=request.data)\n    serializer.is_valid(raise_exception=True)\n\n    email = serializer.validated_data['email']\n\n    template_config = (\n        registration_settings.REGISTER_EMAIL_VERIFICATION_EMAIL_TEMPLATES)\n    if registration_settings.REGISTER_EMAIL_VERIFICATION_ENABLED:\n        signer = RegisterEmailSigner({\n            'user_id': user.pk,\n            'email': email,\n        }, request=request)\n        send_verification_notification(\n            user, signer, template_config, email=email)\n    else:\n        email_field = get_user_setting('EMAIL_FIELD')\n        setattr(user, email_field, email)\n        user.save()\n\n    return get_ok_response('Register email link email sent')", "code_tokens": ["def", "register_email", "(", "request", ")", ":", "user", "=", "request", ".", "user", "serializer", "=", "RegisterEmailSerializer", "(", "data", "=", "request", ".", "data", ")", "serializer", ".", "is_valid", "(", "raise_exception", "=", "True", ")", "email", "=", "serializer", ".", "validated_data", "[", "'email'", "]", "template_config", "=", "(", "registration_settings", ".", "REGISTER_EMAIL_VERIFICATION_EMAIL_TEMPLATES", ")", "if", "registration_settings", ".", "REGISTER_EMAIL_VERIFICATION_ENABLED", ":", "signer", "=", "RegisterEmailSigner", "(", "{", "'user_id'", ":", "user", ".", "pk", ",", "'email'", ":", "email", ",", "}", ",", "request", "=", "request", ")", "send_verification_notification", "(", "user", ",", "signer", ",", "template_config", ",", "email", "=", "email", ")", "else", ":", "email_field", "=", "get_user_setting", "(", "'EMAIL_FIELD'", ")", "setattr", "(", "user", ",", "email_field", ",", "email", ")", "user", ".", "save", "(", ")", "return", "get_ok_response", "(", "'Register email link email sent'", ")"], "docstring": "Register new email.", "docstring_tokens": ["Register", "new", "email", "."], "sha": "7373571264dd567c2a73a97ff4c45b64f113605b", "url": "https://github.com/apragacz/django-rest-registration/blob/7373571264dd567c2a73a97ff4c45b64f113605b/rest_registration/api/views/register_email.py#L33-L58", "partition": "train"}
{"repo": "fadhiilrachman/line-py", "path": "examples/groupbot.py", "func_name": "RECEIVE_MESSAGE", "original_string": "def RECEIVE_MESSAGE(op):\n    '''\n        This is sample for implement BOT in LINE group\n        Invite your BOT to group, then BOT will auto accept your invitation\n        Command availabe :\n        > hi\n        > /author\n    '''\n    msg = op.message\n    \n    text = msg.text\n    msg_id = msg.id\n    receiver = msg.to\n    sender = msg._from\n    \n    try:\n        # Check content only text message\n        if msg.contentType == 0:\n            # Check only group chat\n            if msg.toType == 2:\n                # Chat checked request\n                line.sendChatChecked(receiver, msg_id)\n                # Get sender contact\n                contact = line.getContact(sender)\n                # Command list\n                if text.lower() == 'hi':\n                    line.log('[%s] %s' % (contact.displayName, text))\n                    line.sendMessage(receiver, 'Hi too! How are you?')\n                elif text.lower() == '/author':\n                    line.log('[%s] %s' % (contact.displayName, text))\n                    line.sendMessage(receiver, 'My author is linepy')\n    except Exception as e:\n        line.log(\"[RECEIVE_MESSAGE] ERROR : \" + str(e))", "language": "python", "code": "def RECEIVE_MESSAGE(op):\n    '''\n        This is sample for implement BOT in LINE group\n        Invite your BOT to group, then BOT will auto accept your invitation\n        Command availabe :\n        > hi\n        > /author\n    '''\n    msg = op.message\n    \n    text = msg.text\n    msg_id = msg.id\n    receiver = msg.to\n    sender = msg._from\n    \n    try:\n        # Check content only text message\n        if msg.contentType == 0:\n            # Check only group chat\n            if msg.toType == 2:\n                # Chat checked request\n                line.sendChatChecked(receiver, msg_id)\n                # Get sender contact\n                contact = line.getContact(sender)\n                # Command list\n                if text.lower() == 'hi':\n                    line.log('[%s] %s' % (contact.displayName, text))\n                    line.sendMessage(receiver, 'Hi too! How are you?')\n                elif text.lower() == '/author':\n                    line.log('[%s] %s' % (contact.displayName, text))\n                    line.sendMessage(receiver, 'My author is linepy')\n    except Exception as e:\n        line.log(\"[RECEIVE_MESSAGE] ERROR : \" + str(e))", "code_tokens": ["def", "RECEIVE_MESSAGE", "(", "op", ")", ":", "msg", "=", "op", ".", "message", "text", "=", "msg", ".", "text", "msg_id", "=", "msg", ".", "id", "receiver", "=", "msg", ".", "to", "sender", "=", "msg", ".", "_from", "try", ":", "# Check content only text message", "if", "msg", ".", "contentType", "==", "0", ":", "# Check only group chat", "if", "msg", ".", "toType", "==", "2", ":", "# Chat checked request", "line", ".", "sendChatChecked", "(", "receiver", ",", "msg_id", ")", "# Get sender contact", "contact", "=", "line", ".", "getContact", "(", "sender", ")", "# Command list", "if", "text", ".", "lower", "(", ")", "==", "'hi'", ":", "line", ".", "log", "(", "'[%s] %s'", "%", "(", "contact", ".", "displayName", ",", "text", ")", ")", "line", ".", "sendMessage", "(", "receiver", ",", "'Hi too! How are you?'", ")", "elif", "text", ".", "lower", "(", ")", "==", "'/author'", ":", "line", ".", "log", "(", "'[%s] %s'", "%", "(", "contact", ".", "displayName", ",", "text", ")", ")", "line", ".", "sendMessage", "(", "receiver", ",", "'My author is linepy'", ")", "except", "Exception", "as", "e", ":", "line", ".", "log", "(", "\"[RECEIVE_MESSAGE] ERROR : \"", "+", "str", "(", "e", ")", ")"], "docstring": "This is sample for implement BOT in LINE group\n        Invite your BOT to group, then BOT will auto accept your invitation\n        Command availabe :\n        > hi\n        > /author", "docstring_tokens": ["This", "is", "sample", "for", "implement", "BOT", "in", "LINE", "group", "Invite", "your", "BOT", "to", "group", "then", "BOT", "will", "auto", "accept", "your", "invitation", "Command", "availabe", ":", ">", "hi", ">", "/", "author"], "sha": "b7f5f2b3fc09fa3fbf6088d7ebdaf9e44d96ba69", "url": "https://github.com/fadhiilrachman/line-py/blob/b7f5f2b3fc09fa3fbf6088d7ebdaf9e44d96ba69/examples/groupbot.py#L14-L46", "partition": "train"}
{"repo": "nschloe/matplotlib2tikz", "path": "matplotlib2tikz/axes.py", "func_name": "_get_ticks", "original_string": "def _get_ticks(data, xy, ticks, ticklabels):\n    \"\"\"Gets a {'x','y'}, a number of ticks and ticks labels, and returns the\n    necessary axis options for the given configuration.\n    \"\"\"\n    axis_options = []\n    pgfplots_ticks = []\n    pgfplots_ticklabels = []\n    is_label_required = False\n    for tick, ticklabel in zip(ticks, ticklabels):\n        pgfplots_ticks.append(tick)\n        # store the label anyway\n        label = ticklabel.get_text()\n        if ticklabel.get_visible():\n            label = mpl_backend_pgf.common_texification(label)\n            pgfplots_ticklabels.append(label)\n        else:\n            is_label_required = True\n        # Check if the label is necessary. If one of the labels is, then all of them\n        # must appear in the TikZ plot.\n        if label:\n            try:\n                label_float = float(label.replace(u\"\\N{MINUS SIGN}\", \"-\"))\n                is_label_required = is_label_required or (label and label_float != tick)\n            except ValueError:\n                is_label_required = True\n\n    # Leave the ticks to PGFPlots if not in STRICT mode and if there are no explicit\n    # labels.\n    if data[\"strict\"] or is_label_required:\n        if pgfplots_ticks:\n            ff = data[\"float format\"]\n            axis_options.append(\n                \"{}tick={{{}}}\".format(\n                    xy, \",\".join([ff.format(el) for el in pgfplots_ticks])\n                )\n            )\n        else:\n            val = \"{}\" if \"minor\" in xy else \"\\\\empty\"\n            axis_options.append(\"{}tick={}\".format(xy, val))\n\n        if is_label_required:\n            axis_options.append(\n                \"{}ticklabels={{{}}}\".format(xy, \",\".join(pgfplots_ticklabels))\n            )\n    return axis_options", "language": "python", "code": "def _get_ticks(data, xy, ticks, ticklabels):\n    \"\"\"Gets a {'x','y'}, a number of ticks and ticks labels, and returns the\n    necessary axis options for the given configuration.\n    \"\"\"\n    axis_options = []\n    pgfplots_ticks = []\n    pgfplots_ticklabels = []\n    is_label_required = False\n    for tick, ticklabel in zip(ticks, ticklabels):\n        pgfplots_ticks.append(tick)\n        # store the label anyway\n        label = ticklabel.get_text()\n        if ticklabel.get_visible():\n            label = mpl_backend_pgf.common_texification(label)\n            pgfplots_ticklabels.append(label)\n        else:\n            is_label_required = True\n        # Check if the label is necessary. If one of the labels is, then all of them\n        # must appear in the TikZ plot.\n        if label:\n            try:\n                label_float = float(label.replace(u\"\\N{MINUS SIGN}\", \"-\"))\n                is_label_required = is_label_required or (label and label_float != tick)\n            except ValueError:\n                is_label_required = True\n\n    # Leave the ticks to PGFPlots if not in STRICT mode and if there are no explicit\n    # labels.\n    if data[\"strict\"] or is_label_required:\n        if pgfplots_ticks:\n            ff = data[\"float format\"]\n            axis_options.append(\n                \"{}tick={{{}}}\".format(\n                    xy, \",\".join([ff.format(el) for el in pgfplots_ticks])\n                )\n            )\n        else:\n            val = \"{}\" if \"minor\" in xy else \"\\\\empty\"\n            axis_options.append(\"{}tick={}\".format(xy, val))\n\n        if is_label_required:\n            axis_options.append(\n                \"{}ticklabels={{{}}}\".format(xy, \",\".join(pgfplots_ticklabels))\n            )\n    return axis_options", "code_tokens": ["def", "_get_ticks", "(", "data", ",", "xy", ",", "ticks", ",", "ticklabels", ")", ":", "axis_options", "=", "[", "]", "pgfplots_ticks", "=", "[", "]", "pgfplots_ticklabels", "=", "[", "]", "is_label_required", "=", "False", "for", "tick", ",", "ticklabel", "in", "zip", "(", "ticks", ",", "ticklabels", ")", ":", "pgfplots_ticks", ".", "append", "(", "tick", ")", "# store the label anyway", "label", "=", "ticklabel", ".", "get_text", "(", ")", "if", "ticklabel", ".", "get_visible", "(", ")", ":", "label", "=", "mpl_backend_pgf", ".", "common_texification", "(", "label", ")", "pgfplots_ticklabels", ".", "append", "(", "label", ")", "else", ":", "is_label_required", "=", "True", "# Check if the label is necessary. If one of the labels is, then all of them", "# must appear in the TikZ plot.", "if", "label", ":", "try", ":", "label_float", "=", "float", "(", "label", ".", "replace", "(", "u\"\\N{MINUS SIGN}\"", ",", "\"-\"", ")", ")", "is_label_required", "=", "is_label_required", "or", "(", "label", "and", "label_float", "!=", "tick", ")", "except", "ValueError", ":", "is_label_required", "=", "True", "# Leave the ticks to PGFPlots if not in STRICT mode and if there are no explicit", "# labels.", "if", "data", "[", "\"strict\"", "]", "or", "is_label_required", ":", "if", "pgfplots_ticks", ":", "ff", "=", "data", "[", "\"float format\"", "]", "axis_options", ".", "append", "(", "\"{}tick={{{}}}\"", ".", "format", "(", "xy", ",", "\",\"", ".", "join", "(", "[", "ff", ".", "format", "(", "el", ")", "for", "el", "in", "pgfplots_ticks", "]", ")", ")", ")", "else", ":", "val", "=", "\"{}\"", "if", "\"minor\"", "in", "xy", "else", "\"\\\\empty\"", "axis_options", ".", "append", "(", "\"{}tick={}\"", ".", "format", "(", "xy", ",", "val", ")", ")", "if", "is_label_required", ":", "axis_options", ".", "append", "(", "\"{}ticklabels={{{}}}\"", ".", "format", "(", "xy", ",", "\",\"", ".", "join", "(", "pgfplots_ticklabels", ")", ")", ")", "return", "axis_options"], "docstring": "Gets a {'x','y'}, a number of ticks and ticks labels, and returns the\n    necessary axis options for the given configuration.", "docstring_tokens": ["Gets", "a", "{", "x", "y", "}", "a", "number", "of", "ticks", "and", "ticks", "labels", "and", "returns", "the", "necessary", "axis", "options", "for", "the", "given", "configuration", "."], "sha": "ac5daca6f38b834d757f6c6ae6cc34121956f46b", "url": "https://github.com/nschloe/matplotlib2tikz/blob/ac5daca6f38b834d757f6c6ae6cc34121956f46b/matplotlib2tikz/axes.py#L535-L579", "partition": "train"}
{"repo": "ceph/ceph-deploy", "path": "ceph_deploy/hosts/common.py", "func_name": "start_mon_service", "original_string": "def start_mon_service(distro, cluster, hostname):\n    \"\"\"\n    start mon service depending on distro init\n    \"\"\"\n    if distro.init == 'sysvinit':\n        service = distro.conn.remote_module.which_service()\n        remoto.process.run(\n            distro.conn,\n            [\n                service,\n                'ceph',\n                '-c',\n                '/etc/ceph/{cluster}.conf'.format(cluster=cluster),\n                'start',\n                'mon.{hostname}'.format(hostname=hostname)\n            ],\n            timeout=7,\n        )\n        system.enable_service(distro.conn)\n\n    elif distro.init == 'upstart':\n        remoto.process.run(\n             distro.conn,\n             [\n                 'initctl',\n                 'emit',\n                 'ceph-mon',\n                 'cluster={cluster}'.format(cluster=cluster),\n                 'id={hostname}'.format(hostname=hostname),\n             ],\n             timeout=7,\n         )\n\n    elif distro.init == 'systemd':\n       # enable ceph target for this host (in case it isn't already enabled)\n        remoto.process.run(\n            distro.conn,\n            [\n                'systemctl',\n                'enable',\n                'ceph.target'\n            ],\n            timeout=7,\n        )\n\n        # enable and start this mon instance\n        remoto.process.run(\n            distro.conn,\n            [\n                'systemctl',\n                'enable',\n                'ceph-mon@{hostname}'.format(hostname=hostname),\n            ],\n            timeout=7,\n        )\n        remoto.process.run(\n            distro.conn,\n            [\n                'systemctl',\n                'start',\n                'ceph-mon@{hostname}'.format(hostname=hostname),\n            ],\n            timeout=7,\n        )", "language": "python", "code": "def start_mon_service(distro, cluster, hostname):\n    \"\"\"\n    start mon service depending on distro init\n    \"\"\"\n    if distro.init == 'sysvinit':\n        service = distro.conn.remote_module.which_service()\n        remoto.process.run(\n            distro.conn,\n            [\n                service,\n                'ceph',\n                '-c',\n                '/etc/ceph/{cluster}.conf'.format(cluster=cluster),\n                'start',\n                'mon.{hostname}'.format(hostname=hostname)\n            ],\n            timeout=7,\n        )\n        system.enable_service(distro.conn)\n\n    elif distro.init == 'upstart':\n        remoto.process.run(\n             distro.conn,\n             [\n                 'initctl',\n                 'emit',\n                 'ceph-mon',\n                 'cluster={cluster}'.format(cluster=cluster),\n                 'id={hostname}'.format(hostname=hostname),\n             ],\n             timeout=7,\n         )\n\n    elif distro.init == 'systemd':\n       # enable ceph target for this host (in case it isn't already enabled)\n        remoto.process.run(\n            distro.conn,\n            [\n                'systemctl',\n                'enable',\n                'ceph.target'\n            ],\n            timeout=7,\n        )\n\n        # enable and start this mon instance\n        remoto.process.run(\n            distro.conn,\n            [\n                'systemctl',\n                'enable',\n                'ceph-mon@{hostname}'.format(hostname=hostname),\n            ],\n            timeout=7,\n        )\n        remoto.process.run(\n            distro.conn,\n            [\n                'systemctl',\n                'start',\n                'ceph-mon@{hostname}'.format(hostname=hostname),\n            ],\n            timeout=7,\n        )", "code_tokens": ["def", "start_mon_service", "(", "distro", ",", "cluster", ",", "hostname", ")", ":", "if", "distro", ".", "init", "==", "'sysvinit'", ":", "service", "=", "distro", ".", "conn", ".", "remote_module", ".", "which_service", "(", ")", "remoto", ".", "process", ".", "run", "(", "distro", ".", "conn", ",", "[", "service", ",", "'ceph'", ",", "'-c'", ",", "'/etc/ceph/{cluster}.conf'", ".", "format", "(", "cluster", "=", "cluster", ")", ",", "'start'", ",", "'mon.{hostname}'", ".", "format", "(", "hostname", "=", "hostname", ")", "]", ",", "timeout", "=", "7", ",", ")", "system", ".", "enable_service", "(", "distro", ".", "conn", ")", "elif", "distro", ".", "init", "==", "'upstart'", ":", "remoto", ".", "process", ".", "run", "(", "distro", ".", "conn", ",", "[", "'initctl'", ",", "'emit'", ",", "'ceph-mon'", ",", "'cluster={cluster}'", ".", "format", "(", "cluster", "=", "cluster", ")", ",", "'id={hostname}'", ".", "format", "(", "hostname", "=", "hostname", ")", ",", "]", ",", "timeout", "=", "7", ",", ")", "elif", "distro", ".", "init", "==", "'systemd'", ":", "# enable ceph target for this host (in case it isn't already enabled)", "remoto", ".", "process", ".", "run", "(", "distro", ".", "conn", ",", "[", "'systemctl'", ",", "'enable'", ",", "'ceph.target'", "]", ",", "timeout", "=", "7", ",", ")", "# enable and start this mon instance", "remoto", ".", "process", ".", "run", "(", "distro", ".", "conn", ",", "[", "'systemctl'", ",", "'enable'", ",", "'ceph-mon@{hostname}'", ".", "format", "(", "hostname", "=", "hostname", ")", ",", "]", ",", "timeout", "=", "7", ",", ")", "remoto", ".", "process", ".", "run", "(", "distro", ".", "conn", ",", "[", "'systemctl'", ",", "'start'", ",", "'ceph-mon@{hostname}'", ".", "format", "(", "hostname", "=", "hostname", ")", ",", "]", ",", "timeout", "=", "7", ",", ")"], "docstring": "start mon service depending on distro init", "docstring_tokens": ["start", "mon", "service", "depending", "on", "distro", "init"], "sha": "86943fcc454cd4c99a86e3493e9e93a59c661fef", "url": "https://github.com/ceph/ceph-deploy/blob/86943fcc454cd4c99a86e3493e9e93a59c661fef/ceph_deploy/hosts/common.py#L185-L248", "partition": "train"}
{"repo": "andrea-cuttone/geoplotlib", "path": "geoplotlib/layers.py", "func_name": "VoronoiLayer.__voronoi_finite_polygons_2d", "original_string": "def __voronoi_finite_polygons_2d(vor, radius=None):\n        \"\"\"\n        Reconstruct infinite voronoi regions in a 2D diagram to finite\n        regions.\n\n        Parameters\n        ----------\n        vor : Voronoi\n            Input diagram\n        radius : float, optional\n            Distance to 'points at infinity'.\n\n        Returns\n        -------\n        regions : list of tuples\n            Indices of vertices in each revised Voronoi regions.\n        vertices : list of tuples\n            Coordinates for revised Voronoi vertices. Same as coordinates\n            of input vertices, with 'points at infinity' appended to the\n            end.\n\n        \"\"\"\n\n        if vor.points.shape[1] != 2:\n            raise ValueError(\"Requires 2D input\")\n\n        new_regions = []\n        new_vertices = vor.vertices.tolist()\n\n        center = vor.points.mean(axis=0)\n        if radius is None:\n            radius = vor.points.ptp().max()\n\n        # Construct a map containing all ridges for a given point\n        all_ridges = {}\n        for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n            all_ridges.setdefault(p1, []).append((p2, v1, v2))\n            all_ridges.setdefault(p2, []).append((p1, v1, v2))\n\n        # Reconstruct infinite regions\n        for p1, region in enumerate(vor.point_region):\n            vertices = vor.regions[region]\n\n            if all(v >= 0 for v in vertices):\n                # finite region\n                new_regions.append(vertices)\n                continue\n\n            # reconstruct a non-finite region\n            if p1 not in all_ridges:\n                continue\n\n            ridges = all_ridges[p1]\n            new_region = [v for v in vertices if v >= 0]\n\n            for p2, v1, v2 in ridges:\n                if v2 < 0:\n                    v1, v2 = v2, v1\n                if v1 >= 0:\n                    # finite ridge: already in the region\n                    continue\n\n                # Compute the missing endpoint of an infinite ridge\n\n                t = vor.points[p2] - vor.points[p1] # tangent\n                t /= np.linalg.norm(t)\n                n = np.array([-t[1], t[0]])  # normal\n\n                midpoint = vor.points[[p1, p2]].mean(axis=0)\n                direction = np.sign(np.dot(midpoint - center, n)) * n\n                far_point = vor.vertices[v2] + direction * radius\n\n                new_region.append(len(new_vertices))\n                new_vertices.append(far_point.tolist())\n\n            # sort region counterclockwise\n            vs = np.asarray([new_vertices[v] for v in new_region])\n            c = vs.mean(axis=0)\n            angles = np.arctan2(vs[:,1] - c[1], vs[:,0] - c[0])\n            new_region = np.array(new_region)[np.argsort(angles)]\n\n            # finish\n            new_regions.append(new_region.tolist())\n\n        return new_regions, np.asarray(new_vertices)", "language": "python", "code": "def __voronoi_finite_polygons_2d(vor, radius=None):\n        \"\"\"\n        Reconstruct infinite voronoi regions in a 2D diagram to finite\n        regions.\n\n        Parameters\n        ----------\n        vor : Voronoi\n            Input diagram\n        radius : float, optional\n            Distance to 'points at infinity'.\n\n        Returns\n        -------\n        regions : list of tuples\n            Indices of vertices in each revised Voronoi regions.\n        vertices : list of tuples\n            Coordinates for revised Voronoi vertices. Same as coordinates\n            of input vertices, with 'points at infinity' appended to the\n            end.\n\n        \"\"\"\n\n        if vor.points.shape[1] != 2:\n            raise ValueError(\"Requires 2D input\")\n\n        new_regions = []\n        new_vertices = vor.vertices.tolist()\n\n        center = vor.points.mean(axis=0)\n        if radius is None:\n            radius = vor.points.ptp().max()\n\n        # Construct a map containing all ridges for a given point\n        all_ridges = {}\n        for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n            all_ridges.setdefault(p1, []).append((p2, v1, v2))\n            all_ridges.setdefault(p2, []).append((p1, v1, v2))\n\n        # Reconstruct infinite regions\n        for p1, region in enumerate(vor.point_region):\n            vertices = vor.regions[region]\n\n            if all(v >= 0 for v in vertices):\n                # finite region\n                new_regions.append(vertices)\n                continue\n\n            # reconstruct a non-finite region\n            if p1 not in all_ridges:\n                continue\n\n            ridges = all_ridges[p1]\n            new_region = [v for v in vertices if v >= 0]\n\n            for p2, v1, v2 in ridges:\n                if v2 < 0:\n                    v1, v2 = v2, v1\n                if v1 >= 0:\n                    # finite ridge: already in the region\n                    continue\n\n                # Compute the missing endpoint of an infinite ridge\n\n                t = vor.points[p2] - vor.points[p1] # tangent\n                t /= np.linalg.norm(t)\n                n = np.array([-t[1], t[0]])  # normal\n\n                midpoint = vor.points[[p1, p2]].mean(axis=0)\n                direction = np.sign(np.dot(midpoint - center, n)) * n\n                far_point = vor.vertices[v2] + direction * radius\n\n                new_region.append(len(new_vertices))\n                new_vertices.append(far_point.tolist())\n\n            # sort region counterclockwise\n            vs = np.asarray([new_vertices[v] for v in new_region])\n            c = vs.mean(axis=0)\n            angles = np.arctan2(vs[:,1] - c[1], vs[:,0] - c[0])\n            new_region = np.array(new_region)[np.argsort(angles)]\n\n            # finish\n            new_regions.append(new_region.tolist())\n\n        return new_regions, np.asarray(new_vertices)", "code_tokens": ["def", "__voronoi_finite_polygons_2d", "(", "vor", ",", "radius", "=", "None", ")", ":", "if", "vor", ".", "points", ".", "shape", "[", "1", "]", "!=", "2", ":", "raise", "ValueError", "(", "\"Requires 2D input\"", ")", "new_regions", "=", "[", "]", "new_vertices", "=", "vor", ".", "vertices", ".", "tolist", "(", ")", "center", "=", "vor", ".", "points", ".", "mean", "(", "axis", "=", "0", ")", "if", "radius", "is", "None", ":", "radius", "=", "vor", ".", "points", ".", "ptp", "(", ")", ".", "max", "(", ")", "# Construct a map containing all ridges for a given point", "all_ridges", "=", "{", "}", "for", "(", "p1", ",", "p2", ")", ",", "(", "v1", ",", "v2", ")", "in", "zip", "(", "vor", ".", "ridge_points", ",", "vor", ".", "ridge_vertices", ")", ":", "all_ridges", ".", "setdefault", "(", "p1", ",", "[", "]", ")", ".", "append", "(", "(", "p2", ",", "v1", ",", "v2", ")", ")", "all_ridges", ".", "setdefault", "(", "p2", ",", "[", "]", ")", ".", "append", "(", "(", "p1", ",", "v1", ",", "v2", ")", ")", "# Reconstruct infinite regions", "for", "p1", ",", "region", "in", "enumerate", "(", "vor", ".", "point_region", ")", ":", "vertices", "=", "vor", ".", "regions", "[", "region", "]", "if", "all", "(", "v", ">=", "0", "for", "v", "in", "vertices", ")", ":", "# finite region", "new_regions", ".", "append", "(", "vertices", ")", "continue", "# reconstruct a non-finite region", "if", "p1", "not", "in", "all_ridges", ":", "continue", "ridges", "=", "all_ridges", "[", "p1", "]", "new_region", "=", "[", "v", "for", "v", "in", "vertices", "if", "v", ">=", "0", "]", "for", "p2", ",", "v1", ",", "v2", "in", "ridges", ":", "if", "v2", "<", "0", ":", "v1", ",", "v2", "=", "v2", ",", "v1", "if", "v1", ">=", "0", ":", "# finite ridge: already in the region", "continue", "# Compute the missing endpoint of an infinite ridge", "t", "=", "vor", ".", "points", "[", "p2", "]", "-", "vor", ".", "points", "[", "p1", "]", "# tangent", "t", "/=", "np", ".", "linalg", ".", "norm", "(", "t", ")", "n", "=", "np", ".", "array", "(", "[", "-", "t", "[", "1", "]", ",", "t", "[", "0", "]", "]", ")", "# normal", "midpoint", "=", "vor", ".", "points", "[", "[", "p1", ",", "p2", "]", "]", ".", "mean", "(", "axis", "=", "0", ")", "direction", "=", "np", ".", "sign", "(", "np", ".", "dot", "(", "midpoint", "-", "center", ",", "n", ")", ")", "*", "n", "far_point", "=", "vor", ".", "vertices", "[", "v2", "]", "+", "direction", "*", "radius", "new_region", ".", "append", "(", "len", "(", "new_vertices", ")", ")", "new_vertices", ".", "append", "(", "far_point", ".", "tolist", "(", ")", ")", "# sort region counterclockwise", "vs", "=", "np", ".", "asarray", "(", "[", "new_vertices", "[", "v", "]", "for", "v", "in", "new_region", "]", ")", "c", "=", "vs", ".", "mean", "(", "axis", "=", "0", ")", "angles", "=", "np", ".", "arctan2", "(", "vs", "[", ":", ",", "1", "]", "-", "c", "[", "1", "]", ",", "vs", "[", ":", ",", "0", "]", "-", "c", "[", "0", "]", ")", "new_region", "=", "np", ".", "array", "(", "new_region", ")", "[", "np", ".", "argsort", "(", "angles", ")", "]", "# finish", "new_regions", ".", "append", "(", "new_region", ".", "tolist", "(", ")", ")", "return", "new_regions", ",", "np", ".", "asarray", "(", "new_vertices", ")"], "docstring": "Reconstruct infinite voronoi regions in a 2D diagram to finite\n        regions.\n\n        Parameters\n        ----------\n        vor : Voronoi\n            Input diagram\n        radius : float, optional\n            Distance to 'points at infinity'.\n\n        Returns\n        -------\n        regions : list of tuples\n            Indices of vertices in each revised Voronoi regions.\n        vertices : list of tuples\n            Coordinates for revised Voronoi vertices. Same as coordinates\n            of input vertices, with 'points at infinity' appended to the\n            end.", "docstring_tokens": ["Reconstruct", "infinite", "voronoi", "regions", "in", "a", "2D", "diagram", "to", "finite", "regions", "."], "sha": "a1c355bccec91cabd157569fad6daf53cf7687a1", "url": "https://github.com/andrea-cuttone/geoplotlib/blob/a1c355bccec91cabd157569fad6daf53cf7687a1/geoplotlib/layers.py#L505-L589", "partition": "train"}
{"repo": "sourceperl/pyModbusTCP", "path": "pyModbusTCP/utils.py", "func_name": "crc16", "original_string": "def crc16(frame):\n    \"\"\"Compute CRC16\n\n    :param frame: frame\n    :type frame: str (Python2) or class bytes (Python3)\n    :returns: CRC16\n    :rtype: int\n    \"\"\"\n    crc = 0xFFFF\n    for index, item in enumerate(bytearray(frame)):\n        next_byte = item\n        crc ^= next_byte\n        for i in range(8):\n            lsb = crc & 1\n            crc >>= 1\n            if lsb:\n                crc ^= 0xA001\n    return crc", "language": "python", "code": "def crc16(frame):\n    \"\"\"Compute CRC16\n\n    :param frame: frame\n    :type frame: str (Python2) or class bytes (Python3)\n    :returns: CRC16\n    :rtype: int\n    \"\"\"\n    crc = 0xFFFF\n    for index, item in enumerate(bytearray(frame)):\n        next_byte = item\n        crc ^= next_byte\n        for i in range(8):\n            lsb = crc & 1\n            crc >>= 1\n            if lsb:\n                crc ^= 0xA001\n    return crc", "code_tokens": ["def", "crc16", "(", "frame", ")", ":", "crc", "=", "0xFFFF", "for", "index", ",", "item", "in", "enumerate", "(", "bytearray", "(", "frame", ")", ")", ":", "next_byte", "=", "item", "crc", "^=", "next_byte", "for", "i", "in", "range", "(", "8", ")", ":", "lsb", "=", "crc", "&", "1", "crc", ">>=", "1", "if", "lsb", ":", "crc", "^=", "0xA001", "return", "crc"], "docstring": "Compute CRC16\n\n    :param frame: frame\n    :type frame: str (Python2) or class bytes (Python3)\n    :returns: CRC16\n    :rtype: int", "docstring_tokens": ["Compute", "CRC16"], "sha": "993f6e2f5ab52eba164be049e42cea560c3751a5", "url": "https://github.com/sourceperl/pyModbusTCP/blob/993f6e2f5ab52eba164be049e42cea560c3751a5/pyModbusTCP/utils.py#L153-L170", "partition": "train"}
{"repo": "ihabunek/toot", "path": "toot/wcstring.py", "func_name": "_wc_hard_wrap", "original_string": "def _wc_hard_wrap(line, length):\n    \"\"\"\n    Wrap text to length characters, breaking when target length is reached,\n    taking into account character width.\n\n    Used to wrap lines which cannot be wrapped on whitespace.\n    \"\"\"\n    chars = []\n    chars_len = 0\n    for char in line:\n        char_len = wcwidth(char)\n        if chars_len + char_len > length:\n            yield \"\".join(chars)\n            chars = []\n            chars_len = 0\n\n        chars.append(char)\n        chars_len += char_len\n\n    if chars:\n        yield \"\".join(chars)", "language": "python", "code": "def _wc_hard_wrap(line, length):\n    \"\"\"\n    Wrap text to length characters, breaking when target length is reached,\n    taking into account character width.\n\n    Used to wrap lines which cannot be wrapped on whitespace.\n    \"\"\"\n    chars = []\n    chars_len = 0\n    for char in line:\n        char_len = wcwidth(char)\n        if chars_len + char_len > length:\n            yield \"\".join(chars)\n            chars = []\n            chars_len = 0\n\n        chars.append(char)\n        chars_len += char_len\n\n    if chars:\n        yield \"\".join(chars)", "code_tokens": ["def", "_wc_hard_wrap", "(", "line", ",", "length", ")", ":", "chars", "=", "[", "]", "chars_len", "=", "0", "for", "char", "in", "line", ":", "char_len", "=", "wcwidth", "(", "char", ")", "if", "chars_len", "+", "char_len", ">", "length", ":", "yield", "\"\"", ".", "join", "(", "chars", ")", "chars", "=", "[", "]", "chars_len", "=", "0", "chars", ".", "append", "(", "char", ")", "chars_len", "+=", "char_len", "if", "chars", ":", "yield", "\"\"", ".", "join", "(", "chars", ")"], "docstring": "Wrap text to length characters, breaking when target length is reached,\n    taking into account character width.\n\n    Used to wrap lines which cannot be wrapped on whitespace.", "docstring_tokens": ["Wrap", "text", "to", "length", "characters", "breaking", "when", "target", "length", "is", "reached", "taking", "into", "account", "character", "width", "."], "sha": "d13fa8685b300f96621fa325774913ec0f413a7f", "url": "https://github.com/ihabunek/toot/blob/d13fa8685b300f96621fa325774913ec0f413a7f/toot/wcstring.py#L10-L30", "partition": "train"}
{"repo": "saltstack/salt", "path": "salt/modules/elasticsearch.py", "func_name": "node_info", "original_string": "def node_info(nodes=None, flat_settings=False, hosts=None, profile=None):\n    '''\n    .. versionadded:: 2017.7.0\n\n    Return Elasticsearch node information.\n\n    nodes\n        List of cluster nodes (id or name) to display stats for. Use _local for connected node, empty for all\n    flat_settings\n        Flatten settings keys\n\n    CLI example::\n\n        salt myminion elasticsearch.node_info flat_settings=True\n    '''\n    es = _get_instance(hosts, profile)\n\n    try:\n        return es.nodes.info(node_id=nodes, flat_settings=flat_settings)\n    except elasticsearch.TransportError as e:\n        raise CommandExecutionError(\"Cannot retrieve node information, server returned code {0} with message {1}\".format(e.status_code, e.error))", "language": "python", "code": "def node_info(nodes=None, flat_settings=False, hosts=None, profile=None):\n    '''\n    .. versionadded:: 2017.7.0\n\n    Return Elasticsearch node information.\n\n    nodes\n        List of cluster nodes (id or name) to display stats for. Use _local for connected node, empty for all\n    flat_settings\n        Flatten settings keys\n\n    CLI example::\n\n        salt myminion elasticsearch.node_info flat_settings=True\n    '''\n    es = _get_instance(hosts, profile)\n\n    try:\n        return es.nodes.info(node_id=nodes, flat_settings=flat_settings)\n    except elasticsearch.TransportError as e:\n        raise CommandExecutionError(\"Cannot retrieve node information, server returned code {0} with message {1}\".format(e.status_code, e.error))", "code_tokens": ["def", "node_info", "(", "nodes", "=", "None", ",", "flat_settings", "=", "False", ",", "hosts", "=", "None", ",", "profile", "=", "None", ")", ":", "es", "=", "_get_instance", "(", "hosts", ",", "profile", ")", "try", ":", "return", "es", ".", "nodes", ".", "info", "(", "node_id", "=", "nodes", ",", "flat_settings", "=", "flat_settings", ")", "except", "elasticsearch", ".", "TransportError", "as", "e", ":", "raise", "CommandExecutionError", "(", "\"Cannot retrieve node information, server returned code {0} with message {1}\"", ".", "format", "(", "e", ".", "status_code", ",", "e", ".", "error", ")", ")"], "docstring": ".. versionadded:: 2017.7.0\n\n    Return Elasticsearch node information.\n\n    nodes\n        List of cluster nodes (id or name) to display stats for. Use _local for connected node, empty for all\n    flat_settings\n        Flatten settings keys\n\n    CLI example::\n\n        salt myminion elasticsearch.node_info flat_settings=True", "docstring_tokens": ["..", "versionadded", "::", "2017", ".", "7", ".", "0"], "sha": "e8541fd6e744ab0df786c0f76102e41631f45d46", "url": "https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/elasticsearch.py#L198-L218", "partition": "train"}
{"repo": "saltstack/salt", "path": "salt/modules/riak.py", "func_name": "status", "original_string": "def status():\n    '''\n    Current node status\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' riak.status\n    '''\n    ret = {}\n\n    cmd = __execute_cmd('riak-admin', 'status')\n\n    for i in cmd['stdout'].splitlines():\n        if ':' in i:\n            (name, val) = i.split(':', 1)\n            ret[name.strip()] = val.strip()\n\n    return ret", "language": "python", "code": "def status():\n    '''\n    Current node status\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' riak.status\n    '''\n    ret = {}\n\n    cmd = __execute_cmd('riak-admin', 'status')\n\n    for i in cmd['stdout'].splitlines():\n        if ':' in i:\n            (name, val) = i.split(':', 1)\n            ret[name.strip()] = val.strip()\n\n    return ret", "code_tokens": ["def", "status", "(", ")", ":", "ret", "=", "{", "}", "cmd", "=", "__execute_cmd", "(", "'riak-admin'", ",", "'status'", ")", "for", "i", "in", "cmd", "[", "'stdout'", "]", ".", "splitlines", "(", ")", ":", "if", "':'", "in", "i", ":", "(", "name", ",", "val", ")", "=", "i", ".", "split", "(", "':'", ",", "1", ")", "ret", "[", "name", ".", "strip", "(", ")", "]", "=", "val", ".", "strip", "(", ")", "return", "ret"], "docstring": "Current node status\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' riak.status", "docstring_tokens": ["Current", "node", "status"], "sha": "e8541fd6e744ab0df786c0f76102e41631f45d46", "url": "https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/riak.py#L226-L247", "partition": "train"}
{"repo": "saltstack/salt", "path": "salt/ext/vsan/vsanapiutils.py", "func_name": "WaitForTasks", "original_string": "def WaitForTasks(tasks, si):\n   \"\"\"\n   Given the service instance si and tasks, it returns after all the\n   tasks are complete\n   \"\"\"\n\n   pc = si.content.propertyCollector\n\n   taskList = [str(task) for task in tasks]\n\n   # Create filter\n   objSpecs = [vmodl.query.PropertyCollector.ObjectSpec(obj=task)\n                                                            for task in tasks]\n   propSpec = vmodl.query.PropertyCollector.PropertySpec(type=vim.Task,\n                                                         pathSet=[], all=True)\n   filterSpec = vmodl.query.PropertyCollector.FilterSpec()\n   filterSpec.objectSet = objSpecs\n   filterSpec.propSet = [propSpec]\n   filter = pc.CreateFilter(filterSpec, True)\n\n   try:\n      version, state = None, None\n\n      # Loop looking for updates till the state moves to a completed state.\n      while len(taskList):\n         update = pc.WaitForUpdates(version)\n         for filterSet in update.filterSet:\n            for objSet in filterSet.objectSet:\n               task = objSet.obj\n               for change in objSet.changeSet:\n                  if change.name == 'info':\n                     state = change.val.state\n                  elif change.name == 'info.state':\n                     state = change.val\n                  else:\n                     continue\n\n                  if not str(task) in taskList:\n                     continue\n\n                  if state == vim.TaskInfo.State.success:\n                     # Remove task from taskList\n                     taskList.remove(str(task))\n                  elif state == vim.TaskInfo.State.error:\n                     raise task.info.error\n         # Move to next version\n         version = update.version\n   finally:\n      if filter:\n         filter.Destroy()", "language": "python", "code": "def WaitForTasks(tasks, si):\n   \"\"\"\n   Given the service instance si and tasks, it returns after all the\n   tasks are complete\n   \"\"\"\n\n   pc = si.content.propertyCollector\n\n   taskList = [str(task) for task in tasks]\n\n   # Create filter\n   objSpecs = [vmodl.query.PropertyCollector.ObjectSpec(obj=task)\n                                                            for task in tasks]\n   propSpec = vmodl.query.PropertyCollector.PropertySpec(type=vim.Task,\n                                                         pathSet=[], all=True)\n   filterSpec = vmodl.query.PropertyCollector.FilterSpec()\n   filterSpec.objectSet = objSpecs\n   filterSpec.propSet = [propSpec]\n   filter = pc.CreateFilter(filterSpec, True)\n\n   try:\n      version, state = None, None\n\n      # Loop looking for updates till the state moves to a completed state.\n      while len(taskList):\n         update = pc.WaitForUpdates(version)\n         for filterSet in update.filterSet:\n            for objSet in filterSet.objectSet:\n               task = objSet.obj\n               for change in objSet.changeSet:\n                  if change.name == 'info':\n                     state = change.val.state\n                  elif change.name == 'info.state':\n                     state = change.val\n                  else:\n                     continue\n\n                  if not str(task) in taskList:\n                     continue\n\n                  if state == vim.TaskInfo.State.success:\n                     # Remove task from taskList\n                     taskList.remove(str(task))\n                  elif state == vim.TaskInfo.State.error:\n                     raise task.info.error\n         # Move to next version\n         version = update.version\n   finally:\n      if filter:\n         filter.Destroy()", "code_tokens": ["def", "WaitForTasks", "(", "tasks", ",", "si", ")", ":", "pc", "=", "si", ".", "content", ".", "propertyCollector", "taskList", "=", "[", "str", "(", "task", ")", "for", "task", "in", "tasks", "]", "# Create filter", "objSpecs", "=", "[", "vmodl", ".", "query", ".", "PropertyCollector", ".", "ObjectSpec", "(", "obj", "=", "task", ")", "for", "task", "in", "tasks", "]", "propSpec", "=", "vmodl", ".", "query", ".", "PropertyCollector", ".", "PropertySpec", "(", "type", "=", "vim", ".", "Task", ",", "pathSet", "=", "[", "]", ",", "all", "=", "True", ")", "filterSpec", "=", "vmodl", ".", "query", ".", "PropertyCollector", ".", "FilterSpec", "(", ")", "filterSpec", ".", "objectSet", "=", "objSpecs", "filterSpec", ".", "propSet", "=", "[", "propSpec", "]", "filter", "=", "pc", ".", "CreateFilter", "(", "filterSpec", ",", "True", ")", "try", ":", "version", ",", "state", "=", "None", ",", "None", "# Loop looking for updates till the state moves to a completed state.", "while", "len", "(", "taskList", ")", ":", "update", "=", "pc", ".", "WaitForUpdates", "(", "version", ")", "for", "filterSet", "in", "update", ".", "filterSet", ":", "for", "objSet", "in", "filterSet", ".", "objectSet", ":", "task", "=", "objSet", ".", "obj", "for", "change", "in", "objSet", ".", "changeSet", ":", "if", "change", ".", "name", "==", "'info'", ":", "state", "=", "change", ".", "val", ".", "state", "elif", "change", ".", "name", "==", "'info.state'", ":", "state", "=", "change", ".", "val", "else", ":", "continue", "if", "not", "str", "(", "task", ")", "in", "taskList", ":", "continue", "if", "state", "==", "vim", ".", "TaskInfo", ".", "State", ".", "success", ":", "# Remove task from taskList", "taskList", ".", "remove", "(", "str", "(", "task", ")", ")", "elif", "state", "==", "vim", ".", "TaskInfo", ".", "State", ".", "error", ":", "raise", "task", ".", "info", ".", "error", "# Move to next version", "version", "=", "update", ".", "version", "finally", ":", "if", "filter", ":", "filter", ".", "Destroy", "(", ")"], "docstring": "Given the service instance si and tasks, it returns after all the\n   tasks are complete", "docstring_tokens": ["Given", "the", "service", "instance", "si", "and", "tasks", "it", "returns", "after", "all", "the", "tasks", "are", "complete"], "sha": "e8541fd6e744ab0df786c0f76102e41631f45d46", "url": "https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/ext/vsan/vsanapiutils.py#L116-L165", "partition": "train"}
{"repo": "joke2k/faker", "path": "faker/providers/ssn/it_IT/__init__.py", "func_name": "checksum", "original_string": "def checksum(value):\n    \"\"\"\n    Calculates the checksum char used for the 16th char.\n    Author: Vincenzo Palazzo\n    \"\"\"\n    return chr(65 + sum(CHECKSUM_TABLE[index % 2][ALPHANUMERICS_DICT[char]]\n                        for index, char in enumerate(value)) % 26)", "language": "python", "code": "def checksum(value):\n    \"\"\"\n    Calculates the checksum char used for the 16th char.\n    Author: Vincenzo Palazzo\n    \"\"\"\n    return chr(65 + sum(CHECKSUM_TABLE[index % 2][ALPHANUMERICS_DICT[char]]\n                        for index, char in enumerate(value)) % 26)", "code_tokens": ["def", "checksum", "(", "value", ")", ":", "return", "chr", "(", "65", "+", "sum", "(", "CHECKSUM_TABLE", "[", "index", "%", "2", "]", "[", "ALPHANUMERICS_DICT", "[", "char", "]", "]", "for", "index", ",", "char", "in", "enumerate", "(", "value", ")", ")", "%", "26", ")"], "docstring": "Calculates the checksum char used for the 16th char.\n    Author: Vincenzo Palazzo", "docstring_tokens": ["Calculates", "the", "checksum", "char", "used", "for", "the", "16th", "char", ".", "Author", ":", "Vincenzo", "Palazzo"], "sha": "965824b61132e52d92d1a6ce470396dbbe01c96c", "url": "https://github.com/joke2k/faker/blob/965824b61132e52d92d1a6ce470396dbbe01c96c/faker/providers/ssn/it_IT/__init__.py#L18-L24", "partition": "train"}
{"repo": "angr/angr", "path": "angr/slicer.py", "func_name": "SimSlicer._alias_analysis", "original_string": "def _alias_analysis(self, mock_sp=True, mock_bp=True):\n        \"\"\"\n        Perform a forward execution and perform alias analysis. Note that this analysis is fast, light-weight, and by no\n        means complete. For instance, most arithmetic operations are not supported.\n\n        - Depending on user settings, stack pointer and stack base pointer will be mocked and propagated to individual\n          tmps.\n\n        :param bool mock_sp: propagate stack pointer or not\n        :param bool mock_bp: propagate stack base pointer or not\n        :return: None\n        \"\"\"\n\n        state = SimLightState(\n            regs={\n                self._arch.sp_offset: self._arch.initial_sp,\n                self._arch.bp_offset: self._arch.initial_sp + 0x2000, # TODO: take care of the relation between sp and bp\n            },\n            temps={},\n            options={\n            'mock_sp': mock_sp,\n            'mock_bp': mock_bp,\n            }\n        )\n\n        for stmt_idx, stmt in list(enumerate(self._statements)):\n            self._forward_handler_stmt(stmt, state)", "language": "python", "code": "def _alias_analysis(self, mock_sp=True, mock_bp=True):\n        \"\"\"\n        Perform a forward execution and perform alias analysis. Note that this analysis is fast, light-weight, and by no\n        means complete. For instance, most arithmetic operations are not supported.\n\n        - Depending on user settings, stack pointer and stack base pointer will be mocked and propagated to individual\n          tmps.\n\n        :param bool mock_sp: propagate stack pointer or not\n        :param bool mock_bp: propagate stack base pointer or not\n        :return: None\n        \"\"\"\n\n        state = SimLightState(\n            regs={\n                self._arch.sp_offset: self._arch.initial_sp,\n                self._arch.bp_offset: self._arch.initial_sp + 0x2000, # TODO: take care of the relation between sp and bp\n            },\n            temps={},\n            options={\n            'mock_sp': mock_sp,\n            'mock_bp': mock_bp,\n            }\n        )\n\n        for stmt_idx, stmt in list(enumerate(self._statements)):\n            self._forward_handler_stmt(stmt, state)", "code_tokens": ["def", "_alias_analysis", "(", "self", ",", "mock_sp", "=", "True", ",", "mock_bp", "=", "True", ")", ":", "state", "=", "SimLightState", "(", "regs", "=", "{", "self", ".", "_arch", ".", "sp_offset", ":", "self", ".", "_arch", ".", "initial_sp", ",", "self", ".", "_arch", ".", "bp_offset", ":", "self", ".", "_arch", ".", "initial_sp", "+", "0x2000", ",", "# TODO: take care of the relation between sp and bp", "}", ",", "temps", "=", "{", "}", ",", "options", "=", "{", "'mock_sp'", ":", "mock_sp", ",", "'mock_bp'", ":", "mock_bp", ",", "}", ")", "for", "stmt_idx", ",", "stmt", "in", "list", "(", "enumerate", "(", "self", ".", "_statements", ")", ")", ":", "self", ".", "_forward_handler_stmt", "(", "stmt", ",", "state", ")"], "docstring": "Perform a forward execution and perform alias analysis. Note that this analysis is fast, light-weight, and by no\n        means complete. For instance, most arithmetic operations are not supported.\n\n        - Depending on user settings, stack pointer and stack base pointer will be mocked and propagated to individual\n          tmps.\n\n        :param bool mock_sp: propagate stack pointer or not\n        :param bool mock_bp: propagate stack base pointer or not\n        :return: None", "docstring_tokens": ["Perform", "a", "forward", "execution", "and", "perform", "alias", "analysis", ".", "Note", "that", "this", "analysis", "is", "fast", "light", "-", "weight", "and", "by", "no", "means", "complete", ".", "For", "instance", "most", "arithmetic", "operations", "are", "not", "supported", "."], "sha": "4e2f97d56af5419ee73bdb30482c8dd8ff5f3e40", "url": "https://github.com/angr/angr/blob/4e2f97d56af5419ee73bdb30482c8dd8ff5f3e40/angr/slicer.py#L56-L82", "partition": "train"}
{"repo": "angr/angr", "path": "angr/state_plugins/callstack.py", "func_name": "CallStack.stack_suffix", "original_string": "def stack_suffix(self, context_sensitivity_level):\n        \"\"\"\n        Generate the stack suffix. A stack suffix can be used as the key to a SimRun in CFG recovery.\n\n        :param int context_sensitivity_level: Level of context sensitivity.\n        :return: A tuple of stack suffix.\n        :rtype: tuple\n        \"\"\"\n\n        ret = ()\n\n        for frame in self:\n            if len(ret) >= context_sensitivity_level*2:\n                break\n            ret = (frame.call_site_addr, frame.func_addr) + ret\n\n        while len(ret) < context_sensitivity_level*2:\n            ret = (None, None) + ret\n\n        return ret", "language": "python", "code": "def stack_suffix(self, context_sensitivity_level):\n        \"\"\"\n        Generate the stack suffix. A stack suffix can be used as the key to a SimRun in CFG recovery.\n\n        :param int context_sensitivity_level: Level of context sensitivity.\n        :return: A tuple of stack suffix.\n        :rtype: tuple\n        \"\"\"\n\n        ret = ()\n\n        for frame in self:\n            if len(ret) >= context_sensitivity_level*2:\n                break\n            ret = (frame.call_site_addr, frame.func_addr) + ret\n\n        while len(ret) < context_sensitivity_level*2:\n            ret = (None, None) + ret\n\n        return ret", "code_tokens": ["def", "stack_suffix", "(", "self", ",", "context_sensitivity_level", ")", ":", "ret", "=", "(", ")", "for", "frame", "in", "self", ":", "if", "len", "(", "ret", ")", ">=", "context_sensitivity_level", "*", "2", ":", "break", "ret", "=", "(", "frame", ".", "call_site_addr", ",", "frame", ".", "func_addr", ")", "+", "ret", "while", "len", "(", "ret", ")", "<", "context_sensitivity_level", "*", "2", ":", "ret", "=", "(", "None", ",", "None", ")", "+", "ret", "return", "ret"], "docstring": "Generate the stack suffix. A stack suffix can be used as the key to a SimRun in CFG recovery.\n\n        :param int context_sensitivity_level: Level of context sensitivity.\n        :return: A tuple of stack suffix.\n        :rtype: tuple", "docstring_tokens": ["Generate", "the", "stack", "suffix", ".", "A", "stack", "suffix", "can", "be", "used", "as", "the", "key", "to", "a", "SimRun", "in", "CFG", "recovery", "."], "sha": "4e2f97d56af5419ee73bdb30482c8dd8ff5f3e40", "url": "https://github.com/angr/angr/blob/4e2f97d56af5419ee73bdb30482c8dd8ff5f3e40/angr/state_plugins/callstack.py#L328-L347", "partition": "train"}
{"repo": "shidenggui/easyquotation", "path": "easyquotation/jsl.py", "func_name": "Jsl.qdii", "original_string": "def qdii(self, min_volume=0):\n        \"\"\"\u4ee5\u5b57\u5178\u5f62\u5f0f\u8fd4\u56deQDII\u6570\u636e\n        :param min_volume:\u6700\u5c0f\u4ea4\u6613\u91cf\uff0c\u5355\u4f4d\u4e07\u5143\n        \"\"\"\n        # \u6dfb\u52a0\u5f53\u524d\u7684ctime\n        self.__qdii_url = self.__qdii_url.format(ctime=int(time.time()))\n        # \u8bf7\u6c42\u6570\u636e\n        rep = requests.get(self.__qdii_url)\n        # \u83b7\u53d6\u8fd4\u56de\u7684json\u5b57\u7b26\u4e32\n        fundjson = json.loads(rep.text)\n        # \u683c\u5f0f\u5316\u8fd4\u56de\u7684json\u5b57\u7b26\u4e32\n        data = self.formatjisilujson(fundjson)\n        data = {x: y for x, y in data.items() if y[\"notes\"] != \"\u4f30\u503c\u6709\u95ee\u9898\"}\n        # \u8fc7\u6ee4\u5c0f\u4e8e\u6307\u5b9a\u4ea4\u6613\u91cf\u7684\u6570\u636e\n        if min_volume:\n            data = {\n                k: data[k]\n                for k in data\n                if float(data[k][\"volume\"]) > min_volume\n            }\n\n        self.__qdii = data\n        return self.__qdii", "language": "python", "code": "def qdii(self, min_volume=0):\n        \"\"\"\u4ee5\u5b57\u5178\u5f62\u5f0f\u8fd4\u56deQDII\u6570\u636e\n        :param min_volume:\u6700\u5c0f\u4ea4\u6613\u91cf\uff0c\u5355\u4f4d\u4e07\u5143\n        \"\"\"\n        # \u6dfb\u52a0\u5f53\u524d\u7684ctime\n        self.__qdii_url = self.__qdii_url.format(ctime=int(time.time()))\n        # \u8bf7\u6c42\u6570\u636e\n        rep = requests.get(self.__qdii_url)\n        # \u83b7\u53d6\u8fd4\u56de\u7684json\u5b57\u7b26\u4e32\n        fundjson = json.loads(rep.text)\n        # \u683c\u5f0f\u5316\u8fd4\u56de\u7684json\u5b57\u7b26\u4e32\n        data = self.formatjisilujson(fundjson)\n        data = {x: y for x, y in data.items() if y[\"notes\"] != \"\u4f30\u503c\u6709\u95ee\u9898\"}\n        # \u8fc7\u6ee4\u5c0f\u4e8e\u6307\u5b9a\u4ea4\u6613\u91cf\u7684\u6570\u636e\n        if min_volume:\n            data = {\n                k: data[k]\n                for k in data\n                if float(data[k][\"volume\"]) > min_volume\n            }\n\n        self.__qdii = data\n        return self.__qdii", "code_tokens": ["def", "qdii", "(", "self", ",", "min_volume", "=", "0", ")", ":", "# \u6dfb\u52a0\u5f53\u524d\u7684ctime", "self", ".", "__qdii_url", "=", "self", ".", "__qdii_url", ".", "format", "(", "ctime", "=", "int", "(", "time", ".", "time", "(", ")", ")", ")", "# \u8bf7\u6c42\u6570\u636e", "rep", "=", "requests", ".", "get", "(", "self", ".", "__qdii_url", ")", "# \u83b7\u53d6\u8fd4\u56de\u7684json\u5b57\u7b26\u4e32", "fundjson", "=", "json", ".", "loads", "(", "rep", ".", "text", ")", "# \u683c\u5f0f\u5316\u8fd4\u56de\u7684json\u5b57\u7b26\u4e32", "data", "=", "self", ".", "formatjisilujson", "(", "fundjson", ")", "data", "=", "{", "x", ":", "y", "for", "x", ",", "y", "in", "data", ".", "items", "(", ")", "if", "y", "[", "\"notes\"", "]", "!=", "\"\u4f30\u503c\u6709\u95ee\u9898\"}", "", "# \u8fc7\u6ee4\u5c0f\u4e8e\u6307\u5b9a\u4ea4\u6613\u91cf\u7684\u6570\u636e", "if", "min_volume", ":", "data", "=", "{", "k", ":", "data", "[", "k", "]", "for", "k", "in", "data", "if", "float", "(", "data", "[", "k", "]", "[", "\"volume\"", "]", ")", ">", "min_volume", "}", "self", ".", "__qdii", "=", "data", "return", "self", ".", "__qdii"], "docstring": "\u4ee5\u5b57\u5178\u5f62\u5f0f\u8fd4\u56deQDII\u6570\u636e\n        :param min_volume:\u6700\u5c0f\u4ea4\u6613\u91cf\uff0c\u5355\u4f4d\u4e07\u5143", "docstring_tokens": ["\u4ee5\u5b57\u5178\u5f62\u5f0f\u8fd4\u56deQDII\u6570\u636e", ":", "param", "min_volume", ":", "\u6700\u5c0f\u4ea4\u6613\u91cf\uff0c\u5355\u4f4d\u4e07\u5143"], "sha": "a75820db4f05f5386e1c1024d05b0bfc1de6cbda", "url": "https://github.com/shidenggui/easyquotation/blob/a75820db4f05f5386e1c1024d05b0bfc1de6cbda/easyquotation/jsl.py#L393-L415", "partition": "train"}
{"repo": "shidenggui/easyquotation", "path": "easyquotation/jsl.py", "func_name": "Jsl.cb", "original_string": "def cb(self, min_volume=0):\n        \"\"\"\u4ee5\u5b57\u5178\u5f62\u5f0f\u8fd4\u56deQDII\u6570\u636e\n        :param min_volume:\u6700\u5c0f\u4ea4\u6613\u91cf\uff0c\u5355\u4f4d\u4e07\u5143\n        \"\"\"\n        # \u6dfb\u52a0\u5f53\u524d\u7684ctime\n        self.__cb_url = self.__cb_url.format(ctime=int(time.time()))\n        # \u8bf7\u6c42\u6570\u636e\n        rep = requests.get(self.__cb_url)\n        # \u83b7\u53d6\u8fd4\u56de\u7684json\u5b57\u7b26\u4e32\n        fundjson = json.loads(rep.text)\n        # \u683c\u5f0f\u5316\u8fd4\u56de\u7684json\u5b57\u7b26\u4e32\n        data = self.formatjisilujson(fundjson)\n        # \u8fc7\u6ee4\u5c0f\u4e8e\u6307\u5b9a\u4ea4\u6613\u91cf\u7684\u6570\u636e\n        if min_volume:\n            data = {\n                k: data[k]\n                for k in data\n                if float(data[k][\"volume\"]) > min_volume\n            }\n\n        self.__cb = data\n        return self.__cb", "language": "python", "code": "def cb(self, min_volume=0):\n        \"\"\"\u4ee5\u5b57\u5178\u5f62\u5f0f\u8fd4\u56deQDII\u6570\u636e\n        :param min_volume:\u6700\u5c0f\u4ea4\u6613\u91cf\uff0c\u5355\u4f4d\u4e07\u5143\n        \"\"\"\n        # \u6dfb\u52a0\u5f53\u524d\u7684ctime\n        self.__cb_url = self.__cb_url.format(ctime=int(time.time()))\n        # \u8bf7\u6c42\u6570\u636e\n        rep = requests.get(self.__cb_url)\n        # \u83b7\u53d6\u8fd4\u56de\u7684json\u5b57\u7b26\u4e32\n        fundjson = json.loads(rep.text)\n        # \u683c\u5f0f\u5316\u8fd4\u56de\u7684json\u5b57\u7b26\u4e32\n        data = self.formatjisilujson(fundjson)\n        # \u8fc7\u6ee4\u5c0f\u4e8e\u6307\u5b9a\u4ea4\u6613\u91cf\u7684\u6570\u636e\n        if min_volume:\n            data = {\n                k: data[k]\n                for k in data\n                if float(data[k][\"volume\"]) > min_volume\n            }\n\n        self.__cb = data\n        return self.__cb", "code_tokens": ["def", "cb", "(", "self", ",", "min_volume", "=", "0", ")", ":", "# \u6dfb\u52a0\u5f53\u524d\u7684ctime", "self", ".", "__cb_url", "=", "self", ".", "__cb_url", ".", "format", "(", "ctime", "=", "int", "(", "time", ".", "time", "(", ")", ")", ")", "# \u8bf7\u6c42\u6570\u636e", "rep", "=", "requests", ".", "get", "(", "self", ".", "__cb_url", ")", "# \u83b7\u53d6\u8fd4\u56de\u7684json\u5b57\u7b26\u4e32", "fundjson", "=", "json", ".", "loads", "(", "rep", ".", "text", ")", "# \u683c\u5f0f\u5316\u8fd4\u56de\u7684json\u5b57\u7b26\u4e32", "data", "=", "self", ".", "formatjisilujson", "(", "fundjson", ")", "# \u8fc7\u6ee4\u5c0f\u4e8e\u6307\u5b9a\u4ea4\u6613\u91cf\u7684\u6570\u636e", "if", "min_volume", ":", "data", "=", "{", "k", ":", "data", "[", "k", "]", "for", "k", "in", "data", "if", "float", "(", "data", "[", "k", "]", "[", "\"volume\"", "]", ")", ">", "min_volume", "}", "self", ".", "__cb", "=", "data", "return", "self", ".", "__cb"], "docstring": "\u4ee5\u5b57\u5178\u5f62\u5f0f\u8fd4\u56deQDII\u6570\u636e\n        :param min_volume:\u6700\u5c0f\u4ea4\u6613\u91cf\uff0c\u5355\u4f4d\u4e07\u5143", "docstring_tokens": ["\u4ee5\u5b57\u5178\u5f62\u5f0f\u8fd4\u56deQDII\u6570\u636e", ":", "param", "min_volume", ":", "\u6700\u5c0f\u4ea4\u6613\u91cf\uff0c\u5355\u4f4d\u4e07\u5143"], "sha": "a75820db4f05f5386e1c1024d05b0bfc1de6cbda", "url": "https://github.com/shidenggui/easyquotation/blob/a75820db4f05f5386e1c1024d05b0bfc1de6cbda/easyquotation/jsl.py#L418-L439", "partition": "train"}
{"repo": "nicolargo/glances", "path": "glances/plugins/glances_now.py", "func_name": "Plugin.update", "original_string": "def update(self):\n        \"\"\"Update current date/time.\"\"\"\n        # Had to convert it to string because datetime is not JSON serializable\n        self.stats = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        # Add the time zone (issue #1249 and issue #1337)\n        if 'tmzone' in localtime():\n            self.stats += ' {}'.format(localtime().tm_zone)\n        elif len(tzname) > 0:\n            self.stats += ' {}'.format(tzname[1])\n\n        return self.stats", "language": "python", "code": "def update(self):\n        \"\"\"Update current date/time.\"\"\"\n        # Had to convert it to string because datetime is not JSON serializable\n        self.stats = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        # Add the time zone (issue #1249 and issue #1337)\n        if 'tmzone' in localtime():\n            self.stats += ' {}'.format(localtime().tm_zone)\n        elif len(tzname) > 0:\n            self.stats += ' {}'.format(tzname[1])\n\n        return self.stats", "code_tokens": ["def", "update", "(", "self", ")", ":", "# Had to convert it to string because datetime is not JSON serializable", "self", ".", "stats", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d %H:%M:%S'", ")", "# Add the time zone (issue #1249 and issue #1337)", "if", "'tmzone'", "in", "localtime", "(", ")", ":", "self", ".", "stats", "+=", "' {}'", ".", "format", "(", "localtime", "(", ")", ".", "tm_zone", ")", "elif", "len", "(", "tzname", ")", ">", "0", ":", "self", ".", "stats", "+=", "' {}'", ".", "format", "(", "tzname", "[", "1", "]", ")", "return", "self", ".", "stats"], "docstring": "Update current date/time.", "docstring_tokens": ["Update", "current", "date", "/", "time", "."], "sha": "5bd4d587a736e0d2b03170b56926841d2a3eb7ee", "url": "https://github.com/nicolargo/glances/blob/5bd4d587a736e0d2b03170b56926841d2a3eb7ee/glances/plugins/glances_now.py#L48-L58", "partition": "train"}
{"repo": "nicolargo/glances", "path": "glances/exports/glances_export.py", "func_name": "GlancesExport.update", "original_string": "def update(self, stats):\n        \"\"\"Update stats to a server.\n\n        The method builds two lists: names and values\n        and calls the export method to export the stats.\n\n        Note: this class can be overwrite (for example in CSV and Graph).\n        \"\"\"\n        if not self.export_enable:\n            return False\n\n        # Get all the stats & limits\n        all_stats = stats.getAllExportsAsDict(plugin_list=self.plugins_to_export())\n        all_limits = stats.getAllLimitsAsDict(plugin_list=self.plugins_to_export())\n\n        # Loop over plugins to export\n        for plugin in self.plugins_to_export():\n            if isinstance(all_stats[plugin], dict):\n                all_stats[plugin].update(all_limits[plugin])\n            elif isinstance(all_stats[plugin], list):\n                # TypeError: string indices must be integers (Network plugin) #1054\n                for i in all_stats[plugin]:\n                    i.update(all_limits[plugin])\n            else:\n                continue\n            export_names, export_values = self.__build_export(all_stats[plugin])\n            self.export(plugin, export_names, export_values)\n\n        return True", "language": "python", "code": "def update(self, stats):\n        \"\"\"Update stats to a server.\n\n        The method builds two lists: names and values\n        and calls the export method to export the stats.\n\n        Note: this class can be overwrite (for example in CSV and Graph).\n        \"\"\"\n        if not self.export_enable:\n            return False\n\n        # Get all the stats & limits\n        all_stats = stats.getAllExportsAsDict(plugin_list=self.plugins_to_export())\n        all_limits = stats.getAllLimitsAsDict(plugin_list=self.plugins_to_export())\n\n        # Loop over plugins to export\n        for plugin in self.plugins_to_export():\n            if isinstance(all_stats[plugin], dict):\n                all_stats[plugin].update(all_limits[plugin])\n            elif isinstance(all_stats[plugin], list):\n                # TypeError: string indices must be integers (Network plugin) #1054\n                for i in all_stats[plugin]:\n                    i.update(all_limits[plugin])\n            else:\n                continue\n            export_names, export_values = self.__build_export(all_stats[plugin])\n            self.export(plugin, export_names, export_values)\n\n        return True", "code_tokens": ["def", "update", "(", "self", ",", "stats", ")", ":", "if", "not", "self", ".", "export_enable", ":", "return", "False", "# Get all the stats & limits", "all_stats", "=", "stats", ".", "getAllExportsAsDict", "(", "plugin_list", "=", "self", ".", "plugins_to_export", "(", ")", ")", "all_limits", "=", "stats", ".", "getAllLimitsAsDict", "(", "plugin_list", "=", "self", ".", "plugins_to_export", "(", ")", ")", "# Loop over plugins to export", "for", "plugin", "in", "self", ".", "plugins_to_export", "(", ")", ":", "if", "isinstance", "(", "all_stats", "[", "plugin", "]", ",", "dict", ")", ":", "all_stats", "[", "plugin", "]", ".", "update", "(", "all_limits", "[", "plugin", "]", ")", "elif", "isinstance", "(", "all_stats", "[", "plugin", "]", ",", "list", ")", ":", "# TypeError: string indices must be integers (Network plugin) #1054", "for", "i", "in", "all_stats", "[", "plugin", "]", ":", "i", ".", "update", "(", "all_limits", "[", "plugin", "]", ")", "else", ":", "continue", "export_names", ",", "export_values", "=", "self", ".", "__build_export", "(", "all_stats", "[", "plugin", "]", ")", "self", ".", "export", "(", "plugin", ",", "export_names", ",", "export_values", ")", "return", "True"], "docstring": "Update stats to a server.\n\n        The method builds two lists: names and values\n        and calls the export method to export the stats.\n\n        Note: this class can be overwrite (for example in CSV and Graph).", "docstring_tokens": ["Update", "stats", "to", "a", "server", "."], "sha": "5bd4d587a736e0d2b03170b56926841d2a3eb7ee", "url": "https://github.com/nicolargo/glances/blob/5bd4d587a736e0d2b03170b56926841d2a3eb7ee/glances/exports/glances_export.py#L157-L185", "partition": "train"}
{"repo": "darknessomi/musicbox", "path": "NEMbox/player.py", "func_name": "Player.start_playing", "original_string": "def start_playing(self, on_exit, args):\n        \"\"\"\n        Runs the given args in subprocess.Popen, and then calls the function\n        on_exit when the subprocess completes.\n        on_exit is a callable object, and args is a lists/tuple of args\n        that would give to subprocess.Popen.\n        \"\"\"\n        # log.debug(\"%s,%s,%s\" % (args['song_id'], args['song_name'], args['mp3_url']))\n        if \"cache\" in args.keys() and os.path.isfile(args[\"cache\"]):\n            thread = threading.Thread(\n                target=self.run_mpg123, args=(on_exit, args[\"cache\"])\n            )\n        else:\n            new_url = NetEase().songs_url([args[\"song_id\"]])[0][\"url\"]  #\u4f7f\u7528\u65b0\u5730\u5740\n            if  not new_url:    #\u5982\u679c\u6ca1\u6709\u83b7\u5f97\u65b0\u5730\u5740\n                new_url = args[\"mp3_url\"]  #\u4f7f\u7528\u8001\u5730\u5740\u4f20\u7ed9mpg123\n            thread = threading.Thread(\n                target=self.run_mpg123,\n                args=(on_exit, new_url, args[\"expires\"], args[\"get_time\"]),\n            )\n            cache_thread = threading.Thread(\n                target=self.download_song,\n                args=(\n                    args[\"song_id\"],\n                    args[\"song_name\"],\n                    args[\"artist\"],\n                    args[\"mp3_url\"],\n                ),\n            )\n            cache_thread.start()\n\n        thread.start()\n        lyric_download_thread = threading.Thread(target=self.download_lyric)\n        lyric_download_thread.start()\n        tlyric_download_thread = threading.Thread(\n            target=self.download_lyric, args=(True,)\n        )\n        tlyric_download_thread.start()\n        # returns immediately after the thread starts\n        return thread", "language": "python", "code": "def start_playing(self, on_exit, args):\n        \"\"\"\n        Runs the given args in subprocess.Popen, and then calls the function\n        on_exit when the subprocess completes.\n        on_exit is a callable object, and args is a lists/tuple of args\n        that would give to subprocess.Popen.\n        \"\"\"\n        # log.debug(\"%s,%s,%s\" % (args['song_id'], args['song_name'], args['mp3_url']))\n        if \"cache\" in args.keys() and os.path.isfile(args[\"cache\"]):\n            thread = threading.Thread(\n                target=self.run_mpg123, args=(on_exit, args[\"cache\"])\n            )\n        else:\n            new_url = NetEase().songs_url([args[\"song_id\"]])[0][\"url\"]  #\u4f7f\u7528\u65b0\u5730\u5740\n            if  not new_url:    #\u5982\u679c\u6ca1\u6709\u83b7\u5f97\u65b0\u5730\u5740\n                new_url = args[\"mp3_url\"]  #\u4f7f\u7528\u8001\u5730\u5740\u4f20\u7ed9mpg123\n            thread = threading.Thread(\n                target=self.run_mpg123,\n                args=(on_exit, new_url, args[\"expires\"], args[\"get_time\"]),\n            )\n            cache_thread = threading.Thread(\n                target=self.download_song,\n                args=(\n                    args[\"song_id\"],\n                    args[\"song_name\"],\n                    args[\"artist\"],\n                    args[\"mp3_url\"],\n                ),\n            )\n            cache_thread.start()\n\n        thread.start()\n        lyric_download_thread = threading.Thread(target=self.download_lyric)\n        lyric_download_thread.start()\n        tlyric_download_thread = threading.Thread(\n            target=self.download_lyric, args=(True,)\n        )\n        tlyric_download_thread.start()\n        # returns immediately after the thread starts\n        return thread", "code_tokens": ["def", "start_playing", "(", "self", ",", "on_exit", ",", "args", ")", ":", "# log.debug(\"%s,%s,%s\" % (args['song_id'], args['song_name'], args['mp3_url']))", "if", "\"cache\"", "in", "args", ".", "keys", "(", ")", "and", "os", ".", "path", ".", "isfile", "(", "args", "[", "\"cache\"", "]", ")", ":", "thread", "=", "threading", ".", "Thread", "(", "target", "=", "self", ".", "run_mpg123", ",", "args", "=", "(", "on_exit", ",", "args", "[", "\"cache\"", "]", ")", ")", "else", ":", "new_url", "=", "NetEase", "(", ")", ".", "songs_url", "(", "[", "args", "[", "\"song_id\"", "]", "]", ")", "[", "0", "]", "[", "\"url\"", "]", "#\u4f7f\u7528\u65b0\u5730\u5740", "if", "not", "new_url", ":", "#\u5982\u679c\u6ca1\u6709\u83b7\u5f97\u65b0\u5730\u5740", "new_url", "=", "args", "[", "\"mp3_url\"", "]", "#\u4f7f\u7528\u8001\u5730\u5740\u4f20\u7ed9mpg123", "thread", "=", "threading", ".", "Thread", "(", "target", "=", "self", ".", "run_mpg123", ",", "args", "=", "(", "on_exit", ",", "new_url", ",", "args", "[", "\"expires\"", "]", ",", "args", "[", "\"get_time\"", "]", ")", ",", ")", "cache_thread", "=", "threading", ".", "Thread", "(", "target", "=", "self", ".", "download_song", ",", "args", "=", "(", "args", "[", "\"song_id\"", "]", ",", "args", "[", "\"song_name\"", "]", ",", "args", "[", "\"artist\"", "]", ",", "args", "[", "\"mp3_url\"", "]", ",", ")", ",", ")", "cache_thread", ".", "start", "(", ")", "thread", ".", "start", "(", ")", "lyric_download_thread", "=", "threading", ".", "Thread", "(", "target", "=", "self", ".", "download_lyric", ")", "lyric_download_thread", ".", "start", "(", ")", "tlyric_download_thread", "=", "threading", ".", "Thread", "(", "target", "=", "self", ".", "download_lyric", ",", "args", "=", "(", "True", ",", ")", ")", "tlyric_download_thread", ".", "start", "(", ")", "# returns immediately after the thread starts", "return", "thread"], "docstring": "Runs the given args in subprocess.Popen, and then calls the function\n        on_exit when the subprocess completes.\n        on_exit is a callable object, and args is a lists/tuple of args\n        that would give to subprocess.Popen.", "docstring_tokens": ["Runs", "the", "given", "args", "in", "subprocess", ".", "Popen", "and", "then", "calls", "the", "function", "on_exit", "when", "the", "subprocess", "completes", ".", "on_exit", "is", "a", "callable", "object", "and", "args", "is", "a", "lists", "/", "tuple", "of", "args", "that", "would", "give", "to", "subprocess", ".", "Popen", "."], "sha": "9d58201fa2fefcee99d34d2c7fcc7a57d1e45937", "url": "https://github.com/darknessomi/musicbox/blob/9d58201fa2fefcee99d34d2c7fcc7a57d1e45937/NEMbox/player.py#L309-L348", "partition": "train"}
{"repo": "ninja-build/ninja", "path": "misc/ninja_syntax.py", "func_name": "expand", "original_string": "def expand(string, vars, local_vars={}):\n    \"\"\"Expand a string containing $vars as Ninja would.\n\n    Note: doesn't handle the full Ninja variable syntax, but it's enough\n    to make configure.py's use of it work.\n    \"\"\"\n    def exp(m):\n        var = m.group(1)\n        if var == '$':\n            return '$'\n        return local_vars.get(var, vars.get(var, ''))\n    return re.sub(r'\\$(\\$|\\w*)', exp, string)", "language": "python", "code": "def expand(string, vars, local_vars={}):\n    \"\"\"Expand a string containing $vars as Ninja would.\n\n    Note: doesn't handle the full Ninja variable syntax, but it's enough\n    to make configure.py's use of it work.\n    \"\"\"\n    def exp(m):\n        var = m.group(1)\n        if var == '$':\n            return '$'\n        return local_vars.get(var, vars.get(var, ''))\n    return re.sub(r'\\$(\\$|\\w*)', exp, string)", "code_tokens": ["def", "expand", "(", "string", ",", "vars", ",", "local_vars", "=", "{", "}", ")", ":", "def", "exp", "(", "m", ")", ":", "var", "=", "m", ".", "group", "(", "1", ")", "if", "var", "==", "'$'", ":", "return", "'$'", "return", "local_vars", ".", "get", "(", "var", ",", "vars", ".", "get", "(", "var", ",", "''", ")", ")", "return", "re", ".", "sub", "(", "r'\\$(\\$|\\w*)'", ",", "exp", ",", "string", ")"], "docstring": "Expand a string containing $vars as Ninja would.\n\n    Note: doesn't handle the full Ninja variable syntax, but it's enough\n    to make configure.py's use of it work.", "docstring_tokens": ["Expand", "a", "string", "containing", "$vars", "as", "Ninja", "would", "."], "sha": "2e64645749ff91eff2f999f03f55da360ae5913d", "url": "https://github.com/ninja-build/ninja/blob/2e64645749ff91eff2f999f03f55da360ae5913d/misc/ninja_syntax.py#L172-L183", "partition": "train"}
{"repo": "nteract/nteract", "path": "applications/jupyter-extension/nteract_on_jupyter/semver.py", "func_name": "semver", "original_string": "def semver(version, loose):\n    if isinstance(version, SemVer):\n        if version.loose == loose:\n            return version\n        else:\n            version = version.version\n    elif not isinstance(version, string_type):  # xxx:\n        raise ValueError(\"Invalid Version: {}\".format(version))\n\n    \"\"\"\n    if (!(this instanceof SemVer))\n       return new SemVer(version, loose);\n    \"\"\"\n    return SemVer(version, loose)", "language": "python", "code": "def semver(version, loose):\n    if isinstance(version, SemVer):\n        if version.loose == loose:\n            return version\n        else:\n            version = version.version\n    elif not isinstance(version, string_type):  # xxx:\n        raise ValueError(\"Invalid Version: {}\".format(version))\n\n    \"\"\"\n    if (!(this instanceof SemVer))\n       return new SemVer(version, loose);\n    \"\"\"\n    return SemVer(version, loose)", "code_tokens": ["def", "semver", "(", "version", ",", "loose", ")", ":", "if", "isinstance", "(", "version", ",", "SemVer", ")", ":", "if", "version", ".", "loose", "==", "loose", ":", "return", "version", "else", ":", "version", "=", "version", ".", "version", "elif", "not", "isinstance", "(", "version", ",", "string_type", ")", ":", "# xxx:", "raise", "ValueError", "(", "\"Invalid Version: {}\"", ".", "format", "(", "version", ")", ")", "return", "SemVer", "(", "version", ",", "loose", ")"], "docstring": "if (!(this instanceof SemVer))\n       return new SemVer(version, loose);", "docstring_tokens": ["if", "(", "!", "(", "this", "instanceof", "SemVer", "))", "return", "new", "SemVer", "(", "version", "loose", ")", ";"], "sha": "f63a01c4067b05541ebddd3932345d7102ea3e5b", "url": "https://github.com/nteract/nteract/blob/f63a01c4067b05541ebddd3932345d7102ea3e5b/applications/jupyter-extension/nteract_on_jupyter/semver.py#L357-L370", "partition": "train"}
{"repo": "cjhutto/vaderSentiment", "path": "additional_resources/build_emoji_lexicon.py", "func_name": "get_list_from_file", "original_string": "def get_list_from_file(file_name):\n    \"\"\"read the lines from a file into a list\"\"\"\n    with open(file_name, mode='r', encoding='utf-8') as f1:\n        lst = f1.readlines()\n    return lst", "language": "python", "code": "def get_list_from_file(file_name):\n    \"\"\"read the lines from a file into a list\"\"\"\n    with open(file_name, mode='r', encoding='utf-8') as f1:\n        lst = f1.readlines()\n    return lst", "code_tokens": ["def", "get_list_from_file", "(", "file_name", ")", ":", "with", "open", "(", "file_name", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f1", ":", "lst", "=", "f1", ".", "readlines", "(", ")", "return", "lst"], "docstring": "read the lines from a file into a list", "docstring_tokens": ["read", "the", "lines", "from", "a", "file", "into", "a", "list"], "sha": "cfc2bce747afb2c49799c1de1dcf517358948d71", "url": "https://github.com/cjhutto/vaderSentiment/blob/cfc2bce747afb2c49799c1de1dcf517358948d71/additional_resources/build_emoji_lexicon.py#L7-L11", "partition": "train"}
{"repo": "buildbot/buildbot", "path": "worker/buildbot_worker/base.py", "func_name": "WorkerForBuilderBase.remote_startCommand", "original_string": "def remote_startCommand(self, stepref, stepId, command, args):\n        \"\"\"\n        This gets invoked by L{buildbot.process.step.RemoteCommand.start}, as\n        part of various master-side BuildSteps, to start various commands\n        that actually do the build. I return nothing. Eventually I will call\n        .commandComplete() to notify the master-side RemoteCommand that I'm\n        done.\n        \"\"\"\n        stepId = decode(stepId)\n        command = decode(command)\n        args = decode(args)\n\n        self.activity()\n\n        if self.command:\n            log.msg(\"leftover command, dropping it\")\n            self.stopCommand()\n\n        try:\n            factory = registry.getFactory(command)\n        except KeyError:\n            raise UnknownCommand(u\"unrecognized WorkerCommand '{0}'\".format(command))\n        self.command = factory(self, stepId, args)\n\n        log.msg(u\" startCommand:{0} [id {1}]\".format(command, stepId))\n        self.remoteStep = stepref\n        self.remoteStep.notifyOnDisconnect(self.lostRemoteStep)\n        d = self.command.doStart()\n        d.addCallback(lambda res: None)\n        d.addBoth(self.commandComplete)\n        return None", "language": "python", "code": "def remote_startCommand(self, stepref, stepId, command, args):\n        \"\"\"\n        This gets invoked by L{buildbot.process.step.RemoteCommand.start}, as\n        part of various master-side BuildSteps, to start various commands\n        that actually do the build. I return nothing. Eventually I will call\n        .commandComplete() to notify the master-side RemoteCommand that I'm\n        done.\n        \"\"\"\n        stepId = decode(stepId)\n        command = decode(command)\n        args = decode(args)\n\n        self.activity()\n\n        if self.command:\n            log.msg(\"leftover command, dropping it\")\n            self.stopCommand()\n\n        try:\n            factory = registry.getFactory(command)\n        except KeyError:\n            raise UnknownCommand(u\"unrecognized WorkerCommand '{0}'\".format(command))\n        self.command = factory(self, stepId, args)\n\n        log.msg(u\" startCommand:{0} [id {1}]\".format(command, stepId))\n        self.remoteStep = stepref\n        self.remoteStep.notifyOnDisconnect(self.lostRemoteStep)\n        d = self.command.doStart()\n        d.addCallback(lambda res: None)\n        d.addBoth(self.commandComplete)\n        return None", "code_tokens": ["def", "remote_startCommand", "(", "self", ",", "stepref", ",", "stepId", ",", "command", ",", "args", ")", ":", "stepId", "=", "decode", "(", "stepId", ")", "command", "=", "decode", "(", "command", ")", "args", "=", "decode", "(", "args", ")", "self", ".", "activity", "(", ")", "if", "self", ".", "command", ":", "log", ".", "msg", "(", "\"leftover command, dropping it\"", ")", "self", ".", "stopCommand", "(", ")", "try", ":", "factory", "=", "registry", ".", "getFactory", "(", "command", ")", "except", "KeyError", ":", "raise", "UnknownCommand", "(", "u\"unrecognized WorkerCommand '{0}'\"", ".", "format", "(", "command", ")", ")", "self", ".", "command", "=", "factory", "(", "self", ",", "stepId", ",", "args", ")", "log", ".", "msg", "(", "u\" startCommand:{0} [id {1}]\"", ".", "format", "(", "command", ",", "stepId", ")", ")", "self", ".", "remoteStep", "=", "stepref", "self", ".", "remoteStep", ".", "notifyOnDisconnect", "(", "self", ".", "lostRemoteStep", ")", "d", "=", "self", ".", "command", ".", "doStart", "(", ")", "d", ".", "addCallback", "(", "lambda", "res", ":", "None", ")", "d", ".", "addBoth", "(", "self", ".", "commandComplete", ")", "return", "None"], "docstring": "This gets invoked by L{buildbot.process.step.RemoteCommand.start}, as\n        part of various master-side BuildSteps, to start various commands\n        that actually do the build. I return nothing. Eventually I will call\n        .commandComplete() to notify the master-side RemoteCommand that I'm\n        done.", "docstring_tokens": ["This", "gets", "invoked", "by", "L", "{", "buildbot", ".", "process", ".", "step", ".", "RemoteCommand", ".", "start", "}", "as", "part", "of", "various", "master", "-", "side", "BuildSteps", "to", "start", "various", "commands", "that", "actually", "do", "the", "build", ".", "I", "return", "nothing", ".", "Eventually", "I", "will", "call", ".", "commandComplete", "()", "to", "notify", "the", "master", "-", "side", "RemoteCommand", "that", "I", "m", "done", "."], "sha": "5df3cfae6d760557d99156633c32b1822a1e130c", "url": "https://github.com/buildbot/buildbot/blob/5df3cfae6d760557d99156633c32b1822a1e130c/worker/buildbot_worker/base.py#L126-L156", "partition": "train"}
{"repo": "ktbyers/netmiko", "path": "netmiko/extreme/extreme_netiron.py", "func_name": "ExtremeNetironBase.save_config", "original_string": "def save_config(self, cmd=\"write memory\", confirm=False, confirm_response=\"\"):\n        \"\"\"Save Config\"\"\"\n        return super(ExtremeNetironBase, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )", "language": "python", "code": "def save_config(self, cmd=\"write memory\", confirm=False, confirm_response=\"\"):\n        \"\"\"Save Config\"\"\"\n        return super(ExtremeNetironBase, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )", "code_tokens": ["def", "save_config", "(", "self", ",", "cmd", "=", "\"write memory\"", ",", "confirm", "=", "False", ",", "confirm_response", "=", "\"\"", ")", ":", "return", "super", "(", "ExtremeNetironBase", ",", "self", ")", ".", "save_config", "(", "cmd", "=", "cmd", ",", "confirm", "=", "confirm", ",", "confirm_response", "=", "confirm_response", ")"], "docstring": "Save Config", "docstring_tokens": ["Save", "Config"], "sha": "54e6116c0b4664de2123081937e0a9a27bdfdfea", "url": "https://github.com/ktbyers/netmiko/blob/54e6116c0b4664de2123081937e0a9a27bdfdfea/netmiko/extreme/extreme_netiron.py#L6-L10", "partition": "train"}
{"repo": "cloudtools/troposphere", "path": "examples/ElastiCacheRedis.py", "func_name": "main", "original_string": "def main():\n    \"\"\"\n    Create a ElastiCache Redis Node and EC2 Instance\n    \"\"\"\n\n    template = Template()\n\n    # Description\n    template.add_description(\n        'AWS CloudFormation Sample Template ElastiCache_Redis:'\n        'Sample template showing how to create an Amazon'\n        'ElastiCache Redis Cluster. **WARNING** This template'\n        'creates an Amazon EC2 Instance and an Amazon ElastiCache'\n        'Cluster. You will be billed for the AWS resources used'\n        'if you create a stack from this template.')\n\n    # Mappings\n    template.add_mapping('AWSInstanceType2Arch', {\n        't1.micro':     {'Arch': 'PV64'},\n        't2.micro':     {'Arch': 'HVM64'},\n        't2.small':     {'Arch': 'HVM64'},\n        't2.medium':    {'Arch': 'HVM64'},\n        'm1.small':     {'Arch': 'PV64'},\n        'm1.medium':    {'Arch': 'PV64'},\n        'm1.large':     {'Arch': 'PV64'},\n        'm1.xlarge':    {'Arch': 'PV64'},\n        'm2.xlarge':    {'Arch': 'PV64'},\n        'm2.2xlarge':   {'Arch': 'PV64'},\n        'm2.4xlarge':   {'Arch': 'PV64'},\n        'm3.medium':    {'Arch': 'HVM64'},\n        'm3.large':     {'Arch': 'HVM64'},\n        'm3.xlarge':    {'Arch': 'HVM64'},\n        'm3.2xlarge':   {'Arch': 'HVM64'},\n        'c1.medium':    {'Arch': 'PV64'},\n        'c1.xlarge':    {'Arch': 'PV64'},\n        'c3.large':     {'Arch': 'HVM64'},\n        'c3.xlarge':    {'Arch': 'HVM64'},\n        'c3.2xlarge':   {'Arch': 'HVM64'},\n        'c3.4xlarge':   {'Arch': 'HVM64'},\n        'c3.8xlarge':   {'Arch': 'HVM64'},\n        'c4.large':     {'Arch': 'HVM64'},\n        'c4.xlarge':    {'Arch': 'HVM64'},\n        'c4.2xlarge':   {'Arch': 'HVM64'},\n        'c4.4xlarge':   {'Arch': 'HVM64'},\n        'c4.8xlarge':   {'Arch': 'HVM64'},\n        'g2.2xlarge':   {'Arch': 'HVMG2'},\n        'r3.large':     {'Arch': 'HVM64'},\n        'r3.xlarge':    {'Arch': 'HVM64'},\n        'r3.2xlarge':   {'Arch': 'HVM64'},\n        'r3.4xlarge':   {'Arch': 'HVM64'},\n        'r3.8xlarge':   {'Arch': 'HVM64'},\n        'i2.xlarge':    {'Arch': 'HVM64'},\n        'i2.2xlarge':   {'Arch': 'HVM64'},\n        'i2.4xlarge':   {'Arch': 'HVM64'},\n        'i2.8xlarge':   {'Arch': 'HVM64'},\n        'd2.xlarge':    {'Arch': 'HVM64'},\n        'd2.2xlarge':   {'Arch': 'HVM64'},\n        'd2.4xlarge':   {'Arch': 'HVM64'},\n        'd2.8xlarge':   {'Arch': 'HVM64'},\n        'hi1.4xlarge':  {'Arch': 'HVM64'},\n        'hs1.8xlarge':  {'Arch': 'HVM64'},\n        'cr1.8xlarge':  {'Arch': 'HVM64'},\n        'cc2.8xlarge':  {'Arch': 'HVM64'}\n        })\n\n    template.add_mapping('AWSRegionArch2AMI', {\n        'us-east-1': {'PV64': 'ami-0f4cfd64',\n                      'HVM64': 'ami-0d4cfd66',\n                      'HVMG2': 'ami-5b05ba30'},\n        'us-west-2': {'PV64': 'ami-d3c5d1e3',\n                      'HVM64': 'ami-d5c5d1e5',\n                      'HVMG2': 'ami-a9d6c099'},\n        'us-west-1': {'PV64': 'ami-85ea13c1',\n                      'HVM64': 'ami-87ea13c3',\n                      'HVMG2': 'ami-37827a73'},\n        'eu-west-1': {'PV64': 'ami-d6d18ea1',\n                      'HVM64': 'ami-e4d18e93',\n                      'HVMG2': 'ami-72a9f105'},\n        'eu-central-1': {'PV64': 'ami-a4b0b7b9',\n                         'HVM64': 'ami-a6b0b7bb',\n                         'HVMG2': 'ami-a6c9cfbb'},\n        'ap-northeast-1': {'PV64': 'ami-1a1b9f1a',\n                           'HVM64': 'ami-1c1b9f1c',\n                           'HVMG2': 'ami-f644c4f6'},\n        'ap-southeast-1': {'PV64': 'ami-d24b4280',\n                           'HVM64': 'ami-d44b4286',\n                           'HVMG2': 'ami-12b5bc40'},\n        'ap-southeast-2': {'PV64': 'ami-ef7b39d5',\n                           'HVM64': 'ami-db7b39e1',\n                           'HVMG2': 'ami-b3337e89'},\n        'sa-east-1': {'PV64': 'ami-5b098146',\n                      'HVM64': 'ami-55098148',\n                      'HVMG2': 'NOT_SUPPORTED'},\n        'cn-north-1': {'PV64': 'ami-bec45887',\n                       'HVM64': 'ami-bcc45885',\n                       'HVMG2': 'NOT_SUPPORTED'}\n        })\n\n    template.add_mapping('Region2Principal', {\n        'us-east-1': {'EC2Principal': 'ec2.amazonaws.com',\n                      'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'us-west-2': {'EC2Principal': 'ec2.amazonaws.com',\n                      'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'us-west-1': {'EC2Principal': 'ec2.amazonaws.com',\n                      'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'eu-west-1': {'EC2Principal': 'ec2.amazonaws.com',\n                      'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'ap-southeast-1': {'EC2Principal': 'ec2.amazonaws.com',\n                           'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'ap-northeast-1': {'EC2Principal': 'ec2.amazonaws.com',\n                           'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'ap-southeast-2': {'EC2Principal': 'ec2.amazonaws.com',\n                           'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'sa-east-1': {'EC2Principal': 'ec2.amazonaws.com',\n                      'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'cn-north-1': {'EC2Principal': 'ec2.amazonaws.com.cn',\n                       'OpsWorksPrincipal': 'opsworks.amazonaws.com.cn'},\n        'eu-central-1': {'EC2Principal': 'ec2.amazonaws.com',\n                         'OpsWorksPrincipal': 'opsworks.amazonaws.com'}\n        })\n\n    # Parameters\n    cachenodetype = template.add_parameter(Parameter(\n        'ClusterNodeType',\n        Description='The compute and memory capacity of the nodes in the Redis'\n                    ' Cluster',\n        Type='String',\n        Default='cache.m1.small',\n        AllowedValues=['cache.m1.small',\n                       'cache.m1.large',\n                       'cache.m1.xlarge',\n                       'cache.m2.xlarge',\n                       'cache.m2.2xlarge',\n                       'cache.m2.4xlarge',\n                       'cache.c1.xlarge'],\n        ConstraintDescription='must select a valid Cache Node type.',\n        ))\n\n    instancetype = template.add_parameter(Parameter(\n        'InstanceType',\n        Description='WebServer EC2 instance type',\n        Type='String',\n        Default='t2.micro',\n        AllowedValues=['t1.micro',\n                       't2.micro',\n                       't2.small',\n                       't2.medium',\n                       'm1.small',\n                       'm1.medium',\n                       'm1.large',\n                       'm1.xlarge',\n                       'm2.xlarge',\n                       'm2.2xlarge',\n                       'm2.4xlarge',\n                       'm3.medium',\n                       'm3.large',\n                       'm3.xlarge',\n                       'm3.2xlarge',\n                       'c1.medium',\n                       'c1.xlarge',\n                       'c3.large',\n                       'c3.xlarge',\n                       'c3.2xlarge',\n                       'c3.4xlarge',\n                       'c3.8xlarge',\n                       'c4.large',\n                       'c4.xlarge',\n                       'c4.2xlarge',\n                       'c4.4xlarge',\n                       'c4.8xlarge',\n                       'g2.2xlarge',\n                       'r3.large',\n                       'r3.xlarge',\n                       'r3.2xlarge',\n                       'r3.4xlarge',\n                       'r3.8xlarge',\n                       'i2.xlarge',\n                       'i2.2xlarge',\n                       'i2.4xlarge',\n                       'i2.8xlarge',\n                       'd2.xlarge',\n                       'd2.2xlarge',\n                       'd2.4xlarge',\n                       'd2.8xlarge',\n                       'hi1.4xlarge',\n                       'hs1.8xlarge',\n                       'cr1.8xlarge',\n                       'cc2.8xlarge',\n                       'cg1.4xlarge'],\n        ConstraintDescription='must be a valid EC2 instance type.',\n        ))\n\n    keyname = template.add_parameter(Parameter(\n        'KeyName',\n        Description='Name of an existing EC2 KeyPair to enable SSH access'\n                    ' to the instance',\n        Type='AWS::EC2::KeyPair::KeyName',\n        ConstraintDescription='must be the name of an existing EC2 KeyPair.',\n        ))\n\n    sshlocation = template.add_parameter(Parameter(\n        'SSHLocation',\n        Description='The IP address range that can be used to SSH to'\n                    ' the EC2 instances',\n        Type='String',\n        MinLength='9',\n        MaxLength='18',\n        Default='0.0.0.0/0',\n        AllowedPattern='(\\\\d{1,3})\\\\.(\\\\d{1,3})\\\\.'\n                       '(\\\\d{1,3})\\\\.(\\\\d{1,3})/(\\\\d{1,2})',\n        ConstraintDescription='must be a valid IP CIDR range of the'\n                              ' form x.x.x.x/x.'\n        ))\n\n    # Resources\n    webserverrole = template.add_resource(iam.Role(\n        'WebServerRole',\n        AssumeRolePolicyDocument=Policy(\n            Statement=[\n                Statement(\n                    Effect=Allow,\n                    Action=[AssumeRole],\n                    Principal=Principal('Service',\n                                        [FindInMap('Region2Principal',\n                                                   Ref('AWS::Region'),\n                                                   'EC2Principal')]),\n                    )\n                ]\n            ),\n        Path='/',\n    ))\n\n    template.add_resource(iam.PolicyType(\n        'WebServerRolePolicy',\n        PolicyName='WebServerRole',\n        PolicyDocument=awacs.aws.Policy(\n            Statement=[awacs.aws.Statement(\n                Action=[awacs.aws.Action(\"elasticache\",\n                        \"DescribeCacheClusters\")],\n                Resource=[\"*\"],\n                Effect=awacs.aws.Allow\n            )]\n        ),\n        Roles=[Ref(webserverrole)],\n    ))\n\n    webserverinstanceprofile = template.add_resource(iam.InstanceProfile(\n        'WebServerInstanceProfile',\n        Path='/',\n        Roles=[Ref(webserverrole)],\n    ))\n\n    webserversg = template.add_resource(ec2.SecurityGroup(\n        'WebServerSecurityGroup',\n        GroupDescription='Enable HTTP and SSH access',\n        SecurityGroupIngress=[\n            ec2.SecurityGroupRule(\n                IpProtocol='tcp',\n                FromPort='22',\n                ToPort='22',\n                CidrIp=Ref(sshlocation),\n                ),\n            ec2.SecurityGroupRule(\n                IpProtocol='tcp',\n                FromPort='80',\n                ToPort='80',\n                CidrIp='0.0.0.0/0',\n                )\n            ]\n        ))\n\n    webserverinstance = template.add_resource(ec2.Instance(\n        'WebServerInstance',\n        Metadata=cloudformation.Metadata(\n            cloudformation.Init({\n                'config': cloudformation.InitConfig(\n                    packages={\n                        'yum': {\n                            'httpd':     [],\n                            'php':       [],\n                            'php-devel': [],\n                            'gcc':       [],\n                            'make':      []\n                            }\n                        },\n\n                    files=cloudformation.InitFiles({\n                        '/var/www/html/index.php': cloudformation.InitFile(\n                            content=Join('', [\n                                '<?php\\n',\n                                'echo \\\"<h1>AWS CloudFormation sample'\n                                ' application for Amazon ElastiCache'\n                                ' Redis Cluster</h1>\\\";\\n',\n                                '\\n',\n                                '$cluster_config = json_decode('\n                                'file_get_contents(\\'/tmp/cacheclusterconfig\\''\n                                '), true);\\n',\n                                '$endpoint = $cluster_config[\\'CacheClusters'\n                                '\\'][0][\\'CacheNodes\\'][0][\\'Endpoint\\'][\\'Add'\n                                'ress\\'];\\n',\n                                '$port = $cluster_config[\\'CacheClusters\\'][0]'\n                                '[\\'CacheNodes\\'][0][\\'Endpoint\\'][\\'Port\\'];'\n                                '\\n',\n                                '\\n',\n                                'echo \\\"<p>Connecting to Redis Cache Cluster '\n                                'node \\'{$endpoint}\\' on port {$port}</p>\\\";'\n                                '\\n',\n                                '\\n',\n                                '$redis=new Redis();\\n',\n                                '$redis->connect($endpoint, $port);\\n',\n                                '$redis->set(\\'testkey\\', \\'Hello World!\\');'\n                                '\\n',\n                                '$return = $redis->get(\\'testkey\\');\\n',\n                                '\\n',\n                                'echo \\\"<p>Retrieved value: $return</p>\\\";'\n                                '\\n',\n                                '?>\\n'\n                                ]),\n                            mode='000644',\n                            owner='apache',\n                            group='apache'\n                            ),\n                        '/etc/cron.d/get_cluster_config':\n                            cloudformation.InitFile(\n                                content='*/5 * * * * root'\n                                        ' /usr/local/bin/get_cluster_config',\n                                mode='000644',\n                                owner='root',\n                                group='root'\n                                ),\n                        '/usr/local/bin/get_cluster_config':\n                            cloudformation.InitFile(\n                                content=Join('', [\n                                    '#! /bin/bash\\n',\n                                    'aws elasticache describe-cache-clusters ',\n                                    '         --cache-cluster-id ',\n                                    Ref('RedisCluster'),\n                                    '         --show-cache-node-info'\n                                    ' --region ', Ref('AWS::Region'),\n                                    ' > /tmp/cacheclusterconfig\\n'\n                                    ]),\n                                mode='000755',\n                                owner='root',\n                                group='root'\n                                ),\n                        '/usr/local/bin/install_phpredis':\n                            cloudformation.InitFile(\n                                content=Join('', [\n                                    '#! /bin/bash\\n',\n                                    'cd /tmp\\n',\n                                    'wget https://github.com/nicolasff/'\n                                    'phpredis/zipball/master -O phpredis.zip'\n                                    '\\n',\n                                    'unzip phpredis.zip\\n',\n                                    'cd nicolasff-phpredis-*\\n',\n                                    'phpize\\n',\n                                    './configure\\n',\n                                    'make && make install\\n',\n                                    'touch /etc/php.d/redis.ini\\n',\n                                    'echo extension=redis.so > /etc/php.d/'\n                                    'redis.ini\\n'\n                                    ]),\n                                mode='000755',\n                                owner='root',\n                                group='root'\n                                ),\n                        '/etc/cfn/cfn-hup.conf': cloudformation.InitFile(\n                            content=Join('', [\n                                '[main]\\n',\n                                'stack=', Ref('AWS::StackId'), '\\n',\n                                'region=', Ref('AWS::Region'), '\\n'\n                                ]),\n                            mode='000400',\n                            owner='root',\n                            group='root'\n                            ),\n                        '/etc/cfn/hooks.d/cfn-auto-reloader.conf':\n                            cloudformation.InitFile(\n                                content=Join('', [\n                                    '[cfn-auto-reloader-hook]\\n',\n                                    'triggers=post.update\\n',\n                                    'path=Resources.WebServerInstance.Metadata'\n                                    '.AWS::CloudFormation::Init\\n',\n                                    'action=/opt/aws/bin/cfn-init -v ',\n                                    '         --stack ', Ref('AWS::StackName'),\n                                    '         --resource WebServerInstance ',\n                                    '         --region ', Ref('AWS::Region'),\n                                    '\\n',\n                                    'runas=root\\n'\n                                    ]),\n                                # Why doesn't the Amazon template have this?\n                                # mode='000400',\n                                # owner='root',\n                                # group='root'\n                                ),\n                        }),\n\n                    commands={\n                        '01-install_phpredis': {\n                            'command': '/usr/local/bin/install_phpredis'\n                            },\n                        '02-get-cluster-config': {\n                            'command': '/usr/local/bin/get_cluster_config'\n                            }\n                        },\n\n                    services={\n                        \"sysvinit\": cloudformation.InitServices({\n                            \"httpd\": cloudformation.InitService(\n                                enabled=True,\n                                ensureRunning=True,\n                                ),\n                            \"cfn-hup\": cloudformation.InitService(\n                                enabled=True,\n                                ensureRunning=True,\n                                files=['/etc/cfn/cfn-hup.conf',\n                                       '/etc/cfn/hooks.d/'\n                                       'cfn-auto-reloader.conf']\n                                ),\n                            }),\n                        },\n                    )\n                })\n            ),\n        ImageId=FindInMap('AWSRegionArch2AMI', Ref('AWS::Region'),\n                          FindInMap('AWSInstanceType2Arch',\n                                    Ref(instancetype), 'Arch')),\n        InstanceType=Ref(instancetype),\n        SecurityGroups=[Ref(webserversg)],\n        KeyName=Ref(keyname),\n        IamInstanceProfile=Ref(webserverinstanceprofile),\n        UserData=Base64(Join('', [\n            '#!/bin/bash -xe\\n',\n            'yum update -y aws-cfn-bootstrap\\n',\n\n            '# Setup the PHP sample application\\n',\n            '/opt/aws/bin/cfn-init -v ',\n            '         --stack ', Ref('AWS::StackName'),\n            '         --resource WebServerInstance ',\n            '         --region ', Ref('AWS::Region'), '\\n',\n\n            '# Signal the status of cfn-init\\n',\n            '/opt/aws/bin/cfn-signal -e $? ',\n            '         --stack ', Ref('AWS::StackName'),\n            '         --resource WebServerInstance ',\n            '         --region ', Ref('AWS::Region'), '\\n'\n            ])),\n        CreationPolicy=CreationPolicy(\n            ResourceSignal=ResourceSignal(Timeout='PT15M')\n            ),\n        Tags=Tags(Application=Ref('AWS::StackId'),\n                  Details='Created using Troposhpere')\n        ))\n\n    redisclustersg = template.add_resource(elasticache.SecurityGroup(\n        'RedisClusterSecurityGroup',\n        Description='Lock the cluster down',\n        ))\n\n    template.add_resource(elasticache.SecurityGroupIngress(\n        'RedisClusterSecurityGroupIngress',\n        CacheSecurityGroupName=Ref(redisclustersg),\n        EC2SecurityGroupName=Ref(webserversg),\n        ))\n\n    template.add_resource(elasticache.CacheCluster(\n        'RedisCluster',\n        Engine='redis',\n        CacheNodeType=Ref(cachenodetype),\n        NumCacheNodes='1',\n        CacheSecurityGroupNames=[Ref(redisclustersg)],\n        ))\n\n    # Outputs\n    template.add_output([\n        Output(\n            'WebsiteURL',\n            Description='Application URL',\n            Value=Join('', [\n                'http://',\n                GetAtt(webserverinstance, 'PublicDnsName'),\n\n                ])\n            )\n        ])\n\n    # Print CloudFormation Template\n    print(template.to_json())", "language": "python", "code": "def main():\n    \"\"\"\n    Create a ElastiCache Redis Node and EC2 Instance\n    \"\"\"\n\n    template = Template()\n\n    # Description\n    template.add_description(\n        'AWS CloudFormation Sample Template ElastiCache_Redis:'\n        'Sample template showing how to create an Amazon'\n        'ElastiCache Redis Cluster. **WARNING** This template'\n        'creates an Amazon EC2 Instance and an Amazon ElastiCache'\n        'Cluster. You will be billed for the AWS resources used'\n        'if you create a stack from this template.')\n\n    # Mappings\n    template.add_mapping('AWSInstanceType2Arch', {\n        't1.micro':     {'Arch': 'PV64'},\n        't2.micro':     {'Arch': 'HVM64'},\n        't2.small':     {'Arch': 'HVM64'},\n        't2.medium':    {'Arch': 'HVM64'},\n        'm1.small':     {'Arch': 'PV64'},\n        'm1.medium':    {'Arch': 'PV64'},\n        'm1.large':     {'Arch': 'PV64'},\n        'm1.xlarge':    {'Arch': 'PV64'},\n        'm2.xlarge':    {'Arch': 'PV64'},\n        'm2.2xlarge':   {'Arch': 'PV64'},\n        'm2.4xlarge':   {'Arch': 'PV64'},\n        'm3.medium':    {'Arch': 'HVM64'},\n        'm3.large':     {'Arch': 'HVM64'},\n        'm3.xlarge':    {'Arch': 'HVM64'},\n        'm3.2xlarge':   {'Arch': 'HVM64'},\n        'c1.medium':    {'Arch': 'PV64'},\n        'c1.xlarge':    {'Arch': 'PV64'},\n        'c3.large':     {'Arch': 'HVM64'},\n        'c3.xlarge':    {'Arch': 'HVM64'},\n        'c3.2xlarge':   {'Arch': 'HVM64'},\n        'c3.4xlarge':   {'Arch': 'HVM64'},\n        'c3.8xlarge':   {'Arch': 'HVM64'},\n        'c4.large':     {'Arch': 'HVM64'},\n        'c4.xlarge':    {'Arch': 'HVM64'},\n        'c4.2xlarge':   {'Arch': 'HVM64'},\n        'c4.4xlarge':   {'Arch': 'HVM64'},\n        'c4.8xlarge':   {'Arch': 'HVM64'},\n        'g2.2xlarge':   {'Arch': 'HVMG2'},\n        'r3.large':     {'Arch': 'HVM64'},\n        'r3.xlarge':    {'Arch': 'HVM64'},\n        'r3.2xlarge':   {'Arch': 'HVM64'},\n        'r3.4xlarge':   {'Arch': 'HVM64'},\n        'r3.8xlarge':   {'Arch': 'HVM64'},\n        'i2.xlarge':    {'Arch': 'HVM64'},\n        'i2.2xlarge':   {'Arch': 'HVM64'},\n        'i2.4xlarge':   {'Arch': 'HVM64'},\n        'i2.8xlarge':   {'Arch': 'HVM64'},\n        'd2.xlarge':    {'Arch': 'HVM64'},\n        'd2.2xlarge':   {'Arch': 'HVM64'},\n        'd2.4xlarge':   {'Arch': 'HVM64'},\n        'd2.8xlarge':   {'Arch': 'HVM64'},\n        'hi1.4xlarge':  {'Arch': 'HVM64'},\n        'hs1.8xlarge':  {'Arch': 'HVM64'},\n        'cr1.8xlarge':  {'Arch': 'HVM64'},\n        'cc2.8xlarge':  {'Arch': 'HVM64'}\n        })\n\n    template.add_mapping('AWSRegionArch2AMI', {\n        'us-east-1': {'PV64': 'ami-0f4cfd64',\n                      'HVM64': 'ami-0d4cfd66',\n                      'HVMG2': 'ami-5b05ba30'},\n        'us-west-2': {'PV64': 'ami-d3c5d1e3',\n                      'HVM64': 'ami-d5c5d1e5',\n                      'HVMG2': 'ami-a9d6c099'},\n        'us-west-1': {'PV64': 'ami-85ea13c1',\n                      'HVM64': 'ami-87ea13c3',\n                      'HVMG2': 'ami-37827a73'},\n        'eu-west-1': {'PV64': 'ami-d6d18ea1',\n                      'HVM64': 'ami-e4d18e93',\n                      'HVMG2': 'ami-72a9f105'},\n        'eu-central-1': {'PV64': 'ami-a4b0b7b9',\n                         'HVM64': 'ami-a6b0b7bb',\n                         'HVMG2': 'ami-a6c9cfbb'},\n        'ap-northeast-1': {'PV64': 'ami-1a1b9f1a',\n                           'HVM64': 'ami-1c1b9f1c',\n                           'HVMG2': 'ami-f644c4f6'},\n        'ap-southeast-1': {'PV64': 'ami-d24b4280',\n                           'HVM64': 'ami-d44b4286',\n                           'HVMG2': 'ami-12b5bc40'},\n        'ap-southeast-2': {'PV64': 'ami-ef7b39d5',\n                           'HVM64': 'ami-db7b39e1',\n                           'HVMG2': 'ami-b3337e89'},\n        'sa-east-1': {'PV64': 'ami-5b098146',\n                      'HVM64': 'ami-55098148',\n                      'HVMG2': 'NOT_SUPPORTED'},\n        'cn-north-1': {'PV64': 'ami-bec45887',\n                       'HVM64': 'ami-bcc45885',\n                       'HVMG2': 'NOT_SUPPORTED'}\n        })\n\n    template.add_mapping('Region2Principal', {\n        'us-east-1': {'EC2Principal': 'ec2.amazonaws.com',\n                      'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'us-west-2': {'EC2Principal': 'ec2.amazonaws.com',\n                      'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'us-west-1': {'EC2Principal': 'ec2.amazonaws.com',\n                      'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'eu-west-1': {'EC2Principal': 'ec2.amazonaws.com',\n                      'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'ap-southeast-1': {'EC2Principal': 'ec2.amazonaws.com',\n                           'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'ap-northeast-1': {'EC2Principal': 'ec2.amazonaws.com',\n                           'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'ap-southeast-2': {'EC2Principal': 'ec2.amazonaws.com',\n                           'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'sa-east-1': {'EC2Principal': 'ec2.amazonaws.com',\n                      'OpsWorksPrincipal': 'opsworks.amazonaws.com'},\n        'cn-north-1': {'EC2Principal': 'ec2.amazonaws.com.cn',\n                       'OpsWorksPrincipal': 'opsworks.amazonaws.com.cn'},\n        'eu-central-1': {'EC2Principal': 'ec2.amazonaws.com',\n                         'OpsWorksPrincipal': 'opsworks.amazonaws.com'}\n        })\n\n    # Parameters\n    cachenodetype = template.add_parameter(Parameter(\n        'ClusterNodeType',\n        Description='The compute and memory capacity of the nodes in the Redis'\n                    ' Cluster',\n        Type='String',\n        Default='cache.m1.small',\n        AllowedValues=['cache.m1.small',\n                       'cache.m1.large',\n                       'cache.m1.xlarge',\n                       'cache.m2.xlarge',\n                       'cache.m2.2xlarge',\n                       'cache.m2.4xlarge',\n                       'cache.c1.xlarge'],\n        ConstraintDescription='must select a valid Cache Node type.',\n        ))\n\n    instancetype = template.add_parameter(Parameter(\n        'InstanceType',\n        Description='WebServer EC2 instance type',\n        Type='String',\n        Default='t2.micro',\n        AllowedValues=['t1.micro',\n                       't2.micro',\n                       't2.small',\n                       't2.medium',\n                       'm1.small',\n                       'm1.medium',\n                       'm1.large',\n                       'm1.xlarge',\n                       'm2.xlarge',\n                       'm2.2xlarge',\n                       'm2.4xlarge',\n                       'm3.medium',\n                       'm3.large',\n                       'm3.xlarge',\n                       'm3.2xlarge',\n                       'c1.medium',\n                       'c1.xlarge',\n                       'c3.large',\n                       'c3.xlarge',\n                       'c3.2xlarge',\n                       'c3.4xlarge',\n                       'c3.8xlarge',\n                       'c4.large',\n                       'c4.xlarge',\n                       'c4.2xlarge',\n                       'c4.4xlarge',\n                       'c4.8xlarge',\n                       'g2.2xlarge',\n                       'r3.large',\n                       'r3.xlarge',\n                       'r3.2xlarge',\n                       'r3.4xlarge',\n                       'r3.8xlarge',\n                       'i2.xlarge',\n                       'i2.2xlarge',\n                       'i2.4xlarge',\n                       'i2.8xlarge',\n                       'd2.xlarge',\n                       'd2.2xlarge',\n                       'd2.4xlarge',\n                       'd2.8xlarge',\n                       'hi1.4xlarge',\n                       'hs1.8xlarge',\n                       'cr1.8xlarge',\n                       'cc2.8xlarge',\n                       'cg1.4xlarge'],\n        ConstraintDescription='must be a valid EC2 instance type.',\n        ))\n\n    keyname = template.add_parameter(Parameter(\n        'KeyName',\n        Description='Name of an existing EC2 KeyPair to enable SSH access'\n                    ' to the instance',\n        Type='AWS::EC2::KeyPair::KeyName',\n        ConstraintDescription='must be the name of an existing EC2 KeyPair.',\n        ))\n\n    sshlocation = template.add_parameter(Parameter(\n        'SSHLocation',\n        Description='The IP address range that can be used to SSH to'\n                    ' the EC2 instances',\n        Type='String',\n        MinLength='9',\n        MaxLength='18',\n        Default='0.0.0.0/0',\n        AllowedPattern='(\\\\d{1,3})\\\\.(\\\\d{1,3})\\\\.'\n                       '(\\\\d{1,3})\\\\.(\\\\d{1,3})/(\\\\d{1,2})',\n        ConstraintDescription='must be a valid IP CIDR range of the'\n                              ' form x.x.x.x/x.'\n        ))\n\n    # Resources\n    webserverrole = template.add_resource(iam.Role(\n        'WebServerRole',\n        AssumeRolePolicyDocument=Policy(\n            Statement=[\n                Statement(\n                    Effect=Allow,\n                    Action=[AssumeRole],\n                    Principal=Principal('Service',\n                                        [FindInMap('Region2Principal',\n                                                   Ref('AWS::Region'),\n                                                   'EC2Principal')]),\n                    )\n                ]\n            ),\n        Path='/',\n    ))\n\n    template.add_resource(iam.PolicyType(\n        'WebServerRolePolicy',\n        PolicyName='WebServerRole',\n        PolicyDocument=awacs.aws.Policy(\n            Statement=[awacs.aws.Statement(\n                Action=[awacs.aws.Action(\"elasticache\",\n                        \"DescribeCacheClusters\")],\n                Resource=[\"*\"],\n                Effect=awacs.aws.Allow\n            )]\n        ),\n        Roles=[Ref(webserverrole)],\n    ))\n\n    webserverinstanceprofile = template.add_resource(iam.InstanceProfile(\n        'WebServerInstanceProfile',\n        Path='/',\n        Roles=[Ref(webserverrole)],\n    ))\n\n    webserversg = template.add_resource(ec2.SecurityGroup(\n        'WebServerSecurityGroup',\n        GroupDescription='Enable HTTP and SSH access',\n        SecurityGroupIngress=[\n            ec2.SecurityGroupRule(\n                IpProtocol='tcp',\n                FromPort='22',\n                ToPort='22',\n                CidrIp=Ref(sshlocation),\n                ),\n            ec2.SecurityGroupRule(\n                IpProtocol='tcp',\n                FromPort='80',\n                ToPort='80',\n                CidrIp='0.0.0.0/0',\n                )\n            ]\n        ))\n\n    webserverinstance = template.add_resource(ec2.Instance(\n        'WebServerInstance',\n        Metadata=cloudformation.Metadata(\n            cloudformation.Init({\n                'config': cloudformation.InitConfig(\n                    packages={\n                        'yum': {\n                            'httpd':     [],\n                            'php':       [],\n                            'php-devel': [],\n                            'gcc':       [],\n                            'make':      []\n                            }\n                        },\n\n                    files=cloudformation.InitFiles({\n                        '/var/www/html/index.php': cloudformation.InitFile(\n                            content=Join('', [\n                                '<?php\\n',\n                                'echo \\\"<h1>AWS CloudFormation sample'\n                                ' application for Amazon ElastiCache'\n                                ' Redis Cluster</h1>\\\";\\n',\n                                '\\n',\n                                '$cluster_config = json_decode('\n                                'file_get_contents(\\'/tmp/cacheclusterconfig\\''\n                                '), true);\\n',\n                                '$endpoint = $cluster_config[\\'CacheClusters'\n                                '\\'][0][\\'CacheNodes\\'][0][\\'Endpoint\\'][\\'Add'\n                                'ress\\'];\\n',\n                                '$port = $cluster_config[\\'CacheClusters\\'][0]'\n                                '[\\'CacheNodes\\'][0][\\'Endpoint\\'][\\'Port\\'];'\n                                '\\n',\n                                '\\n',\n                                'echo \\\"<p>Connecting to Redis Cache Cluster '\n                                'node \\'{$endpoint}\\' on port {$port}</p>\\\";'\n                                '\\n',\n                                '\\n',\n                                '$redis=new Redis();\\n',\n                                '$redis->connect($endpoint, $port);\\n',\n                                '$redis->set(\\'testkey\\', \\'Hello World!\\');'\n                                '\\n',\n                                '$return = $redis->get(\\'testkey\\');\\n',\n                                '\\n',\n                                'echo \\\"<p>Retrieved value: $return</p>\\\";'\n                                '\\n',\n                                '?>\\n'\n                                ]),\n                            mode='000644',\n                            owner='apache',\n                            group='apache'\n                            ),\n                        '/etc/cron.d/get_cluster_config':\n                            cloudformation.InitFile(\n                                content='*/5 * * * * root'\n                                        ' /usr/local/bin/get_cluster_config',\n                                mode='000644',\n                                owner='root',\n                                group='root'\n                                ),\n                        '/usr/local/bin/get_cluster_config':\n                            cloudformation.InitFile(\n                                content=Join('', [\n                                    '#! /bin/bash\\n',\n                                    'aws elasticache describe-cache-clusters ',\n                                    '         --cache-cluster-id ',\n                                    Ref('RedisCluster'),\n                                    '         --show-cache-node-info'\n                                    ' --region ', Ref('AWS::Region'),\n                                    ' > /tmp/cacheclusterconfig\\n'\n                                    ]),\n                                mode='000755',\n                                owner='root',\n                                group='root'\n                                ),\n                        '/usr/local/bin/install_phpredis':\n                            cloudformation.InitFile(\n                                content=Join('', [\n                                    '#! /bin/bash\\n',\n                                    'cd /tmp\\n',\n                                    'wget https://github.com/nicolasff/'\n                                    'phpredis/zipball/master -O phpredis.zip'\n                                    '\\n',\n                                    'unzip phpredis.zip\\n',\n                                    'cd nicolasff-phpredis-*\\n',\n                                    'phpize\\n',\n                                    './configure\\n',\n                                    'make && make install\\n',\n                                    'touch /etc/php.d/redis.ini\\n',\n                                    'echo extension=redis.so > /etc/php.d/'\n                                    'redis.ini\\n'\n                                    ]),\n                                mode='000755',\n                                owner='root',\n                                group='root'\n                                ),\n                        '/etc/cfn/cfn-hup.conf': cloudformation.InitFile(\n                            content=Join('', [\n                                '[main]\\n',\n                                'stack=', Ref('AWS::StackId'), '\\n',\n                                'region=', Ref('AWS::Region'), '\\n'\n                                ]),\n                            mode='000400',\n                            owner='root',\n                            group='root'\n                            ),\n                        '/etc/cfn/hooks.d/cfn-auto-reloader.conf':\n                            cloudformation.InitFile(\n                                content=Join('', [\n                                    '[cfn-auto-reloader-hook]\\n',\n                                    'triggers=post.update\\n',\n                                    'path=Resources.WebServerInstance.Metadata'\n                                    '.AWS::CloudFormation::Init\\n',\n                                    'action=/opt/aws/bin/cfn-init -v ',\n                                    '         --stack ', Ref('AWS::StackName'),\n                                    '         --resource WebServerInstance ',\n                                    '         --region ', Ref('AWS::Region'),\n                                    '\\n',\n                                    'runas=root\\n'\n                                    ]),\n                                # Why doesn't the Amazon template have this?\n                                # mode='000400',\n                                # owner='root',\n                                # group='root'\n                                ),\n                        }),\n\n                    commands={\n                        '01-install_phpredis': {\n                            'command': '/usr/local/bin/install_phpredis'\n                            },\n                        '02-get-cluster-config': {\n                            'command': '/usr/local/bin/get_cluster_config'\n                            }\n                        },\n\n                    services={\n                        \"sysvinit\": cloudformation.InitServices({\n                            \"httpd\": cloudformation.InitService(\n                                enabled=True,\n                                ensureRunning=True,\n                                ),\n                            \"cfn-hup\": cloudformation.InitService(\n                                enabled=True,\n                                ensureRunning=True,\n                                files=['/etc/cfn/cfn-hup.conf',\n                                       '/etc/cfn/hooks.d/'\n                                       'cfn-auto-reloader.conf']\n                                ),\n                            }),\n                        },\n                    )\n                })\n            ),\n        ImageId=FindInMap('AWSRegionArch2AMI', Ref('AWS::Region'),\n                          FindInMap('AWSInstanceType2Arch',\n                                    Ref(instancetype), 'Arch')),\n        InstanceType=Ref(instancetype),\n        SecurityGroups=[Ref(webserversg)],\n        KeyName=Ref(keyname),\n        IamInstanceProfile=Ref(webserverinstanceprofile),\n        UserData=Base64(Join('', [\n            '#!/bin/bash -xe\\n',\n            'yum update -y aws-cfn-bootstrap\\n',\n\n            '# Setup the PHP sample application\\n',\n            '/opt/aws/bin/cfn-init -v ',\n            '         --stack ', Ref('AWS::StackName'),\n            '         --resource WebServerInstance ',\n            '         --region ', Ref('AWS::Region'), '\\n',\n\n            '# Signal the status of cfn-init\\n',\n            '/opt/aws/bin/cfn-signal -e $? ',\n            '         --stack ', Ref('AWS::StackName'),\n            '         --resource WebServerInstance ',\n            '         --region ', Ref('AWS::Region'), '\\n'\n            ])),\n        CreationPolicy=CreationPolicy(\n            ResourceSignal=ResourceSignal(Timeout='PT15M')\n            ),\n        Tags=Tags(Application=Ref('AWS::StackId'),\n                  Details='Created using Troposhpere')\n        ))\n\n    redisclustersg = template.add_resource(elasticache.SecurityGroup(\n        'RedisClusterSecurityGroup',\n        Description='Lock the cluster down',\n        ))\n\n    template.add_resource(elasticache.SecurityGroupIngress(\n        'RedisClusterSecurityGroupIngress',\n        CacheSecurityGroupName=Ref(redisclustersg),\n        EC2SecurityGroupName=Ref(webserversg),\n        ))\n\n    template.add_resource(elasticache.CacheCluster(\n        'RedisCluster',\n        Engine='redis',\n        CacheNodeType=Ref(cachenodetype),\n        NumCacheNodes='1',\n        CacheSecurityGroupNames=[Ref(redisclustersg)],\n        ))\n\n    # Outputs\n    template.add_output([\n        Output(\n            'WebsiteURL',\n            Description='Application URL',\n            Value=Join('', [\n                'http://',\n                GetAtt(webserverinstance, 'PublicDnsName'),\n\n                ])\n            )\n        ])\n\n    # Print CloudFormation Template\n    print(template.to_json())", "code_tokens": ["def", "main", "(", ")", ":", "template", "=", "Template", "(", ")", "# Description", "template", ".", "add_description", "(", "'AWS CloudFormation Sample Template ElastiCache_Redis:'", "'Sample template showing how to create an Amazon'", "'ElastiCache Redis Cluster. **WARNING** This template'", "'creates an Amazon EC2 Instance and an Amazon ElastiCache'", "'Cluster. You will be billed for the AWS resources used'", "'if you create a stack from this template.'", ")", "# Mappings", "template", ".", "add_mapping", "(", "'AWSInstanceType2Arch'", ",", "{", "'t1.micro'", ":", "{", "'Arch'", ":", "'PV64'", "}", ",", "'t2.micro'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'t2.small'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'t2.medium'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'m1.small'", ":", "{", "'Arch'", ":", "'PV64'", "}", ",", "'m1.medium'", ":", "{", "'Arch'", ":", "'PV64'", "}", ",", "'m1.large'", ":", "{", "'Arch'", ":", "'PV64'", "}", ",", "'m1.xlarge'", ":", "{", "'Arch'", ":", "'PV64'", "}", ",", "'m2.xlarge'", ":", "{", "'Arch'", ":", "'PV64'", "}", ",", "'m2.2xlarge'", ":", "{", "'Arch'", ":", "'PV64'", "}", ",", "'m2.4xlarge'", ":", "{", "'Arch'", ":", "'PV64'", "}", ",", "'m3.medium'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'m3.large'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'m3.xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'m3.2xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'c1.medium'", ":", "{", "'Arch'", ":", "'PV64'", "}", ",", "'c1.xlarge'", ":", "{", "'Arch'", ":", "'PV64'", "}", ",", "'c3.large'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'c3.xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'c3.2xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'c3.4xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'c3.8xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'c4.large'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'c4.xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'c4.2xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'c4.4xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'c4.8xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'g2.2xlarge'", ":", "{", "'Arch'", ":", "'HVMG2'", "}", ",", "'r3.large'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'r3.xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'r3.2xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'r3.4xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'r3.8xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'i2.xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'i2.2xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'i2.4xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'i2.8xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'d2.xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'d2.2xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'d2.4xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'d2.8xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'hi1.4xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'hs1.8xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'cr1.8xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", ",", "'cc2.8xlarge'", ":", "{", "'Arch'", ":", "'HVM64'", "}", "}", ")", "template", ".", "add_mapping", "(", "'AWSRegionArch2AMI'", ",", "{", "'us-east-1'", ":", "{", "'PV64'", ":", "'ami-0f4cfd64'", ",", "'HVM64'", ":", "'ami-0d4cfd66'", ",", "'HVMG2'", ":", "'ami-5b05ba30'", "}", ",", "'us-west-2'", ":", "{", "'PV64'", ":", "'ami-d3c5d1e3'", ",", "'HVM64'", ":", "'ami-d5c5d1e5'", ",", "'HVMG2'", ":", "'ami-a9d6c099'", "}", ",", "'us-west-1'", ":", "{", "'PV64'", ":", "'ami-85ea13c1'", ",", "'HVM64'", ":", "'ami-87ea13c3'", ",", "'HVMG2'", ":", "'ami-37827a73'", "}", ",", "'eu-west-1'", ":", "{", "'PV64'", ":", "'ami-d6d18ea1'", ",", "'HVM64'", ":", "'ami-e4d18e93'", ",", "'HVMG2'", ":", "'ami-72a9f105'", "}", ",", "'eu-central-1'", ":", "{", "'PV64'", ":", "'ami-a4b0b7b9'", ",", "'HVM64'", ":", "'ami-a6b0b7bb'", ",", "'HVMG2'", ":", "'ami-a6c9cfbb'", "}", ",", "'ap-northeast-1'", ":", "{", "'PV64'", ":", "'ami-1a1b9f1a'", ",", "'HVM64'", ":", "'ami-1c1b9f1c'", ",", "'HVMG2'", ":", "'ami-f644c4f6'", "}", ",", "'ap-southeast-1'", ":", "{", "'PV64'", ":", "'ami-d24b4280'", ",", "'HVM64'", ":", "'ami-d44b4286'", ",", "'HVMG2'", ":", "'ami-12b5bc40'", "}", ",", "'ap-southeast-2'", ":", "{", "'PV64'", ":", "'ami-ef7b39d5'", ",", "'HVM64'", ":", "'ami-db7b39e1'", ",", "'HVMG2'", ":", "'ami-b3337e89'", "}", ",", "'sa-east-1'", ":", "{", "'PV64'", ":", "'ami-5b098146'", ",", "'HVM64'", ":", "'ami-55098148'", ",", "'HVMG2'", ":", "'NOT_SUPPORTED'", "}", ",", "'cn-north-1'", ":", "{", "'PV64'", ":", "'ami-bec45887'", ",", "'HVM64'", ":", "'ami-bcc45885'", ",", "'HVMG2'", ":", "'NOT_SUPPORTED'", "}", "}", ")", "template", ".", "add_mapping", "(", "'Region2Principal'", ",", "{", "'us-east-1'", ":", "{", "'EC2Principal'", ":", "'ec2.amazonaws.com'", ",", "'OpsWorksPrincipal'", ":", "'opsworks.amazonaws.com'", "}", ",", "'us-west-2'", ":", "{", "'EC2Principal'", ":", "'ec2.amazonaws.com'", ",", "'OpsWorksPrincipal'", ":", "'opsworks.amazonaws.com'", "}", ",", "'us-west-1'", ":", "{", "'EC2Principal'", ":", "'ec2.amazonaws.com'", ",", "'OpsWorksPrincipal'", ":", "'opsworks.amazonaws.com'", "}", ",", "'eu-west-1'", ":", "{", "'EC2Principal'", ":", "'ec2.amazonaws.com'", ",", "'OpsWorksPrincipal'", ":", "'opsworks.amazonaws.com'", "}", ",", "'ap-southeast-1'", ":", "{", "'EC2Principal'", ":", "'ec2.amazonaws.com'", ",", "'OpsWorksPrincipal'", ":", "'opsworks.amazonaws.com'", "}", ",", "'ap-northeast-1'", ":", "{", "'EC2Principal'", ":", "'ec2.amazonaws.com'", ",", "'OpsWorksPrincipal'", ":", "'opsworks.amazonaws.com'", "}", ",", "'ap-southeast-2'", ":", "{", "'EC2Principal'", ":", "'ec2.amazonaws.com'", ",", "'OpsWorksPrincipal'", ":", "'opsworks.amazonaws.com'", "}", ",", "'sa-east-1'", ":", "{", "'EC2Principal'", ":", "'ec2.amazonaws.com'", ",", "'OpsWorksPrincipal'", ":", "'opsworks.amazonaws.com'", "}", ",", "'cn-north-1'", ":", "{", "'EC2Principal'", ":", "'ec2.amazonaws.com.cn'", ",", "'OpsWorksPrincipal'", ":", "'opsworks.amazonaws.com.cn'", "}", ",", "'eu-central-1'", ":", "{", "'EC2Principal'", ":", "'ec2.amazonaws.com'", ",", "'OpsWorksPrincipal'", ":", "'opsworks.amazonaws.com'", "}", "}", ")", "# Parameters", "cachenodetype", "=", "template", ".", "add_parameter", "(", "Parameter", "(", "'ClusterNodeType'", ",", "Description", "=", "'The compute and memory capacity of the nodes in the Redis'", "' Cluster'", ",", "Type", "=", "'String'", ",", "Default", "=", "'cache.m1.small'", ",", "AllowedValues", "=", "[", "'cache.m1.small'", ",", "'cache.m1.large'", ",", "'cache.m1.xlarge'", ",", "'cache.m2.xlarge'", ",", "'cache.m2.2xlarge'", ",", "'cache.m2.4xlarge'", ",", "'cache.c1.xlarge'", "]", ",", "ConstraintDescription", "=", "'must select a valid Cache Node type.'", ",", ")", ")", "instancetype", "=", "template", ".", "add_parameter", "(", "Parameter", "(", "'InstanceType'", ",", "Description", "=", "'WebServer EC2 instance type'", ",", "Type", "=", "'String'", ",", "Default", "=", "'t2.micro'", ",", "AllowedValues", "=", "[", "'t1.micro'", ",", "'t2.micro'", ",", "'t2.small'", ",", "'t2.medium'", ",", "'m1.small'", ",", "'m1.medium'", ",", "'m1.large'", ",", "'m1.xlarge'", ",", "'m2.xlarge'", ",", "'m2.2xlarge'", ",", "'m2.4xlarge'", ",", "'m3.medium'", ",", "'m3.large'", ",", "'m3.xlarge'", ",", "'m3.2xlarge'", ",", "'c1.medium'", ",", "'c1.xlarge'", ",", "'c3.large'", ",", "'c3.xlarge'", ",", "'c3.2xlarge'", ",", "'c3.4xlarge'", ",", "'c3.8xlarge'", ",", "'c4.large'", ",", "'c4.xlarge'", ",", "'c4.2xlarge'", ",", "'c4.4xlarge'", ",", "'c4.8xlarge'", ",", "'g2.2xlarge'", ",", "'r3.large'", ",", "'r3.xlarge'", ",", "'r3.2xlarge'", ",", "'r3.4xlarge'", ",", "'r3.8xlarge'", ",", "'i2.xlarge'", ",", "'i2.2xlarge'", ",", "'i2.4xlarge'", ",", "'i2.8xlarge'", ",", "'d2.xlarge'", ",", "'d2.2xlarge'", ",", "'d2.4xlarge'", ",", "'d2.8xlarge'", ",", "'hi1.4xlarge'", ",", "'hs1.8xlarge'", ",", "'cr1.8xlarge'", ",", "'cc2.8xlarge'", ",", "'cg1.4xlarge'", "]", ",", "ConstraintDescription", "=", "'must be a valid EC2 instance type.'", ",", ")", ")", "keyname", "=", "template", ".", "add_parameter", "(", "Parameter", "(", "'KeyName'", ",", "Description", "=", "'Name of an existing EC2 KeyPair to enable SSH access'", "' to the instance'", ",", "Type", "=", "'AWS::EC2::KeyPair::KeyName'", ",", "ConstraintDescription", "=", "'must be the name of an existing EC2 KeyPair.'", ",", ")", ")", "sshlocation", "=", "template", ".", "add_parameter", "(", "Parameter", "(", "'SSHLocation'", ",", "Description", "=", "'The IP address range that can be used to SSH to'", "' the EC2 instances'", ",", "Type", "=", "'String'", ",", "MinLength", "=", "'9'", ",", "MaxLength", "=", "'18'", ",", "Default", "=", "'0.0.0.0/0'", ",", "AllowedPattern", "=", "'(\\\\d{1,3})\\\\.(\\\\d{1,3})\\\\.'", "'(\\\\d{1,3})\\\\.(\\\\d{1,3})/(\\\\d{1,2})'", ",", "ConstraintDescription", "=", "'must be a valid IP CIDR range of the'", "' form x.x.x.x/x.'", ")", ")", "# Resources", "webserverrole", "=", "template", ".", "add_resource", "(", "iam", ".", "Role", "(", "'WebServerRole'", ",", "AssumeRolePolicyDocument", "=", "Policy", "(", "Statement", "=", "[", "Statement", "(", "Effect", "=", "Allow", ",", "Action", "=", "[", "AssumeRole", "]", ",", "Principal", "=", "Principal", "(", "'Service'", ",", "[", "FindInMap", "(", "'Region2Principal'", ",", "Ref", "(", "'AWS::Region'", ")", ",", "'EC2Principal'", ")", "]", ")", ",", ")", "]", ")", ",", "Path", "=", "'/'", ",", ")", ")", "template", ".", "add_resource", "(", "iam", ".", "PolicyType", "(", "'WebServerRolePolicy'", ",", "PolicyName", "=", "'WebServerRole'", ",", "PolicyDocument", "=", "awacs", ".", "aws", ".", "Policy", "(", "Statement", "=", "[", "awacs", ".", "aws", ".", "Statement", "(", "Action", "=", "[", "awacs", ".", "aws", ".", "Action", "(", "\"elasticache\"", ",", "\"DescribeCacheClusters\"", ")", "]", ",", "Resource", "=", "[", "\"*\"", "]", ",", "Effect", "=", "awacs", ".", "aws", ".", "Allow", ")", "]", ")", ",", "Roles", "=", "[", "Ref", "(", "webserverrole", ")", "]", ",", ")", ")", "webserverinstanceprofile", "=", "template", ".", "add_resource", "(", "iam", ".", "InstanceProfile", "(", "'WebServerInstanceProfile'", ",", "Path", "=", "'/'", ",", "Roles", "=", "[", "Ref", "(", "webserverrole", ")", "]", ",", ")", ")", "webserversg", "=", "template", ".", "add_resource", "(", "ec2", ".", "SecurityGroup", "(", "'WebServerSecurityGroup'", ",", "GroupDescription", "=", "'Enable HTTP and SSH access'", ",", "SecurityGroupIngress", "=", "[", "ec2", ".", "SecurityGroupRule", "(", "IpProtocol", "=", "'tcp'", ",", "FromPort", "=", "'22'", ",", "ToPort", "=", "'22'", ",", "CidrIp", "=", "Ref", "(", "sshlocation", ")", ",", ")", ",", "ec2", ".", "SecurityGroupRule", "(", "IpProtocol", "=", "'tcp'", ",", "FromPort", "=", "'80'", ",", "ToPort", "=", "'80'", ",", "CidrIp", "=", "'0.0.0.0/0'", ",", ")", "]", ")", ")", "webserverinstance", "=", "template", ".", "add_resource", "(", "ec2", ".", "Instance", "(", "'WebServerInstance'", ",", "Metadata", "=", "cloudformation", ".", "Metadata", "(", "cloudformation", ".", "Init", "(", "{", "'config'", ":", "cloudformation", ".", "InitConfig", "(", "packages", "=", "{", "'yum'", ":", "{", "'httpd'", ":", "[", "]", ",", "'php'", ":", "[", "]", ",", "'php-devel'", ":", "[", "]", ",", "'gcc'", ":", "[", "]", ",", "'make'", ":", "[", "]", "}", "}", ",", "files", "=", "cloudformation", ".", "InitFiles", "(", "{", "'/var/www/html/index.php'", ":", "cloudformation", ".", "InitFile", "(", "content", "=", "Join", "(", "''", ",", "[", "'<?php\\n'", ",", "'echo \\\"<h1>AWS CloudFormation sample'", "' application for Amazon ElastiCache'", "' Redis Cluster</h1>\\\";\\n'", ",", "'\\n'", ",", "'$cluster_config = json_decode('", "'file_get_contents(\\'/tmp/cacheclusterconfig\\''", "'), true);\\n'", ",", "'$endpoint = $cluster_config[\\'CacheClusters'", "'\\'][0][\\'CacheNodes\\'][0][\\'Endpoint\\'][\\'Add'", "'ress\\'];\\n'", ",", "'$port = $cluster_config[\\'CacheClusters\\'][0]'", "'[\\'CacheNodes\\'][0][\\'Endpoint\\'][\\'Port\\'];'", "'\\n'", ",", "'\\n'", ",", "'echo \\\"<p>Connecting to Redis Cache Cluster '", "'node \\'{$endpoint}\\' on port {$port}</p>\\\";'", "'\\n'", ",", "'\\n'", ",", "'$redis=new Redis();\\n'", ",", "'$redis->connect($endpoint, $port);\\n'", ",", "'$redis->set(\\'testkey\\', \\'Hello World!\\');'", "'\\n'", ",", "'$return = $redis->get(\\'testkey\\');\\n'", ",", "'\\n'", ",", "'echo \\\"<p>Retrieved value: $return</p>\\\";'", "'\\n'", ",", "'?>\\n'", "]", ")", ",", "mode", "=", "'000644'", ",", "owner", "=", "'apache'", ",", "group", "=", "'apache'", ")", ",", "'/etc/cron.d/get_cluster_config'", ":", "cloudformation", ".", "InitFile", "(", "content", "=", "'*/5 * * * * root'", "' /usr/local/bin/get_cluster_config'", ",", "mode", "=", "'000644'", ",", "owner", "=", "'root'", ",", "group", "=", "'root'", ")", ",", "'/usr/local/bin/get_cluster_config'", ":", "cloudformation", ".", "InitFile", "(", "content", "=", "Join", "(", "''", ",", "[", "'#! /bin/bash\\n'", ",", "'aws elasticache describe-cache-clusters '", ",", "'         --cache-cluster-id '", ",", "Ref", "(", "'RedisCluster'", ")", ",", "'         --show-cache-node-info'", "' --region '", ",", "Ref", "(", "'AWS::Region'", ")", ",", "' > /tmp/cacheclusterconfig\\n'", "]", ")", ",", "mode", "=", "'000755'", ",", "owner", "=", "'root'", ",", "group", "=", "'root'", ")", ",", "'/usr/local/bin/install_phpredis'", ":", "cloudformation", ".", "InitFile", "(", "content", "=", "Join", "(", "''", ",", "[", "'#! /bin/bash\\n'", ",", "'cd /tmp\\n'", ",", "'wget https://github.com/nicolasff/'", "'phpredis/zipball/master -O phpredis.zip'", "'\\n'", ",", "'unzip phpredis.zip\\n'", ",", "'cd nicolasff-phpredis-*\\n'", ",", "'phpize\\n'", ",", "'./configure\\n'", ",", "'make && make install\\n'", ",", "'touch /etc/php.d/redis.ini\\n'", ",", "'echo extension=redis.so > /etc/php.d/'", "'redis.ini\\n'", "]", ")", ",", "mode", "=", "'000755'", ",", "owner", "=", "'root'", ",", "group", "=", "'root'", ")", ",", "'/etc/cfn/cfn-hup.conf'", ":", "cloudformation", ".", "InitFile", "(", "content", "=", "Join", "(", "''", ",", "[", "'[main]\\n'", ",", "'stack='", ",", "Ref", "(", "'AWS::StackId'", ")", ",", "'\\n'", ",", "'region='", ",", "Ref", "(", "'AWS::Region'", ")", ",", "'\\n'", "]", ")", ",", "mode", "=", "'000400'", ",", "owner", "=", "'root'", ",", "group", "=", "'root'", ")", ",", "'/etc/cfn/hooks.d/cfn-auto-reloader.conf'", ":", "cloudformation", ".", "InitFile", "(", "content", "=", "Join", "(", "''", ",", "[", "'[cfn-auto-reloader-hook]\\n'", ",", "'triggers=post.update\\n'", ",", "'path=Resources.WebServerInstance.Metadata'", "'.AWS::CloudFormation::Init\\n'", ",", "'action=/opt/aws/bin/cfn-init -v '", ",", "'         --stack '", ",", "Ref", "(", "'AWS::StackName'", ")", ",", "'         --resource WebServerInstance '", ",", "'         --region '", ",", "Ref", "(", "'AWS::Region'", ")", ",", "'\\n'", ",", "'runas=root\\n'", "]", ")", ",", "# Why doesn't the Amazon template have this?", "# mode='000400',", "# owner='root',", "# group='root'", ")", ",", "}", ")", ",", "commands", "=", "{", "'01-install_phpredis'", ":", "{", "'command'", ":", "'/usr/local/bin/install_phpredis'", "}", ",", "'02-get-cluster-config'", ":", "{", "'command'", ":", "'/usr/local/bin/get_cluster_config'", "}", "}", ",", "services", "=", "{", "\"sysvinit\"", ":", "cloudformation", ".", "InitServices", "(", "{", "\"httpd\"", ":", "cloudformation", ".", "InitService", "(", "enabled", "=", "True", ",", "ensureRunning", "=", "True", ",", ")", ",", "\"cfn-hup\"", ":", "cloudformation", ".", "InitService", "(", "enabled", "=", "True", ",", "ensureRunning", "=", "True", ",", "files", "=", "[", "'/etc/cfn/cfn-hup.conf'", ",", "'/etc/cfn/hooks.d/'", "'cfn-auto-reloader.conf'", "]", ")", ",", "}", ")", ",", "}", ",", ")", "}", ")", ")", ",", "ImageId", "=", "FindInMap", "(", "'AWSRegionArch2AMI'", ",", "Ref", "(", "'AWS::Region'", ")", ",", "FindInMap", "(", "'AWSInstanceType2Arch'", ",", "Ref", "(", "instancetype", ")", ",", "'Arch'", ")", ")", ",", "InstanceType", "=", "Ref", "(", "instancetype", ")", ",", "SecurityGroups", "=", "[", "Ref", "(", "webserversg", ")", "]", ",", "KeyName", "=", "Ref", "(", "keyname", ")", ",", "IamInstanceProfile", "=", "Ref", "(", "webserverinstanceprofile", ")", ",", "UserData", "=", "Base64", "(", "Join", "(", "''", ",", "[", "'#!/bin/bash -xe\\n'", ",", "'yum update -y aws-cfn-bootstrap\\n'", ",", "'# Setup the PHP sample application\\n'", ",", "'/opt/aws/bin/cfn-init -v '", ",", "'         --stack '", ",", "Ref", "(", "'AWS::StackName'", ")", ",", "'         --resource WebServerInstance '", ",", "'         --region '", ",", "Ref", "(", "'AWS::Region'", ")", ",", "'\\n'", ",", "'# Signal the status of cfn-init\\n'", ",", "'/opt/aws/bin/cfn-signal -e $? '", ",", "'         --stack '", ",", "Ref", "(", "'AWS::StackName'", ")", ",", "'         --resource WebServerInstance '", ",", "'         --region '", ",", "Ref", "(", "'AWS::Region'", ")", ",", "'\\n'", "]", ")", ")", ",", "CreationPolicy", "=", "CreationPolicy", "(", "ResourceSignal", "=", "ResourceSignal", "(", "Timeout", "=", "'PT15M'", ")", ")", ",", "Tags", "=", "Tags", "(", "Application", "=", "Ref", "(", "'AWS::StackId'", ")", ",", "Details", "=", "'Created using Troposhpere'", ")", ")", ")", "redisclustersg", "=", "template", ".", "add_resource", "(", "elasticache", ".", "SecurityGroup", "(", "'RedisClusterSecurityGroup'", ",", "Description", "=", "'Lock the cluster down'", ",", ")", ")", "template", ".", "add_resource", "(", "elasticache", ".", "SecurityGroupIngress", "(", "'RedisClusterSecurityGroupIngress'", ",", "CacheSecurityGroupName", "=", "Ref", "(", "redisclustersg", ")", ",", "EC2SecurityGroupName", "=", "Ref", "(", "webserversg", ")", ",", ")", ")", "template", ".", "add_resource", "(", "elasticache", ".", "CacheCluster", "(", "'RedisCluster'", ",", "Engine", "=", "'redis'", ",", "CacheNodeType", "=", "Ref", "(", "cachenodetype", ")", ",", "NumCacheNodes", "=", "'1'", ",", "CacheSecurityGroupNames", "=", "[", "Ref", "(", "redisclustersg", ")", "]", ",", ")", ")", "# Outputs", "template", ".", "add_output", "(", "[", "Output", "(", "'WebsiteURL'", ",", "Description", "=", "'Application URL'", ",", "Value", "=", "Join", "(", "''", ",", "[", "'http://'", ",", "GetAtt", "(", "webserverinstance", ",", "'PublicDnsName'", ")", ",", "]", ")", ")", "]", ")", "# Print CloudFormation Template", "print", "(", "template", ".", "to_json", "(", ")", ")"], "docstring": "Create a ElastiCache Redis Node and EC2 Instance", "docstring_tokens": ["Create", "a", "ElastiCache", "Redis", "Node", "and", "EC2", "Instance"], "sha": "f7ea5591a7c287a843adc9c184d2f56064cfc632", "url": "https://github.com/cloudtools/troposphere/blob/f7ea5591a7c287a843adc9c184d2f56064cfc632/examples/ElastiCacheRedis.py#L36-L523", "partition": "train"}
{"repo": "secdev/scapy", "path": "scapy/contrib/diameter.py", "func_name": "DiamReq", "original_string": "def DiamReq(cmd, **fields):\n    \"\"\"Craft Diameter request commands\"\"\"\n    upfields, name = getCmdParams(cmd, True, **fields)\n    p = DiamG(**upfields)\n    p.name = name\n    return p", "language": "python", "code": "def DiamReq(cmd, **fields):\n    \"\"\"Craft Diameter request commands\"\"\"\n    upfields, name = getCmdParams(cmd, True, **fields)\n    p = DiamG(**upfields)\n    p.name = name\n    return p", "code_tokens": ["def", "DiamReq", "(", "cmd", ",", "*", "*", "fields", ")", ":", "upfields", ",", "name", "=", "getCmdParams", "(", "cmd", ",", "True", ",", "*", "*", "fields", ")", "p", "=", "DiamG", "(", "*", "*", "upfields", ")", "p", ".", "name", "=", "name", "return", "p"], "docstring": "Craft Diameter request commands", "docstring_tokens": ["Craft", "Diameter", "request", "commands"], "sha": "3ffe757c184017dd46464593a8f80f85abc1e79a", "url": "https://github.com/secdev/scapy/blob/3ffe757c184017dd46464593a8f80f85abc1e79a/scapy/contrib/diameter.py#L4811-L4816", "partition": "train"}
{"repo": "secdev/scapy", "path": "scapy/contrib/diameter.py", "func_name": "DiamAns", "original_string": "def DiamAns(cmd, **fields):\n    \"\"\"Craft Diameter answer commands\"\"\"\n    upfields, name = getCmdParams(cmd, False, **fields)\n    p = DiamG(**upfields)\n    p.name = name\n    return p", "language": "python", "code": "def DiamAns(cmd, **fields):\n    \"\"\"Craft Diameter answer commands\"\"\"\n    upfields, name = getCmdParams(cmd, False, **fields)\n    p = DiamG(**upfields)\n    p.name = name\n    return p", "code_tokens": ["def", "DiamAns", "(", "cmd", ",", "*", "*", "fields", ")", ":", "upfields", ",", "name", "=", "getCmdParams", "(", "cmd", ",", "False", ",", "*", "*", "fields", ")", "p", "=", "DiamG", "(", "*", "*", "upfields", ")", "p", ".", "name", "=", "name", "return", "p"], "docstring": "Craft Diameter answer commands", "docstring_tokens": ["Craft", "Diameter", "answer", "commands"], "sha": "3ffe757c184017dd46464593a8f80f85abc1e79a", "url": "https://github.com/secdev/scapy/blob/3ffe757c184017dd46464593a8f80f85abc1e79a/scapy/contrib/diameter.py#L4819-L4824", "partition": "train"}
{"repo": "secdev/scapy", "path": "scapy/fields.py", "func_name": "Field.i2m", "original_string": "def i2m(self, pkt, x):\n        \"\"\"Convert internal value to machine value\"\"\"\n        if x is None:\n            x = 0\n        elif isinstance(x, str):\n            return bytes_encode(x)\n        return x", "language": "python", "code": "def i2m(self, pkt, x):\n        \"\"\"Convert internal value to machine value\"\"\"\n        if x is None:\n            x = 0\n        elif isinstance(x, str):\n            return bytes_encode(x)\n        return x", "code_tokens": ["def", "i2m", "(", "self", ",", "pkt", ",", "x", ")", ":", "if", "x", "is", "None", ":", "x", "=", "0", "elif", "isinstance", "(", "x", ",", "str", ")", ":", "return", "bytes_encode", "(", "x", ")", "return", "x"], "docstring": "Convert internal value to machine value", "docstring_tokens": ["Convert", "internal", "value", "to", "machine", "value"], "sha": "3ffe757c184017dd46464593a8f80f85abc1e79a", "url": "https://github.com/secdev/scapy/blob/3ffe757c184017dd46464593a8f80f85abc1e79a/scapy/fields.py#L117-L123", "partition": "train"}
{"repo": "ReFirmLabs/binwalk", "path": "src/binwalk/plugins/hilink.py", "func_name": "HilinkDecryptor._hilink_decrypt", "original_string": "def _hilink_decrypt(self, encrypted_firmware):\n        '''\n        This does the actual decryption.\n        '''\n        cipher = DES.new(self.DES_KEY, DES.MODE_ECB)\n\n        p1 = encrypted_firmware[0:3]\n        p2 = encrypted_firmware[3:]\n        p2 += b\"\\x00\" * (8 - (len(p2) % 8))\n\n        d1 = p1 + cipher.decrypt(p2)\n        d1 += b\"\\x00\" * (8 - (len(d1) % 8))\n\n        return cipher.decrypt(d1)", "language": "python", "code": "def _hilink_decrypt(self, encrypted_firmware):\n        '''\n        This does the actual decryption.\n        '''\n        cipher = DES.new(self.DES_KEY, DES.MODE_ECB)\n\n        p1 = encrypted_firmware[0:3]\n        p2 = encrypted_firmware[3:]\n        p2 += b\"\\x00\" * (8 - (len(p2) % 8))\n\n        d1 = p1 + cipher.decrypt(p2)\n        d1 += b\"\\x00\" * (8 - (len(d1) % 8))\n\n        return cipher.decrypt(d1)", "code_tokens": ["def", "_hilink_decrypt", "(", "self", ",", "encrypted_firmware", ")", ":", "cipher", "=", "DES", ".", "new", "(", "self", ".", "DES_KEY", ",", "DES", ".", "MODE_ECB", ")", "p1", "=", "encrypted_firmware", "[", "0", ":", "3", "]", "p2", "=", "encrypted_firmware", "[", "3", ":", "]", "p2", "+=", "b\"\\x00\"", "*", "(", "8", "-", "(", "len", "(", "p2", ")", "%", "8", ")", ")", "d1", "=", "p1", "+", "cipher", ".", "decrypt", "(", "p2", ")", "d1", "+=", "b\"\\x00\"", "*", "(", "8", "-", "(", "len", "(", "d1", ")", "%", "8", ")", ")", "return", "cipher", ".", "decrypt", "(", "d1", ")"], "docstring": "This does the actual decryption.", "docstring_tokens": ["This", "does", "the", "actual", "decryption", "."], "sha": "a0c5315fd2bae167e5c3d8469ce95d5defc743c2", "url": "https://github.com/ReFirmLabs/binwalk/blob/a0c5315fd2bae167e5c3d8469ce95d5defc743c2/src/binwalk/plugins/hilink.py#L50-L63", "partition": "train"}
{"repo": "ReFirmLabs/binwalk", "path": "src/binwalk/plugins/hilink.py", "func_name": "HilinkDecryptor.scan", "original_string": "def scan(self, result):\n        '''\n        Validate signature results.\n        '''\n        if self.enabled is True:\n            if result.valid is True:\n                if result.description.lower().startswith(self.SIGNATURE_DESCRIPTION) is True:\n                    # Read in the first 64 bytes of the suspected encrypted\n                    # uImage header\n                    fd = self.module.config.open_file(result.file.path, offset=result.offset)\n                    encrypted_header_data = binwalk.core.compat.str2bytes(fd.read(64))\n                    fd.close()\n\n                    # Decrypt the header\n                    decrypted_header_data = self._hilink_decrypt(encrypted_header_data)\n\n                    # Pull out the image size and image name fields from the decrypted uImage header\n                    # and add them to the printed description.\n                    result.size = struct.unpack(b\">L\", decrypted_header_data[12:16])[0]\n                    result.description += \", size: %d\" % (result.size)\n                    # NOTE: The description field should be 32 bytes? Hilink seems to use only 24 bytes for this field,\n                    #       even though the header size is still 64 bytes?\n                    result.description += ', image name: \"%s\"' % binwalk.core.compat.bytes2str(decrypted_header_data[32:56]).strip(\"\\x00\")\n\n                    # Do some basic validation on the decrypted size and image\n                    # name fields\n                    if result.size > (result.file.size - result.offset):\n                        result.valid = False\n                    if not all(c in string.printable for c in result.description):\n                        result.valid = False", "language": "python", "code": "def scan(self, result):\n        '''\n        Validate signature results.\n        '''\n        if self.enabled is True:\n            if result.valid is True:\n                if result.description.lower().startswith(self.SIGNATURE_DESCRIPTION) is True:\n                    # Read in the first 64 bytes of the suspected encrypted\n                    # uImage header\n                    fd = self.module.config.open_file(result.file.path, offset=result.offset)\n                    encrypted_header_data = binwalk.core.compat.str2bytes(fd.read(64))\n                    fd.close()\n\n                    # Decrypt the header\n                    decrypted_header_data = self._hilink_decrypt(encrypted_header_data)\n\n                    # Pull out the image size and image name fields from the decrypted uImage header\n                    # and add them to the printed description.\n                    result.size = struct.unpack(b\">L\", decrypted_header_data[12:16])[0]\n                    result.description += \", size: %d\" % (result.size)\n                    # NOTE: The description field should be 32 bytes? Hilink seems to use only 24 bytes for this field,\n                    #       even though the header size is still 64 bytes?\n                    result.description += ', image name: \"%s\"' % binwalk.core.compat.bytes2str(decrypted_header_data[32:56]).strip(\"\\x00\")\n\n                    # Do some basic validation on the decrypted size and image\n                    # name fields\n                    if result.size > (result.file.size - result.offset):\n                        result.valid = False\n                    if not all(c in string.printable for c in result.description):\n                        result.valid = False", "code_tokens": ["def", "scan", "(", "self", ",", "result", ")", ":", "if", "self", ".", "enabled", "is", "True", ":", "if", "result", ".", "valid", "is", "True", ":", "if", "result", ".", "description", ".", "lower", "(", ")", ".", "startswith", "(", "self", ".", "SIGNATURE_DESCRIPTION", ")", "is", "True", ":", "# Read in the first 64 bytes of the suspected encrypted", "# uImage header", "fd", "=", "self", ".", "module", ".", "config", ".", "open_file", "(", "result", ".", "file", ".", "path", ",", "offset", "=", "result", ".", "offset", ")", "encrypted_header_data", "=", "binwalk", ".", "core", ".", "compat", ".", "str2bytes", "(", "fd", ".", "read", "(", "64", ")", ")", "fd", ".", "close", "(", ")", "# Decrypt the header", "decrypted_header_data", "=", "self", ".", "_hilink_decrypt", "(", "encrypted_header_data", ")", "# Pull out the image size and image name fields from the decrypted uImage header", "# and add them to the printed description.", "result", ".", "size", "=", "struct", ".", "unpack", "(", "b\">L\"", ",", "decrypted_header_data", "[", "12", ":", "16", "]", ")", "[", "0", "]", "result", ".", "description", "+=", "\", size: %d\"", "%", "(", "result", ".", "size", ")", "# NOTE: The description field should be 32 bytes? Hilink seems to use only 24 bytes for this field,", "#       even though the header size is still 64 bytes?", "result", ".", "description", "+=", "', image name: \"%s\"'", "%", "binwalk", ".", "core", ".", "compat", ".", "bytes2str", "(", "decrypted_header_data", "[", "32", ":", "56", "]", ")", ".", "strip", "(", "\"\\x00\"", ")", "# Do some basic validation on the decrypted size and image", "# name fields", "if", "result", ".", "size", ">", "(", "result", ".", "file", ".", "size", "-", "result", ".", "offset", ")", ":", "result", ".", "valid", "=", "False", "if", "not", "all", "(", "c", "in", "string", ".", "printable", "for", "c", "in", "result", ".", "description", ")", ":", "result", ".", "valid", "=", "False"], "docstring": "Validate signature results.", "docstring_tokens": ["Validate", "signature", "results", "."], "sha": "a0c5315fd2bae167e5c3d8469ce95d5defc743c2", "url": "https://github.com/ReFirmLabs/binwalk/blob/a0c5315fd2bae167e5c3d8469ce95d5defc743c2/src/binwalk/plugins/hilink.py#L65-L94", "partition": "train"}
{"repo": "mukulhase/WebWhatsapp-Wrapper", "path": "sample/new_messages_observer.py", "func_name": "run", "original_string": "def run():\n    print(\"Environment\", os.environ)\n    try:\n        os.environ[\"SELENIUM\"]\n    except KeyError:\n        print(\"Please set the environment variable SELENIUM to Selenium URL\")\n        sys.exit(1)\n\n    driver = WhatsAPIDriver(client='remote', command_executor=os.environ[\"SELENIUM\"])\n    print(\"Waiting for QR\")\n    driver.wait_for_login()\n    print(\"Bot started\")\n\n    driver.subscribe_new_messages(NewMessageObserver())\n    print(\"Waiting for new messages...\")\n\n    \"\"\" Locks the main thread while the subscription in running \"\"\"\n    while True:\n        time.sleep(60)", "language": "python", "code": "def run():\n    print(\"Environment\", os.environ)\n    try:\n        os.environ[\"SELENIUM\"]\n    except KeyError:\n        print(\"Please set the environment variable SELENIUM to Selenium URL\")\n        sys.exit(1)\n\n    driver = WhatsAPIDriver(client='remote', command_executor=os.environ[\"SELENIUM\"])\n    print(\"Waiting for QR\")\n    driver.wait_for_login()\n    print(\"Bot started\")\n\n    driver.subscribe_new_messages(NewMessageObserver())\n    print(\"Waiting for new messages...\")\n\n    \"\"\" Locks the main thread while the subscription in running \"\"\"\n    while True:\n        time.sleep(60)", "code_tokens": ["def", "run", "(", ")", ":", "print", "(", "\"Environment\"", ",", "os", ".", "environ", ")", "try", ":", "os", ".", "environ", "[", "\"SELENIUM\"", "]", "except", "KeyError", ":", "print", "(", "\"Please set the environment variable SELENIUM to Selenium URL\"", ")", "sys", ".", "exit", "(", "1", ")", "driver", "=", "WhatsAPIDriver", "(", "client", "=", "'remote'", ",", "command_executor", "=", "os", ".", "environ", "[", "\"SELENIUM\"", "]", ")", "print", "(", "\"Waiting for QR\"", ")", "driver", ".", "wait_for_login", "(", ")", "print", "(", "\"Bot started\"", ")", "driver", ".", "subscribe_new_messages", "(", "NewMessageObserver", "(", ")", ")", "print", "(", "\"Waiting for new messages...\"", ")", "while", "True", ":", "time", ".", "sleep", "(", "60", ")"], "docstring": "Locks the main thread while the subscription in running", "docstring_tokens": ["Locks", "the", "main", "thread", "while", "the", "subscription", "in", "running"], "sha": "81b918ee4e0cd0cb563807a72baa167f670d70cb", "url": "https://github.com/mukulhase/WebWhatsapp-Wrapper/blob/81b918ee4e0cd0cb563807a72baa167f670d70cb/sample/new_messages_observer.py#L8-L26", "partition": "train"}
{"repo": "mukulhase/WebWhatsapp-Wrapper", "path": "webwhatsapi/__init__.py", "func_name": "WhatsAPIDriver.save_firefox_profile", "original_string": "def save_firefox_profile(self, remove_old=False):\n        \"\"\"Function to save the firefox profile to the permanant one\"\"\"\n        self.logger.info(\"Saving profile from %s to %s\" % (self._profile.path, self._profile_path))\n\n        if remove_old:\n            if os.path.exists(self._profile_path):\n                try:\n                    shutil.rmtree(self._profile_path)\n                except OSError:\n                    pass\n\n            shutil.copytree(os.path.join(self._profile.path), self._profile_path,\n                            ignore=shutil.ignore_patterns(\"parent.lock\", \"lock\", \".parentlock\"))\n        else:\n            for item in os.listdir(self._profile.path):\n                if item in [\"parent.lock\", \"lock\", \".parentlock\"]:\n                    continue\n                s = os.path.join(self._profile.path, item)\n                d = os.path.join(self._profile_path, item)\n                if os.path.isdir(s):\n                    shutil.copytree(s, d,\n                                    ignore=shutil.ignore_patterns(\"parent.lock\", \"lock\", \".parentlock\"))\n                else:\n                    shutil.copy2(s, d)\n\n        with open(os.path.join(self._profile_path, self._LOCAL_STORAGE_FILE), 'w') as f:\n            f.write(dumps(self.get_local_storage()))", "language": "python", "code": "def save_firefox_profile(self, remove_old=False):\n        \"\"\"Function to save the firefox profile to the permanant one\"\"\"\n        self.logger.info(\"Saving profile from %s to %s\" % (self._profile.path, self._profile_path))\n\n        if remove_old:\n            if os.path.exists(self._profile_path):\n                try:\n                    shutil.rmtree(self._profile_path)\n                except OSError:\n                    pass\n\n            shutil.copytree(os.path.join(self._profile.path), self._profile_path,\n                            ignore=shutil.ignore_patterns(\"parent.lock\", \"lock\", \".parentlock\"))\n        else:\n            for item in os.listdir(self._profile.path):\n                if item in [\"parent.lock\", \"lock\", \".parentlock\"]:\n                    continue\n                s = os.path.join(self._profile.path, item)\n                d = os.path.join(self._profile_path, item)\n                if os.path.isdir(s):\n                    shutil.copytree(s, d,\n                                    ignore=shutil.ignore_patterns(\"parent.lock\", \"lock\", \".parentlock\"))\n                else:\n                    shutil.copy2(s, d)\n\n        with open(os.path.join(self._profile_path, self._LOCAL_STORAGE_FILE), 'w') as f:\n            f.write(dumps(self.get_local_storage()))", "code_tokens": ["def", "save_firefox_profile", "(", "self", ",", "remove_old", "=", "False", ")", ":", "self", ".", "logger", ".", "info", "(", "\"Saving profile from %s to %s\"", "%", "(", "self", ".", "_profile", ".", "path", ",", "self", ".", "_profile_path", ")", ")", "if", "remove_old", ":", "if", "os", ".", "path", ".", "exists", "(", "self", ".", "_profile_path", ")", ":", "try", ":", "shutil", ".", "rmtree", "(", "self", ".", "_profile_path", ")", "except", "OSError", ":", "pass", "shutil", ".", "copytree", "(", "os", ".", "path", ".", "join", "(", "self", ".", "_profile", ".", "path", ")", ",", "self", ".", "_profile_path", ",", "ignore", "=", "shutil", ".", "ignore_patterns", "(", "\"parent.lock\"", ",", "\"lock\"", ",", "\".parentlock\"", ")", ")", "else", ":", "for", "item", "in", "os", ".", "listdir", "(", "self", ".", "_profile", ".", "path", ")", ":", "if", "item", "in", "[", "\"parent.lock\"", ",", "\"lock\"", ",", "\".parentlock\"", "]", ":", "continue", "s", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_profile", ".", "path", ",", "item", ")", "d", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_profile_path", ",", "item", ")", "if", "os", ".", "path", ".", "isdir", "(", "s", ")", ":", "shutil", ".", "copytree", "(", "s", ",", "d", ",", "ignore", "=", "shutil", ".", "ignore_patterns", "(", "\"parent.lock\"", ",", "\"lock\"", ",", "\".parentlock\"", ")", ")", "else", ":", "shutil", ".", "copy2", "(", "s", ",", "d", ")", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "_profile_path", ",", "self", ".", "_LOCAL_STORAGE_FILE", ")", ",", "'w'", ")", "as", "f", ":", "f", ".", "write", "(", "dumps", "(", "self", ".", "get_local_storage", "(", ")", ")", ")"], "docstring": "Function to save the firefox profile to the permanant one", "docstring_tokens": ["Function", "to", "save", "the", "firefox", "profile", "to", "the", "permanant", "one"], "sha": "81b918ee4e0cd0cb563807a72baa167f670d70cb", "url": "https://github.com/mukulhase/WebWhatsapp-Wrapper/blob/81b918ee4e0cd0cb563807a72baa167f670d70cb/webwhatsapi/__init__.py#L111-L137", "partition": "train"}
{"repo": "kivy/python-for-android", "path": "pythonforandroid/bootstrap.py", "func_name": "Bootstrap.distribute_aars", "original_string": "def distribute_aars(self, arch):\n        '''Process existing .aar bundles and copy to current dist dir.'''\n        info('Unpacking aars')\n        for aar in glob.glob(join(self.ctx.aars_dir, '*.aar')):\n            self._unpack_aar(aar, arch)", "language": "python", "code": "def distribute_aars(self, arch):\n        '''Process existing .aar bundles and copy to current dist dir.'''\n        info('Unpacking aars')\n        for aar in glob.glob(join(self.ctx.aars_dir, '*.aar')):\n            self._unpack_aar(aar, arch)", "code_tokens": ["def", "distribute_aars", "(", "self", ",", "arch", ")", ":", "info", "(", "'Unpacking aars'", ")", "for", "aar", "in", "glob", ".", "glob", "(", "join", "(", "self", ".", "ctx", ".", "aars_dir", ",", "'*.aar'", ")", ")", ":", "self", ".", "_unpack_aar", "(", "aar", ",", "arch", ")"], "docstring": "Process existing .aar bundles and copy to current dist dir.", "docstring_tokens": ["Process", "existing", ".", "aar", "bundles", "and", "copy", "to", "current", "dist", "dir", "."], "sha": "8e0e8056bc22e4d5bd3398a6b0301f38ff167933", "url": "https://github.com/kivy/python-for-android/blob/8e0e8056bc22e4d5bd3398a6b0301f38ff167933/pythonforandroid/bootstrap.py#L231-L235", "partition": "train"}
{"repo": "kivy/python-for-android", "path": "pythonforandroid/bootstraps/pygame/build/buildlib/argparse.py", "func_name": "_ActionsContainer.add_argument", "original_string": "def add_argument(self, *args, **kwargs):\r\n        \"\"\"\r\n        add_argument(dest, ..., name=value, ...)\r\n        add_argument(option_string, option_string, ..., name=value, ...)\r\n        \"\"\"\r\n\r\n        # if no positional args are supplied or only one is supplied and\r\n        # it doesn't look like an option string, parse a positional\r\n        # argument\r\n        chars = self.prefix_chars\r\n        if not args or len(args) == 1 and args[0][0] not in chars:\r\n            if args and 'dest' in kwargs:\r\n                raise ValueError('dest supplied twice for positional argument')\r\n            kwargs = self._get_positional_kwargs(*args, **kwargs)\r\n\r\n        # otherwise, we're adding an optional argument\r\n        else:\r\n            kwargs = self._get_optional_kwargs(*args, **kwargs)\r\n\r\n        # if no default was supplied, use the parser-level default\r\n        if 'default' not in kwargs:\r\n            dest = kwargs['dest']\r\n            if dest in self._defaults:\r\n                kwargs['default'] = self._defaults[dest]\r\n            elif self.argument_default is not None:\r\n                kwargs['default'] = self.argument_default\r\n\r\n        # create the action object, and add it to the parser\r\n        action_class = self._pop_action_class(kwargs)\r\n        if not _callable(action_class):\r\n            raise ValueError('unknown action \"%s\"' % action_class)\r\n        action = action_class(**kwargs)\r\n\r\n        # raise an error if the action type is not callable\r\n        type_func = self._registry_get('type', action.type, action.type)\r\n        if not _callable(type_func):\r\n            raise ValueError('%r is not callable' % type_func)\r\n\r\n        return self._add_action(action)", "language": "python", "code": "def add_argument(self, *args, **kwargs):\r\n        \"\"\"\r\n        add_argument(dest, ..., name=value, ...)\r\n        add_argument(option_string, option_string, ..., name=value, ...)\r\n        \"\"\"\r\n\r\n        # if no positional args are supplied or only one is supplied and\r\n        # it doesn't look like an option string, parse a positional\r\n        # argument\r\n        chars = self.prefix_chars\r\n        if not args or len(args) == 1 and args[0][0] not in chars:\r\n            if args and 'dest' in kwargs:\r\n                raise ValueError('dest supplied twice for positional argument')\r\n            kwargs = self._get_positional_kwargs(*args, **kwargs)\r\n\r\n        # otherwise, we're adding an optional argument\r\n        else:\r\n            kwargs = self._get_optional_kwargs(*args, **kwargs)\r\n\r\n        # if no default was supplied, use the parser-level default\r\n        if 'default' not in kwargs:\r\n            dest = kwargs['dest']\r\n            if dest in self._defaults:\r\n                kwargs['default'] = self._defaults[dest]\r\n            elif self.argument_default is not None:\r\n                kwargs['default'] = self.argument_default\r\n\r\n        # create the action object, and add it to the parser\r\n        action_class = self._pop_action_class(kwargs)\r\n        if not _callable(action_class):\r\n            raise ValueError('unknown action \"%s\"' % action_class)\r\n        action = action_class(**kwargs)\r\n\r\n        # raise an error if the action type is not callable\r\n        type_func = self._registry_get('type', action.type, action.type)\r\n        if not _callable(type_func):\r\n            raise ValueError('%r is not callable' % type_func)\r\n\r\n        return self._add_action(action)", "code_tokens": ["def", "add_argument", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "# if no positional args are supplied or only one is supplied and\r", "# it doesn't look like an option string, parse a positional\r", "# argument\r", "chars", "=", "self", ".", "prefix_chars", "if", "not", "args", "or", "len", "(", "args", ")", "==", "1", "and", "args", "[", "0", "]", "[", "0", "]", "not", "in", "chars", ":", "if", "args", "and", "'dest'", "in", "kwargs", ":", "raise", "ValueError", "(", "'dest supplied twice for positional argument'", ")", "kwargs", "=", "self", ".", "_get_positional_kwargs", "(", "*", "args", ",", "*", "*", "kwargs", ")", "# otherwise, we're adding an optional argument\r", "else", ":", "kwargs", "=", "self", ".", "_get_optional_kwargs", "(", "*", "args", ",", "*", "*", "kwargs", ")", "# if no default was supplied, use the parser-level default\r", "if", "'default'", "not", "in", "kwargs", ":", "dest", "=", "kwargs", "[", "'dest'", "]", "if", "dest", "in", "self", ".", "_defaults", ":", "kwargs", "[", "'default'", "]", "=", "self", ".", "_defaults", "[", "dest", "]", "elif", "self", ".", "argument_default", "is", "not", "None", ":", "kwargs", "[", "'default'", "]", "=", "self", ".", "argument_default", "# create the action object, and add it to the parser\r", "action_class", "=", "self", ".", "_pop_action_class", "(", "kwargs", ")", "if", "not", "_callable", "(", "action_class", ")", ":", "raise", "ValueError", "(", "'unknown action \"%s\"'", "%", "action_class", ")", "action", "=", "action_class", "(", "*", "*", "kwargs", ")", "# raise an error if the action type is not callable\r", "type_func", "=", "self", ".", "_registry_get", "(", "'type'", ",", "action", ".", "type", ",", "action", ".", "type", ")", "if", "not", "_callable", "(", "type_func", ")", ":", "raise", "ValueError", "(", "'%r is not callable'", "%", "type_func", ")", "return", "self", ".", "_add_action", "(", "action", ")"], "docstring": "add_argument(dest, ..., name=value, ...)\r\n        add_argument(option_string, option_string, ..., name=value, ...)", "docstring_tokens": ["add_argument", "(", "dest", "...", "name", "=", "value", "...", ")", "add_argument", "(", "option_string", "option_string", "...", "name", "=", "value", "...", ")"], "sha": "8e0e8056bc22e4d5bd3398a6b0301f38ff167933", "url": "https://github.com/kivy/python-for-android/blob/8e0e8056bc22e4d5bd3398a6b0301f38ff167933/pythonforandroid/bootstraps/pygame/build/buildlib/argparse.py#L1276-L1314", "partition": "train"}
{"repo": "kivy/python-for-android", "path": "pythonforandroid/bootstraps/pygame/build/buildlib/argparse.py", "func_name": "ArgumentParser.error", "original_string": "def error(self, message):\r\n        \"\"\"error(message: string)\r\n\r\n        Prints a usage message incorporating the message to stderr and\r\n        exits.\r\n\r\n        If you override this in a subclass, it should not return -- it\r\n        should either exit or raise an exception.\r\n        \"\"\"\r\n        self.print_usage(_sys.stderr)\r\n        self.exit(2, _('%s: error: %s\\n') % (self.prog, message))", "language": "python", "code": "def error(self, message):\r\n        \"\"\"error(message: string)\r\n\r\n        Prints a usage message incorporating the message to stderr and\r\n        exits.\r\n\r\n        If you override this in a subclass, it should not return -- it\r\n        should either exit or raise an exception.\r\n        \"\"\"\r\n        self.print_usage(_sys.stderr)\r\n        self.exit(2, _('%s: error: %s\\n') % (self.prog, message))", "code_tokens": ["def", "error", "(", "self", ",", "message", ")", ":", "self", ".", "print_usage", "(", "_sys", ".", "stderr", ")", "self", ".", "exit", "(", "2", ",", "_", "(", "'%s: error: %s\\n'", ")", "%", "(", "self", ".", "prog", ",", "message", ")", ")"], "docstring": "error(message: string)\r\n\r\n        Prints a usage message incorporating the message to stderr and\r\n        exits.\r\n\r\n        If you override this in a subclass, it should not return -- it\r\n        should either exit or raise an exception.", "docstring_tokens": ["error", "(", "message", ":", "string", ")", "Prints", "a", "usage", "message", "incorporating", "the", "message", "to", "stderr", "and", "exits", ".", "If", "you", "override", "this", "in", "a", "subclass", "it", "should", "not", "return", "--", "it", "should", "either", "exit", "or", "raise", "an", "exception", "."], "sha": "8e0e8056bc22e4d5bd3398a6b0301f38ff167933", "url": "https://github.com/kivy/python-for-android/blob/8e0e8056bc22e4d5bd3398a6b0301f38ff167933/pythonforandroid/bootstraps/pygame/build/buildlib/argparse.py#L2348-L2358", "partition": "train"}
{"repo": "kennethreitz/records", "path": "records.py", "func_name": "isexception", "original_string": "def isexception(obj):\n    \"\"\"Given an object, return a boolean indicating whether it is an instance\n    or subclass of :py:class:`Exception`.\n    \"\"\"\n    if isinstance(obj, Exception):\n        return True\n    if isclass(obj) and issubclass(obj, Exception):\n        return True\n    return False", "language": "python", "code": "def isexception(obj):\n    \"\"\"Given an object, return a boolean indicating whether it is an instance\n    or subclass of :py:class:`Exception`.\n    \"\"\"\n    if isinstance(obj, Exception):\n        return True\n    if isclass(obj) and issubclass(obj, Exception):\n        return True\n    return False", "code_tokens": ["def", "isexception", "(", "obj", ")", ":", "if", "isinstance", "(", "obj", ",", "Exception", ")", ":", "return", "True", "if", "isclass", "(", "obj", ")", "and", "issubclass", "(", "obj", ",", "Exception", ")", ":", "return", "True", "return", "False"], "docstring": "Given an object, return a boolean indicating whether it is an instance\n    or subclass of :py:class:`Exception`.", "docstring_tokens": ["Given", "an", "object", "return", "a", "boolean", "indicating", "whether", "it", "is", "an", "instance", "or", "subclass", "of", ":", "py", ":", "class", ":", "Exception", "."], "sha": "ecd857266c5e7830d657cbe0196816314790563b", "url": "https://github.com/kennethreitz/records/blob/ecd857266c5e7830d657cbe0196816314790563b/records.py#L16-L24", "partition": "train"}
{"repo": "seatgeek/fuzzywuzzy", "path": "benchmarks.py", "func_name": "print_result_from_timeit", "original_string": "def print_result_from_timeit(stmt='pass', setup='pass', number=1000000):\n    \"\"\"\n    Clean function to know how much time took the execution of one statement\n    \"\"\"\n    units = [\"s\", \"ms\", \"us\", \"ns\"]\n    duration = timeit(stmt, setup, number=int(number))\n    avg_duration = duration / float(number)\n    thousands = int(math.floor(math.log(avg_duration, 1000)))\n\n    print(\"Total time: %fs. Average run: %.3f%s.\" % (\n        duration, avg_duration * (1000 ** -thousands), units[-thousands]))", "language": "python", "code": "def print_result_from_timeit(stmt='pass', setup='pass', number=1000000):\n    \"\"\"\n    Clean function to know how much time took the execution of one statement\n    \"\"\"\n    units = [\"s\", \"ms\", \"us\", \"ns\"]\n    duration = timeit(stmt, setup, number=int(number))\n    avg_duration = duration / float(number)\n    thousands = int(math.floor(math.log(avg_duration, 1000)))\n\n    print(\"Total time: %fs. Average run: %.3f%s.\" % (\n        duration, avg_duration * (1000 ** -thousands), units[-thousands]))", "code_tokens": ["def", "print_result_from_timeit", "(", "stmt", "=", "'pass'", ",", "setup", "=", "'pass'", ",", "number", "=", "1000000", ")", ":", "units", "=", "[", "\"s\"", ",", "\"ms\"", ",", "\"us\"", ",", "\"ns\"", "]", "duration", "=", "timeit", "(", "stmt", ",", "setup", ",", "number", "=", "int", "(", "number", ")", ")", "avg_duration", "=", "duration", "/", "float", "(", "number", ")", "thousands", "=", "int", "(", "math", ".", "floor", "(", "math", ".", "log", "(", "avg_duration", ",", "1000", ")", ")", ")", "print", "(", "\"Total time: %fs. Average run: %.3f%s.\"", "%", "(", "duration", ",", "avg_duration", "*", "(", "1000", "**", "-", "thousands", ")", ",", "units", "[", "-", "thousands", "]", ")", ")"], "docstring": "Clean function to know how much time took the execution of one statement", "docstring_tokens": ["Clean", "function", "to", "know", "how", "much", "time", "took", "the", "execution", "of", "one", "statement"], "sha": "778162c5a73256745eb6ae22f925bc2dbcf7c894", "url": "https://github.com/seatgeek/fuzzywuzzy/blob/778162c5a73256745eb6ae22f925bc2dbcf7c894/benchmarks.py#L47-L57", "partition": "train"}
{"repo": "aws/sagemaker-python-sdk", "path": "src/sagemaker/pipeline.py", "func_name": "PipelineModel.deploy", "original_string": "def deploy(self, initial_instance_count, instance_type, endpoint_name=None, tags=None):\n        \"\"\"Deploy this ``Model`` to an ``Endpoint`` and optionally return a ``Predictor``.\n\n        Create a SageMaker ``Model`` and ``EndpointConfig``, and deploy an ``Endpoint`` from this ``Model``.\n        If ``self.predictor_cls`` is not None, this method returns a the result of invoking\n        ``self.predictor_cls`` on the created endpoint name.\n\n        The name of the created model is accessible in the ``name`` field of this ``Model`` after deploy returns\n\n        The name of the created endpoint is accessible in the ``endpoint_name``\n        field of this ``Model`` after deploy returns.\n\n        Args:\n            instance_type (str): The EC2 instance type to deploy this Model to. For example, 'ml.p2.xlarge'.\n            initial_instance_count (int): The initial number of instances to run in the\n                ``Endpoint`` created from this ``Model``.\n            endpoint_name (str): The name of the endpoint to create (default: None).\n                If not specified, a unique endpoint name will be created.\n            tags(List[dict[str, str]]): The list of tags to attach to this specific endpoint.\n\n        Returns:\n            callable[string, sagemaker.session.Session] or None: Invocation of ``self.predictor_cls`` on\n                the created endpoint name, if ``self.predictor_cls`` is not None. Otherwise, return None.\n        \"\"\"\n        if not self.sagemaker_session:\n            self.sagemaker_session = Session()\n\n        containers = self.pipeline_container_def(instance_type)\n\n        self.name = self.name or name_from_image(containers[0]['Image'])\n        self.sagemaker_session.create_model(self.name, self.role, containers, vpc_config=self.vpc_config)\n\n        production_variant = sagemaker.production_variant(self.name, instance_type, initial_instance_count)\n        self.endpoint_name = endpoint_name or self.name\n        self.sagemaker_session.endpoint_from_production_variants(self.endpoint_name, [production_variant], tags)\n        if self.predictor_cls:\n            return self.predictor_cls(self.endpoint_name, self.sagemaker_session)", "language": "python", "code": "def deploy(self, initial_instance_count, instance_type, endpoint_name=None, tags=None):\n        \"\"\"Deploy this ``Model`` to an ``Endpoint`` and optionally return a ``Predictor``.\n\n        Create a SageMaker ``Model`` and ``EndpointConfig``, and deploy an ``Endpoint`` from this ``Model``.\n        If ``self.predictor_cls`` is not None, this method returns a the result of invoking\n        ``self.predictor_cls`` on the created endpoint name.\n\n        The name of the created model is accessible in the ``name`` field of this ``Model`` after deploy returns\n\n        The name of the created endpoint is accessible in the ``endpoint_name``\n        field of this ``Model`` after deploy returns.\n\n        Args:\n            instance_type (str): The EC2 instance type to deploy this Model to. For example, 'ml.p2.xlarge'.\n            initial_instance_count (int): The initial number of instances to run in the\n                ``Endpoint`` created from this ``Model``.\n            endpoint_name (str): The name of the endpoint to create (default: None).\n                If not specified, a unique endpoint name will be created.\n            tags(List[dict[str, str]]): The list of tags to attach to this specific endpoint.\n\n        Returns:\n            callable[string, sagemaker.session.Session] or None: Invocation of ``self.predictor_cls`` on\n                the created endpoint name, if ``self.predictor_cls`` is not None. Otherwise, return None.\n        \"\"\"\n        if not self.sagemaker_session:\n            self.sagemaker_session = Session()\n\n        containers = self.pipeline_container_def(instance_type)\n\n        self.name = self.name or name_from_image(containers[0]['Image'])\n        self.sagemaker_session.create_model(self.name, self.role, containers, vpc_config=self.vpc_config)\n\n        production_variant = sagemaker.production_variant(self.name, instance_type, initial_instance_count)\n        self.endpoint_name = endpoint_name or self.name\n        self.sagemaker_session.endpoint_from_production_variants(self.endpoint_name, [production_variant], tags)\n        if self.predictor_cls:\n            return self.predictor_cls(self.endpoint_name, self.sagemaker_session)", "code_tokens": ["def", "deploy", "(", "self", ",", "initial_instance_count", ",", "instance_type", ",", "endpoint_name", "=", "None", ",", "tags", "=", "None", ")", ":", "if", "not", "self", ".", "sagemaker_session", ":", "self", ".", "sagemaker_session", "=", "Session", "(", ")", "containers", "=", "self", ".", "pipeline_container_def", "(", "instance_type", ")", "self", ".", "name", "=", "self", ".", "name", "or", "name_from_image", "(", "containers", "[", "0", "]", "[", "'Image'", "]", ")", "self", ".", "sagemaker_session", ".", "create_model", "(", "self", ".", "name", ",", "self", ".", "role", ",", "containers", ",", "vpc_config", "=", "self", ".", "vpc_config", ")", "production_variant", "=", "sagemaker", ".", "production_variant", "(", "self", ".", "name", ",", "instance_type", ",", "initial_instance_count", ")", "self", ".", "endpoint_name", "=", "endpoint_name", "or", "self", ".", "name", "self", ".", "sagemaker_session", ".", "endpoint_from_production_variants", "(", "self", ".", "endpoint_name", ",", "[", "production_variant", "]", ",", "tags", ")", "if", "self", ".", "predictor_cls", ":", "return", "self", ".", "predictor_cls", "(", "self", ".", "endpoint_name", ",", "self", ".", "sagemaker_session", ")"], "docstring": "Deploy this ``Model`` to an ``Endpoint`` and optionally return a ``Predictor``.\n\n        Create a SageMaker ``Model`` and ``EndpointConfig``, and deploy an ``Endpoint`` from this ``Model``.\n        If ``self.predictor_cls`` is not None, this method returns a the result of invoking\n        ``self.predictor_cls`` on the created endpoint name.\n\n        The name of the created model is accessible in the ``name`` field of this ``Model`` after deploy returns\n\n        The name of the created endpoint is accessible in the ``endpoint_name``\n        field of this ``Model`` after deploy returns.\n\n        Args:\n            instance_type (str): The EC2 instance type to deploy this Model to. For example, 'ml.p2.xlarge'.\n            initial_instance_count (int): The initial number of instances to run in the\n                ``Endpoint`` created from this ``Model``.\n            endpoint_name (str): The name of the endpoint to create (default: None).\n                If not specified, a unique endpoint name will be created.\n            tags(List[dict[str, str]]): The list of tags to attach to this specific endpoint.\n\n        Returns:\n            callable[string, sagemaker.session.Session] or None: Invocation of ``self.predictor_cls`` on\n                the created endpoint name, if ``self.predictor_cls`` is not None. Otherwise, return None.", "docstring_tokens": ["Deploy", "this", "Model", "to", "an", "Endpoint", "and", "optionally", "return", "a", "Predictor", "."], "sha": "a9e724c7d3f5572b68c3903548c792a59d99799a", "url": "https://github.com/aws/sagemaker-python-sdk/blob/a9e724c7d3f5572b68c3903548c792a59d99799a/src/sagemaker/pipeline.py#L70-L106", "partition": "train"}
{"repo": "aws/sagemaker-python-sdk", "path": "src/sagemaker/pipeline.py", "func_name": "PipelineModel._create_sagemaker_pipeline_model", "original_string": "def _create_sagemaker_pipeline_model(self, instance_type):\n        \"\"\"Create a SageMaker Model Entity\n\n        Args:\n            instance_type (str): The EC2 instance type that this Model will be used for, this is only\n                used to determine if the image needs GPU support or not.\n            accelerator_type (str): Type of Elastic Inference accelerator to attach to an endpoint for model loading\n                and inference, for example, 'ml.eia1.medium'. If not specified, no Elastic Inference accelerator\n                will be attached to the endpoint.\n        \"\"\"\n        if not self.sagemaker_session:\n            self.sagemaker_session = Session()\n\n        containers = self.pipeline_container_def(instance_type)\n\n        self.name = self.name or name_from_image(containers[0]['Image'])\n        self.sagemaker_session.create_model(self.name, self.role, containers, vpc_config=self.vpc_config)", "language": "python", "code": "def _create_sagemaker_pipeline_model(self, instance_type):\n        \"\"\"Create a SageMaker Model Entity\n\n        Args:\n            instance_type (str): The EC2 instance type that this Model will be used for, this is only\n                used to determine if the image needs GPU support or not.\n            accelerator_type (str): Type of Elastic Inference accelerator to attach to an endpoint for model loading\n                and inference, for example, 'ml.eia1.medium'. If not specified, no Elastic Inference accelerator\n                will be attached to the endpoint.\n        \"\"\"\n        if not self.sagemaker_session:\n            self.sagemaker_session = Session()\n\n        containers = self.pipeline_container_def(instance_type)\n\n        self.name = self.name or name_from_image(containers[0]['Image'])\n        self.sagemaker_session.create_model(self.name, self.role, containers, vpc_config=self.vpc_config)", "code_tokens": ["def", "_create_sagemaker_pipeline_model", "(", "self", ",", "instance_type", ")", ":", "if", "not", "self", ".", "sagemaker_session", ":", "self", ".", "sagemaker_session", "=", "Session", "(", ")", "containers", "=", "self", ".", "pipeline_container_def", "(", "instance_type", ")", "self", ".", "name", "=", "self", ".", "name", "or", "name_from_image", "(", "containers", "[", "0", "]", "[", "'Image'", "]", ")", "self", ".", "sagemaker_session", ".", "create_model", "(", "self", ".", "name", ",", "self", ".", "role", ",", "containers", ",", "vpc_config", "=", "self", ".", "vpc_config", ")"], "docstring": "Create a SageMaker Model Entity\n\n        Args:\n            instance_type (str): The EC2 instance type that this Model will be used for, this is only\n                used to determine if the image needs GPU support or not.\n            accelerator_type (str): Type of Elastic Inference accelerator to attach to an endpoint for model loading\n                and inference, for example, 'ml.eia1.medium'. If not specified, no Elastic Inference accelerator\n                will be attached to the endpoint.", "docstring_tokens": ["Create", "a", "SageMaker", "Model", "Entity"], "sha": "a9e724c7d3f5572b68c3903548c792a59d99799a", "url": "https://github.com/aws/sagemaker-python-sdk/blob/a9e724c7d3f5572b68c3903548c792a59d99799a/src/sagemaker/pipeline.py#L108-L124", "partition": "train"}
{"repo": "deepmind/sonnet", "path": "sonnet/python/modules/util.py", "func_name": "count_variables_by_type", "original_string": "def count_variables_by_type(variables=None):\n  \"\"\"Returns a dict mapping dtypes to number of variables and scalars.\n\n  Args:\n    variables: iterable of `tf.Variable`s, or None. If None is passed, then all\n      global and local variables in the current graph are used.\n\n  Returns:\n    A dict mapping tf.dtype keys to a dict containing the keys 'num_scalars' and\n      'num_variables'.\n  \"\"\"\n  if variables is None:\n    variables = tf.global_variables() + tf.local_variables()\n  unique_types = set(v.dtype.base_dtype for v in variables)\n  results_dict = {}\n  for dtype in unique_types:\n    if dtype == tf.string:\n      tf.logging.warning(\n          \"NB: string Variables present. The memory usage for these  Variables \"\n          \"will not be accurately computed as it depends on the exact strings \"\n          \"stored in a particular session.\")\n    vars_of_type = [v for v in variables if v.dtype.base_dtype == dtype]\n    num_scalars = sum(v.shape.num_elements() for v in vars_of_type)\n    results_dict[dtype] = {\n        \"num_variables\": len(vars_of_type),\n        \"num_scalars\": num_scalars\n    }\n  return results_dict", "language": "python", "code": "def count_variables_by_type(variables=None):\n  \"\"\"Returns a dict mapping dtypes to number of variables and scalars.\n\n  Args:\n    variables: iterable of `tf.Variable`s, or None. If None is passed, then all\n      global and local variables in the current graph are used.\n\n  Returns:\n    A dict mapping tf.dtype keys to a dict containing the keys 'num_scalars' and\n      'num_variables'.\n  \"\"\"\n  if variables is None:\n    variables = tf.global_variables() + tf.local_variables()\n  unique_types = set(v.dtype.base_dtype for v in variables)\n  results_dict = {}\n  for dtype in unique_types:\n    if dtype == tf.string:\n      tf.logging.warning(\n          \"NB: string Variables present. The memory usage for these  Variables \"\n          \"will not be accurately computed as it depends on the exact strings \"\n          \"stored in a particular session.\")\n    vars_of_type = [v for v in variables if v.dtype.base_dtype == dtype]\n    num_scalars = sum(v.shape.num_elements() for v in vars_of_type)\n    results_dict[dtype] = {\n        \"num_variables\": len(vars_of_type),\n        \"num_scalars\": num_scalars\n    }\n  return results_dict", "code_tokens": ["def", "count_variables_by_type", "(", "variables", "=", "None", ")", ":", "if", "variables", "is", "None", ":", "variables", "=", "tf", ".", "global_variables", "(", ")", "+", "tf", ".", "local_variables", "(", ")", "unique_types", "=", "set", "(", "v", ".", "dtype", ".", "base_dtype", "for", "v", "in", "variables", ")", "results_dict", "=", "{", "}", "for", "dtype", "in", "unique_types", ":", "if", "dtype", "==", "tf", ".", "string", ":", "tf", ".", "logging", ".", "warning", "(", "\"NB: string Variables present. The memory usage for these  Variables \"", "\"will not be accurately computed as it depends on the exact strings \"", "\"stored in a particular session.\"", ")", "vars_of_type", "=", "[", "v", "for", "v", "in", "variables", "if", "v", ".", "dtype", ".", "base_dtype", "==", "dtype", "]", "num_scalars", "=", "sum", "(", "v", ".", "shape", ".", "num_elements", "(", ")", "for", "v", "in", "vars_of_type", ")", "results_dict", "[", "dtype", "]", "=", "{", "\"num_variables\"", ":", "len", "(", "vars_of_type", ")", ",", "\"num_scalars\"", ":", "num_scalars", "}", "return", "results_dict"], "docstring": "Returns a dict mapping dtypes to number of variables and scalars.\n\n  Args:\n    variables: iterable of `tf.Variable`s, or None. If None is passed, then all\n      global and local variables in the current graph are used.\n\n  Returns:\n    A dict mapping tf.dtype keys to a dict containing the keys 'num_scalars' and\n      'num_variables'.", "docstring_tokens": ["Returns", "a", "dict", "mapping", "dtypes", "to", "number", "of", "variables", "and", "scalars", "."], "sha": "00612ca3178964d86b556e062694d808ff81fcca", "url": "https://github.com/deepmind/sonnet/blob/00612ca3178964d86b556e062694d808ff81fcca/sonnet/python/modules/util.py#L630-L657", "partition": "train"}
{"repo": "deepmind/sonnet", "path": "sonnet/python/modules/util.py", "func_name": "count_variables_by_type", "original_string": "def count_variables_by_type(variables=None):\n  \"\"\"Returns a dict mapping dtypes to number of variables and scalars.\n\n  Args:\n    variables: iterable of `tf.Variable`s, or None. If None is passed, then all\n      global and local variables in the current graph are used.\n\n  Returns:\n    A dict mapping tf.dtype keys to a dict containing the keys 'num_scalars' and\n      'num_variables'.\n  \"\"\"\n  if variables is None:\n    variables = tf.global_variables() + tf.local_variables()\n  unique_types = set(v.dtype.base_dtype for v in variables)\n  results_dict = {}\n  for dtype in unique_types:\n    if dtype == tf.string:\n      tf.logging.warning(\n          \"NB: string Variables present. The memory usage for these  Variables \"\n          \"will not be accurately computed as it depends on the exact strings \"\n          \"stored in a particular session.\")\n    vars_of_type = [v for v in variables if v.dtype.base_dtype == dtype]\n    num_scalars = sum(v.shape.num_elements() for v in vars_of_type)\n    results_dict[dtype] = {\n        \"num_variables\": len(vars_of_type),\n        \"num_scalars\": num_scalars\n    }\n  return results_dict", "language": "python", "code": "def count_variables_by_type(variables=None):\n  \"\"\"Returns a dict mapping dtypes to number of variables and scalars.\n\n  Args:\n    variables: iterable of `tf.Variable`s, or None. If None is passed, then all\n      global and local variables in the current graph are used.\n\n  Returns:\n    A dict mapping tf.dtype keys to a dict containing the keys 'num_scalars' and\n      'num_variables'.\n  \"\"\"\n  if variables is None:\n    variables = tf.global_variables() + tf.local_variables()\n  unique_types = set(v.dtype.base_dtype for v in variables)\n  results_dict = {}\n  for dtype in unique_types:\n    if dtype == tf.string:\n      tf.logging.warning(\n          \"NB: string Variables present. The memory usage for these  Variables \"\n          \"will not be accurately computed as it depends on the exact strings \"\n          \"stored in a particular session.\")\n    vars_of_type = [v for v in variables if v.dtype.base_dtype == dtype]\n    num_scalars = sum(v.shape.num_elements() for v in vars_of_type)\n    results_dict[dtype] = {\n        \"num_variables\": len(vars_of_type),\n        \"num_scalars\": num_scalars\n    }\n  return results_dict", "code_tokens": ["def", "count_variables_by_type", "(", "variables", "=", "None", ")", ":", "if", "variables", "is", "None", ":", "variables", "=", "tf", ".", "global_variables", "(", ")", "+", "tf", ".", "local_variables", "(", ")", "unique_types", "=", "set", "(", "v", ".", "dtype", ".", "base_dtype", "for", "v", "in", "variables", ")", "results_dict", "=", "{", "}", "for", "dtype", "in", "unique_types", ":", "if", "dtype", "==", "tf", ".", "string", ":", "tf", ".", "logging", ".", "warning", "(", "\"NB: string Variables present. The memory usage for these  Variables \"", "\"will not be accurately computed as it depends on the exact strings \"", "\"stored in a particular session.\"", ")", "vars_of_type", "=", "[", "v", "for", "v", "in", "variables", "if", "v", ".", "dtype", ".", "base_dtype", "==", "dtype", "]", "num_scalars", "=", "sum", "(", "v", ".", "shape", ".", "num_elements", "(", ")", "for", "v", "in", "vars_of_type", ")", "results_dict", "[", "dtype", "]", "=", "{", "\"num_variables\"", ":", "len", "(", "vars_of_type", ")", ",", "\"num_scalars\"", ":", "num_scalars", "}", "return", "results_dict"], "docstring": "Returns a dict mapping dtypes to number of variables and scalars.\n\n  Args:\n    variables: iterable of `tf.Variable`s, or None. If None is passed, then all\n      global and local variables in the current graph are used.\n\n  Returns:\n    A dict mapping tf.dtype keys to a dict containing the keys 'num_scalars' and\n      'num_variables'.", "docstring_tokens": ["Returns", "a", "dict", "mapping", "dtypes", "to", "number", "of", "variables", "and", "scalars", "."], "sha": "00612ca3178964d86b556e062694d808ff81fcca", "url": "https://github.com/deepmind/sonnet/blob/00612ca3178964d86b556e062694d808ff81fcca/sonnet/python/modules/util.py#L630-L657", "partition": "train"}
{"repo": "burnash/gspread", "path": "gspread/client.py", "func_name": "Client.insert_permission", "original_string": "def insert_permission(\n        self,\n        file_id,\n        value,\n        perm_type,\n        role,\n        notify=True,\n        email_message=None,\n        with_link=False\n    ):\n        \"\"\"Creates a new permission for a file.\n\n        :param file_id: a spreadsheet ID (aka file ID.)\n        :type file_id: str\n        :param value: user or group e-mail address, domain name\n                      or None for 'default' type.\n        :type value: str, None\n        :param perm_type: (optional) The account type.\n               Allowed values are: ``user``, ``group``, ``domain``,\n               ``anyone``\n        :type perm_type: str\n        :param role: (optional) The primary role for this user.\n               Allowed values are: ``owner``, ``writer``, ``reader``\n        :type str:\n\n        :param notify: (optional) Whether to send an email to the target user/domain.\n        :type notify: str\n        :param email_message: (optional) An email message to be sent if notify=True.\n        :type email_message: str\n        \n        :param with_link: (optional) Whether the link is required for this permission to be active.\n        :type with_link: bool\n\n        Examples::\n\n            # Give write permissions to otto@example.com\n\n            gc.insert_permission(\n                '0BmgG6nO_6dprnRRUWl1UFE',\n                'otto@example.org',\n                perm_type='user',\n                role='writer'\n            )\n\n            # Make the spreadsheet publicly readable\n\n            gc.insert_permission(\n                '0BmgG6nO_6dprnRRUWl1UFE',\n                None,\n                perm_type='anyone',\n                role='reader'\n            )\n\n        \"\"\"\n\n        url = '{0}/{1}/permissions'.format(DRIVE_FILES_API_V2_URL, file_id)\n\n        payload = {\n            'value': value,\n            'type': perm_type,\n            'role': role,\n            'withLink': with_link\n        }\n\n        params = {\n            'sendNotificationEmails': notify,\n            'emailMessage': email_message\n        }\n\n        self.request(\n            'post',\n            url,\n            json=payload,\n            params=params\n        )", "language": "python", "code": "def insert_permission(\n        self,\n        file_id,\n        value,\n        perm_type,\n        role,\n        notify=True,\n        email_message=None,\n        with_link=False\n    ):\n        \"\"\"Creates a new permission for a file.\n\n        :param file_id: a spreadsheet ID (aka file ID.)\n        :type file_id: str\n        :param value: user or group e-mail address, domain name\n                      or None for 'default' type.\n        :type value: str, None\n        :param perm_type: (optional) The account type.\n               Allowed values are: ``user``, ``group``, ``domain``,\n               ``anyone``\n        :type perm_type: str\n        :param role: (optional) The primary role for this user.\n               Allowed values are: ``owner``, ``writer``, ``reader``\n        :type str:\n\n        :param notify: (optional) Whether to send an email to the target user/domain.\n        :type notify: str\n        :param email_message: (optional) An email message to be sent if notify=True.\n        :type email_message: str\n        \n        :param with_link: (optional) Whether the link is required for this permission to be active.\n        :type with_link: bool\n\n        Examples::\n\n            # Give write permissions to otto@example.com\n\n            gc.insert_permission(\n                '0BmgG6nO_6dprnRRUWl1UFE',\n                'otto@example.org',\n                perm_type='user',\n                role='writer'\n            )\n\n            # Make the spreadsheet publicly readable\n\n            gc.insert_permission(\n                '0BmgG6nO_6dprnRRUWl1UFE',\n                None,\n                perm_type='anyone',\n                role='reader'\n            )\n\n        \"\"\"\n\n        url = '{0}/{1}/permissions'.format(DRIVE_FILES_API_V2_URL, file_id)\n\n        payload = {\n            'value': value,\n            'type': perm_type,\n            'role': role,\n            'withLink': with_link\n        }\n\n        params = {\n            'sendNotificationEmails': notify,\n            'emailMessage': email_message\n        }\n\n        self.request(\n            'post',\n            url,\n            json=payload,\n            params=params\n        )", "code_tokens": ["def", "insert_permission", "(", "self", ",", "file_id", ",", "value", ",", "perm_type", ",", "role", ",", "notify", "=", "True", ",", "email_message", "=", "None", ",", "with_link", "=", "False", ")", ":", "url", "=", "'{0}/{1}/permissions'", ".", "format", "(", "DRIVE_FILES_API_V2_URL", ",", "file_id", ")", "payload", "=", "{", "'value'", ":", "value", ",", "'type'", ":", "perm_type", ",", "'role'", ":", "role", ",", "'withLink'", ":", "with_link", "}", "params", "=", "{", "'sendNotificationEmails'", ":", "notify", ",", "'emailMessage'", ":", "email_message", "}", "self", ".", "request", "(", "'post'", ",", "url", ",", "json", "=", "payload", ",", "params", "=", "params", ")"], "docstring": "Creates a new permission for a file.\n\n        :param file_id: a spreadsheet ID (aka file ID.)\n        :type file_id: str\n        :param value: user or group e-mail address, domain name\n                      or None for 'default' type.\n        :type value: str, None\n        :param perm_type: (optional) The account type.\n               Allowed values are: ``user``, ``group``, ``domain``,\n               ``anyone``\n        :type perm_type: str\n        :param role: (optional) The primary role for this user.\n               Allowed values are: ``owner``, ``writer``, ``reader``\n        :type str:\n\n        :param notify: (optional) Whether to send an email to the target user/domain.\n        :type notify: str\n        :param email_message: (optional) An email message to be sent if notify=True.\n        :type email_message: str\n        \n        :param with_link: (optional) Whether the link is required for this permission to be active.\n        :type with_link: bool\n\n        Examples::\n\n            # Give write permissions to otto@example.com\n\n            gc.insert_permission(\n                '0BmgG6nO_6dprnRRUWl1UFE',\n                'otto@example.org',\n                perm_type='user',\n                role='writer'\n            )\n\n            # Make the spreadsheet publicly readable\n\n            gc.insert_permission(\n                '0BmgG6nO_6dprnRRUWl1UFE',\n                None,\n                perm_type='anyone',\n                role='reader'\n            )", "docstring_tokens": ["Creates", "a", "new", "permission", "for", "a", "file", "."], "sha": "0e8debe208095aeed3e3e7136c2fa5cd74090946", "url": "https://github.com/burnash/gspread/blob/0e8debe208095aeed3e3e7136c2fa5cd74090946/gspread/client.py#L347-L421", "partition": "train"}
{"repo": "burnash/gspread", "path": "gspread/client.py", "func_name": "Client.remove_permission", "original_string": "def remove_permission(self, file_id, permission_id):\n        \"\"\"Deletes a permission from a file.\n\n        :param file_id: a spreadsheet ID (aka file ID.)\n        :type file_id: str\n        :param permission_id: an ID for the permission.\n        :type permission_id: str\n        \"\"\"\n        url = '{0}/{1}/permissions/{2}'.format(\n            DRIVE_FILES_API_V2_URL,\n            file_id,\n            permission_id\n        )\n\n        self.request('delete', url)", "language": "python", "code": "def remove_permission(self, file_id, permission_id):\n        \"\"\"Deletes a permission from a file.\n\n        :param file_id: a spreadsheet ID (aka file ID.)\n        :type file_id: str\n        :param permission_id: an ID for the permission.\n        :type permission_id: str\n        \"\"\"\n        url = '{0}/{1}/permissions/{2}'.format(\n            DRIVE_FILES_API_V2_URL,\n            file_id,\n            permission_id\n        )\n\n        self.request('delete', url)", "code_tokens": ["def", "remove_permission", "(", "self", ",", "file_id", ",", "permission_id", ")", ":", "url", "=", "'{0}/{1}/permissions/{2}'", ".", "format", "(", "DRIVE_FILES_API_V2_URL", ",", "file_id", ",", "permission_id", ")", "self", ".", "request", "(", "'delete'", ",", "url", ")"], "docstring": "Deletes a permission from a file.\n\n        :param file_id: a spreadsheet ID (aka file ID.)\n        :type file_id: str\n        :param permission_id: an ID for the permission.\n        :type permission_id: str", "docstring_tokens": ["Deletes", "a", "permission", "from", "a", "file", "."], "sha": "0e8debe208095aeed3e3e7136c2fa5cd74090946", "url": "https://github.com/burnash/gspread/blob/0e8debe208095aeed3e3e7136c2fa5cd74090946/gspread/client.py#L423-L437", "partition": "train"}
{"repo": "lmcinnes/umap", "path": "umap/utils.py", "func_name": "tau_rand_int", "original_string": "def tau_rand_int(state):\n    \"\"\"A fast (pseudo)-random number generator.\n\n    Parameters\n    ----------\n    state: array of int64, shape (3,)\n        The internal state of the rng\n\n    Returns\n    -------\n    A (pseudo)-random int32 value\n    \"\"\"\n    state[0] = (((state[0] & 4294967294) << 12) & 0xffffffff) ^ (\n        (((state[0] << 13) & 0xffffffff) ^ state[0]) >> 19\n    )\n    state[1] = (((state[1] & 4294967288) << 4) & 0xffffffff) ^ (\n        (((state[1] << 2) & 0xffffffff) ^ state[1]) >> 25\n    )\n    state[2] = (((state[2] & 4294967280) << 17) & 0xffffffff) ^ (\n        (((state[2] << 3) & 0xffffffff) ^ state[2]) >> 11\n    )\n\n    return state[0] ^ state[1] ^ state[2]", "language": "python", "code": "def tau_rand_int(state):\n    \"\"\"A fast (pseudo)-random number generator.\n\n    Parameters\n    ----------\n    state: array of int64, shape (3,)\n        The internal state of the rng\n\n    Returns\n    -------\n    A (pseudo)-random int32 value\n    \"\"\"\n    state[0] = (((state[0] & 4294967294) << 12) & 0xffffffff) ^ (\n        (((state[0] << 13) & 0xffffffff) ^ state[0]) >> 19\n    )\n    state[1] = (((state[1] & 4294967288) << 4) & 0xffffffff) ^ (\n        (((state[1] << 2) & 0xffffffff) ^ state[1]) >> 25\n    )\n    state[2] = (((state[2] & 4294967280) << 17) & 0xffffffff) ^ (\n        (((state[2] << 3) & 0xffffffff) ^ state[2]) >> 11\n    )\n\n    return state[0] ^ state[1] ^ state[2]", "code_tokens": ["def", "tau_rand_int", "(", "state", ")", ":", "state", "[", "0", "]", "=", "(", "(", "(", "state", "[", "0", "]", "&", "4294967294", ")", "<<", "12", ")", "&", "0xffffffff", ")", "^", "(", "(", "(", "(", "state", "[", "0", "]", "<<", "13", ")", "&", "0xffffffff", ")", "^", "state", "[", "0", "]", ")", ">>", "19", ")", "state", "[", "1", "]", "=", "(", "(", "(", "state", "[", "1", "]", "&", "4294967288", ")", "<<", "4", ")", "&", "0xffffffff", ")", "^", "(", "(", "(", "(", "state", "[", "1", "]", "<<", "2", ")", "&", "0xffffffff", ")", "^", "state", "[", "1", "]", ")", ">>", "25", ")", "state", "[", "2", "]", "=", "(", "(", "(", "state", "[", "2", "]", "&", "4294967280", ")", "<<", "17", ")", "&", "0xffffffff", ")", "^", "(", "(", "(", "(", "state", "[", "2", "]", "<<", "3", ")", "&", "0xffffffff", ")", "^", "state", "[", "2", "]", ")", ">>", "11", ")", "return", "state", "[", "0", "]", "^", "state", "[", "1", "]", "^", "state", "[", "2", "]"], "docstring": "A fast (pseudo)-random number generator.\n\n    Parameters\n    ----------\n    state: array of int64, shape (3,)\n        The internal state of the rng\n\n    Returns\n    -------\n    A (pseudo)-random int32 value", "docstring_tokens": ["A", "fast", "(", "pseudo", ")", "-", "random", "number", "generator", "."], "sha": "bbb01c03ba49f7bff8f77fd662d00e50d6686c77", "url": "https://github.com/lmcinnes/umap/blob/bbb01c03ba49f7bff8f77fd662d00e50d6686c77/umap/utils.py#L12-L34", "partition": "train"}
{"repo": "lmcinnes/umap", "path": "umap/utils.py", "func_name": "norm", "original_string": "def norm(vec):\n    \"\"\"Compute the (standard l2) norm of a vector.\n\n    Parameters\n    ----------\n    vec: array of shape (dim,)\n\n    Returns\n    -------\n    The l2 norm of vec.\n    \"\"\"\n    result = 0.0\n    for i in range(vec.shape[0]):\n        result += vec[i] ** 2\n    return np.sqrt(result)", "language": "python", "code": "def norm(vec):\n    \"\"\"Compute the (standard l2) norm of a vector.\n\n    Parameters\n    ----------\n    vec: array of shape (dim,)\n\n    Returns\n    -------\n    The l2 norm of vec.\n    \"\"\"\n    result = 0.0\n    for i in range(vec.shape[0]):\n        result += vec[i] ** 2\n    return np.sqrt(result)", "code_tokens": ["def", "norm", "(", "vec", ")", ":", "result", "=", "0.0", "for", "i", "in", "range", "(", "vec", ".", "shape", "[", "0", "]", ")", ":", "result", "+=", "vec", "[", "i", "]", "**", "2", "return", "np", ".", "sqrt", "(", "result", ")"], "docstring": "Compute the (standard l2) norm of a vector.\n\n    Parameters\n    ----------\n    vec: array of shape (dim,)\n\n    Returns\n    -------\n    The l2 norm of vec.", "docstring_tokens": ["Compute", "the", "(", "standard", "l2", ")", "norm", "of", "a", "vector", "."], "sha": "bbb01c03ba49f7bff8f77fd662d00e50d6686c77", "url": "https://github.com/lmcinnes/umap/blob/bbb01c03ba49f7bff8f77fd662d00e50d6686c77/umap/utils.py#L55-L69", "partition": "train"}
{"repo": "lark-parser/lark", "path": "lark/parsers/earley.py", "func_name": "Parser.predict_and_complete", "original_string": "def predict_and_complete(self, i, to_scan, columns, transitives):\n        \"\"\"The core Earley Predictor and Completer.\n\n        At each stage of the input, we handling any completed items (things\n        that matched on the last cycle) and use those to predict what should\n        come next in the input stream. The completions and any predicted\n        non-terminals are recursively processed until we reach a set of,\n        which can be added to the scan list for the next scanner cycle.\"\"\"\n        # Held Completions (H in E.Scotts paper).\n        node_cache = {}\n        held_completions = {}\n\n        column = columns[i]\n        # R (items) = Ei (column.items)\n        items = deque(column)\n        while items:\n            item = items.pop()    # remove an element, A say, from R\n\n            ### The Earley completer\n            if item.is_complete:   ### (item.s == string)\n                if item.node is None:\n                    label = (item.s, item.start, i)\n                    item.node = node_cache[label] if label in node_cache else node_cache.setdefault(label, SymbolNode(*label))\n                    item.node.add_family(item.s, item.rule, item.start, None, None)\n\n                # create_leo_transitives(item.rule.origin, item.start)\n\n                ###R Joop Leo right recursion Completer\n                if item.rule.origin in transitives[item.start]:\n                    transitive = transitives[item.start][item.s]\n                    if transitive.previous in transitives[transitive.column]:\n                        root_transitive = transitives[transitive.column][transitive.previous]\n                    else:\n                        root_transitive = transitive\n\n                    new_item = Item(transitive.rule, transitive.ptr, transitive.start)\n                    label = (root_transitive.s, root_transitive.start, i)\n                    new_item.node = node_cache[label] if label in node_cache else node_cache.setdefault(label, SymbolNode(*label))\n                    new_item.node.add_path(root_transitive, item.node)\n                    if new_item.expect in self.TERMINALS:\n                        # Add (B :: aC.B, h, y) to Q\n                        to_scan.add(new_item)\n                    elif new_item not in column:\n                        # Add (B :: aC.B, h, y) to Ei and R\n                        column.add(new_item)\n                        items.append(new_item)\n                ###R Regular Earley completer\n                else:\n                    # Empty has 0 length. If we complete an empty symbol in a particular\n                    # parse step, we need to be able to use that same empty symbol to complete\n                    # any predictions that result, that themselves require empty. Avoids\n                    # infinite recursion on empty symbols.\n                    # held_completions is 'H' in E.Scott's paper.\n                    is_empty_item = item.start == i\n                    if is_empty_item:\n                        held_completions[item.rule.origin] = item.node\n\n                    originators = [originator for originator in columns[item.start] if originator.expect is not None and originator.expect == item.s]\n                    for originator in originators:\n                        new_item = originator.advance()\n                        label = (new_item.s, originator.start, i)\n                        new_item.node = node_cache[label] if label in node_cache else node_cache.setdefault(label, SymbolNode(*label))\n                        new_item.node.add_family(new_item.s, new_item.rule, i, originator.node, item.node)\n                        if new_item.expect in self.TERMINALS:\n                            # Add (B :: aC.B, h, y) to Q\n                            to_scan.add(new_item)\n                        elif new_item not in column:\n                            # Add (B :: aC.B, h, y) to Ei and R\n                            column.add(new_item)\n                            items.append(new_item)\n\n            ### The Earley predictor\n            elif item.expect in self.NON_TERMINALS: ### (item.s == lr0)\n                new_items = []\n                for rule in self.predictions[item.expect]:\n                    new_item = Item(rule, 0, i)\n                    new_items.append(new_item)\n\n                # Process any held completions (H).\n                if item.expect in held_completions:\n                    new_item = item.advance()\n                    label = (new_item.s, item.start, i)\n                    new_item.node = node_cache[label] if label in node_cache else node_cache.setdefault(label, SymbolNode(*label))\n                    new_item.node.add_family(new_item.s, new_item.rule, new_item.start, item.node, held_completions[item.expect])\n                    new_items.append(new_item)\n\n                for new_item in new_items:\n                    if new_item.expect in self.TERMINALS:\n                        to_scan.add(new_item)\n                    elif new_item not in column:\n                        column.add(new_item)\n                        items.append(new_item)", "language": "python", "code": "def predict_and_complete(self, i, to_scan, columns, transitives):\n        \"\"\"The core Earley Predictor and Completer.\n\n        At each stage of the input, we handling any completed items (things\n        that matched on the last cycle) and use those to predict what should\n        come next in the input stream. The completions and any predicted\n        non-terminals are recursively processed until we reach a set of,\n        which can be added to the scan list for the next scanner cycle.\"\"\"\n        # Held Completions (H in E.Scotts paper).\n        node_cache = {}\n        held_completions = {}\n\n        column = columns[i]\n        # R (items) = Ei (column.items)\n        items = deque(column)\n        while items:\n            item = items.pop()    # remove an element, A say, from R\n\n            ### The Earley completer\n            if item.is_complete:   ### (item.s == string)\n                if item.node is None:\n                    label = (item.s, item.start, i)\n                    item.node = node_cache[label] if label in node_cache else node_cache.setdefault(label, SymbolNode(*label))\n                    item.node.add_family(item.s, item.rule, item.start, None, None)\n\n                # create_leo_transitives(item.rule.origin, item.start)\n\n                ###R Joop Leo right recursion Completer\n                if item.rule.origin in transitives[item.start]:\n                    transitive = transitives[item.start][item.s]\n                    if transitive.previous in transitives[transitive.column]:\n                        root_transitive = transitives[transitive.column][transitive.previous]\n                    else:\n                        root_transitive = transitive\n\n                    new_item = Item(transitive.rule, transitive.ptr, transitive.start)\n                    label = (root_transitive.s, root_transitive.start, i)\n                    new_item.node = node_cache[label] if label in node_cache else node_cache.setdefault(label, SymbolNode(*label))\n                    new_item.node.add_path(root_transitive, item.node)\n                    if new_item.expect in self.TERMINALS:\n                        # Add (B :: aC.B, h, y) to Q\n                        to_scan.add(new_item)\n                    elif new_item not in column:\n                        # Add (B :: aC.B, h, y) to Ei and R\n                        column.add(new_item)\n                        items.append(new_item)\n                ###R Regular Earley completer\n                else:\n                    # Empty has 0 length. If we complete an empty symbol in a particular\n                    # parse step, we need to be able to use that same empty symbol to complete\n                    # any predictions that result, that themselves require empty. Avoids\n                    # infinite recursion on empty symbols.\n                    # held_completions is 'H' in E.Scott's paper.\n                    is_empty_item = item.start == i\n                    if is_empty_item:\n                        held_completions[item.rule.origin] = item.node\n\n                    originators = [originator for originator in columns[item.start] if originator.expect is not None and originator.expect == item.s]\n                    for originator in originators:\n                        new_item = originator.advance()\n                        label = (new_item.s, originator.start, i)\n                        new_item.node = node_cache[label] if label in node_cache else node_cache.setdefault(label, SymbolNode(*label))\n                        new_item.node.add_family(new_item.s, new_item.rule, i, originator.node, item.node)\n                        if new_item.expect in self.TERMINALS:\n                            # Add (B :: aC.B, h, y) to Q\n                            to_scan.add(new_item)\n                        elif new_item not in column:\n                            # Add (B :: aC.B, h, y) to Ei and R\n                            column.add(new_item)\n                            items.append(new_item)\n\n            ### The Earley predictor\n            elif item.expect in self.NON_TERMINALS: ### (item.s == lr0)\n                new_items = []\n                for rule in self.predictions[item.expect]:\n                    new_item = Item(rule, 0, i)\n                    new_items.append(new_item)\n\n                # Process any held completions (H).\n                if item.expect in held_completions:\n                    new_item = item.advance()\n                    label = (new_item.s, item.start, i)\n                    new_item.node = node_cache[label] if label in node_cache else node_cache.setdefault(label, SymbolNode(*label))\n                    new_item.node.add_family(new_item.s, new_item.rule, new_item.start, item.node, held_completions[item.expect])\n                    new_items.append(new_item)\n\n                for new_item in new_items:\n                    if new_item.expect in self.TERMINALS:\n                        to_scan.add(new_item)\n                    elif new_item not in column:\n                        column.add(new_item)\n                        items.append(new_item)", "code_tokens": ["def", "predict_and_complete", "(", "self", ",", "i", ",", "to_scan", ",", "columns", ",", "transitives", ")", ":", "# Held Completions (H in E.Scotts paper).", "node_cache", "=", "{", "}", "held_completions", "=", "{", "}", "column", "=", "columns", "[", "i", "]", "# R (items) = Ei (column.items)", "items", "=", "deque", "(", "column", ")", "while", "items", ":", "item", "=", "items", ".", "pop", "(", ")", "# remove an element, A say, from R", "### The Earley completer", "if", "item", ".", "is_complete", ":", "### (item.s == string)", "if", "item", ".", "node", "is", "None", ":", "label", "=", "(", "item", ".", "s", ",", "item", ".", "start", ",", "i", ")", "item", ".", "node", "=", "node_cache", "[", "label", "]", "if", "label", "in", "node_cache", "else", "node_cache", ".", "setdefault", "(", "label", ",", "SymbolNode", "(", "*", "label", ")", ")", "item", ".", "node", ".", "add_family", "(", "item", ".", "s", ",", "item", ".", "rule", ",", "item", ".", "start", ",", "None", ",", "None", ")", "# create_leo_transitives(item.rule.origin, item.start)", "###R Joop Leo right recursion Completer", "if", "item", ".", "rule", ".", "origin", "in", "transitives", "[", "item", ".", "start", "]", ":", "transitive", "=", "transitives", "[", "item", ".", "start", "]", "[", "item", ".", "s", "]", "if", "transitive", ".", "previous", "in", "transitives", "[", "transitive", ".", "column", "]", ":", "root_transitive", "=", "transitives", "[", "transitive", ".", "column", "]", "[", "transitive", ".", "previous", "]", "else", ":", "root_transitive", "=", "transitive", "new_item", "=", "Item", "(", "transitive", ".", "rule", ",", "transitive", ".", "ptr", ",", "transitive", ".", "start", ")", "label", "=", "(", "root_transitive", ".", "s", ",", "root_transitive", ".", "start", ",", "i", ")", "new_item", ".", "node", "=", "node_cache", "[", "label", "]", "if", "label", "in", "node_cache", "else", "node_cache", ".", "setdefault", "(", "label", ",", "SymbolNode", "(", "*", "label", ")", ")", "new_item", ".", "node", ".", "add_path", "(", "root_transitive", ",", "item", ".", "node", ")", "if", "new_item", ".", "expect", "in", "self", ".", "TERMINALS", ":", "# Add (B :: aC.B, h, y) to Q", "to_scan", ".", "add", "(", "new_item", ")", "elif", "new_item", "not", "in", "column", ":", "# Add (B :: aC.B, h, y) to Ei and R", "column", ".", "add", "(", "new_item", ")", "items", ".", "append", "(", "new_item", ")", "###R Regular Earley completer", "else", ":", "# Empty has 0 length. If we complete an empty symbol in a particular", "# parse step, we need to be able to use that same empty symbol to complete", "# any predictions that result, that themselves require empty. Avoids", "# infinite recursion on empty symbols.", "# held_completions is 'H' in E.Scott's paper.", "is_empty_item", "=", "item", ".", "start", "==", "i", "if", "is_empty_item", ":", "held_completions", "[", "item", ".", "rule", ".", "origin", "]", "=", "item", ".", "node", "originators", "=", "[", "originator", "for", "originator", "in", "columns", "[", "item", ".", "start", "]", "if", "originator", ".", "expect", "is", "not", "None", "and", "originator", ".", "expect", "==", "item", ".", "s", "]", "for", "originator", "in", "originators", ":", "new_item", "=", "originator", ".", "advance", "(", ")", "label", "=", "(", "new_item", ".", "s", ",", "originator", ".", "start", ",", "i", ")", "new_item", ".", "node", "=", "node_cache", "[", "label", "]", "if", "label", "in", "node_cache", "else", "node_cache", ".", "setdefault", "(", "label", ",", "SymbolNode", "(", "*", "label", ")", ")", "new_item", ".", "node", ".", "add_family", "(", "new_item", ".", "s", ",", "new_item", ".", "rule", ",", "i", ",", "originator", ".", "node", ",", "item", ".", "node", ")", "if", "new_item", ".", "expect", "in", "self", ".", "TERMINALS", ":", "# Add (B :: aC.B, h, y) to Q", "to_scan", ".", "add", "(", "new_item", ")", "elif", "new_item", "not", "in", "column", ":", "# Add (B :: aC.B, h, y) to Ei and R", "column", ".", "add", "(", "new_item", ")", "items", ".", "append", "(", "new_item", ")", "### The Earley predictor", "elif", "item", ".", "expect", "in", "self", ".", "NON_TERMINALS", ":", "### (item.s == lr0)", "new_items", "=", "[", "]", "for", "rule", "in", "self", ".", "predictions", "[", "item", ".", "expect", "]", ":", "new_item", "=", "Item", "(", "rule", ",", "0", ",", "i", ")", "new_items", ".", "append", "(", "new_item", ")", "# Process any held completions (H).", "if", "item", ".", "expect", "in", "held_completions", ":", "new_item", "=", "item", ".", "advance", "(", ")", "label", "=", "(", "new_item", ".", "s", ",", "item", ".", "start", ",", "i", ")", "new_item", ".", "node", "=", "node_cache", "[", "label", "]", "if", "label", "in", "node_cache", "else", "node_cache", ".", "setdefault", "(", "label", ",", "SymbolNode", "(", "*", "label", ")", ")", "new_item", ".", "node", ".", "add_family", "(", "new_item", ".", "s", ",", "new_item", ".", "rule", ",", "new_item", ".", "start", ",", "item", ".", "node", ",", "held_completions", "[", "item", ".", "expect", "]", ")", "new_items", ".", "append", "(", "new_item", ")", "for", "new_item", "in", "new_items", ":", "if", "new_item", ".", "expect", "in", "self", ".", "TERMINALS", ":", "to_scan", ".", "add", "(", "new_item", ")", "elif", "new_item", "not", "in", "column", ":", "column", ".", "add", "(", "new_item", ")", "items", ".", "append", "(", "new_item", ")"], "docstring": "The core Earley Predictor and Completer.\n\n        At each stage of the input, we handling any completed items (things\n        that matched on the last cycle) and use those to predict what should\n        come next in the input stream. The completions and any predicted\n        non-terminals are recursively processed until we reach a set of,\n        which can be added to the scan list for the next scanner cycle.", "docstring_tokens": ["The", "core", "Earley", "Predictor", "and", "Completer", "."], "sha": "a798dec77907e74520dd7e90c7b6a4acc680633a", "url": "https://github.com/lark-parser/lark/blob/a798dec77907e74520dd7e90c7b6a4acc680633a/lark/parsers/earley.py#L56-L147", "partition": "train"}
{"repo": "chriskiehl/Gooey", "path": "gooey/gui/components/widgets/radio_group.py", "func_name": "RadioGroup.applyStyleRules", "original_string": "def applyStyleRules(self):\r\n        \"\"\"\r\n        Conditionally disabled/enables form fields based on the current\r\n        section in the radio group\r\n        \"\"\"\r\n        for button, widget in zip(self.radioButtons, self.widgets):\r\n            if isinstance(widget, CheckBox):\r\n                widget.hideInput()\r\n            if not button.GetValue(): # not checked\r\n                widget.widget.Disable()\r\n            else:\r\n                widget.widget.Enable()", "language": "python", "code": "def applyStyleRules(self):\r\n        \"\"\"\r\n        Conditionally disabled/enables form fields based on the current\r\n        section in the radio group\r\n        \"\"\"\r\n        for button, widget in zip(self.radioButtons, self.widgets):\r\n            if isinstance(widget, CheckBox):\r\n                widget.hideInput()\r\n            if not button.GetValue(): # not checked\r\n                widget.widget.Disable()\r\n            else:\r\n                widget.widget.Enable()", "code_tokens": ["def", "applyStyleRules", "(", "self", ")", ":", "for", "button", ",", "widget", "in", "zip", "(", "self", ".", "radioButtons", ",", "self", ".", "widgets", ")", ":", "if", "isinstance", "(", "widget", ",", "CheckBox", ")", ":", "widget", ".", "hideInput", "(", ")", "if", "not", "button", ".", "GetValue", "(", ")", ":", "# not checked\r", "widget", ".", "widget", ".", "Disable", "(", ")", "else", ":", "widget", ".", "widget", ".", "Enable", "(", ")"], "docstring": "Conditionally disabled/enables form fields based on the current\r\n        section in the radio group", "docstring_tokens": ["Conditionally", "disabled", "/", "enables", "form", "fields", "based", "on", "the", "current", "section", "in", "the", "radio", "group"], "sha": "e598573c6519b953e0ccfc1f3663f827f8cd7e22", "url": "https://github.com/chriskiehl/Gooey/blob/e598573c6519b953e0ccfc1f3663f827f8cd7e22/gooey/gui/components/widgets/radio_group.py#L95-L106", "partition": "train"}
{"repo": "raghakot/keras-vis", "path": "vis/visualization/activation_maximization.py", "func_name": "visualize_activation_with_losses", "original_string": "def visualize_activation_with_losses(input_tensor, losses, wrt_tensor=None,\n                                     seed_input=None, input_range=(0, 255),\n                                     **optimizer_params):\n    \"\"\"Generates the `input_tensor` that minimizes the weighted `losses`. This function is intended for advanced\n    use cases where a custom loss is desired.\n\n    Args:\n        input_tensor: An input tensor of shape: `(samples, channels, image_dims...)` if `image_data_format=\n            channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n        wrt_tensor: Short for, with respect to. The gradients of losses are computed with respect to this tensor.\n            When None, this is assumed to be the same as `input_tensor` (Default value: None)\n        losses: List of ([Loss](vis.losses#Loss), weight) tuples.\n        seed_input: Seeds the optimization with a starting image. Initialized with a random value when set to None.\n            (Default value = None)\n        input_range: Specifies the input range as a `(min, max)` tuple. This is used to rescale the\n            final optimized input to the given range. (Default value=(0, 255))\n        optimizer_params: The **kwargs for optimizer [params](vis.optimizer#optimizerminimize). Will default to\n            reasonable values when required keys are not found.\n\n    Returns:\n        The model input that minimizes the weighted `losses`.\n    \"\"\"\n    # Default optimizer kwargs.\n    optimizer_params = utils.add_defaults_to_kwargs({\n        'seed_input': seed_input,\n        'max_iter': 200,\n        'verbose': False\n    }, **optimizer_params)\n\n    opt = Optimizer(input_tensor, losses, input_range, wrt_tensor=wrt_tensor)\n    img = opt.minimize(**optimizer_params)[0]\n\n    # If range has integer numbers, cast to 'uint8'\n    if isinstance(input_range[0], int) and isinstance(input_range[1], int):\n        img = np.clip(img, input_range[0], input_range[1]).astype('uint8')\n\n    if K.image_data_format() == 'channels_first':\n        img = np.moveaxis(img, 0, -1)\n    return img", "language": "python", "code": "def visualize_activation_with_losses(input_tensor, losses, wrt_tensor=None,\n                                     seed_input=None, input_range=(0, 255),\n                                     **optimizer_params):\n    \"\"\"Generates the `input_tensor` that minimizes the weighted `losses`. This function is intended for advanced\n    use cases where a custom loss is desired.\n\n    Args:\n        input_tensor: An input tensor of shape: `(samples, channels, image_dims...)` if `image_data_format=\n            channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n        wrt_tensor: Short for, with respect to. The gradients of losses are computed with respect to this tensor.\n            When None, this is assumed to be the same as `input_tensor` (Default value: None)\n        losses: List of ([Loss](vis.losses#Loss), weight) tuples.\n        seed_input: Seeds the optimization with a starting image. Initialized with a random value when set to None.\n            (Default value = None)\n        input_range: Specifies the input range as a `(min, max)` tuple. This is used to rescale the\n            final optimized input to the given range. (Default value=(0, 255))\n        optimizer_params: The **kwargs for optimizer [params](vis.optimizer#optimizerminimize). Will default to\n            reasonable values when required keys are not found.\n\n    Returns:\n        The model input that minimizes the weighted `losses`.\n    \"\"\"\n    # Default optimizer kwargs.\n    optimizer_params = utils.add_defaults_to_kwargs({\n        'seed_input': seed_input,\n        'max_iter': 200,\n        'verbose': False\n    }, **optimizer_params)\n\n    opt = Optimizer(input_tensor, losses, input_range, wrt_tensor=wrt_tensor)\n    img = opt.minimize(**optimizer_params)[0]\n\n    # If range has integer numbers, cast to 'uint8'\n    if isinstance(input_range[0], int) and isinstance(input_range[1], int):\n        img = np.clip(img, input_range[0], input_range[1]).astype('uint8')\n\n    if K.image_data_format() == 'channels_first':\n        img = np.moveaxis(img, 0, -1)\n    return img", "code_tokens": ["def", "visualize_activation_with_losses", "(", "input_tensor", ",", "losses", ",", "wrt_tensor", "=", "None", ",", "seed_input", "=", "None", ",", "input_range", "=", "(", "0", ",", "255", ")", ",", "*", "*", "optimizer_params", ")", ":", "# Default optimizer kwargs.", "optimizer_params", "=", "utils", ".", "add_defaults_to_kwargs", "(", "{", "'seed_input'", ":", "seed_input", ",", "'max_iter'", ":", "200", ",", "'verbose'", ":", "False", "}", ",", "*", "*", "optimizer_params", ")", "opt", "=", "Optimizer", "(", "input_tensor", ",", "losses", ",", "input_range", ",", "wrt_tensor", "=", "wrt_tensor", ")", "img", "=", "opt", ".", "minimize", "(", "*", "*", "optimizer_params", ")", "[", "0", "]", "# If range has integer numbers, cast to 'uint8'", "if", "isinstance", "(", "input_range", "[", "0", "]", ",", "int", ")", "and", "isinstance", "(", "input_range", "[", "1", "]", ",", "int", ")", ":", "img", "=", "np", ".", "clip", "(", "img", ",", "input_range", "[", "0", "]", ",", "input_range", "[", "1", "]", ")", ".", "astype", "(", "'uint8'", ")", "if", "K", ".", "image_data_format", "(", ")", "==", "'channels_first'", ":", "img", "=", "np", ".", "moveaxis", "(", "img", ",", "0", ",", "-", "1", ")", "return", "img"], "docstring": "Generates the `input_tensor` that minimizes the weighted `losses`. This function is intended for advanced\n    use cases where a custom loss is desired.\n\n    Args:\n        input_tensor: An input tensor of shape: `(samples, channels, image_dims...)` if `image_data_format=\n            channels_first` or `(samples, image_dims..., channels)` if `image_data_format=channels_last`.\n        wrt_tensor: Short for, with respect to. The gradients of losses are computed with respect to this tensor.\n            When None, this is assumed to be the same as `input_tensor` (Default value: None)\n        losses: List of ([Loss](vis.losses#Loss), weight) tuples.\n        seed_input: Seeds the optimization with a starting image. Initialized with a random value when set to None.\n            (Default value = None)\n        input_range: Specifies the input range as a `(min, max)` tuple. This is used to rescale the\n            final optimized input to the given range. (Default value=(0, 255))\n        optimizer_params: The **kwargs for optimizer [params](vis.optimizer#optimizerminimize). Will default to\n            reasonable values when required keys are not found.\n\n    Returns:\n        The model input that minimizes the weighted `losses`.", "docstring_tokens": ["Generates", "the", "input_tensor", "that", "minimizes", "the", "weighted", "losses", ".", "This", "function", "is", "intended", "for", "advanced", "use", "cases", "where", "a", "custom", "loss", "is", "desired", "."], "sha": "668b0e11dab93f3487f23c17e07f40554a8939e9", "url": "https://github.com/raghakot/keras-vis/blob/668b0e11dab93f3487f23c17e07f40554a8939e9/vis/visualization/activation_maximization.py#L13-L51", "partition": "train"}
{"repo": "dpkp/kafka-python", "path": "kafka/client.py", "func_name": "SimpleClient.send_fetch_request", "original_string": "def send_fetch_request(self, payloads=(), fail_on_error=True,\n                           callback=None, max_wait_time=100, min_bytes=4096):\n        \"\"\"\n        Encode and send a FetchRequest\n\n        Payloads are grouped by topic and partition so they can be pipelined\n        to the same brokers.\n        \"\"\"\n\n        encoder = functools.partial(KafkaProtocol.encode_fetch_request,\n                          max_wait_time=max_wait_time,\n                          min_bytes=min_bytes)\n\n        resps = self._send_broker_aware_request(\n            payloads, encoder,\n            KafkaProtocol.decode_fetch_response)\n\n        return [resp if not callback else callback(resp) for resp in resps\n                if not fail_on_error or not self._raise_on_response_error(resp)]", "language": "python", "code": "def send_fetch_request(self, payloads=(), fail_on_error=True,\n                           callback=None, max_wait_time=100, min_bytes=4096):\n        \"\"\"\n        Encode and send a FetchRequest\n\n        Payloads are grouped by topic and partition so they can be pipelined\n        to the same brokers.\n        \"\"\"\n\n        encoder = functools.partial(KafkaProtocol.encode_fetch_request,\n                          max_wait_time=max_wait_time,\n                          min_bytes=min_bytes)\n\n        resps = self._send_broker_aware_request(\n            payloads, encoder,\n            KafkaProtocol.decode_fetch_response)\n\n        return [resp if not callback else callback(resp) for resp in resps\n                if not fail_on_error or not self._raise_on_response_error(resp)]", "code_tokens": ["def", "send_fetch_request", "(", "self", ",", "payloads", "=", "(", ")", ",", "fail_on_error", "=", "True", ",", "callback", "=", "None", ",", "max_wait_time", "=", "100", ",", "min_bytes", "=", "4096", ")", ":", "encoder", "=", "functools", ".", "partial", "(", "KafkaProtocol", ".", "encode_fetch_request", ",", "max_wait_time", "=", "max_wait_time", ",", "min_bytes", "=", "min_bytes", ")", "resps", "=", "self", ".", "_send_broker_aware_request", "(", "payloads", ",", "encoder", ",", "KafkaProtocol", ".", "decode_fetch_response", ")", "return", "[", "resp", "if", "not", "callback", "else", "callback", "(", "resp", ")", "for", "resp", "in", "resps", "if", "not", "fail_on_error", "or", "not", "self", ".", "_raise_on_response_error", "(", "resp", ")", "]"], "docstring": "Encode and send a FetchRequest\n\n        Payloads are grouped by topic and partition so they can be pipelined\n        to the same brokers.", "docstring_tokens": ["Encode", "and", "send", "a", "FetchRequest"], "sha": "f6a8a38937688ea2cc5dc13d3d1039493be5c9b5", "url": "https://github.com/dpkp/kafka-python/blob/f6a8a38937688ea2cc5dc13d3d1039493be5c9b5/kafka/client.py#L649-L667", "partition": "train"}
{"repo": "davesque/django-rest-framework-simplejwt", "path": "rest_framework_simplejwt/tokens.py", "func_name": "Token.verify", "original_string": "def verify(self):\n        \"\"\"\n        Performs additional validation steps which were not performed when this\n        token was decoded.  This method is part of the \"public\" API to indicate\n        the intention that it may be overridden in subclasses.\n        \"\"\"\n        # According to RFC 7519, the \"exp\" claim is OPTIONAL\n        # (https://tools.ietf.org/html/rfc7519#section-4.1.4).  As a more\n        # correct behavior for authorization tokens, we require an \"exp\"\n        # claim.  We don't want any zombie tokens walking around.\n        self.check_exp()\n\n        # Ensure token id is present\n        if api_settings.JTI_CLAIM not in self.payload:\n            raise TokenError(_('Token has no id'))\n\n        self.verify_token_type()", "language": "python", "code": "def verify(self):\n        \"\"\"\n        Performs additional validation steps which were not performed when this\n        token was decoded.  This method is part of the \"public\" API to indicate\n        the intention that it may be overridden in subclasses.\n        \"\"\"\n        # According to RFC 7519, the \"exp\" claim is OPTIONAL\n        # (https://tools.ietf.org/html/rfc7519#section-4.1.4).  As a more\n        # correct behavior for authorization tokens, we require an \"exp\"\n        # claim.  We don't want any zombie tokens walking around.\n        self.check_exp()\n\n        # Ensure token id is present\n        if api_settings.JTI_CLAIM not in self.payload:\n            raise TokenError(_('Token has no id'))\n\n        self.verify_token_type()", "code_tokens": ["def", "verify", "(", "self", ")", ":", "# According to RFC 7519, the \"exp\" claim is OPTIONAL", "# (https://tools.ietf.org/html/rfc7519#section-4.1.4).  As a more", "# correct behavior for authorization tokens, we require an \"exp\"", "# claim.  We don't want any zombie tokens walking around.", "self", ".", "check_exp", "(", ")", "# Ensure token id is present", "if", "api_settings", ".", "JTI_CLAIM", "not", "in", "self", ".", "payload", ":", "raise", "TokenError", "(", "_", "(", "'Token has no id'", ")", ")", "self", ".", "verify_token_type", "(", ")"], "docstring": "Performs additional validation steps which were not performed when this\n        token was decoded.  This method is part of the \"public\" API to indicate\n        the intention that it may be overridden in subclasses.", "docstring_tokens": ["Performs", "additional", "validation", "steps", "which", "were", "not", "performed", "when", "this", "token", "was", "decoded", ".", "This", "method", "is", "part", "of", "the", "public", "API", "to", "indicate", "the", "intention", "that", "it", "may", "be", "overridden", "in", "subclasses", "."], "sha": "d6084c595aefbf97865d15254b56017e710e8e47", "url": "https://github.com/davesque/django-rest-framework-simplejwt/blob/d6084c595aefbf97865d15254b56017e710e8e47/rest_framework_simplejwt/tokens.py#L84-L100", "partition": "train"}
{"repo": "python-openxml/python-docx", "path": "docx/parts/image.py", "func_name": "ImagePart.from_image", "original_string": "def from_image(cls, image, partname):\n        \"\"\"\n        Return an |ImagePart| instance newly created from *image* and\n        assigned *partname*.\n        \"\"\"\n        return ImagePart(partname, image.content_type, image.blob, image)", "language": "python", "code": "def from_image(cls, image, partname):\n        \"\"\"\n        Return an |ImagePart| instance newly created from *image* and\n        assigned *partname*.\n        \"\"\"\n        return ImagePart(partname, image.content_type, image.blob, image)", "code_tokens": ["def", "from_image", "(", "cls", ",", "image", ",", "partname", ")", ":", "return", "ImagePart", "(", "partname", ",", "image", ".", "content_type", ",", "image", ".", "blob", ",", "image", ")"], "docstring": "Return an |ImagePart| instance newly created from *image* and\n        assigned *partname*.", "docstring_tokens": ["Return", "an", "|ImagePart|", "instance", "newly", "created", "from", "*", "image", "*", "and", "assigned", "*", "partname", "*", "."], "sha": "6756f6cd145511d3eb6d1d188beea391b1ddfd53", "url": "https://github.com/python-openxml/python-docx/blob/6756f6cd145511d3eb6d1d188beea391b1ddfd53/docx/parts/image.py#L63-L68", "partition": "train"}
{"repo": "python-openxml/python-docx", "path": "docx/parts/image.py", "func_name": "ImagePart.load", "original_string": "def load(cls, partname, content_type, blob, package):\n        \"\"\"\n        Called by ``docx.opc.package.PartFactory`` to load an image part from\n        a package being opened by ``Document(...)`` call.\n        \"\"\"\n        return cls(partname, content_type, blob)", "language": "python", "code": "def load(cls, partname, content_type, blob, package):\n        \"\"\"\n        Called by ``docx.opc.package.PartFactory`` to load an image part from\n        a package being opened by ``Document(...)`` call.\n        \"\"\"\n        return cls(partname, content_type, blob)", "code_tokens": ["def", "load", "(", "cls", ",", "partname", ",", "content_type", ",", "blob", ",", "package", ")", ":", "return", "cls", "(", "partname", ",", "content_type", ",", "blob", ")"], "docstring": "Called by ``docx.opc.package.PartFactory`` to load an image part from\n        a package being opened by ``Document(...)`` call.", "docstring_tokens": ["Called", "by", "docx", ".", "opc", ".", "package", ".", "PartFactory", "to", "load", "an", "image", "part", "from", "a", "package", "being", "opened", "by", "Document", "(", "...", ")", "call", "."], "sha": "6756f6cd145511d3eb6d1d188beea391b1ddfd53", "url": "https://github.com/python-openxml/python-docx/blob/6756f6cd145511d3eb6d1d188beea391b1ddfd53/docx/parts/image.py#L77-L82", "partition": "train"}
{"repo": "python-openxml/python-docx", "path": "docx/text/tabstops.py", "func_name": "TabStops.add_tab_stop", "original_string": "def add_tab_stop(self, position, alignment=WD_TAB_ALIGNMENT.LEFT,\n                     leader=WD_TAB_LEADER.SPACES):\n        \"\"\"\n        Add a new tab stop at *position*, a |Length| object specifying the\n        location of the tab stop relative to the paragraph edge. A negative\n        *position* value is valid and appears in hanging indentation. Tab\n        alignment defaults to left, but may be specified by passing a member\n        of the :ref:`WdTabAlignment` enumeration as *alignment*. An optional\n        leader character can be specified by passing a member of the\n        :ref:`WdTabLeader` enumeration as *leader*.\n        \"\"\"\n        tabs = self._pPr.get_or_add_tabs()\n        tab = tabs.insert_tab_in_order(position, alignment, leader)\n        return TabStop(tab)", "language": "python", "code": "def add_tab_stop(self, position, alignment=WD_TAB_ALIGNMENT.LEFT,\n                     leader=WD_TAB_LEADER.SPACES):\n        \"\"\"\n        Add a new tab stop at *position*, a |Length| object specifying the\n        location of the tab stop relative to the paragraph edge. A negative\n        *position* value is valid and appears in hanging indentation. Tab\n        alignment defaults to left, but may be specified by passing a member\n        of the :ref:`WdTabAlignment` enumeration as *alignment*. An optional\n        leader character can be specified by passing a member of the\n        :ref:`WdTabLeader` enumeration as *leader*.\n        \"\"\"\n        tabs = self._pPr.get_or_add_tabs()\n        tab = tabs.insert_tab_in_order(position, alignment, leader)\n        return TabStop(tab)", "code_tokens": ["def", "add_tab_stop", "(", "self", ",", "position", ",", "alignment", "=", "WD_TAB_ALIGNMENT", ".", "LEFT", ",", "leader", "=", "WD_TAB_LEADER", ".", "SPACES", ")", ":", "tabs", "=", "self", ".", "_pPr", ".", "get_or_add_tabs", "(", ")", "tab", "=", "tabs", ".", "insert_tab_in_order", "(", "position", ",", "alignment", ",", "leader", ")", "return", "TabStop", "(", "tab", ")"], "docstring": "Add a new tab stop at *position*, a |Length| object specifying the\n        location of the tab stop relative to the paragraph edge. A negative\n        *position* value is valid and appears in hanging indentation. Tab\n        alignment defaults to left, but may be specified by passing a member\n        of the :ref:`WdTabAlignment` enumeration as *alignment*. An optional\n        leader character can be specified by passing a member of the\n        :ref:`WdTabLeader` enumeration as *leader*.", "docstring_tokens": ["Add", "a", "new", "tab", "stop", "at", "*", "position", "*", "a", "|Length|", "object", "specifying", "the", "location", "of", "the", "tab", "stop", "relative", "to", "the", "paragraph", "edge", ".", "A", "negative", "*", "position", "*", "value", "is", "valid", "and", "appears", "in", "hanging", "indentation", ".", "Tab", "alignment", "defaults", "to", "left", "but", "may", "be", "specified", "by", "passing", "a", "member", "of", "the", ":", "ref", ":", "WdTabAlignment", "enumeration", "as", "*", "alignment", "*", ".", "An", "optional", "leader", "character", "can", "be", "specified", "by", "passing", "a", "member", "of", "the", ":", "ref", ":", "WdTabLeader", "enumeration", "as", "*", "leader", "*", "."], "sha": "6756f6cd145511d3eb6d1d188beea391b1ddfd53", "url": "https://github.com/python-openxml/python-docx/blob/6756f6cd145511d3eb6d1d188beea391b1ddfd53/docx/text/tabstops.py#L69-L82", "partition": "train"}
{"repo": "gboeing/osmnx", "path": "osmnx/stats.py", "func_name": "extended_stats", "original_string": "def extended_stats(G, connectivity=False, anc=False, ecc=False, bc=False, cc=False):\n    \"\"\"\n    Calculate extended topological stats and metrics for a graph.\n\n    Many of these algorithms have an inherently high time complexity. Global\n    topological analysis of large complex networks is extremely time consuming\n    and may exhaust computer memory. Consider using function arguments to not\n    run metrics that require computation of a full matrix of paths if they\n    will not be needed.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    connectivity : bool\n        if True, calculate node and edge connectivity\n    anc : bool\n        if True, calculate average node connectivity\n    ecc : bool\n        if True, calculate shortest paths, eccentricity, and topological metrics\n        that use eccentricity\n    bc : bool\n        if True, calculate node betweenness centrality\n    cc : bool\n        if True, calculate node closeness centrality\n\n    Returns\n    -------\n    stats : dict\n        dictionary of network measures containing the following elements (some\n        only calculated/returned optionally, based on passed parameters):\n\n          - avg_neighbor_degree\n          - avg_neighbor_degree_avg\n          - avg_weighted_neighbor_degree\n          - avg_weighted_neighbor_degree_avg\n          - degree_centrality\n          - degree_centrality_avg\n          - clustering_coefficient\n          - clustering_coefficient_avg\n          - clustering_coefficient_weighted\n          - clustering_coefficient_weighted_avg\n          - pagerank\n          - pagerank_max_node\n          - pagerank_max\n          - pagerank_min_node\n          - pagerank_min\n          - node_connectivity\n          - node_connectivity_avg\n          - edge_connectivity\n          - eccentricity\n          - diameter\n          - radius\n          - center\n          - periphery\n          - closeness_centrality\n          - closeness_centrality_avg\n          - betweenness_centrality\n          - betweenness_centrality_avg\n\n    \"\"\"\n\n    stats = {}\n    full_start_time = time.time()\n\n    # create a DiGraph from the MultiDiGraph, for those metrics that require it\n    G_dir = nx.DiGraph(G)\n\n    # create an undirected Graph from the MultiDiGraph, for those metrics that\n    # require it\n    G_undir = nx.Graph(G)\n\n    # get the largest strongly connected component, for those metrics that\n    # require strongly connected graphs\n    G_strong = get_largest_component(G, strongly=True)\n\n    # average degree of the neighborhood of each node, and average for the graph\n    avg_neighbor_degree = nx.average_neighbor_degree(G)\n    stats['avg_neighbor_degree'] = avg_neighbor_degree\n    stats['avg_neighbor_degree_avg'] = sum(avg_neighbor_degree.values())/len(avg_neighbor_degree)\n\n    # average weighted degree of the neighborhood of each node, and average for\n    # the graph\n    avg_weighted_neighbor_degree = nx.average_neighbor_degree(G, weight='length')\n    stats['avg_weighted_neighbor_degree'] = avg_weighted_neighbor_degree\n    stats['avg_weighted_neighbor_degree_avg'] = sum(avg_weighted_neighbor_degree.values())/len(avg_weighted_neighbor_degree)\n\n    # degree centrality for a node is the fraction of nodes it is connected to\n    degree_centrality = nx.degree_centrality(G)\n    stats['degree_centrality'] = degree_centrality\n    stats['degree_centrality_avg'] = sum(degree_centrality.values())/len(degree_centrality)\n\n    # calculate clustering coefficient for the nodes\n    stats['clustering_coefficient'] = nx.clustering(G_undir)\n\n    # average clustering coefficient for the graph\n    stats['clustering_coefficient_avg'] = nx.average_clustering(G_undir)\n\n    # calculate weighted clustering coefficient for the nodes\n    stats['clustering_coefficient_weighted'] = nx.clustering(G_undir, weight='length')\n\n    # average clustering coefficient (weighted) for the graph\n    stats['clustering_coefficient_weighted_avg'] = nx.average_clustering(G_undir, weight='length')\n\n    # pagerank: a ranking of the nodes in the graph based on the structure of\n    # the incoming links\n    pagerank = nx.pagerank(G_dir, weight='length')\n    stats['pagerank'] = pagerank\n\n    # node with the highest page rank, and its value\n    pagerank_max_node = max(pagerank, key=lambda x: pagerank[x])\n    stats['pagerank_max_node'] = pagerank_max_node\n    stats['pagerank_max'] = pagerank[pagerank_max_node]\n\n    # node with the lowest page rank, and its value\n    pagerank_min_node = min(pagerank, key=lambda x: pagerank[x])\n    stats['pagerank_min_node'] = pagerank_min_node\n    stats['pagerank_min'] = pagerank[pagerank_min_node]\n\n    # if True, calculate node and edge connectivity\n    if connectivity:\n        start_time = time.time()\n\n        # node connectivity is the minimum number of nodes that must be removed\n        # to disconnect G or render it trivial\n        stats['node_connectivity'] = nx.node_connectivity(G_strong)\n\n        # edge connectivity is equal to the minimum number of edges that must be\n        # removed to disconnect G or render it trivial\n        stats['edge_connectivity'] = nx.edge_connectivity(G_strong)\n        log('Calculated node and edge connectivity in {:,.2f} seconds'.format(time.time() - start_time))\n\n    # if True, calculate average node connectivity\n    if anc:\n        # mean number of internally node-disjoint paths between each pair of\n        # nodes in G, i.e., the expected number of nodes that must be removed to\n        # disconnect a randomly selected pair of non-adjacent nodes\n        start_time = time.time()\n        stats['node_connectivity_avg'] = nx.average_node_connectivity(G)\n        log('Calculated average node connectivity in {:,.2f} seconds'.format(time.time() - start_time))\n\n    # if True, calculate shortest paths, eccentricity, and topological metrics\n    # that use eccentricity\n    if ecc:\n        # precompute shortest paths between all nodes for eccentricity-based\n        # stats\n        start_time = time.time()\n        sp = {source:dict(nx.single_source_dijkstra_path_length(G_strong, source, weight='length')) for source in G_strong.nodes()}\n\n        log('Calculated shortest path lengths in {:,.2f} seconds'.format(time.time() - start_time))\n\n        # eccentricity of a node v is the maximum distance from v to all other\n        # nodes in G\n        eccentricity = nx.eccentricity(G_strong, sp=sp)\n        stats['eccentricity'] = eccentricity\n\n        # diameter is the maximum eccentricity\n        diameter = nx.diameter(G_strong, e=eccentricity)\n        stats['diameter'] = diameter\n\n        # radius is the minimum eccentricity\n        radius = nx.radius(G_strong, e=eccentricity)\n        stats['radius'] = radius\n\n        # center is the set of nodes with eccentricity equal to radius\n        center = nx.center(G_strong, e=eccentricity)\n        stats['center'] = center\n\n        # periphery is the set of nodes with eccentricity equal to the diameter\n        periphery = nx.periphery(G_strong, e=eccentricity)\n        stats['periphery'] = periphery\n\n    # if True, calculate node closeness centrality\n    if cc:\n        # closeness centrality of a node is the reciprocal of the sum of the\n        # shortest path distances from u to all other nodes\n        start_time = time.time()\n        closeness_centrality = nx.closeness_centrality(G, distance='length')\n        stats['closeness_centrality'] = closeness_centrality\n        stats['closeness_centrality_avg'] = sum(closeness_centrality.values())/len(closeness_centrality)\n        log('Calculated closeness centrality in {:,.2f} seconds'.format(time.time() - start_time))\n\n    # if True, calculate node betweenness centrality\n    if bc:\n        # betweenness centrality of a node is the sum of the fraction of\n        # all-pairs shortest paths that pass through node\n        start_time = time.time()\n        betweenness_centrality = nx.betweenness_centrality(G, weight='length')\n        stats['betweenness_centrality'] = betweenness_centrality\n        stats['betweenness_centrality_avg'] = sum(betweenness_centrality.values())/len(betweenness_centrality)\n        log('Calculated betweenness centrality in {:,.2f} seconds'.format(time.time() - start_time))\n\n    log('Calculated extended stats in {:,.2f} seconds'.format(time.time()-full_start_time))\n    return stats", "language": "python", "code": "def extended_stats(G, connectivity=False, anc=False, ecc=False, bc=False, cc=False):\n    \"\"\"\n    Calculate extended topological stats and metrics for a graph.\n\n    Many of these algorithms have an inherently high time complexity. Global\n    topological analysis of large complex networks is extremely time consuming\n    and may exhaust computer memory. Consider using function arguments to not\n    run metrics that require computation of a full matrix of paths if they\n    will not be needed.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    connectivity : bool\n        if True, calculate node and edge connectivity\n    anc : bool\n        if True, calculate average node connectivity\n    ecc : bool\n        if True, calculate shortest paths, eccentricity, and topological metrics\n        that use eccentricity\n    bc : bool\n        if True, calculate node betweenness centrality\n    cc : bool\n        if True, calculate node closeness centrality\n\n    Returns\n    -------\n    stats : dict\n        dictionary of network measures containing the following elements (some\n        only calculated/returned optionally, based on passed parameters):\n\n          - avg_neighbor_degree\n          - avg_neighbor_degree_avg\n          - avg_weighted_neighbor_degree\n          - avg_weighted_neighbor_degree_avg\n          - degree_centrality\n          - degree_centrality_avg\n          - clustering_coefficient\n          - clustering_coefficient_avg\n          - clustering_coefficient_weighted\n          - clustering_coefficient_weighted_avg\n          - pagerank\n          - pagerank_max_node\n          - pagerank_max\n          - pagerank_min_node\n          - pagerank_min\n          - node_connectivity\n          - node_connectivity_avg\n          - edge_connectivity\n          - eccentricity\n          - diameter\n          - radius\n          - center\n          - periphery\n          - closeness_centrality\n          - closeness_centrality_avg\n          - betweenness_centrality\n          - betweenness_centrality_avg\n\n    \"\"\"\n\n    stats = {}\n    full_start_time = time.time()\n\n    # create a DiGraph from the MultiDiGraph, for those metrics that require it\n    G_dir = nx.DiGraph(G)\n\n    # create an undirected Graph from the MultiDiGraph, for those metrics that\n    # require it\n    G_undir = nx.Graph(G)\n\n    # get the largest strongly connected component, for those metrics that\n    # require strongly connected graphs\n    G_strong = get_largest_component(G, strongly=True)\n\n    # average degree of the neighborhood of each node, and average for the graph\n    avg_neighbor_degree = nx.average_neighbor_degree(G)\n    stats['avg_neighbor_degree'] = avg_neighbor_degree\n    stats['avg_neighbor_degree_avg'] = sum(avg_neighbor_degree.values())/len(avg_neighbor_degree)\n\n    # average weighted degree of the neighborhood of each node, and average for\n    # the graph\n    avg_weighted_neighbor_degree = nx.average_neighbor_degree(G, weight='length')\n    stats['avg_weighted_neighbor_degree'] = avg_weighted_neighbor_degree\n    stats['avg_weighted_neighbor_degree_avg'] = sum(avg_weighted_neighbor_degree.values())/len(avg_weighted_neighbor_degree)\n\n    # degree centrality for a node is the fraction of nodes it is connected to\n    degree_centrality = nx.degree_centrality(G)\n    stats['degree_centrality'] = degree_centrality\n    stats['degree_centrality_avg'] = sum(degree_centrality.values())/len(degree_centrality)\n\n    # calculate clustering coefficient for the nodes\n    stats['clustering_coefficient'] = nx.clustering(G_undir)\n\n    # average clustering coefficient for the graph\n    stats['clustering_coefficient_avg'] = nx.average_clustering(G_undir)\n\n    # calculate weighted clustering coefficient for the nodes\n    stats['clustering_coefficient_weighted'] = nx.clustering(G_undir, weight='length')\n\n    # average clustering coefficient (weighted) for the graph\n    stats['clustering_coefficient_weighted_avg'] = nx.average_clustering(G_undir, weight='length')\n\n    # pagerank: a ranking of the nodes in the graph based on the structure of\n    # the incoming links\n    pagerank = nx.pagerank(G_dir, weight='length')\n    stats['pagerank'] = pagerank\n\n    # node with the highest page rank, and its value\n    pagerank_max_node = max(pagerank, key=lambda x: pagerank[x])\n    stats['pagerank_max_node'] = pagerank_max_node\n    stats['pagerank_max'] = pagerank[pagerank_max_node]\n\n    # node with the lowest page rank, and its value\n    pagerank_min_node = min(pagerank, key=lambda x: pagerank[x])\n    stats['pagerank_min_node'] = pagerank_min_node\n    stats['pagerank_min'] = pagerank[pagerank_min_node]\n\n    # if True, calculate node and edge connectivity\n    if connectivity:\n        start_time = time.time()\n\n        # node connectivity is the minimum number of nodes that must be removed\n        # to disconnect G or render it trivial\n        stats['node_connectivity'] = nx.node_connectivity(G_strong)\n\n        # edge connectivity is equal to the minimum number of edges that must be\n        # removed to disconnect G or render it trivial\n        stats['edge_connectivity'] = nx.edge_connectivity(G_strong)\n        log('Calculated node and edge connectivity in {:,.2f} seconds'.format(time.time() - start_time))\n\n    # if True, calculate average node connectivity\n    if anc:\n        # mean number of internally node-disjoint paths between each pair of\n        # nodes in G, i.e., the expected number of nodes that must be removed to\n        # disconnect a randomly selected pair of non-adjacent nodes\n        start_time = time.time()\n        stats['node_connectivity_avg'] = nx.average_node_connectivity(G)\n        log('Calculated average node connectivity in {:,.2f} seconds'.format(time.time() - start_time))\n\n    # if True, calculate shortest paths, eccentricity, and topological metrics\n    # that use eccentricity\n    if ecc:\n        # precompute shortest paths between all nodes for eccentricity-based\n        # stats\n        start_time = time.time()\n        sp = {source:dict(nx.single_source_dijkstra_path_length(G_strong, source, weight='length')) for source in G_strong.nodes()}\n\n        log('Calculated shortest path lengths in {:,.2f} seconds'.format(time.time() - start_time))\n\n        # eccentricity of a node v is the maximum distance from v to all other\n        # nodes in G\n        eccentricity = nx.eccentricity(G_strong, sp=sp)\n        stats['eccentricity'] = eccentricity\n\n        # diameter is the maximum eccentricity\n        diameter = nx.diameter(G_strong, e=eccentricity)\n        stats['diameter'] = diameter\n\n        # radius is the minimum eccentricity\n        radius = nx.radius(G_strong, e=eccentricity)\n        stats['radius'] = radius\n\n        # center is the set of nodes with eccentricity equal to radius\n        center = nx.center(G_strong, e=eccentricity)\n        stats['center'] = center\n\n        # periphery is the set of nodes with eccentricity equal to the diameter\n        periphery = nx.periphery(G_strong, e=eccentricity)\n        stats['periphery'] = periphery\n\n    # if True, calculate node closeness centrality\n    if cc:\n        # closeness centrality of a node is the reciprocal of the sum of the\n        # shortest path distances from u to all other nodes\n        start_time = time.time()\n        closeness_centrality = nx.closeness_centrality(G, distance='length')\n        stats['closeness_centrality'] = closeness_centrality\n        stats['closeness_centrality_avg'] = sum(closeness_centrality.values())/len(closeness_centrality)\n        log('Calculated closeness centrality in {:,.2f} seconds'.format(time.time() - start_time))\n\n    # if True, calculate node betweenness centrality\n    if bc:\n        # betweenness centrality of a node is the sum of the fraction of\n        # all-pairs shortest paths that pass through node\n        start_time = time.time()\n        betweenness_centrality = nx.betweenness_centrality(G, weight='length')\n        stats['betweenness_centrality'] = betweenness_centrality\n        stats['betweenness_centrality_avg'] = sum(betweenness_centrality.values())/len(betweenness_centrality)\n        log('Calculated betweenness centrality in {:,.2f} seconds'.format(time.time() - start_time))\n\n    log('Calculated extended stats in {:,.2f} seconds'.format(time.time()-full_start_time))\n    return stats", "code_tokens": ["def", "extended_stats", "(", "G", ",", "connectivity", "=", "False", ",", "anc", "=", "False", ",", "ecc", "=", "False", ",", "bc", "=", "False", ",", "cc", "=", "False", ")", ":", "stats", "=", "{", "}", "full_start_time", "=", "time", ".", "time", "(", ")", "# create a DiGraph from the MultiDiGraph, for those metrics that require it", "G_dir", "=", "nx", ".", "DiGraph", "(", "G", ")", "# create an undirected Graph from the MultiDiGraph, for those metrics that", "# require it", "G_undir", "=", "nx", ".", "Graph", "(", "G", ")", "# get the largest strongly connected component, for those metrics that", "# require strongly connected graphs", "G_strong", "=", "get_largest_component", "(", "G", ",", "strongly", "=", "True", ")", "# average degree of the neighborhood of each node, and average for the graph", "avg_neighbor_degree", "=", "nx", ".", "average_neighbor_degree", "(", "G", ")", "stats", "[", "'avg_neighbor_degree'", "]", "=", "avg_neighbor_degree", "stats", "[", "'avg_neighbor_degree_avg'", "]", "=", "sum", "(", "avg_neighbor_degree", ".", "values", "(", ")", ")", "/", "len", "(", "avg_neighbor_degree", ")", "# average weighted degree of the neighborhood of each node, and average for", "# the graph", "avg_weighted_neighbor_degree", "=", "nx", ".", "average_neighbor_degree", "(", "G", ",", "weight", "=", "'length'", ")", "stats", "[", "'avg_weighted_neighbor_degree'", "]", "=", "avg_weighted_neighbor_degree", "stats", "[", "'avg_weighted_neighbor_degree_avg'", "]", "=", "sum", "(", "avg_weighted_neighbor_degree", ".", "values", "(", ")", ")", "/", "len", "(", "avg_weighted_neighbor_degree", ")", "# degree centrality for a node is the fraction of nodes it is connected to", "degree_centrality", "=", "nx", ".", "degree_centrality", "(", "G", ")", "stats", "[", "'degree_centrality'", "]", "=", "degree_centrality", "stats", "[", "'degree_centrality_avg'", "]", "=", "sum", "(", "degree_centrality", ".", "values", "(", ")", ")", "/", "len", "(", "degree_centrality", ")", "# calculate clustering coefficient for the nodes", "stats", "[", "'clustering_coefficient'", "]", "=", "nx", ".", "clustering", "(", "G_undir", ")", "# average clustering coefficient for the graph", "stats", "[", "'clustering_coefficient_avg'", "]", "=", "nx", ".", "average_clustering", "(", "G_undir", ")", "# calculate weighted clustering coefficient for the nodes", "stats", "[", "'clustering_coefficient_weighted'", "]", "=", "nx", ".", "clustering", "(", "G_undir", ",", "weight", "=", "'length'", ")", "# average clustering coefficient (weighted) for the graph", "stats", "[", "'clustering_coefficient_weighted_avg'", "]", "=", "nx", ".", "average_clustering", "(", "G_undir", ",", "weight", "=", "'length'", ")", "# pagerank: a ranking of the nodes in the graph based on the structure of", "# the incoming links", "pagerank", "=", "nx", ".", "pagerank", "(", "G_dir", ",", "weight", "=", "'length'", ")", "stats", "[", "'pagerank'", "]", "=", "pagerank", "# node with the highest page rank, and its value", "pagerank_max_node", "=", "max", "(", "pagerank", ",", "key", "=", "lambda", "x", ":", "pagerank", "[", "x", "]", ")", "stats", "[", "'pagerank_max_node'", "]", "=", "pagerank_max_node", "stats", "[", "'pagerank_max'", "]", "=", "pagerank", "[", "pagerank_max_node", "]", "# node with the lowest page rank, and its value", "pagerank_min_node", "=", "min", "(", "pagerank", ",", "key", "=", "lambda", "x", ":", "pagerank", "[", "x", "]", ")", "stats", "[", "'pagerank_min_node'", "]", "=", "pagerank_min_node", "stats", "[", "'pagerank_min'", "]", "=", "pagerank", "[", "pagerank_min_node", "]", "# if True, calculate node and edge connectivity", "if", "connectivity", ":", "start_time", "=", "time", ".", "time", "(", ")", "# node connectivity is the minimum number of nodes that must be removed", "# to disconnect G or render it trivial", "stats", "[", "'node_connectivity'", "]", "=", "nx", ".", "node_connectivity", "(", "G_strong", ")", "# edge connectivity is equal to the minimum number of edges that must be", "# removed to disconnect G or render it trivial", "stats", "[", "'edge_connectivity'", "]", "=", "nx", ".", "edge_connectivity", "(", "G_strong", ")", "log", "(", "'Calculated node and edge connectivity in {:,.2f} seconds'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "# if True, calculate average node connectivity", "if", "anc", ":", "# mean number of internally node-disjoint paths between each pair of", "# nodes in G, i.e., the expected number of nodes that must be removed to", "# disconnect a randomly selected pair of non-adjacent nodes", "start_time", "=", "time", ".", "time", "(", ")", "stats", "[", "'node_connectivity_avg'", "]", "=", "nx", ".", "average_node_connectivity", "(", "G", ")", "log", "(", "'Calculated average node connectivity in {:,.2f} seconds'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "# if True, calculate shortest paths, eccentricity, and topological metrics", "# that use eccentricity", "if", "ecc", ":", "# precompute shortest paths between all nodes for eccentricity-based", "# stats", "start_time", "=", "time", ".", "time", "(", ")", "sp", "=", "{", "source", ":", "dict", "(", "nx", ".", "single_source_dijkstra_path_length", "(", "G_strong", ",", "source", ",", "weight", "=", "'length'", ")", ")", "for", "source", "in", "G_strong", ".", "nodes", "(", ")", "}", "log", "(", "'Calculated shortest path lengths in {:,.2f} seconds'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "# eccentricity of a node v is the maximum distance from v to all other", "# nodes in G", "eccentricity", "=", "nx", ".", "eccentricity", "(", "G_strong", ",", "sp", "=", "sp", ")", "stats", "[", "'eccentricity'", "]", "=", "eccentricity", "# diameter is the maximum eccentricity", "diameter", "=", "nx", ".", "diameter", "(", "G_strong", ",", "e", "=", "eccentricity", ")", "stats", "[", "'diameter'", "]", "=", "diameter", "# radius is the minimum eccentricity", "radius", "=", "nx", ".", "radius", "(", "G_strong", ",", "e", "=", "eccentricity", ")", "stats", "[", "'radius'", "]", "=", "radius", "# center is the set of nodes with eccentricity equal to radius", "center", "=", "nx", ".", "center", "(", "G_strong", ",", "e", "=", "eccentricity", ")", "stats", "[", "'center'", "]", "=", "center", "# periphery is the set of nodes with eccentricity equal to the diameter", "periphery", "=", "nx", ".", "periphery", "(", "G_strong", ",", "e", "=", "eccentricity", ")", "stats", "[", "'periphery'", "]", "=", "periphery", "# if True, calculate node closeness centrality", "if", "cc", ":", "# closeness centrality of a node is the reciprocal of the sum of the", "# shortest path distances from u to all other nodes", "start_time", "=", "time", ".", "time", "(", ")", "closeness_centrality", "=", "nx", ".", "closeness_centrality", "(", "G", ",", "distance", "=", "'length'", ")", "stats", "[", "'closeness_centrality'", "]", "=", "closeness_centrality", "stats", "[", "'closeness_centrality_avg'", "]", "=", "sum", "(", "closeness_centrality", ".", "values", "(", ")", ")", "/", "len", "(", "closeness_centrality", ")", "log", "(", "'Calculated closeness centrality in {:,.2f} seconds'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "# if True, calculate node betweenness centrality", "if", "bc", ":", "# betweenness centrality of a node is the sum of the fraction of", "# all-pairs shortest paths that pass through node", "start_time", "=", "time", ".", "time", "(", ")", "betweenness_centrality", "=", "nx", ".", "betweenness_centrality", "(", "G", ",", "weight", "=", "'length'", ")", "stats", "[", "'betweenness_centrality'", "]", "=", "betweenness_centrality", "stats", "[", "'betweenness_centrality_avg'", "]", "=", "sum", "(", "betweenness_centrality", ".", "values", "(", ")", ")", "/", "len", "(", "betweenness_centrality", ")", "log", "(", "'Calculated betweenness centrality in {:,.2f} seconds'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "log", "(", "'Calculated extended stats in {:,.2f} seconds'", ".", "format", "(", "time", ".", "time", "(", ")", "-", "full_start_time", ")", ")", "return", "stats"], "docstring": "Calculate extended topological stats and metrics for a graph.\n\n    Many of these algorithms have an inherently high time complexity. Global\n    topological analysis of large complex networks is extremely time consuming\n    and may exhaust computer memory. Consider using function arguments to not\n    run metrics that require computation of a full matrix of paths if they\n    will not be needed.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    connectivity : bool\n        if True, calculate node and edge connectivity\n    anc : bool\n        if True, calculate average node connectivity\n    ecc : bool\n        if True, calculate shortest paths, eccentricity, and topological metrics\n        that use eccentricity\n    bc : bool\n        if True, calculate node betweenness centrality\n    cc : bool\n        if True, calculate node closeness centrality\n\n    Returns\n    -------\n    stats : dict\n        dictionary of network measures containing the following elements (some\n        only calculated/returned optionally, based on passed parameters):\n\n          - avg_neighbor_degree\n          - avg_neighbor_degree_avg\n          - avg_weighted_neighbor_degree\n          - avg_weighted_neighbor_degree_avg\n          - degree_centrality\n          - degree_centrality_avg\n          - clustering_coefficient\n          - clustering_coefficient_avg\n          - clustering_coefficient_weighted\n          - clustering_coefficient_weighted_avg\n          - pagerank\n          - pagerank_max_node\n          - pagerank_max\n          - pagerank_min_node\n          - pagerank_min\n          - node_connectivity\n          - node_connectivity_avg\n          - edge_connectivity\n          - eccentricity\n          - diameter\n          - radius\n          - center\n          - periphery\n          - closeness_centrality\n          - closeness_centrality_avg\n          - betweenness_centrality\n          - betweenness_centrality_avg", "docstring_tokens": ["Calculate", "extended", "topological", "stats", "and", "metrics", "for", "a", "graph", "."], "sha": "be59fd313bcb68af8fc79242c56194f1247e26e2", "url": "https://github.com/gboeing/osmnx/blob/be59fd313bcb68af8fc79242c56194f1247e26e2/osmnx/stats.py#L236-L428", "partition": "train"}
{"repo": "QuantEcon/QuantEcon.py", "path": "quantecon/markov/ddp.py", "func_name": "backward_induction", "original_string": "def backward_induction(ddp, T, v_term=None):\n    r\"\"\"\n    Solve by backward induction a :math:`T`-period finite horizon\n    discrete dynamic program with stationary reward and transition\n    probability functions :math:`r` and :math:`q` and discount factor\n    :math:`\\beta \\in [0, 1]`.\n\n    The optimal value functions :math:`v^*_0, \\ldots, v^*_T` and policy\n    functions :math:`\\sigma^*_0, \\ldots, \\sigma^*_{T-1}` are obtained by\n    :math:`v^*_T = v_T`, and\n\n    .. math::\n\n        v^*_{t-1}(s) = \\max_{a \\in A(s)} r(s, a) +\n            \\beta \\sum_{s' \\in S} q(s'|s, a) v^*_t(s')\n            \\quad (s \\in S)\n\n    and\n\n    .. math::\n\n        \\sigma^*_{t-1}(s) \\in \\operatorname*{arg\\,max}_{a \\in A(s)}\n            r(s, a) + \\beta \\sum_{s' \\in S} q(s'|s, a) v^*_t(s')\n            \\quad (s \\in S)\n\n    for :math:`t = T, \\ldots, 1`, where the terminal value function\n    :math:`v_T` is exogenously given.\n\n    Parameters\n    ----------\n    ddp : DiscreteDP\n        DiscreteDP instance storing reward array `R`, transition\n        probability array `Q`, and discount factor `beta`.\n\n    T : scalar(int)\n        Number of decision periods.\n\n    v_term : array_like(float, ndim=1), optional(default=None)\n        Terminal value function, of length equal to n (the number of\n        states). If None, it defaults to the vector of zeros.\n\n    Returns\n    -------\n    vs : ndarray(float, ndim=2)\n        Array of shape (T+1, n) where `vs[t]` contains the optimal\n        value function at period `t = 0, ..., T`.\n\n    sigmas : ndarray(int, ndim=2)\n        Array of shape (T, n) where `sigmas[t]` contains the optimal\n        policy function at period `t = 0, ..., T-1`.\n\n    \"\"\"\n    n = ddp.num_states\n    vs = np.empty((T+1, n))\n    sigmas = np.empty((T, n), dtype=int)\n\n    if v_term is None:\n        v_term = np.zeros(n)\n    vs[T, :] = v_term\n\n    for t in range(T, 0, -1):\n        ddp.bellman_operator(vs[t, :], Tv=vs[t-1, :], sigma=sigmas[t-1, :])\n\n    return vs, sigmas", "language": "python", "code": "def backward_induction(ddp, T, v_term=None):\n    r\"\"\"\n    Solve by backward induction a :math:`T`-period finite horizon\n    discrete dynamic program with stationary reward and transition\n    probability functions :math:`r` and :math:`q` and discount factor\n    :math:`\\beta \\in [0, 1]`.\n\n    The optimal value functions :math:`v^*_0, \\ldots, v^*_T` and policy\n    functions :math:`\\sigma^*_0, \\ldots, \\sigma^*_{T-1}` are obtained by\n    :math:`v^*_T = v_T`, and\n\n    .. math::\n\n        v^*_{t-1}(s) = \\max_{a \\in A(s)} r(s, a) +\n            \\beta \\sum_{s' \\in S} q(s'|s, a) v^*_t(s')\n            \\quad (s \\in S)\n\n    and\n\n    .. math::\n\n        \\sigma^*_{t-1}(s) \\in \\operatorname*{arg\\,max}_{a \\in A(s)}\n            r(s, a) + \\beta \\sum_{s' \\in S} q(s'|s, a) v^*_t(s')\n            \\quad (s \\in S)\n\n    for :math:`t = T, \\ldots, 1`, where the terminal value function\n    :math:`v_T` is exogenously given.\n\n    Parameters\n    ----------\n    ddp : DiscreteDP\n        DiscreteDP instance storing reward array `R`, transition\n        probability array `Q`, and discount factor `beta`.\n\n    T : scalar(int)\n        Number of decision periods.\n\n    v_term : array_like(float, ndim=1), optional(default=None)\n        Terminal value function, of length equal to n (the number of\n        states). If None, it defaults to the vector of zeros.\n\n    Returns\n    -------\n    vs : ndarray(float, ndim=2)\n        Array of shape (T+1, n) where `vs[t]` contains the optimal\n        value function at period `t = 0, ..., T`.\n\n    sigmas : ndarray(int, ndim=2)\n        Array of shape (T, n) where `sigmas[t]` contains the optimal\n        policy function at period `t = 0, ..., T-1`.\n\n    \"\"\"\n    n = ddp.num_states\n    vs = np.empty((T+1, n))\n    sigmas = np.empty((T, n), dtype=int)\n\n    if v_term is None:\n        v_term = np.zeros(n)\n    vs[T, :] = v_term\n\n    for t in range(T, 0, -1):\n        ddp.bellman_operator(vs[t, :], Tv=vs[t-1, :], sigma=sigmas[t-1, :])\n\n    return vs, sigmas", "code_tokens": ["def", "backward_induction", "(", "ddp", ",", "T", ",", "v_term", "=", "None", ")", ":", "n", "=", "ddp", ".", "num_states", "vs", "=", "np", ".", "empty", "(", "(", "T", "+", "1", ",", "n", ")", ")", "sigmas", "=", "np", ".", "empty", "(", "(", "T", ",", "n", ")", ",", "dtype", "=", "int", ")", "if", "v_term", "is", "None", ":", "v_term", "=", "np", ".", "zeros", "(", "n", ")", "vs", "[", "T", ",", ":", "]", "=", "v_term", "for", "t", "in", "range", "(", "T", ",", "0", ",", "-", "1", ")", ":", "ddp", ".", "bellman_operator", "(", "vs", "[", "t", ",", ":", "]", ",", "Tv", "=", "vs", "[", "t", "-", "1", ",", ":", "]", ",", "sigma", "=", "sigmas", "[", "t", "-", "1", ",", ":", "]", ")", "return", "vs", ",", "sigmas"], "docstring": "r\"\"\"\n    Solve by backward induction a :math:`T`-period finite horizon\n    discrete dynamic program with stationary reward and transition\n    probability functions :math:`r` and :math:`q` and discount factor\n    :math:`\\beta \\in [0, 1]`.\n\n    The optimal value functions :math:`v^*_0, \\ldots, v^*_T` and policy\n    functions :math:`\\sigma^*_0, \\ldots, \\sigma^*_{T-1}` are obtained by\n    :math:`v^*_T = v_T`, and\n\n    .. math::\n\n        v^*_{t-1}(s) = \\max_{a \\in A(s)} r(s, a) +\n            \\beta \\sum_{s' \\in S} q(s'|s, a) v^*_t(s')\n            \\quad (s \\in S)\n\n    and\n\n    .. math::\n\n        \\sigma^*_{t-1}(s) \\in \\operatorname*{arg\\,max}_{a \\in A(s)}\n            r(s, a) + \\beta \\sum_{s' \\in S} q(s'|s, a) v^*_t(s')\n            \\quad (s \\in S)\n\n    for :math:`t = T, \\ldots, 1`, where the terminal value function\n    :math:`v_T` is exogenously given.\n\n    Parameters\n    ----------\n    ddp : DiscreteDP\n        DiscreteDP instance storing reward array `R`, transition\n        probability array `Q`, and discount factor `beta`.\n\n    T : scalar(int)\n        Number of decision periods.\n\n    v_term : array_like(float, ndim=1), optional(default=None)\n        Terminal value function, of length equal to n (the number of\n        states). If None, it defaults to the vector of zeros.\n\n    Returns\n    -------\n    vs : ndarray(float, ndim=2)\n        Array of shape (T+1, n) where `vs[t]` contains the optimal\n        value function at period `t = 0, ..., T`.\n\n    sigmas : ndarray(int, ndim=2)\n        Array of shape (T, n) where `sigmas[t]` contains the optimal\n        policy function at period `t = 0, ..., T-1`.", "docstring_tokens": ["r", "Solve", "by", "backward", "induction", "a", ":", "math", ":", "T", "-", "period", "finite", "horizon", "discrete", "dynamic", "program", "with", "stationary", "reward", "and", "transition", "probability", "functions", ":", "math", ":", "r", "and", ":", "math", ":", "q", "and", "discount", "factor", ":", "math", ":", "\\", "beta", "\\", "in", "[", "0", "1", "]", "."], "sha": "26a66c552f2a73967d7efb6e1f4b4c4985a12643", "url": "https://github.com/QuantEcon/QuantEcon.py/blob/26a66c552f2a73967d7efb6e1f4b4c4985a12643/quantecon/markov/ddp.py#L964-L1027", "partition": "train"}
{"repo": "QuantEcon/QuantEcon.py", "path": "quantecon/markov/ddp.py", "func_name": "_has_sorted_sa_indices", "original_string": "def _has_sorted_sa_indices(s_indices, a_indices):\n    \"\"\"\n    Check whether `s_indices` and `a_indices` are sorted in\n    lexicographic order.\n\n    Parameters\n    ----------\n    s_indices, a_indices : ndarray(ndim=1)\n\n    Returns\n    -------\n    bool\n        Whether `s_indices` and `a_indices` are sorted.\n\n    \"\"\"\n    L = len(s_indices)\n    for i in range(L-1):\n        if s_indices[i] > s_indices[i+1]:\n            return False\n        if s_indices[i] == s_indices[i+1]:\n            if a_indices[i] >= a_indices[i+1]:\n                return False\n    return True", "language": "python", "code": "def _has_sorted_sa_indices(s_indices, a_indices):\n    \"\"\"\n    Check whether `s_indices` and `a_indices` are sorted in\n    lexicographic order.\n\n    Parameters\n    ----------\n    s_indices, a_indices : ndarray(ndim=1)\n\n    Returns\n    -------\n    bool\n        Whether `s_indices` and `a_indices` are sorted.\n\n    \"\"\"\n    L = len(s_indices)\n    for i in range(L-1):\n        if s_indices[i] > s_indices[i+1]:\n            return False\n        if s_indices[i] == s_indices[i+1]:\n            if a_indices[i] >= a_indices[i+1]:\n                return False\n    return True", "code_tokens": ["def", "_has_sorted_sa_indices", "(", "s_indices", ",", "a_indices", ")", ":", "L", "=", "len", "(", "s_indices", ")", "for", "i", "in", "range", "(", "L", "-", "1", ")", ":", "if", "s_indices", "[", "i", "]", ">", "s_indices", "[", "i", "+", "1", "]", ":", "return", "False", "if", "s_indices", "[", "i", "]", "==", "s_indices", "[", "i", "+", "1", "]", ":", "if", "a_indices", "[", "i", "]", ">=", "a_indices", "[", "i", "+", "1", "]", ":", "return", "False", "return", "True"], "docstring": "Check whether `s_indices` and `a_indices` are sorted in\n    lexicographic order.\n\n    Parameters\n    ----------\n    s_indices, a_indices : ndarray(ndim=1)\n\n    Returns\n    -------\n    bool\n        Whether `s_indices` and `a_indices` are sorted.", "docstring_tokens": ["Check", "whether", "s_indices", "and", "a_indices", "are", "sorted", "in", "lexicographic", "order", "."], "sha": "26a66c552f2a73967d7efb6e1f4b4c4985a12643", "url": "https://github.com/QuantEcon/QuantEcon.py/blob/26a66c552f2a73967d7efb6e1f4b4c4985a12643/quantecon/markov/ddp.py#L1074-L1096", "partition": "train"}
{"repo": "amueller/word_cloud", "path": "wordcloud/wordcloud.py", "func_name": "WordCloud._draw_contour", "original_string": "def _draw_contour(self, img):\n        \"\"\"Draw mask contour on a pillow image.\"\"\"\n        if self.mask is None or self.contour_width == 0:\n            return img\n\n        mask = self._get_bolean_mask(self.mask) * 255\n        contour = Image.fromarray(mask.astype(np.uint8))\n        contour = contour.resize(img.size)\n        contour = contour.filter(ImageFilter.FIND_EDGES)\n        contour = np.array(contour)\n\n        # make sure borders are not drawn before changing width\n        contour[[0, -1], :] = 0\n        contour[:, [0, -1]] = 0\n\n        # use gaussian to change width, divide by 10 to give more resolution\n        radius = self.contour_width / 10\n        contour = Image.fromarray(contour)\n        contour = contour.filter(ImageFilter.GaussianBlur(radius=radius))\n        contour = np.array(contour) > 0\n        contour = np.dstack((contour, contour, contour))\n\n        # color the contour\n        ret = np.array(img) * np.invert(contour)\n        if self.contour_color != 'black':\n            color = Image.new(img.mode, img.size, self.contour_color)\n            ret += np.array(color) * contour\n\n        return Image.fromarray(ret)", "language": "python", "code": "def _draw_contour(self, img):\n        \"\"\"Draw mask contour on a pillow image.\"\"\"\n        if self.mask is None or self.contour_width == 0:\n            return img\n\n        mask = self._get_bolean_mask(self.mask) * 255\n        contour = Image.fromarray(mask.astype(np.uint8))\n        contour = contour.resize(img.size)\n        contour = contour.filter(ImageFilter.FIND_EDGES)\n        contour = np.array(contour)\n\n        # make sure borders are not drawn before changing width\n        contour[[0, -1], :] = 0\n        contour[:, [0, -1]] = 0\n\n        # use gaussian to change width, divide by 10 to give more resolution\n        radius = self.contour_width / 10\n        contour = Image.fromarray(contour)\n        contour = contour.filter(ImageFilter.GaussianBlur(radius=radius))\n        contour = np.array(contour) > 0\n        contour = np.dstack((contour, contour, contour))\n\n        # color the contour\n        ret = np.array(img) * np.invert(contour)\n        if self.contour_color != 'black':\n            color = Image.new(img.mode, img.size, self.contour_color)\n            ret += np.array(color) * contour\n\n        return Image.fromarray(ret)", "code_tokens": ["def", "_draw_contour", "(", "self", ",", "img", ")", ":", "if", "self", ".", "mask", "is", "None", "or", "self", ".", "contour_width", "==", "0", ":", "return", "img", "mask", "=", "self", ".", "_get_bolean_mask", "(", "self", ".", "mask", ")", "*", "255", "contour", "=", "Image", ".", "fromarray", "(", "mask", ".", "astype", "(", "np", ".", "uint8", ")", ")", "contour", "=", "contour", ".", "resize", "(", "img", ".", "size", ")", "contour", "=", "contour", ".", "filter", "(", "ImageFilter", ".", "FIND_EDGES", ")", "contour", "=", "np", ".", "array", "(", "contour", ")", "# make sure borders are not drawn before changing width", "contour", "[", "[", "0", ",", "-", "1", "]", ",", ":", "]", "=", "0", "contour", "[", ":", ",", "[", "0", ",", "-", "1", "]", "]", "=", "0", "# use gaussian to change width, divide by 10 to give more resolution", "radius", "=", "self", ".", "contour_width", "/", "10", "contour", "=", "Image", ".", "fromarray", "(", "contour", ")", "contour", "=", "contour", ".", "filter", "(", "ImageFilter", ".", "GaussianBlur", "(", "radius", "=", "radius", ")", ")", "contour", "=", "np", ".", "array", "(", "contour", ")", ">", "0", "contour", "=", "np", ".", "dstack", "(", "(", "contour", ",", "contour", ",", "contour", ")", ")", "# color the contour", "ret", "=", "np", ".", "array", "(", "img", ")", "*", "np", ".", "invert", "(", "contour", ")", "if", "self", ".", "contour_color", "!=", "'black'", ":", "color", "=", "Image", ".", "new", "(", "img", ".", "mode", ",", "img", ".", "size", ",", "self", ".", "contour_color", ")", "ret", "+=", "np", ".", "array", "(", "color", ")", "*", "contour", "return", "Image", ".", "fromarray", "(", "ret", ")"], "docstring": "Draw mask contour on a pillow image.", "docstring_tokens": ["Draw", "mask", "contour", "on", "a", "pillow", "image", "."], "sha": "d36f526e3d8346e6d7a2656631f05f68e402517d", "url": "https://github.com/amueller/word_cloud/blob/d36f526e3d8346e6d7a2656631f05f68e402517d/wordcloud/wordcloud.py#L744-L772", "partition": "train"}
{"repo": "DataDog/integrations-core", "path": "datadog_checks_base/datadog_checks/base/checks/kube_leader/mixins.py", "func_name": "KubeLeaderElectionMixin.check_election_status", "original_string": "def check_election_status(self, config):\n        \"\"\"\n        Retrieves the leader-election annotation from a given object, and\n        submits metrics and a service check.\n\n        An integration warning is sent if the object is not retrievable,\n        or no record is found. Monitors on the service-check should have\n        no-data alerts enabled to account for this.\n\n        The config objet requires the following fields:\n            namespace (prefix for the metrics and check)\n            record_kind (endpoints or configmap)\n            record_name\n            record_namespace\n            tags (optional)\n\n        It reads the following agent configuration:\n            kubernetes_kubeconfig_path: defaut is to use in-cluster config\n        \"\"\"\n        try:\n            record = self._get_record(\n                config.get(\"record_kind\", \"\"), config.get(\"record_name\", \"\"), config.get(\"record_namespace\", \"\")\n            )\n            self._report_status(config, record)\n        except Exception as e:\n            self.warning(\"Cannot retrieve leader election record {}: {}\".format(config.get(\"record_name\", \"\"), e))", "language": "python", "code": "def check_election_status(self, config):\n        \"\"\"\n        Retrieves the leader-election annotation from a given object, and\n        submits metrics and a service check.\n\n        An integration warning is sent if the object is not retrievable,\n        or no record is found. Monitors on the service-check should have\n        no-data alerts enabled to account for this.\n\n        The config objet requires the following fields:\n            namespace (prefix for the metrics and check)\n            record_kind (endpoints or configmap)\n            record_name\n            record_namespace\n            tags (optional)\n\n        It reads the following agent configuration:\n            kubernetes_kubeconfig_path: defaut is to use in-cluster config\n        \"\"\"\n        try:\n            record = self._get_record(\n                config.get(\"record_kind\", \"\"), config.get(\"record_name\", \"\"), config.get(\"record_namespace\", \"\")\n            )\n            self._report_status(config, record)\n        except Exception as e:\n            self.warning(\"Cannot retrieve leader election record {}: {}\".format(config.get(\"record_name\", \"\"), e))", "code_tokens": ["def", "check_election_status", "(", "self", ",", "config", ")", ":", "try", ":", "record", "=", "self", ".", "_get_record", "(", "config", ".", "get", "(", "\"record_kind\"", ",", "\"\"", ")", ",", "config", ".", "get", "(", "\"record_name\"", ",", "\"\"", ")", ",", "config", ".", "get", "(", "\"record_namespace\"", ",", "\"\"", ")", ")", "self", ".", "_report_status", "(", "config", ",", "record", ")", "except", "Exception", "as", "e", ":", "self", ".", "warning", "(", "\"Cannot retrieve leader election record {}: {}\"", ".", "format", "(", "config", ".", "get", "(", "\"record_name\"", ",", "\"\"", ")", ",", "e", ")", ")"], "docstring": "Retrieves the leader-election annotation from a given object, and\n        submits metrics and a service check.\n\n        An integration warning is sent if the object is not retrievable,\n        or no record is found. Monitors on the service-check should have\n        no-data alerts enabled to account for this.\n\n        The config objet requires the following fields:\n            namespace (prefix for the metrics and check)\n            record_kind (endpoints or configmap)\n            record_name\n            record_namespace\n            tags (optional)\n\n        It reads the following agent configuration:\n            kubernetes_kubeconfig_path: defaut is to use in-cluster config", "docstring_tokens": ["Retrieves", "the", "leader", "-", "election", "annotation", "from", "a", "given", "object", "and", "submits", "metrics", "and", "a", "service", "check", "."], "sha": "ebd41c873cf9f97a8c51bf9459bc6a7536af8acd", "url": "https://github.com/DataDog/integrations-core/blob/ebd41c873cf9f97a8c51bf9459bc6a7536af8acd/datadog_checks_base/datadog_checks/base/checks/kube_leader/mixins.py#L26-L51", "partition": "train"}
{"repo": "mlperf/training", "path": "translation/tensorflow/transformer/transformer_main.py", "func_name": "translate_and_compute_bleu", "original_string": "def translate_and_compute_bleu(estimator, subtokenizer, bleu_source, bleu_ref):\n  \"\"\"Translate file and report the cased and uncased bleu scores.\"\"\"\n  # Create temporary file to store translation.\n  tmp = tempfile.NamedTemporaryFile(delete=False)\n  tmp_filename = tmp.name\n\n  translate.translate_file(\n      estimator, subtokenizer, bleu_source, output_file=tmp_filename,\n      print_all_translations=False)\n\n  # Compute uncased and cased bleu scores.\n  uncased_score = compute_bleu.bleu_wrapper(bleu_ref, tmp_filename, False)\n  cased_score = compute_bleu.bleu_wrapper(bleu_ref, tmp_filename, True)\n  os.remove(tmp_filename)\n  return uncased_score, cased_score", "language": "python", "code": "def translate_and_compute_bleu(estimator, subtokenizer, bleu_source, bleu_ref):\n  \"\"\"Translate file and report the cased and uncased bleu scores.\"\"\"\n  # Create temporary file to store translation.\n  tmp = tempfile.NamedTemporaryFile(delete=False)\n  tmp_filename = tmp.name\n\n  translate.translate_file(\n      estimator, subtokenizer, bleu_source, output_file=tmp_filename,\n      print_all_translations=False)\n\n  # Compute uncased and cased bleu scores.\n  uncased_score = compute_bleu.bleu_wrapper(bleu_ref, tmp_filename, False)\n  cased_score = compute_bleu.bleu_wrapper(bleu_ref, tmp_filename, True)\n  os.remove(tmp_filename)\n  return uncased_score, cased_score", "code_tokens": ["def", "translate_and_compute_bleu", "(", "estimator", ",", "subtokenizer", ",", "bleu_source", ",", "bleu_ref", ")", ":", "# Create temporary file to store translation.", "tmp", "=", "tempfile", ".", "NamedTemporaryFile", "(", "delete", "=", "False", ")", "tmp_filename", "=", "tmp", ".", "name", "translate", ".", "translate_file", "(", "estimator", ",", "subtokenizer", ",", "bleu_source", ",", "output_file", "=", "tmp_filename", ",", "print_all_translations", "=", "False", ")", "# Compute uncased and cased bleu scores.", "uncased_score", "=", "compute_bleu", ".", "bleu_wrapper", "(", "bleu_ref", ",", "tmp_filename", ",", "False", ")", "cased_score", "=", "compute_bleu", ".", "bleu_wrapper", "(", "bleu_ref", ",", "tmp_filename", ",", "True", ")", "os", ".", "remove", "(", "tmp_filename", ")", "return", "uncased_score", ",", "cased_score"], "docstring": "Translate file and report the cased and uncased bleu scores.", "docstring_tokens": ["Translate", "file", "and", "report", "the", "cased", "and", "uncased", "bleu", "scores", "."], "sha": "1c6ae725a81d15437a2b2df05cac0673fde5c3a4", "url": "https://github.com/mlperf/training/blob/1c6ae725a81d15437a2b2df05cac0673fde5c3a4/translation/tensorflow/transformer/transformer_main.py#L144-L158", "partition": "train"}
{"repo": "mlperf/training", "path": "translation/tensorflow/transformer/transformer_main.py", "func_name": "evaluate_and_log_bleu", "original_string": "def evaluate_and_log_bleu(estimator, bleu_writer, bleu_source, bleu_ref):\n  \"\"\"Calculate and record the BLEU score.\"\"\"\n  subtokenizer = tokenizer.Subtokenizer(\n      os.path.join(FLAGS.data_dir, FLAGS.vocab_file))\n\n  uncased_score, cased_score = translate_and_compute_bleu(\n      estimator, subtokenizer, bleu_source, bleu_ref)\n\n  print(\"Bleu score (uncased):\", uncased_score)\n  print(\"Bleu score (cased):\", cased_score)\n\n  summary = tf.Summary(value=[\n      tf.Summary.Value(tag=\"bleu/uncased\", simple_value=uncased_score),\n      tf.Summary.Value(tag=\"bleu/cased\", simple_value=cased_score),\n  ])\n\n  bleu_writer.add_summary(summary, get_global_step(estimator))\n  bleu_writer.flush()\n  return uncased_score, cased_score", "language": "python", "code": "def evaluate_and_log_bleu(estimator, bleu_writer, bleu_source, bleu_ref):\n  \"\"\"Calculate and record the BLEU score.\"\"\"\n  subtokenizer = tokenizer.Subtokenizer(\n      os.path.join(FLAGS.data_dir, FLAGS.vocab_file))\n\n  uncased_score, cased_score = translate_and_compute_bleu(\n      estimator, subtokenizer, bleu_source, bleu_ref)\n\n  print(\"Bleu score (uncased):\", uncased_score)\n  print(\"Bleu score (cased):\", cased_score)\n\n  summary = tf.Summary(value=[\n      tf.Summary.Value(tag=\"bleu/uncased\", simple_value=uncased_score),\n      tf.Summary.Value(tag=\"bleu/cased\", simple_value=cased_score),\n  ])\n\n  bleu_writer.add_summary(summary, get_global_step(estimator))\n  bleu_writer.flush()\n  return uncased_score, cased_score", "code_tokens": ["def", "evaluate_and_log_bleu", "(", "estimator", ",", "bleu_writer", ",", "bleu_source", ",", "bleu_ref", ")", ":", "subtokenizer", "=", "tokenizer", ".", "Subtokenizer", "(", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "data_dir", ",", "FLAGS", ".", "vocab_file", ")", ")", "uncased_score", ",", "cased_score", "=", "translate_and_compute_bleu", "(", "estimator", ",", "subtokenizer", ",", "bleu_source", ",", "bleu_ref", ")", "print", "(", "\"Bleu score (uncased):\"", ",", "uncased_score", ")", "print", "(", "\"Bleu score (cased):\"", ",", "cased_score", ")", "summary", "=", "tf", ".", "Summary", "(", "value", "=", "[", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "\"bleu/uncased\"", ",", "simple_value", "=", "uncased_score", ")", ",", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "\"bleu/cased\"", ",", "simple_value", "=", "cased_score", ")", ",", "]", ")", "bleu_writer", ".", "add_summary", "(", "summary", ",", "get_global_step", "(", "estimator", ")", ")", "bleu_writer", ".", "flush", "(", ")", "return", "uncased_score", ",", "cased_score"], "docstring": "Calculate and record the BLEU score.", "docstring_tokens": ["Calculate", "and", "record", "the", "BLEU", "score", "."], "sha": "1c6ae725a81d15437a2b2df05cac0673fde5c3a4", "url": "https://github.com/mlperf/training/blob/1c6ae725a81d15437a2b2df05cac0673fde5c3a4/translation/tensorflow/transformer/transformer_main.py#L166-L184", "partition": "train"}
{"repo": "mlperf/training", "path": "translation/tensorflow/transformer/transformer_main.py", "func_name": "train_schedule", "original_string": "def train_schedule(\n    estimator, train_eval_iterations, single_iteration_train_steps=None,\n    single_iteration_train_epochs=None, bleu_source=None, bleu_ref=None,\n    bleu_threshold=None):\n  \"\"\"Train and evaluate model, and optionally compute model's BLEU score.\n\n  **Step vs. Epoch vs. Iteration**\n\n  Steps and epochs are canonical terms used in TensorFlow and general machine\n  learning. They are used to describe running a single process (train/eval):\n    - Step refers to running the process through a single or batch of examples.\n    - Epoch refers to running the process through an entire dataset.\n\n  E.g. training a dataset with 100 examples. The dataset is\n  divided into 20 batches with 5 examples per batch. A single training step\n  trains the model on one batch. After 20 training steps, the model will have\n  trained on every batch in the dataset, or, in other words, one epoch.\n\n  Meanwhile, iteration is used in this implementation to describe running\n  multiple processes (training and eval).\n    - A single iteration:\n      1. trains the model for a specific number of steps or epochs.\n      2. evaluates the model.\n      3. (if source and ref files are provided) compute BLEU score.\n\n  This function runs through multiple train+eval+bleu iterations.\n\n  Args:\n    estimator: tf.Estimator containing model to train.\n    train_eval_iterations: Number of times to repeat the train+eval iteration.\n    single_iteration_train_steps: Number of steps to train in one iteration.\n    single_iteration_train_epochs: Number of epochs to train in one iteration.\n    bleu_source: File containing text to be translated for BLEU calculation.\n    bleu_ref: File containing reference translations for BLEU calculation.\n    bleu_threshold: minimum BLEU score before training is stopped.\n\n  Raises:\n    ValueError: if both or none of single_iteration_train_steps and\n      single_iteration_train_epochs were defined.\n  \"\"\"\n  # Ensure that exactly one of single_iteration_train_steps and\n  # single_iteration_train_epochs is defined.\n  if single_iteration_train_steps is None:\n    if single_iteration_train_epochs is None:\n      raise ValueError(\n          \"Exactly one of single_iteration_train_steps or \"\n          \"single_iteration_train_epochs must be defined. Both were none.\")\n  else:\n    if single_iteration_train_epochs is not None:\n      raise ValueError(\n          \"Exactly one of single_iteration_train_steps or \"\n          \"single_iteration_train_epochs must be defined. Both were defined.\")\n\n  evaluate_bleu = bleu_source is not None and bleu_ref is not None\n\n  # Print out training schedule\n  print(\"Training schedule:\")\n  if single_iteration_train_epochs is not None:\n    print(\"\\t1. Train for %d epochs.\" % single_iteration_train_epochs)\n  else:\n    print(\"\\t1. Train for %d steps.\" % single_iteration_train_steps)\n  print(\"\\t2. Evaluate model.\")\n  if evaluate_bleu:\n    print(\"\\t3. Compute BLEU score.\")\n    if bleu_threshold is not None:\n      print(\"Repeat above steps until the BLEU score reaches\", bleu_threshold)\n  if not evaluate_bleu or bleu_threshold is None:\n    print(\"Repeat above steps %d times.\" % train_eval_iterations)\n\n  if evaluate_bleu:\n    # Set summary writer to log bleu score.\n    bleu_writer = tf.summary.FileWriter(\n        os.path.join(estimator.model_dir, BLEU_DIR))\n    if bleu_threshold is not None:\n      # Change loop stopping condition if bleu_threshold is defined.\n      train_eval_iterations = INF\n\n  # Loop training/evaluation/bleu cycles\n  mlperf_log.transformer_print(key=mlperf_log.TRAIN_LOOP)\n  for i in xrange(train_eval_iterations):\n    print(\"Starting iteration\", i + 1)\n\n    mlperf_log.transformer_print(key=mlperf_log.TRAIN_EPOCH,\n                                 value=i * single_iteration_train_epochs + 1)\n\n    # Train the model for single_iteration_train_steps or until the input fn\n    # runs out of examples (if single_iteration_train_steps is None).\n    estimator.train(dataset.train_input_fn, steps=single_iteration_train_steps)\n\n    mlperf_log.transformer_print(key=mlperf_log.EVAL_START)\n    eval_results = estimator.evaluate(dataset.eval_input_fn)\n    print(\"Evaluation results (iter %d/%d):\" % (i + 1, train_eval_iterations),\n          eval_results)\n\n    if evaluate_bleu:\n      uncased_score, _ = evaluate_and_log_bleu(\n          estimator, bleu_writer, bleu_source, bleu_ref)\n      if bleu_threshold is not None and uncased_score > bleu_threshold:\n        bleu_writer.close()\n        break\n      mlperf_log.transformer_print(key=mlperf_log.EVAL_TARGET, value=bleu_threshold)\n      mlperf_log.transformer_print(key=mlperf_log.EVAL_ACCURACY, value=uncased_score)\n    mlperf_log.transformer_print(key=mlperf_log.EVAL_STOP)", "language": "python", "code": "def train_schedule(\n    estimator, train_eval_iterations, single_iteration_train_steps=None,\n    single_iteration_train_epochs=None, bleu_source=None, bleu_ref=None,\n    bleu_threshold=None):\n  \"\"\"Train and evaluate model, and optionally compute model's BLEU score.\n\n  **Step vs. Epoch vs. Iteration**\n\n  Steps and epochs are canonical terms used in TensorFlow and general machine\n  learning. They are used to describe running a single process (train/eval):\n    - Step refers to running the process through a single or batch of examples.\n    - Epoch refers to running the process through an entire dataset.\n\n  E.g. training a dataset with 100 examples. The dataset is\n  divided into 20 batches with 5 examples per batch. A single training step\n  trains the model on one batch. After 20 training steps, the model will have\n  trained on every batch in the dataset, or, in other words, one epoch.\n\n  Meanwhile, iteration is used in this implementation to describe running\n  multiple processes (training and eval).\n    - A single iteration:\n      1. trains the model for a specific number of steps or epochs.\n      2. evaluates the model.\n      3. (if source and ref files are provided) compute BLEU score.\n\n  This function runs through multiple train+eval+bleu iterations.\n\n  Args:\n    estimator: tf.Estimator containing model to train.\n    train_eval_iterations: Number of times to repeat the train+eval iteration.\n    single_iteration_train_steps: Number of steps to train in one iteration.\n    single_iteration_train_epochs: Number of epochs to train in one iteration.\n    bleu_source: File containing text to be translated for BLEU calculation.\n    bleu_ref: File containing reference translations for BLEU calculation.\n    bleu_threshold: minimum BLEU score before training is stopped.\n\n  Raises:\n    ValueError: if both or none of single_iteration_train_steps and\n      single_iteration_train_epochs were defined.\n  \"\"\"\n  # Ensure that exactly one of single_iteration_train_steps and\n  # single_iteration_train_epochs is defined.\n  if single_iteration_train_steps is None:\n    if single_iteration_train_epochs is None:\n      raise ValueError(\n          \"Exactly one of single_iteration_train_steps or \"\n          \"single_iteration_train_epochs must be defined. Both were none.\")\n  else:\n    if single_iteration_train_epochs is not None:\n      raise ValueError(\n          \"Exactly one of single_iteration_train_steps or \"\n          \"single_iteration_train_epochs must be defined. Both were defined.\")\n\n  evaluate_bleu = bleu_source is not None and bleu_ref is not None\n\n  # Print out training schedule\n  print(\"Training schedule:\")\n  if single_iteration_train_epochs is not None:\n    print(\"\\t1. Train for %d epochs.\" % single_iteration_train_epochs)\n  else:\n    print(\"\\t1. Train for %d steps.\" % single_iteration_train_steps)\n  print(\"\\t2. Evaluate model.\")\n  if evaluate_bleu:\n    print(\"\\t3. Compute BLEU score.\")\n    if bleu_threshold is not None:\n      print(\"Repeat above steps until the BLEU score reaches\", bleu_threshold)\n  if not evaluate_bleu or bleu_threshold is None:\n    print(\"Repeat above steps %d times.\" % train_eval_iterations)\n\n  if evaluate_bleu:\n    # Set summary writer to log bleu score.\n    bleu_writer = tf.summary.FileWriter(\n        os.path.join(estimator.model_dir, BLEU_DIR))\n    if bleu_threshold is not None:\n      # Change loop stopping condition if bleu_threshold is defined.\n      train_eval_iterations = INF\n\n  # Loop training/evaluation/bleu cycles\n  mlperf_log.transformer_print(key=mlperf_log.TRAIN_LOOP)\n  for i in xrange(train_eval_iterations):\n    print(\"Starting iteration\", i + 1)\n\n    mlperf_log.transformer_print(key=mlperf_log.TRAIN_EPOCH,\n                                 value=i * single_iteration_train_epochs + 1)\n\n    # Train the model for single_iteration_train_steps or until the input fn\n    # runs out of examples (if single_iteration_train_steps is None).\n    estimator.train(dataset.train_input_fn, steps=single_iteration_train_steps)\n\n    mlperf_log.transformer_print(key=mlperf_log.EVAL_START)\n    eval_results = estimator.evaluate(dataset.eval_input_fn)\n    print(\"Evaluation results (iter %d/%d):\" % (i + 1, train_eval_iterations),\n          eval_results)\n\n    if evaluate_bleu:\n      uncased_score, _ = evaluate_and_log_bleu(\n          estimator, bleu_writer, bleu_source, bleu_ref)\n      if bleu_threshold is not None and uncased_score > bleu_threshold:\n        bleu_writer.close()\n        break\n      mlperf_log.transformer_print(key=mlperf_log.EVAL_TARGET, value=bleu_threshold)\n      mlperf_log.transformer_print(key=mlperf_log.EVAL_ACCURACY, value=uncased_score)\n    mlperf_log.transformer_print(key=mlperf_log.EVAL_STOP)", "code_tokens": ["def", "train_schedule", "(", "estimator", ",", "train_eval_iterations", ",", "single_iteration_train_steps", "=", "None", ",", "single_iteration_train_epochs", "=", "None", ",", "bleu_source", "=", "None", ",", "bleu_ref", "=", "None", ",", "bleu_threshold", "=", "None", ")", ":", "# Ensure that exactly one of single_iteration_train_steps and", "# single_iteration_train_epochs is defined.", "if", "single_iteration_train_steps", "is", "None", ":", "if", "single_iteration_train_epochs", "is", "None", ":", "raise", "ValueError", "(", "\"Exactly one of single_iteration_train_steps or \"", "\"single_iteration_train_epochs must be defined. Both were none.\"", ")", "else", ":", "if", "single_iteration_train_epochs", "is", "not", "None", ":", "raise", "ValueError", "(", "\"Exactly one of single_iteration_train_steps or \"", "\"single_iteration_train_epochs must be defined. Both were defined.\"", ")", "evaluate_bleu", "=", "bleu_source", "is", "not", "None", "and", "bleu_ref", "is", "not", "None", "# Print out training schedule", "print", "(", "\"Training schedule:\"", ")", "if", "single_iteration_train_epochs", "is", "not", "None", ":", "print", "(", "\"\\t1. Train for %d epochs.\"", "%", "single_iteration_train_epochs", ")", "else", ":", "print", "(", "\"\\t1. Train for %d steps.\"", "%", "single_iteration_train_steps", ")", "print", "(", "\"\\t2. Evaluate model.\"", ")", "if", "evaluate_bleu", ":", "print", "(", "\"\\t3. Compute BLEU score.\"", ")", "if", "bleu_threshold", "is", "not", "None", ":", "print", "(", "\"Repeat above steps until the BLEU score reaches\"", ",", "bleu_threshold", ")", "if", "not", "evaluate_bleu", "or", "bleu_threshold", "is", "None", ":", "print", "(", "\"Repeat above steps %d times.\"", "%", "train_eval_iterations", ")", "if", "evaluate_bleu", ":", "# Set summary writer to log bleu score.", "bleu_writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "os", ".", "path", ".", "join", "(", "estimator", ".", "model_dir", ",", "BLEU_DIR", ")", ")", "if", "bleu_threshold", "is", "not", "None", ":", "# Change loop stopping condition if bleu_threshold is defined.", "train_eval_iterations", "=", "INF", "# Loop training/evaluation/bleu cycles", "mlperf_log", ".", "transformer_print", "(", "key", "=", "mlperf_log", ".", "TRAIN_LOOP", ")", "for", "i", "in", "xrange", "(", "train_eval_iterations", ")", ":", "print", "(", "\"Starting iteration\"", ",", "i", "+", "1", ")", "mlperf_log", ".", "transformer_print", "(", "key", "=", "mlperf_log", ".", "TRAIN_EPOCH", ",", "value", "=", "i", "*", "single_iteration_train_epochs", "+", "1", ")", "# Train the model for single_iteration_train_steps or until the input fn", "# runs out of examples (if single_iteration_train_steps is None).", "estimator", ".", "train", "(", "dataset", ".", "train_input_fn", ",", "steps", "=", "single_iteration_train_steps", ")", "mlperf_log", ".", "transformer_print", "(", "key", "=", "mlperf_log", ".", "EVAL_START", ")", "eval_results", "=", "estimator", ".", "evaluate", "(", "dataset", ".", "eval_input_fn", ")", "print", "(", "\"Evaluation results (iter %d/%d):\"", "%", "(", "i", "+", "1", ",", "train_eval_iterations", ")", ",", "eval_results", ")", "if", "evaluate_bleu", ":", "uncased_score", ",", "_", "=", "evaluate_and_log_bleu", "(", "estimator", ",", "bleu_writer", ",", "bleu_source", ",", "bleu_ref", ")", "if", "bleu_threshold", "is", "not", "None", "and", "uncased_score", ">", "bleu_threshold", ":", "bleu_writer", ".", "close", "(", ")", "break", "mlperf_log", ".", "transformer_print", "(", "key", "=", "mlperf_log", ".", "EVAL_TARGET", ",", "value", "=", "bleu_threshold", ")", "mlperf_log", ".", "transformer_print", "(", "key", "=", "mlperf_log", ".", "EVAL_ACCURACY", ",", "value", "=", "uncased_score", ")", "mlperf_log", ".", "transformer_print", "(", "key", "=", "mlperf_log", ".", "EVAL_STOP", ")"], "docstring": "Train and evaluate model, and optionally compute model's BLEU score.\n\n  **Step vs. Epoch vs. Iteration**\n\n  Steps and epochs are canonical terms used in TensorFlow and general machine\n  learning. They are used to describe running a single process (train/eval):\n    - Step refers to running the process through a single or batch of examples.\n    - Epoch refers to running the process through an entire dataset.\n\n  E.g. training a dataset with 100 examples. The dataset is\n  divided into 20 batches with 5 examples per batch. A single training step\n  trains the model on one batch. After 20 training steps, the model will have\n  trained on every batch in the dataset, or, in other words, one epoch.\n\n  Meanwhile, iteration is used in this implementation to describe running\n  multiple processes (training and eval).\n    - A single iteration:\n      1. trains the model for a specific number of steps or epochs.\n      2. evaluates the model.\n      3. (if source and ref files are provided) compute BLEU score.\n\n  This function runs through multiple train+eval+bleu iterations.\n\n  Args:\n    estimator: tf.Estimator containing model to train.\n    train_eval_iterations: Number of times to repeat the train+eval iteration.\n    single_iteration_train_steps: Number of steps to train in one iteration.\n    single_iteration_train_epochs: Number of epochs to train in one iteration.\n    bleu_source: File containing text to be translated for BLEU calculation.\n    bleu_ref: File containing reference translations for BLEU calculation.\n    bleu_threshold: minimum BLEU score before training is stopped.\n\n  Raises:\n    ValueError: if both or none of single_iteration_train_steps and\n      single_iteration_train_epochs were defined.", "docstring_tokens": ["Train", "and", "evaluate", "model", "and", "optionally", "compute", "model", "s", "BLEU", "score", "."], "sha": "1c6ae725a81d15437a2b2df05cac0673fde5c3a4", "url": "https://github.com/mlperf/training/blob/1c6ae725a81d15437a2b2df05cac0673fde5c3a4/translation/tensorflow/transformer/transformer_main.py#L187-L289", "partition": "train"}
{"repo": "PyGithub/PyGithub", "path": "github/Team.py", "func_name": "Team.add_membership", "original_string": "def add_membership(self, member, role=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `PUT /teams/:id/memberships/:user <http://developer.github.com/v3/orgs/teams>`_\n        :param member: :class:`github.Nameduser.NamedUser`\n        :param role: string\n        :rtype: None\n        \"\"\"\n        assert isinstance(member, github.NamedUser.NamedUser), member\n        assert role is github.GithubObject.NotSet or isinstance(\n            role, (str, unicode)), role\n        if role is not github.GithubObject.NotSet:\n            assert role in ['member', 'maintainer']\n            put_parameters = {\n                \"role\": role,\n            }\n        else:\n            put_parameters = {\n                \"role\": \"member\",\n            }\n        headers, data = self._requester.requestJsonAndCheck(\n            \"PUT\",\n            self.url + \"/memberships/\" + member._identity,\n            input=put_parameters\n        )", "language": "python", "code": "def add_membership(self, member, role=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `PUT /teams/:id/memberships/:user <http://developer.github.com/v3/orgs/teams>`_\n        :param member: :class:`github.Nameduser.NamedUser`\n        :param role: string\n        :rtype: None\n        \"\"\"\n        assert isinstance(member, github.NamedUser.NamedUser), member\n        assert role is github.GithubObject.NotSet or isinstance(\n            role, (str, unicode)), role\n        if role is not github.GithubObject.NotSet:\n            assert role in ['member', 'maintainer']\n            put_parameters = {\n                \"role\": role,\n            }\n        else:\n            put_parameters = {\n                \"role\": \"member\",\n            }\n        headers, data = self._requester.requestJsonAndCheck(\n            \"PUT\",\n            self.url + \"/memberships/\" + member._identity,\n            input=put_parameters\n        )", "code_tokens": ["def", "add_membership", "(", "self", ",", "member", ",", "role", "=", "github", ".", "GithubObject", ".", "NotSet", ")", ":", "assert", "isinstance", "(", "member", ",", "github", ".", "NamedUser", ".", "NamedUser", ")", ",", "member", "assert", "role", "is", "github", ".", "GithubObject", ".", "NotSet", "or", "isinstance", "(", "role", ",", "(", "str", ",", "unicode", ")", ")", ",", "role", "if", "role", "is", "not", "github", ".", "GithubObject", ".", "NotSet", ":", "assert", "role", "in", "[", "'member'", ",", "'maintainer'", "]", "put_parameters", "=", "{", "\"role\"", ":", "role", ",", "}", "else", ":", "put_parameters", "=", "{", "\"role\"", ":", "\"member\"", ",", "}", "headers", ",", "data", "=", "self", ".", "_requester", ".", "requestJsonAndCheck", "(", "\"PUT\"", ",", "self", ".", "url", "+", "\"/memberships/\"", "+", "member", ".", "_identity", ",", "input", "=", "put_parameters", ")"], "docstring": ":calls: `PUT /teams/:id/memberships/:user <http://developer.github.com/v3/orgs/teams>`_\n        :param member: :class:`github.Nameduser.NamedUser`\n        :param role: string\n        :rtype: None", "docstring_tokens": [":", "calls", ":", "PUT", "/", "teams", "/", ":", "id", "/", "memberships", "/", ":", "user", "<http", ":", "//", "developer", ".", "github", ".", "com", "/", "v3", "/", "orgs", "/", "teams", ">", "_", ":", "param", "member", ":", ":", "class", ":", "github", ".", "Nameduser", ".", "NamedUser", ":", "param", "role", ":", "string", ":", "rtype", ":", "None"], "sha": "f716df86bbe7dc276c6596699fa9712b61ef974c", "url": "https://github.com/PyGithub/PyGithub/blob/f716df86bbe7dc276c6596699fa9712b61ef974c/github/Team.py#L171-L194", "partition": "train"}
{"repo": "PyGithub/PyGithub", "path": "github/Team.py", "func_name": "Team.add_to_repos", "original_string": "def add_to_repos(self, repo):\n        \"\"\"\n        :calls: `PUT /teams/:id/repos/:org/:repo <http://developer.github.com/v3/orgs/teams>`_\n        :param repo: :class:`github.Repository.Repository`\n        :rtype: None\n        \"\"\"\n        assert isinstance(repo, github.Repository.Repository), repo\n        headers, data = self._requester.requestJsonAndCheck(\n            \"PUT\",\n            self.url + \"/repos/\" + repo._identity\n        )", "language": "python", "code": "def add_to_repos(self, repo):\n        \"\"\"\n        :calls: `PUT /teams/:id/repos/:org/:repo <http://developer.github.com/v3/orgs/teams>`_\n        :param repo: :class:`github.Repository.Repository`\n        :rtype: None\n        \"\"\"\n        assert isinstance(repo, github.Repository.Repository), repo\n        headers, data = self._requester.requestJsonAndCheck(\n            \"PUT\",\n            self.url + \"/repos/\" + repo._identity\n        )", "code_tokens": ["def", "add_to_repos", "(", "self", ",", "repo", ")", ":", "assert", "isinstance", "(", "repo", ",", "github", ".", "Repository", ".", "Repository", ")", ",", "repo", "headers", ",", "data", "=", "self", ".", "_requester", ".", "requestJsonAndCheck", "(", "\"PUT\"", ",", "self", ".", "url", "+", "\"/repos/\"", "+", "repo", ".", "_identity", ")"], "docstring": ":calls: `PUT /teams/:id/repos/:org/:repo <http://developer.github.com/v3/orgs/teams>`_\n        :param repo: :class:`github.Repository.Repository`\n        :rtype: None", "docstring_tokens": [":", "calls", ":", "PUT", "/", "teams", "/", ":", "id", "/", "repos", "/", ":", "org", "/", ":", "repo", "<http", ":", "//", "developer", ".", "github", ".", "com", "/", "v3", "/", "orgs", "/", "teams", ">", "_", ":", "param", "repo", ":", ":", "class", ":", "github", ".", "Repository", ".", "Repository", ":", "rtype", ":", "None"], "sha": "f716df86bbe7dc276c6596699fa9712b61ef974c", "url": "https://github.com/PyGithub/PyGithub/blob/f716df86bbe7dc276c6596699fa9712b61ef974c/github/Team.py#L196-L206", "partition": "train"}
{"repo": "PyGithub/PyGithub", "path": "github/Team.py", "func_name": "Team.set_repo_permission", "original_string": "def set_repo_permission(self, repo, permission):\n        \"\"\"\n        :calls: `PUT /teams/:id/repos/:org/:repo <http://developer.github.com/v3/orgs/teams>`_\n        :param repo: :class:`github.Repository.Repository`\n        :param permission: string\n        :rtype: None\n        \"\"\"\n        assert isinstance(repo, github.Repository.Repository), repo\n        put_parameters = {\n            \"permission\": permission,\n        }\n        headers, data = self._requester.requestJsonAndCheck(\n            \"PUT\",\n            self.url + \"/repos/\" + repo._identity,\n            input=put_parameters\n        )", "language": "python", "code": "def set_repo_permission(self, repo, permission):\n        \"\"\"\n        :calls: `PUT /teams/:id/repos/:org/:repo <http://developer.github.com/v3/orgs/teams>`_\n        :param repo: :class:`github.Repository.Repository`\n        :param permission: string\n        :rtype: None\n        \"\"\"\n        assert isinstance(repo, github.Repository.Repository), repo\n        put_parameters = {\n            \"permission\": permission,\n        }\n        headers, data = self._requester.requestJsonAndCheck(\n            \"PUT\",\n            self.url + \"/repos/\" + repo._identity,\n            input=put_parameters\n        )", "code_tokens": ["def", "set_repo_permission", "(", "self", ",", "repo", ",", "permission", ")", ":", "assert", "isinstance", "(", "repo", ",", "github", ".", "Repository", ".", "Repository", ")", ",", "repo", "put_parameters", "=", "{", "\"permission\"", ":", "permission", ",", "}", "headers", ",", "data", "=", "self", ".", "_requester", ".", "requestJsonAndCheck", "(", "\"PUT\"", ",", "self", ".", "url", "+", "\"/repos/\"", "+", "repo", ".", "_identity", ",", "input", "=", "put_parameters", ")"], "docstring": ":calls: `PUT /teams/:id/repos/:org/:repo <http://developer.github.com/v3/orgs/teams>`_\n        :param repo: :class:`github.Repository.Repository`\n        :param permission: string\n        :rtype: None", "docstring_tokens": [":", "calls", ":", "PUT", "/", "teams", "/", ":", "id", "/", "repos", "/", ":", "org", "/", ":", "repo", "<http", ":", "//", "developer", ".", "github", ".", "com", "/", "v3", "/", "orgs", "/", "teams", ">", "_", ":", "param", "repo", ":", ":", "class", ":", "github", ".", "Repository", ".", "Repository", ":", "param", "permission", ":", "string", ":", "rtype", ":", "None"], "sha": "f716df86bbe7dc276c6596699fa9712b61ef974c", "url": "https://github.com/PyGithub/PyGithub/blob/f716df86bbe7dc276c6596699fa9712b61ef974c/github/Team.py#L208-L223", "partition": "train"}
{"repo": "openai/universe", "path": "universe/vncdriver/vendor/pydes.py", "func_name": "triple_des.setIV", "original_string": "def setIV(self, IV):\n\t\t\"\"\"Will set the Initial Value, used in conjunction with CBC mode\"\"\"\n\t\t_baseDes.setIV(self, IV)\n\t\tfor key in (self.__key1, self.__key2, self.__key3):\n\t\t\tkey.setIV(IV)", "language": "python", "code": "def setIV(self, IV):\n\t\t\"\"\"Will set the Initial Value, used in conjunction with CBC mode\"\"\"\n\t\t_baseDes.setIV(self, IV)\n\t\tfor key in (self.__key1, self.__key2, self.__key3):\n\t\t\tkey.setIV(IV)", "code_tokens": ["def", "setIV", "(", "self", ",", "IV", ")", ":", "_baseDes", ".", "setIV", "(", "self", ",", "IV", ")", "for", "key", "in", "(", "self", ".", "__key1", ",", "self", ".", "__key2", ",", "self", ".", "__key3", ")", ":", "key", ".", "setIV", "(", "IV", ")"], "docstring": "Will set the Initial Value, used in conjunction with CBC mode", "docstring_tokens": ["Will", "set", "the", "Initial", "Value", "used", "in", "conjunction", "with", "CBC", "mode"], "sha": "cc9ce6ec241821bfb0f3b85dd455bd36e4ee7a8c", "url": "https://github.com/openai/universe/blob/cc9ce6ec241821bfb0f3b85dd455bd36e4ee7a8c/universe/vncdriver/vendor/pydes.py#L757-L761", "partition": "train"}
{"repo": "openai/universe", "path": "universe/vncdriver/vendor/pydes.py", "func_name": "triple_des.decrypt", "original_string": "def decrypt(self, data, pad=None, padmode=None):\n\t\t\"\"\"decrypt(data, [pad], [padmode]) -> bytes\n\n\t\tdata : bytes to be encrypted\n\t\tpad  : Optional argument for decryption padding. Must only be one byte\n\t\tpadmode : Optional argument for overriding the padding mode.\n\n\t\tThe data must be a multiple of 8 bytes and will be decrypted\n\t\twith the already specified key. In PAD_NORMAL mode, if the\n\t\toptional padding character is supplied, then the un-encrypted\n\t\tdata will have the padding characters removed from the end of\n\t\tthe bytes. This pad removal only occurs on the last 8 bytes of\n\t\tthe data (last data block). In PAD_PKCS5 mode, the special\n\t\tpadding end markers will be removed from the data after\n\t\tdecrypting, no pad character is required for PAD_PKCS5.\n\t\t\"\"\"\n\t\tENCRYPT = des.ENCRYPT\n\t\tDECRYPT = des.DECRYPT\n\t\tdata = self._guardAgainstUnicode(data)\n\t\tif pad is not None:\n\t\t\tpad = self._guardAgainstUnicode(pad)\n\t\tif self.getMode() == CBC:\n\t\t\tself.__key1.setIV(self.getIV())\n\t\t\tself.__key2.setIV(self.getIV())\n\t\t\tself.__key3.setIV(self.getIV())\n\t\t\ti = 0\n\t\t\tresult = []\n\t\t\twhile i < len(data):\n\t\t\t\tiv = data[i:i+8]\n\t\t\t\tblock = self.__key3.crypt(iv,    DECRYPT)\n\t\t\t\tblock = self.__key2.crypt(block, ENCRYPT)\n\t\t\t\tblock = self.__key1.crypt(block, DECRYPT)\n\t\t\t\tself.__key1.setIV(iv)\n\t\t\t\tself.__key2.setIV(iv)\n\t\t\t\tself.__key3.setIV(iv)\n\t\t\t\tresult.append(block)\n\t\t\t\ti += 8\n\t\t\tif _pythonMajorVersion < 3:\n\t\t\t\tdata = ''.join(result)\n\t\t\telse:\n\t\t\t\tdata = bytes.fromhex('').join(result)\n\t\telse:\n\t\t\tdata = self.__key3.crypt(data, DECRYPT)\n\t\t\tdata = self.__key2.crypt(data, ENCRYPT)\n\t\t\tdata = self.__key1.crypt(data, DECRYPT)\n\t\treturn self._unpadData(data, pad, padmode)", "language": "python", "code": "def decrypt(self, data, pad=None, padmode=None):\n\t\t\"\"\"decrypt(data, [pad], [padmode]) -> bytes\n\n\t\tdata : bytes to be encrypted\n\t\tpad  : Optional argument for decryption padding. Must only be one byte\n\t\tpadmode : Optional argument for overriding the padding mode.\n\n\t\tThe data must be a multiple of 8 bytes and will be decrypted\n\t\twith the already specified key. In PAD_NORMAL mode, if the\n\t\toptional padding character is supplied, then the un-encrypted\n\t\tdata will have the padding characters removed from the end of\n\t\tthe bytes. This pad removal only occurs on the last 8 bytes of\n\t\tthe data (last data block). In PAD_PKCS5 mode, the special\n\t\tpadding end markers will be removed from the data after\n\t\tdecrypting, no pad character is required for PAD_PKCS5.\n\t\t\"\"\"\n\t\tENCRYPT = des.ENCRYPT\n\t\tDECRYPT = des.DECRYPT\n\t\tdata = self._guardAgainstUnicode(data)\n\t\tif pad is not None:\n\t\t\tpad = self._guardAgainstUnicode(pad)\n\t\tif self.getMode() == CBC:\n\t\t\tself.__key1.setIV(self.getIV())\n\t\t\tself.__key2.setIV(self.getIV())\n\t\t\tself.__key3.setIV(self.getIV())\n\t\t\ti = 0\n\t\t\tresult = []\n\t\t\twhile i < len(data):\n\t\t\t\tiv = data[i:i+8]\n\t\t\t\tblock = self.__key3.crypt(iv,    DECRYPT)\n\t\t\t\tblock = self.__key2.crypt(block, ENCRYPT)\n\t\t\t\tblock = self.__key1.crypt(block, DECRYPT)\n\t\t\t\tself.__key1.setIV(iv)\n\t\t\t\tself.__key2.setIV(iv)\n\t\t\t\tself.__key3.setIV(iv)\n\t\t\t\tresult.append(block)\n\t\t\t\ti += 8\n\t\t\tif _pythonMajorVersion < 3:\n\t\t\t\tdata = ''.join(result)\n\t\t\telse:\n\t\t\t\tdata = bytes.fromhex('').join(result)\n\t\telse:\n\t\t\tdata = self.__key3.crypt(data, DECRYPT)\n\t\t\tdata = self.__key2.crypt(data, ENCRYPT)\n\t\t\tdata = self.__key1.crypt(data, DECRYPT)\n\t\treturn self._unpadData(data, pad, padmode)", "code_tokens": ["def", "decrypt", "(", "self", ",", "data", ",", "pad", "=", "None", ",", "padmode", "=", "None", ")", ":", "ENCRYPT", "=", "des", ".", "ENCRYPT", "DECRYPT", "=", "des", ".", "DECRYPT", "data", "=", "self", ".", "_guardAgainstUnicode", "(", "data", ")", "if", "pad", "is", "not", "None", ":", "pad", "=", "self", ".", "_guardAgainstUnicode", "(", "pad", ")", "if", "self", ".", "getMode", "(", ")", "==", "CBC", ":", "self", ".", "__key1", ".", "setIV", "(", "self", ".", "getIV", "(", ")", ")", "self", ".", "__key2", ".", "setIV", "(", "self", ".", "getIV", "(", ")", ")", "self", ".", "__key3", ".", "setIV", "(", "self", ".", "getIV", "(", ")", ")", "i", "=", "0", "result", "=", "[", "]", "while", "i", "<", "len", "(", "data", ")", ":", "iv", "=", "data", "[", "i", ":", "i", "+", "8", "]", "block", "=", "self", ".", "__key3", ".", "crypt", "(", "iv", ",", "DECRYPT", ")", "block", "=", "self", ".", "__key2", ".", "crypt", "(", "block", ",", "ENCRYPT", ")", "block", "=", "self", ".", "__key1", ".", "crypt", "(", "block", ",", "DECRYPT", ")", "self", ".", "__key1", ".", "setIV", "(", "iv", ")", "self", ".", "__key2", ".", "setIV", "(", "iv", ")", "self", ".", "__key3", ".", "setIV", "(", "iv", ")", "result", ".", "append", "(", "block", ")", "i", "+=", "8", "if", "_pythonMajorVersion", "<", "3", ":", "data", "=", "''", ".", "join", "(", "result", ")", "else", ":", "data", "=", "bytes", ".", "fromhex", "(", "''", ")", ".", "join", "(", "result", ")", "else", ":", "data", "=", "self", ".", "__key3", ".", "crypt", "(", "data", ",", "DECRYPT", ")", "data", "=", "self", ".", "__key2", ".", "crypt", "(", "data", ",", "ENCRYPT", ")", "data", "=", "self", ".", "__key1", ".", "crypt", "(", "data", ",", "DECRYPT", ")", "return", "self", ".", "_unpadData", "(", "data", ",", "pad", ",", "padmode", ")"], "docstring": "decrypt(data, [pad], [padmode]) -> bytes\n\n\t\tdata : bytes to be encrypted\n\t\tpad  : Optional argument for decryption padding. Must only be one byte\n\t\tpadmode : Optional argument for overriding the padding mode.\n\n\t\tThe data must be a multiple of 8 bytes and will be decrypted\n\t\twith the already specified key. In PAD_NORMAL mode, if the\n\t\toptional padding character is supplied, then the un-encrypted\n\t\tdata will have the padding characters removed from the end of\n\t\tthe bytes. This pad removal only occurs on the last 8 bytes of\n\t\tthe data (last data block). In PAD_PKCS5 mode, the special\n\t\tpadding end markers will be removed from the data after\n\t\tdecrypting, no pad character is required for PAD_PKCS5.", "docstring_tokens": ["decrypt", "(", "data", "[", "pad", "]", "[", "padmode", "]", ")", "-", ">", "bytes"], "sha": "cc9ce6ec241821bfb0f3b85dd455bd36e4ee7a8c", "url": "https://github.com/openai/universe/blob/cc9ce6ec241821bfb0f3b85dd455bd36e4ee7a8c/universe/vncdriver/vendor/pydes.py#L807-L852", "partition": "train"}
{"repo": "jxtech/wechatpy", "path": "wechatpy/pay/__init__.py", "func_name": "WeChatPay.parse_payment_result", "original_string": "def parse_payment_result(self, xml):\n        \"\"\"\u89e3\u6790\u5fae\u4fe1\u652f\u4ed8\u7ed3\u679c\u901a\u77e5\"\"\"\n        try:\n            data = xmltodict.parse(xml)\n        except (xmltodict.ParsingInterrupted, ExpatError):\n            raise InvalidSignatureException()\n\n        if not data or 'xml' not in data:\n            raise InvalidSignatureException()\n\n        data = data['xml']\n        sign = data.pop('sign', None)\n        real_sign = calculate_signature(data, self.api_key if not self.sandbox else self.sandbox_api_key)\n        if sign != real_sign:\n            raise InvalidSignatureException()\n\n        for key in ('total_fee', 'settlement_total_fee', 'cash_fee', 'coupon_fee', 'coupon_count'):\n            if key in data:\n                data[key] = int(data[key])\n        data['sign'] = sign\n        return data", "language": "python", "code": "def parse_payment_result(self, xml):\n        \"\"\"\u89e3\u6790\u5fae\u4fe1\u652f\u4ed8\u7ed3\u679c\u901a\u77e5\"\"\"\n        try:\n            data = xmltodict.parse(xml)\n        except (xmltodict.ParsingInterrupted, ExpatError):\n            raise InvalidSignatureException()\n\n        if not data or 'xml' not in data:\n            raise InvalidSignatureException()\n\n        data = data['xml']\n        sign = data.pop('sign', None)\n        real_sign = calculate_signature(data, self.api_key if not self.sandbox else self.sandbox_api_key)\n        if sign != real_sign:\n            raise InvalidSignatureException()\n\n        for key in ('total_fee', 'settlement_total_fee', 'cash_fee', 'coupon_fee', 'coupon_count'):\n            if key in data:\n                data[key] = int(data[key])\n        data['sign'] = sign\n        return data", "code_tokens": ["def", "parse_payment_result", "(", "self", ",", "xml", ")", ":", "try", ":", "data", "=", "xmltodict", ".", "parse", "(", "xml", ")", "except", "(", "xmltodict", ".", "ParsingInterrupted", ",", "ExpatError", ")", ":", "raise", "InvalidSignatureException", "(", ")", "if", "not", "data", "or", "'xml'", "not", "in", "data", ":", "raise", "InvalidSignatureException", "(", ")", "data", "=", "data", "[", "'xml'", "]", "sign", "=", "data", ".", "pop", "(", "'sign'", ",", "None", ")", "real_sign", "=", "calculate_signature", "(", "data", ",", "self", ".", "api_key", "if", "not", "self", ".", "sandbox", "else", "self", ".", "sandbox_api_key", ")", "if", "sign", "!=", "real_sign", ":", "raise", "InvalidSignatureException", "(", ")", "for", "key", "in", "(", "'total_fee'", ",", "'settlement_total_fee'", ",", "'cash_fee'", ",", "'coupon_fee'", ",", "'coupon_count'", ")", ":", "if", "key", "in", "data", ":", "data", "[", "key", "]", "=", "int", "(", "data", "[", "key", "]", ")", "data", "[", "'sign'", "]", "=", "sign", "return", "data"], "docstring": "\u89e3\u6790\u5fae\u4fe1\u652f\u4ed8\u7ed3\u679c\u901a\u77e5", "docstring_tokens": ["\u89e3\u6790\u5fae\u4fe1\u652f\u4ed8\u7ed3\u679c\u901a\u77e5"], "sha": "4df0da795618c0895a10f1c2cde9e9d5c0a93aaa", "url": "https://github.com/jxtech/wechatpy/blob/4df0da795618c0895a10f1c2cde9e9d5c0a93aaa/wechatpy/pay/__init__.py#L196-L216", "partition": "train"}
{"repo": "jxtech/wechatpy", "path": "wechatpy/client/api/material.py", "func_name": "WeChatMaterial.add_articles", "original_string": "def add_articles(self, articles):\n        \"\"\"\n        \u65b0\u589e\u6c38\u4e45\u56fe\u6587\u7d20\u6750\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://mp.weixin.qq.com/wiki?id=mp1494572718_WzHIY\n\n        :param articles: \u56fe\u6587\u7d20\u6750\u6570\u7ec4\n        :type articles: list[dict]\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        articles_data = []\n        for article in articles:\n            articles_data.append({\n                'thumb_media_id': article['thumb_media_id'],\n                'title': article['title'],\n                'content': article['content'],\n                'author': article.get('author', ''),\n                'content_source_url': article.get('content_source_url', ''),\n                'digest': article.get('digest', ''),\n                'show_cover_pic': article.get('show_cover_pic', 0),\n                'need_open_comment': int(article.get('need_open_comment', False)),\n                'only_fans_can_comment': int(article.get('only_fans_can_comment', False)),\n            })\n        return self._post(\n            'material/add_news',\n            data={\n                'articles': articles_data\n            }\n        )", "language": "python", "code": "def add_articles(self, articles):\n        \"\"\"\n        \u65b0\u589e\u6c38\u4e45\u56fe\u6587\u7d20\u6750\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://mp.weixin.qq.com/wiki?id=mp1494572718_WzHIY\n\n        :param articles: \u56fe\u6587\u7d20\u6750\u6570\u7ec4\n        :type articles: list[dict]\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        articles_data = []\n        for article in articles:\n            articles_data.append({\n                'thumb_media_id': article['thumb_media_id'],\n                'title': article['title'],\n                'content': article['content'],\n                'author': article.get('author', ''),\n                'content_source_url': article.get('content_source_url', ''),\n                'digest': article.get('digest', ''),\n                'show_cover_pic': article.get('show_cover_pic', 0),\n                'need_open_comment': int(article.get('need_open_comment', False)),\n                'only_fans_can_comment': int(article.get('only_fans_can_comment', False)),\n            })\n        return self._post(\n            'material/add_news',\n            data={\n                'articles': articles_data\n            }\n        )", "code_tokens": ["def", "add_articles", "(", "self", ",", "articles", ")", ":", "articles_data", "=", "[", "]", "for", "article", "in", "articles", ":", "articles_data", ".", "append", "(", "{", "'thumb_media_id'", ":", "article", "[", "'thumb_media_id'", "]", ",", "'title'", ":", "article", "[", "'title'", "]", ",", "'content'", ":", "article", "[", "'content'", "]", ",", "'author'", ":", "article", ".", "get", "(", "'author'", ",", "''", ")", ",", "'content_source_url'", ":", "article", ".", "get", "(", "'content_source_url'", ",", "''", ")", ",", "'digest'", ":", "article", ".", "get", "(", "'digest'", ",", "''", ")", ",", "'show_cover_pic'", ":", "article", ".", "get", "(", "'show_cover_pic'", ",", "0", ")", ",", "'need_open_comment'", ":", "int", "(", "article", ".", "get", "(", "'need_open_comment'", ",", "False", ")", ")", ",", "'only_fans_can_comment'", ":", "int", "(", "article", ".", "get", "(", "'only_fans_can_comment'", ",", "False", ")", ")", ",", "}", ")", "return", "self", ".", "_post", "(", "'material/add_news'", ",", "data", "=", "{", "'articles'", ":", "articles_data", "}", ")"], "docstring": "\u65b0\u589e\u6c38\u4e45\u56fe\u6587\u7d20\u6750\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://mp.weixin.qq.com/wiki?id=mp1494572718_WzHIY\n\n        :param articles: \u56fe\u6587\u7d20\u6750\u6570\u7ec4\n        :type articles: list[dict]\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305", "docstring_tokens": ["\u65b0\u589e\u6c38\u4e45\u56fe\u6587\u7d20\u6750", "\u8be6\u60c5\u8bf7\u53c2\u8003", "https", ":", "//", "mp", ".", "weixin", ".", "qq", ".", "com", "/", "wiki?id", "=", "mp1494572718_WzHIY"], "sha": "4df0da795618c0895a10f1c2cde9e9d5c0a93aaa", "url": "https://github.com/jxtech/wechatpy/blob/4df0da795618c0895a10f1c2cde9e9d5c0a93aaa/wechatpy/client/api/material.py#L9-L37", "partition": "train"}
{"repo": "GPflow/GPflow", "path": "gpflow/core/parentable.py", "func_name": "Parentable._set_parent", "original_string": "def _set_parent(self, parent=None):\n        \"\"\"\n        Set parent.\n\n        :param parent: Parentable object.\n        :raises ValueError: Self-reference object passed.\n        :raises ValueError: Non-Parentable object passed.\n        \"\"\"\n        if parent is not None:\n            if not isinstance(parent, Parentable):\n                raise ValueError('Parent object must implement Parentable interface.')\n            if parent is self or parent._contains(self):\n                raise ValueError('Self references are not allowed.')\n        self._parent = parent if parent is not None else None", "language": "python", "code": "def _set_parent(self, parent=None):\n        \"\"\"\n        Set parent.\n\n        :param parent: Parentable object.\n        :raises ValueError: Self-reference object passed.\n        :raises ValueError: Non-Parentable object passed.\n        \"\"\"\n        if parent is not None:\n            if not isinstance(parent, Parentable):\n                raise ValueError('Parent object must implement Parentable interface.')\n            if parent is self or parent._contains(self):\n                raise ValueError('Self references are not allowed.')\n        self._parent = parent if parent is not None else None", "code_tokens": ["def", "_set_parent", "(", "self", ",", "parent", "=", "None", ")", ":", "if", "parent", "is", "not", "None", ":", "if", "not", "isinstance", "(", "parent", ",", "Parentable", ")", ":", "raise", "ValueError", "(", "'Parent object must implement Parentable interface.'", ")", "if", "parent", "is", "self", "or", "parent", ".", "_contains", "(", "self", ")", ":", "raise", "ValueError", "(", "'Self references are not allowed.'", ")", "self", ".", "_parent", "=", "parent", "if", "parent", "is", "not", "None", "else", "None"], "docstring": "Set parent.\n\n        :param parent: Parentable object.\n        :raises ValueError: Self-reference object passed.\n        :raises ValueError: Non-Parentable object passed.", "docstring_tokens": ["Set", "parent", "."], "sha": "549394f0b1b0696c7b521a065e49bdae6e7acf27", "url": "https://github.com/GPflow/GPflow/blob/549394f0b1b0696c7b521a065e49bdae6e7acf27/gpflow/core/parentable.py#L180-L193", "partition": "train"}
{"repo": "GPflow/GPflow", "path": "gpflow/models/model.py", "func_name": "GPModel.predict_f_samples", "original_string": "def predict_f_samples(self, Xnew, num_samples):\n        \"\"\"\n        Produce samples from the posterior latent function(s) at the points\n        Xnew.\n        \"\"\"\n        mu, var = self._build_predict(Xnew, full_cov=True)  # N x P, # P x N x N\n        jitter = tf.eye(tf.shape(mu)[0], dtype=settings.float_type) * settings.jitter\n        samples = []\n        for i in range(self.num_latent):\n            L = tf.cholesky(var[i, :, :] + jitter)\n            shape = tf.stack([tf.shape(L)[0], num_samples])\n            V = tf.random_normal(shape, dtype=settings.float_type)\n            samples.append(mu[:, i:i + 1] + tf.matmul(L, V))\n        return tf.transpose(tf.stack(samples))", "language": "python", "code": "def predict_f_samples(self, Xnew, num_samples):\n        \"\"\"\n        Produce samples from the posterior latent function(s) at the points\n        Xnew.\n        \"\"\"\n        mu, var = self._build_predict(Xnew, full_cov=True)  # N x P, # P x N x N\n        jitter = tf.eye(tf.shape(mu)[0], dtype=settings.float_type) * settings.jitter\n        samples = []\n        for i in range(self.num_latent):\n            L = tf.cholesky(var[i, :, :] + jitter)\n            shape = tf.stack([tf.shape(L)[0], num_samples])\n            V = tf.random_normal(shape, dtype=settings.float_type)\n            samples.append(mu[:, i:i + 1] + tf.matmul(L, V))\n        return tf.transpose(tf.stack(samples))", "code_tokens": ["def", "predict_f_samples", "(", "self", ",", "Xnew", ",", "num_samples", ")", ":", "mu", ",", "var", "=", "self", ".", "_build_predict", "(", "Xnew", ",", "full_cov", "=", "True", ")", "# N x P, # P x N x N", "jitter", "=", "tf", ".", "eye", "(", "tf", ".", "shape", "(", "mu", ")", "[", "0", "]", ",", "dtype", "=", "settings", ".", "float_type", ")", "*", "settings", ".", "jitter", "samples", "=", "[", "]", "for", "i", "in", "range", "(", "self", ".", "num_latent", ")", ":", "L", "=", "tf", ".", "cholesky", "(", "var", "[", "i", ",", ":", ",", ":", "]", "+", "jitter", ")", "shape", "=", "tf", ".", "stack", "(", "[", "tf", ".", "shape", "(", "L", ")", "[", "0", "]", ",", "num_samples", "]", ")", "V", "=", "tf", ".", "random_normal", "(", "shape", ",", "dtype", "=", "settings", ".", "float_type", ")", "samples", ".", "append", "(", "mu", "[", ":", ",", "i", ":", "i", "+", "1", "]", "+", "tf", ".", "matmul", "(", "L", ",", "V", ")", ")", "return", "tf", ".", "transpose", "(", "tf", ".", "stack", "(", "samples", ")", ")"], "docstring": "Produce samples from the posterior latent function(s) at the points\n        Xnew.", "docstring_tokens": ["Produce", "samples", "from", "the", "posterior", "latent", "function", "(", "s", ")", "at", "the", "points", "Xnew", "."], "sha": "549394f0b1b0696c7b521a065e49bdae6e7acf27", "url": "https://github.com/GPflow/GPflow/blob/549394f0b1b0696c7b521a065e49bdae6e7acf27/gpflow/models/model.py#L185-L198", "partition": "train"}
{"repo": "quantumlib/Cirq", "path": "cirq/ops/eigen_gate.py", "func_name": "_approximate_common_period", "original_string": "def _approximate_common_period(periods: List[float],\n                               approx_denom: int = 60,\n                               reject_atol: float = 1e-8) -> Optional[float]:\n    \"\"\"Finds a value that is nearly an integer multiple of multiple periods.\n\n    The returned value should be the smallest non-negative number with this\n    property. If `approx_denom` is too small the computation can fail to satisfy\n    the `reject_atol` criteria and return `None`. This is actually desirable\n    behavior, since otherwise the code would e.g. return a nonsense value when\n    asked to compute the common period of `np.e` and `np.pi`.\n\n    Args:\n        periods: The result must be an approximate integer multiple of each of\n            these.\n        approx_denom: Determines how the floating point values are rounded into\n            rational values (so that integer methods such as lcm can be used).\n            Each floating point value f_k will be rounded to a rational number\n            of the form n_k / approx_denom. If you want to recognize rational\n            periods of the form i/d then d should divide `approx_denom`.\n        reject_atol: If the computed approximate common period is at least this\n            far from an integer multiple of any of the given periods, then it\n            is discarded and `None` is returned instead.\n\n    Returns:\n        The approximate common period, or else `None` if the given\n        `approx_denom` wasn't sufficient to approximate the common period to\n        within the given `reject_atol`.\n    \"\"\"\n    if not periods:\n        return None\n    if any(e == 0 for e in periods):\n        return None\n    if len(periods) == 1:\n        return abs(periods[0])\n    approx_rational_periods = [\n        fractions.Fraction(int(np.round(abs(p) * approx_denom)), approx_denom)\n        for p in periods\n    ]\n    common = float(_common_rational_period(approx_rational_periods))\n\n    for p in periods:\n        if p != 0 and abs(p * np.round(common / p) - common) > reject_atol:\n            return None\n\n    return common", "language": "python", "code": "def _approximate_common_period(periods: List[float],\n                               approx_denom: int = 60,\n                               reject_atol: float = 1e-8) -> Optional[float]:\n    \"\"\"Finds a value that is nearly an integer multiple of multiple periods.\n\n    The returned value should be the smallest non-negative number with this\n    property. If `approx_denom` is too small the computation can fail to satisfy\n    the `reject_atol` criteria and return `None`. This is actually desirable\n    behavior, since otherwise the code would e.g. return a nonsense value when\n    asked to compute the common period of `np.e` and `np.pi`.\n\n    Args:\n        periods: The result must be an approximate integer multiple of each of\n            these.\n        approx_denom: Determines how the floating point values are rounded into\n            rational values (so that integer methods such as lcm can be used).\n            Each floating point value f_k will be rounded to a rational number\n            of the form n_k / approx_denom. If you want to recognize rational\n            periods of the form i/d then d should divide `approx_denom`.\n        reject_atol: If the computed approximate common period is at least this\n            far from an integer multiple of any of the given periods, then it\n            is discarded and `None` is returned instead.\n\n    Returns:\n        The approximate common period, or else `None` if the given\n        `approx_denom` wasn't sufficient to approximate the common period to\n        within the given `reject_atol`.\n    \"\"\"\n    if not periods:\n        return None\n    if any(e == 0 for e in periods):\n        return None\n    if len(periods) == 1:\n        return abs(periods[0])\n    approx_rational_periods = [\n        fractions.Fraction(int(np.round(abs(p) * approx_denom)), approx_denom)\n        for p in periods\n    ]\n    common = float(_common_rational_period(approx_rational_periods))\n\n    for p in periods:\n        if p != 0 and abs(p * np.round(common / p) - common) > reject_atol:\n            return None\n\n    return common", "code_tokens": ["def", "_approximate_common_period", "(", "periods", ":", "List", "[", "float", "]", ",", "approx_denom", ":", "int", "=", "60", ",", "reject_atol", ":", "float", "=", "1e-8", ")", "->", "Optional", "[", "float", "]", ":", "if", "not", "periods", ":", "return", "None", "if", "any", "(", "e", "==", "0", "for", "e", "in", "periods", ")", ":", "return", "None", "if", "len", "(", "periods", ")", "==", "1", ":", "return", "abs", "(", "periods", "[", "0", "]", ")", "approx_rational_periods", "=", "[", "fractions", ".", "Fraction", "(", "int", "(", "np", ".", "round", "(", "abs", "(", "p", ")", "*", "approx_denom", ")", ")", ",", "approx_denom", ")", "for", "p", "in", "periods", "]", "common", "=", "float", "(", "_common_rational_period", "(", "approx_rational_periods", ")", ")", "for", "p", "in", "periods", ":", "if", "p", "!=", "0", "and", "abs", "(", "p", "*", "np", ".", "round", "(", "common", "/", "p", ")", "-", "common", ")", ">", "reject_atol", ":", "return", "None", "return", "common"], "docstring": "Finds a value that is nearly an integer multiple of multiple periods.\n\n    The returned value should be the smallest non-negative number with this\n    property. If `approx_denom` is too small the computation can fail to satisfy\n    the `reject_atol` criteria and return `None`. This is actually desirable\n    behavior, since otherwise the code would e.g. return a nonsense value when\n    asked to compute the common period of `np.e` and `np.pi`.\n\n    Args:\n        periods: The result must be an approximate integer multiple of each of\n            these.\n        approx_denom: Determines how the floating point values are rounded into\n            rational values (so that integer methods such as lcm can be used).\n            Each floating point value f_k will be rounded to a rational number\n            of the form n_k / approx_denom. If you want to recognize rational\n            periods of the form i/d then d should divide `approx_denom`.\n        reject_atol: If the computed approximate common period is at least this\n            far from an integer multiple of any of the given periods, then it\n            is discarded and `None` is returned instead.\n\n    Returns:\n        The approximate common period, or else `None` if the given\n        `approx_denom` wasn't sufficient to approximate the common period to\n        within the given `reject_atol`.", "docstring_tokens": ["Finds", "a", "value", "that", "is", "nearly", "an", "integer", "multiple", "of", "multiple", "periods", "."], "sha": "0827da80dd7880e5b923eb69407e980ed9bc0bd2", "url": "https://github.com/quantumlib/Cirq/blob/0827da80dd7880e5b923eb69407e980ed9bc0bd2/cirq/ops/eigen_gate.py#L330-L374", "partition": "train"}
{"repo": "quantumlib/Cirq", "path": "cirq/ops/eigen_gate.py", "func_name": "_common_rational_period", "original_string": "def _common_rational_period(rational_periods: List[fractions.Fraction]\n                            ) -> fractions.Fraction:\n    \"\"\"Finds the least common integer multiple of some fractions.\n\n    The solution is the smallest positive integer c such that there\n    exists integers n_k satisfying p_k * n_k = c for all k.\n    \"\"\"\n    assert rational_periods, \"no well-defined solution for an empty list\"\n    common_denom = _lcm(p.denominator for p in rational_periods)\n    int_periods = [p.numerator * common_denom // p.denominator\n                   for p in rational_periods]\n    int_common_period = _lcm(int_periods)\n    return fractions.Fraction(int_common_period, common_denom)", "language": "python", "code": "def _common_rational_period(rational_periods: List[fractions.Fraction]\n                            ) -> fractions.Fraction:\n    \"\"\"Finds the least common integer multiple of some fractions.\n\n    The solution is the smallest positive integer c such that there\n    exists integers n_k satisfying p_k * n_k = c for all k.\n    \"\"\"\n    assert rational_periods, \"no well-defined solution for an empty list\"\n    common_denom = _lcm(p.denominator for p in rational_periods)\n    int_periods = [p.numerator * common_denom // p.denominator\n                   for p in rational_periods]\n    int_common_period = _lcm(int_periods)\n    return fractions.Fraction(int_common_period, common_denom)", "code_tokens": ["def", "_common_rational_period", "(", "rational_periods", ":", "List", "[", "fractions", ".", "Fraction", "]", ")", "->", "fractions", ".", "Fraction", ":", "assert", "rational_periods", ",", "\"no well-defined solution for an empty list\"", "common_denom", "=", "_lcm", "(", "p", ".", "denominator", "for", "p", "in", "rational_periods", ")", "int_periods", "=", "[", "p", ".", "numerator", "*", "common_denom", "//", "p", ".", "denominator", "for", "p", "in", "rational_periods", "]", "int_common_period", "=", "_lcm", "(", "int_periods", ")", "return", "fractions", ".", "Fraction", "(", "int_common_period", ",", "common_denom", ")"], "docstring": "Finds the least common integer multiple of some fractions.\n\n    The solution is the smallest positive integer c such that there\n    exists integers n_k satisfying p_k * n_k = c for all k.", "docstring_tokens": ["Finds", "the", "least", "common", "integer", "multiple", "of", "some", "fractions", "."], "sha": "0827da80dd7880e5b923eb69407e980ed9bc0bd2", "url": "https://github.com/quantumlib/Cirq/blob/0827da80dd7880e5b923eb69407e980ed9bc0bd2/cirq/ops/eigen_gate.py#L377-L389", "partition": "train"}
{"repo": "combust/mleap", "path": "python/mleap/sklearn/preprocessing/data.py", "func_name": "LabelEncoder.fit", "original_string": "def fit(self, X):\n        \"\"\"Fit label encoder\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : returns an instance of self.\n        \"\"\"\n        X = column_or_1d(X, warn=True)\n        _check_numpy_unicode_bug(X)\n        self.classes_ = np.unique(X)\n        return self", "language": "python", "code": "def fit(self, X):\n        \"\"\"Fit label encoder\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : returns an instance of self.\n        \"\"\"\n        X = column_or_1d(X, warn=True)\n        _check_numpy_unicode_bug(X)\n        self.classes_ = np.unique(X)\n        return self", "code_tokens": ["def", "fit", "(", "self", ",", "X", ")", ":", "X", "=", "column_or_1d", "(", "X", ",", "warn", "=", "True", ")", "_check_numpy_unicode_bug", "(", "X", ")", "self", ".", "classes_", "=", "np", ".", "unique", "(", "X", ")", "return", "self"], "docstring": "Fit label encoder\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : returns an instance of self.", "docstring_tokens": ["Fit", "label", "encoder"], "sha": "dc6b79db03ec27a0ba08b289842551e73d517ab3", "url": "https://github.com/combust/mleap/blob/dc6b79db03ec27a0ba08b289842551e73d517ab3/python/mleap/sklearn/preprocessing/data.py#L361-L376", "partition": "train"}
{"repo": "openthread/openthread", "path": "tools/harness-thci/OpenThread.py", "func_name": "OpenThread.getMAC", "original_string": "def getMAC(self, bType=MacType.RandomMac):\n        \"\"\"get one specific type of MAC address\n           currently OpenThread only supports Random MAC address\n\n        Args:\n            bType: indicate which kind of MAC address is required\n\n        Returns:\n            specific type of MAC address\n        \"\"\"\n        print '%s call getMAC' % self.port\n        print bType\n        # if power down happens, return extended address assigned previously\n        if self.isPowerDown:\n            macAddr64 = self.mac\n        else:\n            if bType == MacType.FactoryMac:\n                macAddr64 = self.__sendCommand('eui64')[0]\n            elif bType == MacType.HashMac:\n                macAddr64 = self.__sendCommand('joinerid')[0]\n            else:\n                macAddr64 = self.__sendCommand('extaddr')[0]\n        print macAddr64\n\n        return int(macAddr64, 16)", "language": "python", "code": "def getMAC(self, bType=MacType.RandomMac):\n        \"\"\"get one specific type of MAC address\n           currently OpenThread only supports Random MAC address\n\n        Args:\n            bType: indicate which kind of MAC address is required\n\n        Returns:\n            specific type of MAC address\n        \"\"\"\n        print '%s call getMAC' % self.port\n        print bType\n        # if power down happens, return extended address assigned previously\n        if self.isPowerDown:\n            macAddr64 = self.mac\n        else:\n            if bType == MacType.FactoryMac:\n                macAddr64 = self.__sendCommand('eui64')[0]\n            elif bType == MacType.HashMac:\n                macAddr64 = self.__sendCommand('joinerid')[0]\n            else:\n                macAddr64 = self.__sendCommand('extaddr')[0]\n        print macAddr64\n\n        return int(macAddr64, 16)", "code_tokens": ["def", "getMAC", "(", "self", ",", "bType", "=", "MacType", ".", "RandomMac", ")", ":", "print", "'%s call getMAC'", "%", "self", ".", "port", "print", "bType", "# if power down happens, return extended address assigned previously", "if", "self", ".", "isPowerDown", ":", "macAddr64", "=", "self", ".", "mac", "else", ":", "if", "bType", "==", "MacType", ".", "FactoryMac", ":", "macAddr64", "=", "self", ".", "__sendCommand", "(", "'eui64'", ")", "[", "0", "]", "elif", "bType", "==", "MacType", ".", "HashMac", ":", "macAddr64", "=", "self", ".", "__sendCommand", "(", "'joinerid'", ")", "[", "0", "]", "else", ":", "macAddr64", "=", "self", ".", "__sendCommand", "(", "'extaddr'", ")", "[", "0", "]", "print", "macAddr64", "return", "int", "(", "macAddr64", ",", "16", ")"], "docstring": "get one specific type of MAC address\n           currently OpenThread only supports Random MAC address\n\n        Args:\n            bType: indicate which kind of MAC address is required\n\n        Returns:\n            specific type of MAC address", "docstring_tokens": ["get", "one", "specific", "type", "of", "MAC", "address", "currently", "OpenThread", "only", "supports", "Random", "MAC", "address"], "sha": "0208d10563aa21c518092985c78ecf9cd223ab74", "url": "https://github.com/openthread/openthread/blob/0208d10563aa21c518092985c78ecf9cd223ab74/tools/harness-thci/OpenThread.py#L761-L785", "partition": "train"}
{"repo": "ranaroussi/qtpylib", "path": "qtpylib/instrument.py", "func_name": "Instrument.buy", "original_string": "def buy(self, quantity, **kwargs):\n        \"\"\" Shortcut for ``instrument.order(\"BUY\", ...)`` and accepts all of its\n        `optional parameters <#qtpylib.instrument.Instrument.order>`_\n\n        :Parameters:\n            quantity : int\n                Order quantity\n        \"\"\"\n        self.parent.order(\"BUY\", self, quantity=quantity, **kwargs)", "language": "python", "code": "def buy(self, quantity, **kwargs):\n        \"\"\" Shortcut for ``instrument.order(\"BUY\", ...)`` and accepts all of its\n        `optional parameters <#qtpylib.instrument.Instrument.order>`_\n\n        :Parameters:\n            quantity : int\n                Order quantity\n        \"\"\"\n        self.parent.order(\"BUY\", self, quantity=quantity, **kwargs)", "code_tokens": ["def", "buy", "(", "self", ",", "quantity", ",", "*", "*", "kwargs", ")", ":", "self", ".", "parent", ".", "order", "(", "\"BUY\"", ",", "self", ",", "quantity", "=", "quantity", ",", "*", "*", "kwargs", ")"], "docstring": "Shortcut for ``instrument.order(\"BUY\", ...)`` and accepts all of its\n        `optional parameters <#qtpylib.instrument.Instrument.order>`_\n\n        :Parameters:\n            quantity : int\n                Order quantity", "docstring_tokens": ["Shortcut", "for", "instrument", ".", "order", "(", "BUY", "...", ")", "and", "accepts", "all", "of", "its", "optional", "parameters", "<#qtpylib", ".", "instrument", ".", "Instrument", ".", "order", ">", "_"], "sha": "0dbbc465fafd9cb9b0f4d10e1e07fae4e15032dd", "url": "https://github.com/ranaroussi/qtpylib/blob/0dbbc465fafd9cb9b0f4d10e1e07fae4e15032dd/qtpylib/instrument.py#L267-L275", "partition": "train"}
{"repo": "google/grr", "path": "api_client/python/grr_api_client/vfs.py", "func_name": "FileBase.Get", "original_string": "def Get(self):\n    \"\"\"Fetch file's data and return proper File object.\"\"\"\n\n    args = vfs_pb2.ApiGetFileDetailsArgs(\n        client_id=self.client_id, file_path=self.path)\n    data = self._context.SendRequest(\"GetFileDetails\", args).file\n    return File(client_id=self.client_id, data=data, context=self._context)", "language": "python", "code": "def Get(self):\n    \"\"\"Fetch file's data and return proper File object.\"\"\"\n\n    args = vfs_pb2.ApiGetFileDetailsArgs(\n        client_id=self.client_id, file_path=self.path)\n    data = self._context.SendRequest(\"GetFileDetails\", args).file\n    return File(client_id=self.client_id, data=data, context=self._context)", "code_tokens": ["def", "Get", "(", "self", ")", ":", "args", "=", "vfs_pb2", ".", "ApiGetFileDetailsArgs", "(", "client_id", "=", "self", ".", "client_id", ",", "file_path", "=", "self", ".", "path", ")", "data", "=", "self", ".", "_context", ".", "SendRequest", "(", "\"GetFileDetails\"", ",", "args", ")", ".", "file", "return", "File", "(", "client_id", "=", "self", ".", "client_id", ",", "data", "=", "data", ",", "context", "=", "self", ".", "_context", ")"], "docstring": "Fetch file's data and return proper File object.", "docstring_tokens": ["Fetch", "file", "s", "data", "and", "return", "proper", "File", "object", "."], "sha": "5cef4e8e2f0d5df43ea4877e9c798e0bf60bfe74", "url": "https://github.com/google/grr/blob/5cef4e8e2f0d5df43ea4877e9c798e0bf60bfe74/api_client/python/grr_api_client/vfs.py#L165-L171", "partition": "train"}
{"repo": "google/grr", "path": "grr/server/grr_response_server/gui/api_plugins/flow.py", "func_name": "GetOutputPluginIndex", "original_string": "def GetOutputPluginIndex(\n    plugin_descriptors,\n    plugin_id):\n  \"\"\"Gets an output plugin index for a plugin with a given id.\n\n  Historically output plugins descriptors were stored in dicts-like\n  structures with unique identifiers as keys. In REL_DB-based implementation,\n  however, both plugin descriptors and their states are stored in flat\n  lists (see Flow definition in flows.proto).\n\n  The ids were formed as \"<plugin name>_<plugin index>\" where plugin index\n  was incremented for every plugin with a same name. For example, if we had\n  EmailOutputPlugin and 2 BigQueryOutputPlugins, their ids would be:\n  EmailOutputPlugin_0, BigQueryOutputPlugin_0, BigQueryOutputPlugin_1.\n\n  To preserve backwards API compatibility, we emulate the old behavior by\n  identifying plugins with same plugin ids as before..\n\n  Args:\n    plugin_descriptors: An iterable of OutputPluginDescriptor objects.\n    plugin_id: Plugin id to search for.\n\n  Returns:\n    An index of a plugin in plugin_descriptors iterable corresponding to a\n    given plugin_id.\n\n  Raises:\n    OutputPluginNotFoundError: if no plugin corresponding to a given plugin_id\n    was found.\n  \"\"\"\n\n  used_names = collections.Counter()\n  for (index, desc) in enumerate(plugin_descriptors):\n    cur_plugin_id = \"%s_%d\" % (desc.plugin_name, used_names[desc.plugin_name])\n    used_names[desc.plugin_name] += 1\n\n    if cur_plugin_id == plugin_id:\n      return index\n\n  raise OutputPluginNotFoundError(\"Can't find output plugin %s\" % plugin_id)", "language": "python", "code": "def GetOutputPluginIndex(\n    plugin_descriptors,\n    plugin_id):\n  \"\"\"Gets an output plugin index for a plugin with a given id.\n\n  Historically output plugins descriptors were stored in dicts-like\n  structures with unique identifiers as keys. In REL_DB-based implementation,\n  however, both plugin descriptors and their states are stored in flat\n  lists (see Flow definition in flows.proto).\n\n  The ids were formed as \"<plugin name>_<plugin index>\" where plugin index\n  was incremented for every plugin with a same name. For example, if we had\n  EmailOutputPlugin and 2 BigQueryOutputPlugins, their ids would be:\n  EmailOutputPlugin_0, BigQueryOutputPlugin_0, BigQueryOutputPlugin_1.\n\n  To preserve backwards API compatibility, we emulate the old behavior by\n  identifying plugins with same plugin ids as before..\n\n  Args:\n    plugin_descriptors: An iterable of OutputPluginDescriptor objects.\n    plugin_id: Plugin id to search for.\n\n  Returns:\n    An index of a plugin in plugin_descriptors iterable corresponding to a\n    given plugin_id.\n\n  Raises:\n    OutputPluginNotFoundError: if no plugin corresponding to a given plugin_id\n    was found.\n  \"\"\"\n\n  used_names = collections.Counter()\n  for (index, desc) in enumerate(plugin_descriptors):\n    cur_plugin_id = \"%s_%d\" % (desc.plugin_name, used_names[desc.plugin_name])\n    used_names[desc.plugin_name] += 1\n\n    if cur_plugin_id == plugin_id:\n      return index\n\n  raise OutputPluginNotFoundError(\"Can't find output plugin %s\" % plugin_id)", "code_tokens": ["def", "GetOutputPluginIndex", "(", "plugin_descriptors", ",", "plugin_id", ")", ":", "used_names", "=", "collections", ".", "Counter", "(", ")", "for", "(", "index", ",", "desc", ")", "in", "enumerate", "(", "plugin_descriptors", ")", ":", "cur_plugin_id", "=", "\"%s_%d\"", "%", "(", "desc", ".", "plugin_name", ",", "used_names", "[", "desc", ".", "plugin_name", "]", ")", "used_names", "[", "desc", ".", "plugin_name", "]", "+=", "1", "if", "cur_plugin_id", "==", "plugin_id", ":", "return", "index", "raise", "OutputPluginNotFoundError", "(", "\"Can't find output plugin %s\"", "%", "plugin_id", ")"], "docstring": "Gets an output plugin index for a plugin with a given id.\n\n  Historically output plugins descriptors were stored in dicts-like\n  structures with unique identifiers as keys. In REL_DB-based implementation,\n  however, both plugin descriptors and their states are stored in flat\n  lists (see Flow definition in flows.proto).\n\n  The ids were formed as \"<plugin name>_<plugin index>\" where plugin index\n  was incremented for every plugin with a same name. For example, if we had\n  EmailOutputPlugin and 2 BigQueryOutputPlugins, their ids would be:\n  EmailOutputPlugin_0, BigQueryOutputPlugin_0, BigQueryOutputPlugin_1.\n\n  To preserve backwards API compatibility, we emulate the old behavior by\n  identifying plugins with same plugin ids as before..\n\n  Args:\n    plugin_descriptors: An iterable of OutputPluginDescriptor objects.\n    plugin_id: Plugin id to search for.\n\n  Returns:\n    An index of a plugin in plugin_descriptors iterable corresponding to a\n    given plugin_id.\n\n  Raises:\n    OutputPluginNotFoundError: if no plugin corresponding to a given plugin_id\n    was found.", "docstring_tokens": ["Gets", "an", "output", "plugin", "index", "for", "a", "plugin", "with", "a", "given", "id", "."], "sha": "5cef4e8e2f0d5df43ea4877e9c798e0bf60bfe74", "url": "https://github.com/google/grr/blob/5cef4e8e2f0d5df43ea4877e9c798e0bf60bfe74/grr/server/grr_response_server/gui/api_plugins/flow.py#L936-L975", "partition": "train"}
{"repo": "watson-developer-cloud/python-sdk", "path": "ibm_watson/discovery_v1.py", "func_name": "DeleteEnvironmentResponse._to_dict", "original_string": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'environment_id') and self.environment_id is not None:\n            _dict['environment_id'] = self.environment_id\n        if hasattr(self, 'status') and self.status is not None:\n            _dict['status'] = self.status\n        return _dict", "language": "python", "code": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'environment_id') and self.environment_id is not None:\n            _dict['environment_id'] = self.environment_id\n        if hasattr(self, 'status') and self.status is not None:\n            _dict['status'] = self.status\n        return _dict", "code_tokens": ["def", "_to_dict", "(", "self", ")", ":", "_dict", "=", "{", "}", "if", "hasattr", "(", "self", ",", "'environment_id'", ")", "and", "self", ".", "environment_id", "is", "not", "None", ":", "_dict", "[", "'environment_id'", "]", "=", "self", ".", "environment_id", "if", "hasattr", "(", "self", ",", "'status'", ")", "and", "self", ".", "status", "is", "not", "None", ":", "_dict", "[", "'status'", "]", "=", "self", ".", "status", "return", "_dict"], "docstring": "Return a json dictionary representing this model.", "docstring_tokens": ["Return", "a", "json", "dictionary", "representing", "this", "model", "."], "sha": "4c2c9df4466fcde88975da9ecd834e6ba95eb353", "url": "https://github.com/watson-developer-cloud/python-sdk/blob/4c2c9df4466fcde88975da9ecd834e6ba95eb353/ibm_watson/discovery_v1.py#L4906-L4913", "partition": "train"}
{"repo": "watson-developer-cloud/python-sdk", "path": "ibm_watson/discovery_v1.py", "func_name": "DiskUsage._to_dict", "original_string": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'used_bytes') and self.used_bytes is not None:\n            _dict['used_bytes'] = self.used_bytes\n        if hasattr(self, 'maximum_allowed_bytes'\n                  ) and self.maximum_allowed_bytes is not None:\n            _dict['maximum_allowed_bytes'] = self.maximum_allowed_bytes\n        return _dict", "language": "python", "code": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'used_bytes') and self.used_bytes is not None:\n            _dict['used_bytes'] = self.used_bytes\n        if hasattr(self, 'maximum_allowed_bytes'\n                  ) and self.maximum_allowed_bytes is not None:\n            _dict['maximum_allowed_bytes'] = self.maximum_allowed_bytes\n        return _dict", "code_tokens": ["def", "_to_dict", "(", "self", ")", ":", "_dict", "=", "{", "}", "if", "hasattr", "(", "self", ",", "'used_bytes'", ")", "and", "self", ".", "used_bytes", "is", "not", "None", ":", "_dict", "[", "'used_bytes'", "]", "=", "self", ".", "used_bytes", "if", "hasattr", "(", "self", ",", "'maximum_allowed_bytes'", ")", "and", "self", ".", "maximum_allowed_bytes", "is", "not", "None", ":", "_dict", "[", "'maximum_allowed_bytes'", "]", "=", "self", ".", "maximum_allowed_bytes", "return", "_dict"], "docstring": "Return a json dictionary representing this model.", "docstring_tokens": ["Return", "a", "json", "dictionary", "representing", "this", "model", "."], "sha": "4c2c9df4466fcde88975da9ecd834e6ba95eb353", "url": "https://github.com/watson-developer-cloud/python-sdk/blob/4c2c9df4466fcde88975da9ecd834e6ba95eb353/ibm_watson/discovery_v1.py#L4962-L4970", "partition": "train"}
{"repo": "espressif/esptool", "path": "ecdsa/util.py", "func_name": "randrange", "original_string": "def randrange(order, entropy=None):\n    \"\"\"Return a random integer k such that 1 <= k < order, uniformly\n    distributed across that range. For simplicity, this only behaves well if\n    'order' is fairly close (but below) a power of 256. The try-try-again\n    algorithm we use takes longer and longer time (on average) to complete as\n    'order' falls, rising to a maximum of avg=512 loops for the worst-case\n    (256**k)+1 . All of the standard curves behave well. There is a cutoff at\n    10k loops (which raises RuntimeError) to prevent an infinite loop when\n    something is really broken like the entropy function not working.\n\n    Note that this function is not declared to be forwards-compatible: we may\n    change the behavior in future releases. The entropy= argument (which\n    should get a callable that behaves like os.urandom) can be used to\n    achieve stability within a given release (for repeatable unit tests), but\n    should not be used as a long-term-compatible key generation algorithm.\n    \"\"\"\n    # we could handle arbitrary orders (even 256**k+1) better if we created\n    # candidates bit-wise instead of byte-wise, which would reduce the\n    # worst-case behavior to avg=2 loops, but that would be more complex. The\n    # change would be to round the order up to a power of 256, subtract one\n    # (to get 0xffff..), use that to get a byte-long mask for the top byte,\n    # generate the len-1 entropy bytes, generate one extra byte and mask off\n    # the top bits, then combine it with the rest. Requires jumping back and\n    # forth between strings and integers a lot.\n\n    if entropy is None:\n        entropy = os.urandom\n    assert order > 1\n    bytes = orderlen(order)\n    dont_try_forever = 10000 # gives about 2**-60 failures for worst case\n    while dont_try_forever > 0:\n        dont_try_forever -= 1\n        candidate = string_to_number(entropy(bytes)) + 1\n        if 1 <= candidate < order:\n            return candidate\n        continue\n    raise RuntimeError(\"randrange() tried hard but gave up, either something\"\n                       \" is very wrong or you got realllly unlucky. Order was\"\n                       \" %x\" % order)", "language": "python", "code": "def randrange(order, entropy=None):\n    \"\"\"Return a random integer k such that 1 <= k < order, uniformly\n    distributed across that range. For simplicity, this only behaves well if\n    'order' is fairly close (but below) a power of 256. The try-try-again\n    algorithm we use takes longer and longer time (on average) to complete as\n    'order' falls, rising to a maximum of avg=512 loops for the worst-case\n    (256**k)+1 . All of the standard curves behave well. There is a cutoff at\n    10k loops (which raises RuntimeError) to prevent an infinite loop when\n    something is really broken like the entropy function not working.\n\n    Note that this function is not declared to be forwards-compatible: we may\n    change the behavior in future releases. The entropy= argument (which\n    should get a callable that behaves like os.urandom) can be used to\n    achieve stability within a given release (for repeatable unit tests), but\n    should not be used as a long-term-compatible key generation algorithm.\n    \"\"\"\n    # we could handle arbitrary orders (even 256**k+1) better if we created\n    # candidates bit-wise instead of byte-wise, which would reduce the\n    # worst-case behavior to avg=2 loops, but that would be more complex. The\n    # change would be to round the order up to a power of 256, subtract one\n    # (to get 0xffff..), use that to get a byte-long mask for the top byte,\n    # generate the len-1 entropy bytes, generate one extra byte and mask off\n    # the top bits, then combine it with the rest. Requires jumping back and\n    # forth between strings and integers a lot.\n\n    if entropy is None:\n        entropy = os.urandom\n    assert order > 1\n    bytes = orderlen(order)\n    dont_try_forever = 10000 # gives about 2**-60 failures for worst case\n    while dont_try_forever > 0:\n        dont_try_forever -= 1\n        candidate = string_to_number(entropy(bytes)) + 1\n        if 1 <= candidate < order:\n            return candidate\n        continue\n    raise RuntimeError(\"randrange() tried hard but gave up, either something\"\n                       \" is very wrong or you got realllly unlucky. Order was\"\n                       \" %x\" % order)", "code_tokens": ["def", "randrange", "(", "order", ",", "entropy", "=", "None", ")", ":", "# we could handle arbitrary orders (even 256**k+1) better if we created", "# candidates bit-wise instead of byte-wise, which would reduce the", "# worst-case behavior to avg=2 loops, but that would be more complex. The", "# change would be to round the order up to a power of 256, subtract one", "# (to get 0xffff..), use that to get a byte-long mask for the top byte,", "# generate the len-1 entropy bytes, generate one extra byte and mask off", "# the top bits, then combine it with the rest. Requires jumping back and", "# forth between strings and integers a lot.", "if", "entropy", "is", "None", ":", "entropy", "=", "os", ".", "urandom", "assert", "order", ">", "1", "bytes", "=", "orderlen", "(", "order", ")", "dont_try_forever", "=", "10000", "# gives about 2**-60 failures for worst case", "while", "dont_try_forever", ">", "0", ":", "dont_try_forever", "-=", "1", "candidate", "=", "string_to_number", "(", "entropy", "(", "bytes", ")", ")", "+", "1", "if", "1", "<=", "candidate", "<", "order", ":", "return", "candidate", "continue", "raise", "RuntimeError", "(", "\"randrange() tried hard but gave up, either something\"", "\" is very wrong or you got realllly unlucky. Order was\"", "\" %x\"", "%", "order", ")"], "docstring": "Return a random integer k such that 1 <= k < order, uniformly\n    distributed across that range. For simplicity, this only behaves well if\n    'order' is fairly close (but below) a power of 256. The try-try-again\n    algorithm we use takes longer and longer time (on average) to complete as\n    'order' falls, rising to a maximum of avg=512 loops for the worst-case\n    (256**k)+1 . All of the standard curves behave well. There is a cutoff at\n    10k loops (which raises RuntimeError) to prevent an infinite loop when\n    something is really broken like the entropy function not working.\n\n    Note that this function is not declared to be forwards-compatible: we may\n    change the behavior in future releases. The entropy= argument (which\n    should get a callable that behaves like os.urandom) can be used to\n    achieve stability within a given release (for repeatable unit tests), but\n    should not be used as a long-term-compatible key generation algorithm.", "docstring_tokens": ["Return", "a", "random", "integer", "k", "such", "that", "1", "<", "=", "k", "<", "order", "uniformly", "distributed", "across", "that", "range", ".", "For", "simplicity", "this", "only", "behaves", "well", "if", "order", "is", "fairly", "close", "(", "but", "below", ")", "a", "power", "of", "256", ".", "The", "try", "-", "try", "-", "again", "algorithm", "we", "use", "takes", "longer", "and", "longer", "time", "(", "on", "average", ")", "to", "complete", "as", "order", "falls", "rising", "to", "a", "maximum", "of", "avg", "=", "512", "loops", "for", "the", "worst", "-", "case", "(", "256", "**", "k", ")", "+", "1", ".", "All", "of", "the", "standard", "curves", "behave", "well", ".", "There", "is", "a", "cutoff", "at", "10k", "loops", "(", "which", "raises", "RuntimeError", ")", "to", "prevent", "an", "infinite", "loop", "when", "something", "is", "really", "broken", "like", "the", "entropy", "function", "not", "working", "."], "sha": "c583756c118039cfcfe256f7a3285618914d16a5", "url": "https://github.com/espressif/esptool/blob/c583756c118039cfcfe256f7a3285618914d16a5/ecdsa/util.py#L19-L57", "partition": "train"}
{"repo": "Qiskit/qiskit", "path": "tools/generate_authors.py", "func_name": "generate_authors", "original_string": "def generate_authors(git_dir):\n    \"\"\"Create AUTHORS file using git commits.\"\"\"\n    authors = []\n    emails = []\n    git_log_cmd = ['git', 'log', '--format=%aN|%aE']\n    tmp_authors = _run_shell_command(git_log_cmd, git_dir).split('\\n')\n    for author_str in tmp_authors:\n        author, email = author_str.split('|')\n        author = author.strip()\n        email = email.strip()\n        if author.lower() not in [x.lower() for x in authors]:\n            if email.lower() not in [x.lower() for x in emails]:\n                authors.append(author)\n                emails.append(email)\n    co_authors_raw = _run_shell_command(['git', 'log'], git_dir)\n    co_authors = re.findall('Co-authored-by:.+', co_authors_raw,\n                            re.MULTILINE)\n    co_authors = [signed.split(\":\", 1)[1].strip().split('<')\n                  for signed in co_authors if signed]\n    for author_str in co_authors:\n        author, email = author_str.split('<')\n        author = author.strip()\n        email = email[:-1].strip()\n        if author.lower() not in [x.lower() for x in authors]:\n            if email.lower() not in [x.lower() for x in emails]:\n                authors.append(author)\n                emails.append(email)\n    authors = sorted(set(authors))\n    return authors", "language": "python", "code": "def generate_authors(git_dir):\n    \"\"\"Create AUTHORS file using git commits.\"\"\"\n    authors = []\n    emails = []\n    git_log_cmd = ['git', 'log', '--format=%aN|%aE']\n    tmp_authors = _run_shell_command(git_log_cmd, git_dir).split('\\n')\n    for author_str in tmp_authors:\n        author, email = author_str.split('|')\n        author = author.strip()\n        email = email.strip()\n        if author.lower() not in [x.lower() for x in authors]:\n            if email.lower() not in [x.lower() for x in emails]:\n                authors.append(author)\n                emails.append(email)\n    co_authors_raw = _run_shell_command(['git', 'log'], git_dir)\n    co_authors = re.findall('Co-authored-by:.+', co_authors_raw,\n                            re.MULTILINE)\n    co_authors = [signed.split(\":\", 1)[1].strip().split('<')\n                  for signed in co_authors if signed]\n    for author_str in co_authors:\n        author, email = author_str.split('<')\n        author = author.strip()\n        email = email[:-1].strip()\n        if author.lower() not in [x.lower() for x in authors]:\n            if email.lower() not in [x.lower() for x in emails]:\n                authors.append(author)\n                emails.append(email)\n    authors = sorted(set(authors))\n    return authors", "code_tokens": ["def", "generate_authors", "(", "git_dir", ")", ":", "authors", "=", "[", "]", "emails", "=", "[", "]", "git_log_cmd", "=", "[", "'git'", ",", "'log'", ",", "'--format=%aN|%aE'", "]", "tmp_authors", "=", "_run_shell_command", "(", "git_log_cmd", ",", "git_dir", ")", ".", "split", "(", "'\\n'", ")", "for", "author_str", "in", "tmp_authors", ":", "author", ",", "email", "=", "author_str", ".", "split", "(", "'|'", ")", "author", "=", "author", ".", "strip", "(", ")", "email", "=", "email", ".", "strip", "(", ")", "if", "author", ".", "lower", "(", ")", "not", "in", "[", "x", ".", "lower", "(", ")", "for", "x", "in", "authors", "]", ":", "if", "email", ".", "lower", "(", ")", "not", "in", "[", "x", ".", "lower", "(", ")", "for", "x", "in", "emails", "]", ":", "authors", ".", "append", "(", "author", ")", "emails", ".", "append", "(", "email", ")", "co_authors_raw", "=", "_run_shell_command", "(", "[", "'git'", ",", "'log'", "]", ",", "git_dir", ")", "co_authors", "=", "re", ".", "findall", "(", "'Co-authored-by:.+'", ",", "co_authors_raw", ",", "re", ".", "MULTILINE", ")", "co_authors", "=", "[", "signed", ".", "split", "(", "\":\"", ",", "1", ")", "[", "1", "]", ".", "strip", "(", ")", ".", "split", "(", "'<'", ")", "for", "signed", "in", "co_authors", "if", "signed", "]", "for", "author_str", "in", "co_authors", ":", "author", ",", "email", "=", "author_str", ".", "split", "(", "'<'", ")", "author", "=", "author", ".", "strip", "(", ")", "email", "=", "email", "[", ":", "-", "1", "]", ".", "strip", "(", ")", "if", "author", ".", "lower", "(", ")", "not", "in", "[", "x", ".", "lower", "(", ")", "for", "x", "in", "authors", "]", ":", "if", "email", ".", "lower", "(", ")", "not", "in", "[", "x", ".", "lower", "(", ")", "for", "x", "in", "emails", "]", ":", "authors", ".", "append", "(", "author", ")", "emails", ".", "append", "(", "email", ")", "authors", "=", "sorted", "(", "set", "(", "authors", ")", ")", "return", "authors"], "docstring": "Create AUTHORS file using git commits.", "docstring_tokens": ["Create", "AUTHORS", "file", "using", "git", "commits", "."], "sha": "f1c3b63b23f912c1026dc886a723b58cf0c3b3df", "url": "https://github.com/Qiskit/qiskit/blob/f1c3b63b23f912c1026dc886a723b58cf0c3b3df/tools/generate_authors.py#L22-L50", "partition": "train"}
{"repo": "pgmpy/pgmpy", "path": "pgmpy/sampling/NUTS.py", "func_name": "NoUTurnSampler._initalize_tree", "original_string": "def _initalize_tree(self, position, momentum, slice_var, stepsize):\n        \"\"\"\n        Initalizes root node of the tree, i.e depth = 0\n        \"\"\"\n\n        position_bar, momentum_bar, _ = self.simulate_dynamics(self.model, position, momentum, stepsize,\n                                                               self.grad_log_pdf).get_proposed_values()\n\n        _, logp_bar = self.grad_log_pdf(position_bar, self.model).get_gradient_log_pdf()\n\n        hamiltonian = logp_bar - 0.5 * np.dot(momentum_bar, momentum_bar)\n\n        candidate_set_size = slice_var < np.exp(hamiltonian)\n        accept_set_bool = hamiltonian > np.log(slice_var) - 10000  # delta_max = 10000\n\n        return position_bar, momentum_bar, candidate_set_size, accept_set_bool", "language": "python", "code": "def _initalize_tree(self, position, momentum, slice_var, stepsize):\n        \"\"\"\n        Initalizes root node of the tree, i.e depth = 0\n        \"\"\"\n\n        position_bar, momentum_bar, _ = self.simulate_dynamics(self.model, position, momentum, stepsize,\n                                                               self.grad_log_pdf).get_proposed_values()\n\n        _, logp_bar = self.grad_log_pdf(position_bar, self.model).get_gradient_log_pdf()\n\n        hamiltonian = logp_bar - 0.5 * np.dot(momentum_bar, momentum_bar)\n\n        candidate_set_size = slice_var < np.exp(hamiltonian)\n        accept_set_bool = hamiltonian > np.log(slice_var) - 10000  # delta_max = 10000\n\n        return position_bar, momentum_bar, candidate_set_size, accept_set_bool", "code_tokens": ["def", "_initalize_tree", "(", "self", ",", "position", ",", "momentum", ",", "slice_var", ",", "stepsize", ")", ":", "position_bar", ",", "momentum_bar", ",", "_", "=", "self", ".", "simulate_dynamics", "(", "self", ".", "model", ",", "position", ",", "momentum", ",", "stepsize", ",", "self", ".", "grad_log_pdf", ")", ".", "get_proposed_values", "(", ")", "_", ",", "logp_bar", "=", "self", ".", "grad_log_pdf", "(", "position_bar", ",", "self", ".", "model", ")", ".", "get_gradient_log_pdf", "(", ")", "hamiltonian", "=", "logp_bar", "-", "0.5", "*", "np", ".", "dot", "(", "momentum_bar", ",", "momentum_bar", ")", "candidate_set_size", "=", "slice_var", "<", "np", ".", "exp", "(", "hamiltonian", ")", "accept_set_bool", "=", "hamiltonian", ">", "np", ".", "log", "(", "slice_var", ")", "-", "10000", "# delta_max = 10000", "return", "position_bar", ",", "momentum_bar", ",", "candidate_set_size", ",", "accept_set_bool"], "docstring": "Initalizes root node of the tree, i.e depth = 0", "docstring_tokens": ["Initalizes", "root", "node", "of", "the", "tree", "i", ".", "e", "depth", "=", "0"], "sha": "9381a66aba3c3871d3ccd00672b148d17d63239e", "url": "https://github.com/pgmpy/pgmpy/blob/9381a66aba3c3871d3ccd00672b148d17d63239e/pgmpy/sampling/NUTS.py#L66-L81", "partition": "train"}
{"repo": "pgmpy/pgmpy", "path": "pgmpy/sampling/NUTS.py", "func_name": "NoUTurnSampler._build_tree", "original_string": "def _build_tree(self, position, momentum, slice_var, direction, depth, stepsize):\n        \"\"\"\n        Recursively builds a tree for proposing new position and momentum\n        \"\"\"\n\n        # Parameter names in algorithm (here -> representation in algorithm)\n        # position -> theta, momentum -> r, slice_var -> u, direction -> v, depth ->j, stepsize -> epsilon\n        # candidate_set_size -> n, accept_set_bool -> s\n        if depth == 0:\n            # Take single leapfrog step in the given direction (direction * stepsize)\n            position_bar, momentum_bar, candidate_set_size, accept_set_bool =\\\n                self._initalize_tree(position, momentum, slice_var, direction * stepsize)\n\n            return (position_bar, momentum_bar, position_bar, momentum_bar, position_bar,\n                    candidate_set_size, accept_set_bool)\n\n        else:\n            # Build left and right subtrees\n            (position_backward, momentum_backward, position_forward, momentum_forward, position_bar,\n             candidate_set_size, accept_set_bool) = self._build_tree(position, momentum,\n                                                                     slice_var, direction, depth - 1, stepsize)\n            if accept_set_bool == 1:\n                if direction == -1:\n                    # Build tree in backward direction\n                    (position_backward, momentum_backward, _, _, position_bar2, candidate_set_size2,\n                     accept_set_bool2) = self._build_tree(position_backward, momentum_backward,\n                                                          slice_var, direction, depth - 1, stepsize)\n                else:\n                    # Build tree in forward direction\n                    (_, _, position_forward, momentum_forward, position_bar2, candidate_set_size2,\n                     accept_set_bool2) = self._build_tree(position_forward, momentum_forward,\n                                                          slice_var, direction, depth - 1, stepsize)\n\n                if np.random.rand() < candidate_set_size2 / (candidate_set_size2 + candidate_set_size):\n                    position_bar = position_bar2\n\n                accept_set_bool, candidate_set_size =\\\n                    self._update_acceptance_criteria(position_forward, position_backward, momentum_forward,\n                                                     momentum_backward, accept_set_bool2, candidate_set_size,\n                                                     candidate_set_size2)\n\n            return (position_backward, momentum_backward, position_forward, momentum_forward,\n                    position_bar, candidate_set_size, accept_set_bool)", "language": "python", "code": "def _build_tree(self, position, momentum, slice_var, direction, depth, stepsize):\n        \"\"\"\n        Recursively builds a tree for proposing new position and momentum\n        \"\"\"\n\n        # Parameter names in algorithm (here -> representation in algorithm)\n        # position -> theta, momentum -> r, slice_var -> u, direction -> v, depth ->j, stepsize -> epsilon\n        # candidate_set_size -> n, accept_set_bool -> s\n        if depth == 0:\n            # Take single leapfrog step in the given direction (direction * stepsize)\n            position_bar, momentum_bar, candidate_set_size, accept_set_bool =\\\n                self._initalize_tree(position, momentum, slice_var, direction * stepsize)\n\n            return (position_bar, momentum_bar, position_bar, momentum_bar, position_bar,\n                    candidate_set_size, accept_set_bool)\n\n        else:\n            # Build left and right subtrees\n            (position_backward, momentum_backward, position_forward, momentum_forward, position_bar,\n             candidate_set_size, accept_set_bool) = self._build_tree(position, momentum,\n                                                                     slice_var, direction, depth - 1, stepsize)\n            if accept_set_bool == 1:\n                if direction == -1:\n                    # Build tree in backward direction\n                    (position_backward, momentum_backward, _, _, position_bar2, candidate_set_size2,\n                     accept_set_bool2) = self._build_tree(position_backward, momentum_backward,\n                                                          slice_var, direction, depth - 1, stepsize)\n                else:\n                    # Build tree in forward direction\n                    (_, _, position_forward, momentum_forward, position_bar2, candidate_set_size2,\n                     accept_set_bool2) = self._build_tree(position_forward, momentum_forward,\n                                                          slice_var, direction, depth - 1, stepsize)\n\n                if np.random.rand() < candidate_set_size2 / (candidate_set_size2 + candidate_set_size):\n                    position_bar = position_bar2\n\n                accept_set_bool, candidate_set_size =\\\n                    self._update_acceptance_criteria(position_forward, position_backward, momentum_forward,\n                                                     momentum_backward, accept_set_bool2, candidate_set_size,\n                                                     candidate_set_size2)\n\n            return (position_backward, momentum_backward, position_forward, momentum_forward,\n                    position_bar, candidate_set_size, accept_set_bool)", "code_tokens": ["def", "_build_tree", "(", "self", ",", "position", ",", "momentum", ",", "slice_var", ",", "direction", ",", "depth", ",", "stepsize", ")", ":", "# Parameter names in algorithm (here -> representation in algorithm)", "# position -> theta, momentum -> r, slice_var -> u, direction -> v, depth ->j, stepsize -> epsilon", "# candidate_set_size -> n, accept_set_bool -> s", "if", "depth", "==", "0", ":", "# Take single leapfrog step in the given direction (direction * stepsize)", "position_bar", ",", "momentum_bar", ",", "candidate_set_size", ",", "accept_set_bool", "=", "self", ".", "_initalize_tree", "(", "position", ",", "momentum", ",", "slice_var", ",", "direction", "*", "stepsize", ")", "return", "(", "position_bar", ",", "momentum_bar", ",", "position_bar", ",", "momentum_bar", ",", "position_bar", ",", "candidate_set_size", ",", "accept_set_bool", ")", "else", ":", "# Build left and right subtrees", "(", "position_backward", ",", "momentum_backward", ",", "position_forward", ",", "momentum_forward", ",", "position_bar", ",", "candidate_set_size", ",", "accept_set_bool", ")", "=", "self", ".", "_build_tree", "(", "position", ",", "momentum", ",", "slice_var", ",", "direction", ",", "depth", "-", "1", ",", "stepsize", ")", "if", "accept_set_bool", "==", "1", ":", "if", "direction", "==", "-", "1", ":", "# Build tree in backward direction", "(", "position_backward", ",", "momentum_backward", ",", "_", ",", "_", ",", "position_bar2", ",", "candidate_set_size2", ",", "accept_set_bool2", ")", "=", "self", ".", "_build_tree", "(", "position_backward", ",", "momentum_backward", ",", "slice_var", ",", "direction", ",", "depth", "-", "1", ",", "stepsize", ")", "else", ":", "# Build tree in forward direction", "(", "_", ",", "_", ",", "position_forward", ",", "momentum_forward", ",", "position_bar2", ",", "candidate_set_size2", ",", "accept_set_bool2", ")", "=", "self", ".", "_build_tree", "(", "position_forward", ",", "momentum_forward", ",", "slice_var", ",", "direction", ",", "depth", "-", "1", ",", "stepsize", ")", "if", "np", ".", "random", ".", "rand", "(", ")", "<", "candidate_set_size2", "/", "(", "candidate_set_size2", "+", "candidate_set_size", ")", ":", "position_bar", "=", "position_bar2", "accept_set_bool", ",", "candidate_set_size", "=", "self", ".", "_update_acceptance_criteria", "(", "position_forward", ",", "position_backward", ",", "momentum_forward", ",", "momentum_backward", ",", "accept_set_bool2", ",", "candidate_set_size", ",", "candidate_set_size2", ")", "return", "(", "position_backward", ",", "momentum_backward", ",", "position_forward", ",", "momentum_forward", ",", "position_bar", ",", "candidate_set_size", ",", "accept_set_bool", ")"], "docstring": "Recursively builds a tree for proposing new position and momentum", "docstring_tokens": ["Recursively", "builds", "a", "tree", "for", "proposing", "new", "position", "and", "momentum"], "sha": "9381a66aba3c3871d3ccd00672b148d17d63239e", "url": "https://github.com/pgmpy/pgmpy/blob/9381a66aba3c3871d3ccd00672b148d17d63239e/pgmpy/sampling/NUTS.py#L97-L139", "partition": "train"}
{"repo": "apache/spark", "path": "python/pyspark/streaming/context.py", "func_name": "StreamingContext.addStreamingListener", "original_string": "def addStreamingListener(self, streamingListener):\n        \"\"\"\n        Add a [[org.apache.spark.streaming.scheduler.StreamingListener]] object for\n        receiving system events related to streaming.\n        \"\"\"\n        self._jssc.addStreamingListener(self._jvm.JavaStreamingListenerWrapper(\n            self._jvm.PythonStreamingListenerWrapper(streamingListener)))", "language": "python", "code": "def addStreamingListener(self, streamingListener):\n        \"\"\"\n        Add a [[org.apache.spark.streaming.scheduler.StreamingListener]] object for\n        receiving system events related to streaming.\n        \"\"\"\n        self._jssc.addStreamingListener(self._jvm.JavaStreamingListenerWrapper(\n            self._jvm.PythonStreamingListenerWrapper(streamingListener)))", "code_tokens": ["def", "addStreamingListener", "(", "self", ",", "streamingListener", ")", ":", "self", ".", "_jssc", ".", "addStreamingListener", "(", "self", ".", "_jvm", ".", "JavaStreamingListenerWrapper", "(", "self", ".", "_jvm", ".", "PythonStreamingListenerWrapper", "(", "streamingListener", ")", ")", ")"], "docstring": "Add a [[org.apache.spark.streaming.scheduler.StreamingListener]] object for\n        receiving system events related to streaming.", "docstring_tokens": ["Add", "a", "[[", "org", ".", "apache", ".", "spark", ".", "streaming", ".", "scheduler", ".", "StreamingListener", "]]", "object", "for", "receiving", "system", "events", "related", "to", "streaming", "."], "sha": "618d6bff71073c8c93501ab7392c3cc579730f0b", "url": "https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/streaming/context.py#L350-L356", "partition": "train"}
{"repo": "huggingface/pytorch-pretrained-BERT", "path": "pytorch_pretrained_bert/modeling_gpt2.py", "func_name": "load_tf_weights_in_gpt2", "original_string": "def load_tf_weights_in_gpt2(model, gpt2_checkpoint_path):\n    \"\"\" Load tf checkpoints in a pytorch model\n    \"\"\"\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\")\n        raise\n    tf_path = os.path.abspath(gpt2_checkpoint_path)\n    print(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        print(\"Loading TF weight {} with shape {}\".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array.squeeze())\n\n    for name, array in zip(names, arrays):\n        name = name[6:]  # skip \"model/\"\n        name = name.split('/')\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r'[A-Za-z]+\\d+', m_name):\n                l = re.split(r'(\\d+)', m_name)\n            else:\n                l = [m_name]\n            if l[0] == 'w' or l[0] == 'g':\n                pointer = getattr(pointer, 'weight')\n            elif l[0] == 'b':\n                pointer = getattr(pointer, 'bias')\n            elif l[0] == 'wpe' or l[0] == 'wte':\n                pointer = getattr(pointer, l[0])\n                pointer = getattr(pointer, 'weight')\n            else:\n                pointer = getattr(pointer, l[0])\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(\"Initialize PyTorch weight {}\".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model", "language": "python", "code": "def load_tf_weights_in_gpt2(model, gpt2_checkpoint_path):\n    \"\"\" Load tf checkpoints in a pytorch model\n    \"\"\"\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\")\n        raise\n    tf_path = os.path.abspath(gpt2_checkpoint_path)\n    print(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        print(\"Loading TF weight {} with shape {}\".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array.squeeze())\n\n    for name, array in zip(names, arrays):\n        name = name[6:]  # skip \"model/\"\n        name = name.split('/')\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r'[A-Za-z]+\\d+', m_name):\n                l = re.split(r'(\\d+)', m_name)\n            else:\n                l = [m_name]\n            if l[0] == 'w' or l[0] == 'g':\n                pointer = getattr(pointer, 'weight')\n            elif l[0] == 'b':\n                pointer = getattr(pointer, 'bias')\n            elif l[0] == 'wpe' or l[0] == 'wte':\n                pointer = getattr(pointer, l[0])\n                pointer = getattr(pointer, 'weight')\n            else:\n                pointer = getattr(pointer, l[0])\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(\"Initialize PyTorch weight {}\".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model", "code_tokens": ["def", "load_tf_weights_in_gpt2", "(", "model", ",", "gpt2_checkpoint_path", ")", ":", "try", ":", "import", "re", "import", "numpy", "as", "np", "import", "tensorflow", "as", "tf", "except", "ImportError", ":", "print", "(", "\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "raise", "tf_path", "=", "os", ".", "path", ".", "abspath", "(", "gpt2_checkpoint_path", ")", "print", "(", "\"Converting TensorFlow checkpoint from {}\"", ".", "format", "(", "tf_path", ")", ")", "# Load weights from TF model", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "names", "=", "[", "]", "arrays", "=", "[", "]", "for", "name", ",", "shape", "in", "init_vars", ":", "print", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "names", ".", "append", "(", "name", ")", "arrays", ".", "append", "(", "array", ".", "squeeze", "(", ")", ")", "for", "name", ",", "array", "in", "zip", "(", "names", ",", "arrays", ")", ":", "name", "=", "name", "[", "6", ":", "]", "# skip \"model/\"", "name", "=", "name", ".", "split", "(", "'/'", ")", "pointer", "=", "model", "for", "m_name", "in", "name", ":", "if", "re", ".", "fullmatch", "(", "r'[A-Za-z]+\\d+'", ",", "m_name", ")", ":", "l", "=", "re", ".", "split", "(", "r'(\\d+)'", ",", "m_name", ")", "else", ":", "l", "=", "[", "m_name", "]", "if", "l", "[", "0", "]", "==", "'w'", "or", "l", "[", "0", "]", "==", "'g'", ":", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "elif", "l", "[", "0", "]", "==", "'b'", ":", "pointer", "=", "getattr", "(", "pointer", ",", "'bias'", ")", "elif", "l", "[", "0", "]", "==", "'wpe'", "or", "l", "[", "0", "]", "==", "'wte'", ":", "pointer", "=", "getattr", "(", "pointer", ",", "l", "[", "0", "]", ")", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "else", ":", "pointer", "=", "getattr", "(", "pointer", ",", "l", "[", "0", "]", ")", "if", "len", "(", "l", ")", ">=", "2", ":", "num", "=", "int", "(", "l", "[", "1", "]", ")", "pointer", "=", "pointer", "[", "num", "]", "try", ":", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "except", "AssertionError", "as", "e", ":", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "raise", "print", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "return", "model"], "docstring": "Load tf checkpoints in a pytorch model", "docstring_tokens": ["Load", "tf", "checkpoints", "in", "a", "pytorch", "model"], "sha": "b832d5bb8a6dfc5965015b828e577677eace601e", "url": "https://github.com/huggingface/pytorch-pretrained-BERT/blob/b832d5bb8a6dfc5965015b828e577677eace601e/pytorch_pretrained_bert/modeling_gpt2.py#L45-L96", "partition": "train"}
{"repo": "apple/turicreate", "path": "src/external/coremltools_wrap/coremltools/deps/protobuf/python/google/protobuf/internal/encoder.py", "func_name": "GroupSizer", "original_string": "def GroupSizer(field_number, is_repeated, is_packed):\n  \"\"\"Returns a sizer for a group field.\"\"\"\n\n  tag_size = _TagSize(field_number) * 2\n  assert not is_packed\n  if is_repeated:\n    def RepeatedFieldSize(value):\n      result = tag_size * len(value)\n      for element in value:\n        result += element.ByteSize()\n      return result\n    return RepeatedFieldSize\n  else:\n    def FieldSize(value):\n      return tag_size + value.ByteSize()\n    return FieldSize", "language": "python", "code": "def GroupSizer(field_number, is_repeated, is_packed):\n  \"\"\"Returns a sizer for a group field.\"\"\"\n\n  tag_size = _TagSize(field_number) * 2\n  assert not is_packed\n  if is_repeated:\n    def RepeatedFieldSize(value):\n      result = tag_size * len(value)\n      for element in value:\n        result += element.ByteSize()\n      return result\n    return RepeatedFieldSize\n  else:\n    def FieldSize(value):\n      return tag_size + value.ByteSize()\n    return FieldSize", "code_tokens": ["def", "GroupSizer", "(", "field_number", ",", "is_repeated", ",", "is_packed", ")", ":", "tag_size", "=", "_TagSize", "(", "field_number", ")", "*", "2", "assert", "not", "is_packed", "if", "is_repeated", ":", "def", "RepeatedFieldSize", "(", "value", ")", ":", "result", "=", "tag_size", "*", "len", "(", "value", ")", "for", "element", "in", "value", ":", "result", "+=", "element", ".", "ByteSize", "(", ")", "return", "result", "return", "RepeatedFieldSize", "else", ":", "def", "FieldSize", "(", "value", ")", ":", "return", "tag_size", "+", "value", ".", "ByteSize", "(", ")", "return", "FieldSize"], "docstring": "Returns a sizer for a group field.", "docstring_tokens": ["Returns", "a", "sizer", "for", "a", "group", "field", "."], "sha": "74514c3f99e25b46f22c6e02977fe3da69221c2e", "url": "https://github.com/apple/turicreate/blob/74514c3f99e25b46f22c6e02977fe3da69221c2e/src/external/coremltools_wrap/coremltools/deps/protobuf/python/google/protobuf/internal/encoder.py#L274-L289", "partition": "train"}
{"repo": "apple/turicreate", "path": "deps/src/boost_1_68_0/tools/build/src/build/property.py", "func_name": "__validate1", "original_string": "def __validate1 (property):\n    \"\"\" Exit with error if property is not valid.\n    \"\"\"\n    assert isinstance(property, Property)\n    msg = None\n\n    if not property.feature.free:\n        feature.validate_value_string (property.feature, property.value)", "language": "python", "code": "def __validate1 (property):\n    \"\"\" Exit with error if property is not valid.\n    \"\"\"\n    assert isinstance(property, Property)\n    msg = None\n\n    if not property.feature.free:\n        feature.validate_value_string (property.feature, property.value)", "code_tokens": ["def", "__validate1", "(", "property", ")", ":", "assert", "isinstance", "(", "property", ",", "Property", ")", "msg", "=", "None", "if", "not", "property", ".", "feature", ".", "free", ":", "feature", ".", "validate_value_string", "(", "property", ".", "feature", ",", "property", ".", "value", ")"], "docstring": "Exit with error if property is not valid.", "docstring_tokens": ["Exit", "with", "error", "if", "property", "is", "not", "valid", "."], "sha": "74514c3f99e25b46f22c6e02977fe3da69221c2e", "url": "https://github.com/apple/turicreate/blob/74514c3f99e25b46f22c6e02977fe3da69221c2e/deps/src/boost_1_68_0/tools/build/src/build/property.py#L483-L490", "partition": "train"}
{"repo": "binux/pyspider", "path": "pyspider/scheduler/scheduler.py", "func_name": "Scheduler.xmlrpc_run", "original_string": "def xmlrpc_run(self, port=23333, bind='127.0.0.1', logRequests=False):\n        '''Start xmlrpc interface'''\n        from pyspider.libs.wsgi_xmlrpc import WSGIXMLRPCApplication\n\n        application = WSGIXMLRPCApplication()\n\n        application.register_function(self.quit, '_quit')\n        application.register_function(self.__len__, 'size')\n\n        def dump_counter(_time, _type):\n            try:\n                return self._cnt[_time].to_dict(_type)\n            except:\n                logger.exception('')\n        application.register_function(dump_counter, 'counter')\n\n        def new_task(task):\n            if self.task_verify(task):\n                self.newtask_queue.put(task)\n                return True\n            return False\n        application.register_function(new_task, 'newtask')\n\n        def send_task(task):\n            '''dispatch task to fetcher'''\n            self.send_task(task)\n            return True\n        application.register_function(send_task, 'send_task')\n\n        def update_project():\n            self._force_update_project = True\n        application.register_function(update_project, 'update_project')\n\n        def get_active_tasks(project=None, limit=100):\n            allowed_keys = set((\n                'type',\n                'taskid',\n                'project',\n                'status',\n                'url',\n                'lastcrawltime',\n                'updatetime',\n                'track',\n            ))\n            track_allowed_keys = set((\n                'ok',\n                'time',\n                'follows',\n                'status_code',\n            ))\n\n            iters = [iter(x.active_tasks) for k, x in iteritems(self.projects)\n                     if x and (k == project if project else True)]\n            tasks = [next(x, None) for x in iters]\n            result = []\n\n            while len(result) < limit and tasks and not all(x is None for x in tasks):\n                updatetime, task = t = max(t for t in tasks if t)\n                i = tasks.index(t)\n                tasks[i] = next(iters[i], None)\n                for key in list(task):\n                    if key == 'track':\n                        for k in list(task[key].get('fetch', [])):\n                            if k not in track_allowed_keys:\n                                del task[key]['fetch'][k]\n                        for k in list(task[key].get('process', [])):\n                            if k not in track_allowed_keys:\n                                del task[key]['process'][k]\n                    if key in allowed_keys:\n                        continue\n                    del task[key]\n                result.append(t)\n            # fix for \"<type 'exceptions.TypeError'>:dictionary key must be string\"\n            # have no idea why\n            return json.loads(json.dumps(result))\n        application.register_function(get_active_tasks, 'get_active_tasks')\n\n        def get_projects_pause_status():\n            result = {}\n            for project_name, project in iteritems(self.projects):\n                result[project_name] = project.paused\n            return result\n        application.register_function(get_projects_pause_status, 'get_projects_pause_status')\n\n        def webui_update():\n            return {\n                'pause_status': get_projects_pause_status(),\n                'counter': {\n                    '5m_time': dump_counter('5m_time', 'avg'),\n                    '5m': dump_counter('5m', 'sum'),\n                    '1h': dump_counter('1h', 'sum'),\n                    '1d': dump_counter('1d', 'sum'),\n                    'all': dump_counter('all', 'sum'),\n                },\n            }\n        application.register_function(webui_update, 'webui_update')\n\n        import tornado.wsgi\n        import tornado.ioloop\n        import tornado.httpserver\n\n        container = tornado.wsgi.WSGIContainer(application)\n        self.xmlrpc_ioloop = tornado.ioloop.IOLoop()\n        self.xmlrpc_server = tornado.httpserver.HTTPServer(container, io_loop=self.xmlrpc_ioloop)\n        self.xmlrpc_server.listen(port=port, address=bind)\n        logger.info('scheduler.xmlrpc listening on %s:%s', bind, port)\n        self.xmlrpc_ioloop.start()", "language": "python", "code": "def xmlrpc_run(self, port=23333, bind='127.0.0.1', logRequests=False):\n        '''Start xmlrpc interface'''\n        from pyspider.libs.wsgi_xmlrpc import WSGIXMLRPCApplication\n\n        application = WSGIXMLRPCApplication()\n\n        application.register_function(self.quit, '_quit')\n        application.register_function(self.__len__, 'size')\n\n        def dump_counter(_time, _type):\n            try:\n                return self._cnt[_time].to_dict(_type)\n            except:\n                logger.exception('')\n        application.register_function(dump_counter, 'counter')\n\n        def new_task(task):\n            if self.task_verify(task):\n                self.newtask_queue.put(task)\n                return True\n            return False\n        application.register_function(new_task, 'newtask')\n\n        def send_task(task):\n            '''dispatch task to fetcher'''\n            self.send_task(task)\n            return True\n        application.register_function(send_task, 'send_task')\n\n        def update_project():\n            self._force_update_project = True\n        application.register_function(update_project, 'update_project')\n\n        def get_active_tasks(project=None, limit=100):\n            allowed_keys = set((\n                'type',\n                'taskid',\n                'project',\n                'status',\n                'url',\n                'lastcrawltime',\n                'updatetime',\n                'track',\n            ))\n            track_allowed_keys = set((\n                'ok',\n                'time',\n                'follows',\n                'status_code',\n            ))\n\n            iters = [iter(x.active_tasks) for k, x in iteritems(self.projects)\n                     if x and (k == project if project else True)]\n            tasks = [next(x, None) for x in iters]\n            result = []\n\n            while len(result) < limit and tasks and not all(x is None for x in tasks):\n                updatetime, task = t = max(t for t in tasks if t)\n                i = tasks.index(t)\n                tasks[i] = next(iters[i], None)\n                for key in list(task):\n                    if key == 'track':\n                        for k in list(task[key].get('fetch', [])):\n                            if k not in track_allowed_keys:\n                                del task[key]['fetch'][k]\n                        for k in list(task[key].get('process', [])):\n                            if k not in track_allowed_keys:\n                                del task[key]['process'][k]\n                    if key in allowed_keys:\n                        continue\n                    del task[key]\n                result.append(t)\n            # fix for \"<type 'exceptions.TypeError'>:dictionary key must be string\"\n            # have no idea why\n            return json.loads(json.dumps(result))\n        application.register_function(get_active_tasks, 'get_active_tasks')\n\n        def get_projects_pause_status():\n            result = {}\n            for project_name, project in iteritems(self.projects):\n                result[project_name] = project.paused\n            return result\n        application.register_function(get_projects_pause_status, 'get_projects_pause_status')\n\n        def webui_update():\n            return {\n                'pause_status': get_projects_pause_status(),\n                'counter': {\n                    '5m_time': dump_counter('5m_time', 'avg'),\n                    '5m': dump_counter('5m', 'sum'),\n                    '1h': dump_counter('1h', 'sum'),\n                    '1d': dump_counter('1d', 'sum'),\n                    'all': dump_counter('all', 'sum'),\n                },\n            }\n        application.register_function(webui_update, 'webui_update')\n\n        import tornado.wsgi\n        import tornado.ioloop\n        import tornado.httpserver\n\n        container = tornado.wsgi.WSGIContainer(application)\n        self.xmlrpc_ioloop = tornado.ioloop.IOLoop()\n        self.xmlrpc_server = tornado.httpserver.HTTPServer(container, io_loop=self.xmlrpc_ioloop)\n        self.xmlrpc_server.listen(port=port, address=bind)\n        logger.info('scheduler.xmlrpc listening on %s:%s', bind, port)\n        self.xmlrpc_ioloop.start()", "code_tokens": ["def", "xmlrpc_run", "(", "self", ",", "port", "=", "23333", ",", "bind", "=", "'127.0.0.1'", ",", "logRequests", "=", "False", ")", ":", "from", "pyspider", ".", "libs", ".", "wsgi_xmlrpc", "import", "WSGIXMLRPCApplication", "application", "=", "WSGIXMLRPCApplication", "(", ")", "application", ".", "register_function", "(", "self", ".", "quit", ",", "'_quit'", ")", "application", ".", "register_function", "(", "self", ".", "__len__", ",", "'size'", ")", "def", "dump_counter", "(", "_time", ",", "_type", ")", ":", "try", ":", "return", "self", ".", "_cnt", "[", "_time", "]", ".", "to_dict", "(", "_type", ")", "except", ":", "logger", ".", "exception", "(", "''", ")", "application", ".", "register_function", "(", "dump_counter", ",", "'counter'", ")", "def", "new_task", "(", "task", ")", ":", "if", "self", ".", "task_verify", "(", "task", ")", ":", "self", ".", "newtask_queue", ".", "put", "(", "task", ")", "return", "True", "return", "False", "application", ".", "register_function", "(", "new_task", ",", "'newtask'", ")", "def", "send_task", "(", "task", ")", ":", "'''dispatch task to fetcher'''", "self", ".", "send_task", "(", "task", ")", "return", "True", "application", ".", "register_function", "(", "send_task", ",", "'send_task'", ")", "def", "update_project", "(", ")", ":", "self", ".", "_force_update_project", "=", "True", "application", ".", "register_function", "(", "update_project", ",", "'update_project'", ")", "def", "get_active_tasks", "(", "project", "=", "None", ",", "limit", "=", "100", ")", ":", "allowed_keys", "=", "set", "(", "(", "'type'", ",", "'taskid'", ",", "'project'", ",", "'status'", ",", "'url'", ",", "'lastcrawltime'", ",", "'updatetime'", ",", "'track'", ",", ")", ")", "track_allowed_keys", "=", "set", "(", "(", "'ok'", ",", "'time'", ",", "'follows'", ",", "'status_code'", ",", ")", ")", "iters", "=", "[", "iter", "(", "x", ".", "active_tasks", ")", "for", "k", ",", "x", "in", "iteritems", "(", "self", ".", "projects", ")", "if", "x", "and", "(", "k", "==", "project", "if", "project", "else", "True", ")", "]", "tasks", "=", "[", "next", "(", "x", ",", "None", ")", "for", "x", "in", "iters", "]", "result", "=", "[", "]", "while", "len", "(", "result", ")", "<", "limit", "and", "tasks", "and", "not", "all", "(", "x", "is", "None", "for", "x", "in", "tasks", ")", ":", "updatetime", ",", "task", "=", "t", "=", "max", "(", "t", "for", "t", "in", "tasks", "if", "t", ")", "i", "=", "tasks", ".", "index", "(", "t", ")", "tasks", "[", "i", "]", "=", "next", "(", "iters", "[", "i", "]", ",", "None", ")", "for", "key", "in", "list", "(", "task", ")", ":", "if", "key", "==", "'track'", ":", "for", "k", "in", "list", "(", "task", "[", "key", "]", ".", "get", "(", "'fetch'", ",", "[", "]", ")", ")", ":", "if", "k", "not", "in", "track_allowed_keys", ":", "del", "task", "[", "key", "]", "[", "'fetch'", "]", "[", "k", "]", "for", "k", "in", "list", "(", "task", "[", "key", "]", ".", "get", "(", "'process'", ",", "[", "]", ")", ")", ":", "if", "k", "not", "in", "track_allowed_keys", ":", "del", "task", "[", "key", "]", "[", "'process'", "]", "[", "k", "]", "if", "key", "in", "allowed_keys", ":", "continue", "del", "task", "[", "key", "]", "result", ".", "append", "(", "t", ")", "# fix for \"<type 'exceptions.TypeError'>:dictionary key must be string\"", "# have no idea why", "return", "json", ".", "loads", "(", "json", ".", "dumps", "(", "result", ")", ")", "application", ".", "register_function", "(", "get_active_tasks", ",", "'get_active_tasks'", ")", "def", "get_projects_pause_status", "(", ")", ":", "result", "=", "{", "}", "for", "project_name", ",", "project", "in", "iteritems", "(", "self", ".", "projects", ")", ":", "result", "[", "project_name", "]", "=", "project", ".", "paused", "return", "result", "application", ".", "register_function", "(", "get_projects_pause_status", ",", "'get_projects_pause_status'", ")", "def", "webui_update", "(", ")", ":", "return", "{", "'pause_status'", ":", "get_projects_pause_status", "(", ")", ",", "'counter'", ":", "{", "'5m_time'", ":", "dump_counter", "(", "'5m_time'", ",", "'avg'", ")", ",", "'5m'", ":", "dump_counter", "(", "'5m'", ",", "'sum'", ")", ",", "'1h'", ":", "dump_counter", "(", "'1h'", ",", "'sum'", ")", ",", "'1d'", ":", "dump_counter", "(", "'1d'", ",", "'sum'", ")", ",", "'all'", ":", "dump_counter", "(", "'all'", ",", "'sum'", ")", ",", "}", ",", "}", "application", ".", "register_function", "(", "webui_update", ",", "'webui_update'", ")", "import", "tornado", ".", "wsgi", "import", "tornado", ".", "ioloop", "import", "tornado", ".", "httpserver", "container", "=", "tornado", ".", "wsgi", ".", "WSGIContainer", "(", "application", ")", "self", ".", "xmlrpc_ioloop", "=", "tornado", ".", "ioloop", ".", "IOLoop", "(", ")", "self", ".", "xmlrpc_server", "=", "tornado", ".", "httpserver", ".", "HTTPServer", "(", "container", ",", "io_loop", "=", "self", ".", "xmlrpc_ioloop", ")", "self", ".", "xmlrpc_server", ".", "listen", "(", "port", "=", "port", ",", "address", "=", "bind", ")", "logger", ".", "info", "(", "'scheduler.xmlrpc listening on %s:%s'", ",", "bind", ",", "port", ")", "self", ".", "xmlrpc_ioloop", ".", "start", "(", ")"], "docstring": "Start xmlrpc interface", "docstring_tokens": ["Start", "xmlrpc", "interface"], "sha": "3fccfabe2b057b7a56d4a4c79dc0dd6cd2239fe9", "url": "https://github.com/binux/pyspider/blob/3fccfabe2b057b7a56d4a4c79dc0dd6cd2239fe9/pyspider/scheduler/scheduler.py#L705-L811", "partition": "train"}
{"repo": "lanpa/tensorboardX", "path": "examples/demo_caffe2.py", "func_name": "AddTrainingOperators", "original_string": "def AddTrainingOperators(model, softmax, label):\n    \"\"\"Adds training operators to the model.\"\"\"\n    xent = model.LabelCrossEntropy([softmax, label], 'xent')\n    # compute the expected loss\n    loss = model.AveragedLoss(xent, \"loss\")\n    # track the accuracy of the model\n    AddAccuracy(model, softmax, label)\n    # use the average loss we just computed to add gradient operators to the\n    # model\n    model.AddGradientOperators([loss])\n    # do a simple stochastic gradient descent\n    ITER = brew.iter(model, \"iter\")\n    # set the learning rate schedule\n    LR = model.LearningRate(\n        ITER, \"LR\", base_lr=-0.1, policy=\"step\", stepsize=1, gamma=0.999)\n    # ONE is a constant value that is used in the gradient update. We only need\n    # to create it once, so it is explicitly placed in param_init_net.\n    ONE = model.param_init_net.ConstantFill([], \"ONE\", shape=[1], value=1.0)\n    # Now, for each parameter, we do the gradient updates.\n    for param in model.params:\n        # Note how we get the gradient of each parameter - ModelHelper keeps\n        # track of that.\n        param_grad = model.param_to_grad[param]\n        # The update is a simple weighted sum: param = param + param_grad * LR\n        model.WeightedSum([param, ONE, param_grad, LR], param)", "language": "python", "code": "def AddTrainingOperators(model, softmax, label):\n    \"\"\"Adds training operators to the model.\"\"\"\n    xent = model.LabelCrossEntropy([softmax, label], 'xent')\n    # compute the expected loss\n    loss = model.AveragedLoss(xent, \"loss\")\n    # track the accuracy of the model\n    AddAccuracy(model, softmax, label)\n    # use the average loss we just computed to add gradient operators to the\n    # model\n    model.AddGradientOperators([loss])\n    # do a simple stochastic gradient descent\n    ITER = brew.iter(model, \"iter\")\n    # set the learning rate schedule\n    LR = model.LearningRate(\n        ITER, \"LR\", base_lr=-0.1, policy=\"step\", stepsize=1, gamma=0.999)\n    # ONE is a constant value that is used in the gradient update. We only need\n    # to create it once, so it is explicitly placed in param_init_net.\n    ONE = model.param_init_net.ConstantFill([], \"ONE\", shape=[1], value=1.0)\n    # Now, for each parameter, we do the gradient updates.\n    for param in model.params:\n        # Note how we get the gradient of each parameter - ModelHelper keeps\n        # track of that.\n        param_grad = model.param_to_grad[param]\n        # The update is a simple weighted sum: param = param + param_grad * LR\n        model.WeightedSum([param, ONE, param_grad, LR], param)", "code_tokens": ["def", "AddTrainingOperators", "(", "model", ",", "softmax", ",", "label", ")", ":", "xent", "=", "model", ".", "LabelCrossEntropy", "(", "[", "softmax", ",", "label", "]", ",", "'xent'", ")", "# compute the expected loss", "loss", "=", "model", ".", "AveragedLoss", "(", "xent", ",", "\"loss\"", ")", "# track the accuracy of the model", "AddAccuracy", "(", "model", ",", "softmax", ",", "label", ")", "# use the average loss we just computed to add gradient operators to the", "# model", "model", ".", "AddGradientOperators", "(", "[", "loss", "]", ")", "# do a simple stochastic gradient descent", "ITER", "=", "brew", ".", "iter", "(", "model", ",", "\"iter\"", ")", "# set the learning rate schedule", "LR", "=", "model", ".", "LearningRate", "(", "ITER", ",", "\"LR\"", ",", "base_lr", "=", "-", "0.1", ",", "policy", "=", "\"step\"", ",", "stepsize", "=", "1", ",", "gamma", "=", "0.999", ")", "# ONE is a constant value that is used in the gradient update. We only need", "# to create it once, so it is explicitly placed in param_init_net.", "ONE", "=", "model", ".", "param_init_net", ".", "ConstantFill", "(", "[", "]", ",", "\"ONE\"", ",", "shape", "=", "[", "1", "]", ",", "value", "=", "1.0", ")", "# Now, for each parameter, we do the gradient updates.", "for", "param", "in", "model", ".", "params", ":", "# Note how we get the gradient of each parameter - ModelHelper keeps", "# track of that.", "param_grad", "=", "model", ".", "param_to_grad", "[", "param", "]", "# The update is a simple weighted sum: param = param + param_grad * LR", "model", ".", "WeightedSum", "(", "[", "param", ",", "ONE", ",", "param_grad", ",", "LR", "]", ",", "param", ")"], "docstring": "Adds training operators to the model.", "docstring_tokens": ["Adds", "training", "operators", "to", "the", "model", "."], "sha": "0bf6c07d97b0745654fd9fab8ee3261ec707f253", "url": "https://github.com/lanpa/tensorboardX/blob/0bf6c07d97b0745654fd9fab8ee3261ec707f253/examples/demo_caffe2.py#L136-L160", "partition": "train"}
{"repo": "keras-rl/keras-rl", "path": "rl/policy.py", "func_name": "LinearAnnealedPolicy.get_config", "original_string": "def get_config(self):\n        \"\"\"Return configurations of LinearAnnealedPolicy\n\n        # Returns\n            Dict of config\n        \"\"\"\n        config = super(LinearAnnealedPolicy, self).get_config()\n        config['attr'] = self.attr\n        config['value_max'] = self.value_max\n        config['value_min'] = self.value_min\n        config['value_test'] = self.value_test\n        config['nb_steps'] = self.nb_steps\n        config['inner_policy'] = get_object_config(self.inner_policy)\n        return config", "language": "python", "code": "def get_config(self):\n        \"\"\"Return configurations of LinearAnnealedPolicy\n\n        # Returns\n            Dict of config\n        \"\"\"\n        config = super(LinearAnnealedPolicy, self).get_config()\n        config['attr'] = self.attr\n        config['value_max'] = self.value_max\n        config['value_min'] = self.value_min\n        config['value_test'] = self.value_test\n        config['nb_steps'] = self.nb_steps\n        config['inner_policy'] = get_object_config(self.inner_policy)\n        return config", "code_tokens": ["def", "get_config", "(", "self", ")", ":", "config", "=", "super", "(", "LinearAnnealedPolicy", ",", "self", ")", ".", "get_config", "(", ")", "config", "[", "'attr'", "]", "=", "self", ".", "attr", "config", "[", "'value_max'", "]", "=", "self", ".", "value_max", "config", "[", "'value_min'", "]", "=", "self", ".", "value_min", "config", "[", "'value_test'", "]", "=", "self", ".", "value_test", "config", "[", "'nb_steps'", "]", "=", "self", ".", "nb_steps", "config", "[", "'inner_policy'", "]", "=", "get_object_config", "(", "self", ".", "inner_policy", ")", "return", "config"], "docstring": "Return configurations of LinearAnnealedPolicy\n\n        # Returns\n            Dict of config", "docstring_tokens": ["Return", "configurations", "of", "LinearAnnealedPolicy"], "sha": "e6efb0d8297ec38d704a3110b5d6ed74d09a05e3", "url": "https://github.com/keras-rl/keras-rl/blob/e6efb0d8297ec38d704a3110b5d6ed74d09a05e3/rl/policy.py#L105-L118", "partition": "train"}
{"repo": "awslabs/aws-sam-cli", "path": "samcli/lib/logs/formatter.py", "func_name": "LambdaLogMsgFormatters.colorize_errors", "original_string": "def colorize_errors(event, colored):\n        \"\"\"\n        Highlights some commonly known Lambda error cases in red:\n            - Nodejs process crashes\n            - Lambda function timeouts\n        \"\"\"\n\n        nodejs_crash_msg = \"Process exited before completing request\"\n        timeout_msg = \"Task timed out\"\n\n        if nodejs_crash_msg in event.message \\\n                or timeout_msg in event.message:\n            event.message = colored.red(event.message)\n\n        return event", "language": "python", "code": "def colorize_errors(event, colored):\n        \"\"\"\n        Highlights some commonly known Lambda error cases in red:\n            - Nodejs process crashes\n            - Lambda function timeouts\n        \"\"\"\n\n        nodejs_crash_msg = \"Process exited before completing request\"\n        timeout_msg = \"Task timed out\"\n\n        if nodejs_crash_msg in event.message \\\n                or timeout_msg in event.message:\n            event.message = colored.red(event.message)\n\n        return event", "code_tokens": ["def", "colorize_errors", "(", "event", ",", "colored", ")", ":", "nodejs_crash_msg", "=", "\"Process exited before completing request\"", "timeout_msg", "=", "\"Task timed out\"", "if", "nodejs_crash_msg", "in", "event", ".", "message", "or", "timeout_msg", "in", "event", ".", "message", ":", "event", ".", "message", "=", "colored", ".", "red", "(", "event", ".", "message", ")", "return", "event"], "docstring": "Highlights some commonly known Lambda error cases in red:\n            - Nodejs process crashes\n            - Lambda function timeouts", "docstring_tokens": ["Highlights", "some", "commonly", "known", "Lambda", "error", "cases", "in", "red", ":", "-", "Nodejs", "process", "crashes", "-", "Lambda", "function", "timeouts"], "sha": "c05af5e7378c6f05f7d82ad3f0bca17204177db6", "url": "https://github.com/awslabs/aws-sam-cli/blob/c05af5e7378c6f05f7d82ad3f0bca17204177db6/samcli/lib/logs/formatter.py#L132-L146", "partition": "train"}
{"repo": "awslabs/aws-sam-cli", "path": "samcli/lib/logs/formatter.py", "func_name": "KeywordHighlighter.highlight_keywords", "original_string": "def highlight_keywords(self, event, colored):\n        \"\"\"\n        Highlight the keyword in the log statement by drawing an underline\n        \"\"\"\n        if self.keyword:\n            highlight = colored.underline(self.keyword)\n            event.message = event.message.replace(self.keyword, highlight)\n\n        return event", "language": "python", "code": "def highlight_keywords(self, event, colored):\n        \"\"\"\n        Highlight the keyword in the log statement by drawing an underline\n        \"\"\"\n        if self.keyword:\n            highlight = colored.underline(self.keyword)\n            event.message = event.message.replace(self.keyword, highlight)\n\n        return event", "code_tokens": ["def", "highlight_keywords", "(", "self", ",", "event", ",", "colored", ")", ":", "if", "self", ".", "keyword", ":", "highlight", "=", "colored", ".", "underline", "(", "self", ".", "keyword", ")", "event", ".", "message", "=", "event", ".", "message", ".", "replace", "(", "self", ".", "keyword", ",", "highlight", ")", "return", "event"], "docstring": "Highlight the keyword in the log statement by drawing an underline", "docstring_tokens": ["Highlight", "the", "keyword", "in", "the", "log", "statement", "by", "drawing", "an", "underline"], "sha": "c05af5e7378c6f05f7d82ad3f0bca17204177db6", "url": "https://github.com/awslabs/aws-sam-cli/blob/c05af5e7378c6f05f7d82ad3f0bca17204177db6/samcli/lib/logs/formatter.py#L157-L165", "partition": "train"}
{"repo": "iterative/dvc", "path": "dvc/lock.py", "func_name": "Lock.lock", "original_string": "def lock(self):\n        \"\"\"Acquire lock for dvc repo.\"\"\"\n        try:\n            self._do_lock()\n            return\n        except LockError:\n            time.sleep(self.TIMEOUT)\n\n        self._do_lock()", "language": "python", "code": "def lock(self):\n        \"\"\"Acquire lock for dvc repo.\"\"\"\n        try:\n            self._do_lock()\n            return\n        except LockError:\n            time.sleep(self.TIMEOUT)\n\n        self._do_lock()", "code_tokens": ["def", "lock", "(", "self", ")", ":", "try", ":", "self", ".", "_do_lock", "(", ")", "return", "except", "LockError", ":", "time", ".", "sleep", "(", "self", ".", "TIMEOUT", ")", "self", ".", "_do_lock", "(", ")"], "docstring": "Acquire lock for dvc repo.", "docstring_tokens": ["Acquire", "lock", "for", "dvc", "repo", "."], "sha": "8bb21261e34c9632453e09090de7ebe50e38d341", "url": "https://github.com/iterative/dvc/blob/8bb21261e34c9632453e09090de7ebe50e38d341/dvc/lock.py#L41-L49", "partition": "train"}
{"repo": "PySimpleGUI/PySimpleGUI", "path": "DemoPrograms/Demo_Desktop_Widget_Email_Notification.py", "func_name": "read_mail", "original_string": "def read_mail(window):\n    \"\"\"\n    Reads late emails from IMAP server and displays them in the Window\n    :param window: window to display emails in\n    :return:\n    \"\"\"\n    mail = imaplib.IMAP4_SSL(IMAP_SERVER)\n\n    (retcode, capabilities) = mail.login(LOGIN_EMAIL, LOGIN_PASSWORD)\n    mail.list()\n    typ, data = mail.select('Inbox')\n    n = 0\n    now = datetime.now()\n    # get messages from today\n    search_string = '(SENTON {}-{}-{})'.format(now.day, calendar.month_abbr[now.month], now.year)\n    (retcode, messages) = mail.search(None, search_string)\n    if retcode == 'OK':\n        msg_list = messages[0].split()  # message numbers are separated by spaces, turn into list\n        msg_list.sort(reverse=True)  # sort messages descending\n        for n, message in enumerate(msg_list):\n            if n >= MAX_EMAILS:\n                break\n            from_elem = window.FindElement('{}from'.format(n))\n            date_elem = window.FindElement('{}date'.format(n))\n            from_elem.Update('')  # erase them so you know they're changing\n            date_elem.Update('')\n            window.Refresh()\n            typ, data = mail.fetch(message, '(RFC822)')\n            for response_part in data:\n                if isinstance(response_part, tuple):\n                    original = email.message_from_bytes(response_part[1])\n                    date_str = original['Date'][:22]\n                    from_elem.Update(original['From'])\n                    date_elem.Update(date_str)\n                    window.Refresh()", "language": "python", "code": "def read_mail(window):\n    \"\"\"\n    Reads late emails from IMAP server and displays them in the Window\n    :param window: window to display emails in\n    :return:\n    \"\"\"\n    mail = imaplib.IMAP4_SSL(IMAP_SERVER)\n\n    (retcode, capabilities) = mail.login(LOGIN_EMAIL, LOGIN_PASSWORD)\n    mail.list()\n    typ, data = mail.select('Inbox')\n    n = 0\n    now = datetime.now()\n    # get messages from today\n    search_string = '(SENTON {}-{}-{})'.format(now.day, calendar.month_abbr[now.month], now.year)\n    (retcode, messages) = mail.search(None, search_string)\n    if retcode == 'OK':\n        msg_list = messages[0].split()  # message numbers are separated by spaces, turn into list\n        msg_list.sort(reverse=True)  # sort messages descending\n        for n, message in enumerate(msg_list):\n            if n >= MAX_EMAILS:\n                break\n            from_elem = window.FindElement('{}from'.format(n))\n            date_elem = window.FindElement('{}date'.format(n))\n            from_elem.Update('')  # erase them so you know they're changing\n            date_elem.Update('')\n            window.Refresh()\n            typ, data = mail.fetch(message, '(RFC822)')\n            for response_part in data:\n                if isinstance(response_part, tuple):\n                    original = email.message_from_bytes(response_part[1])\n                    date_str = original['Date'][:22]\n                    from_elem.Update(original['From'])\n                    date_elem.Update(date_str)\n                    window.Refresh()", "code_tokens": ["def", "read_mail", "(", "window", ")", ":", "mail", "=", "imaplib", ".", "IMAP4_SSL", "(", "IMAP_SERVER", ")", "(", "retcode", ",", "capabilities", ")", "=", "mail", ".", "login", "(", "LOGIN_EMAIL", ",", "LOGIN_PASSWORD", ")", "mail", ".", "list", "(", ")", "typ", ",", "data", "=", "mail", ".", "select", "(", "'Inbox'", ")", "n", "=", "0", "now", "=", "datetime", ".", "now", "(", ")", "# get messages from today", "search_string", "=", "'(SENTON {}-{}-{})'", ".", "format", "(", "now", ".", "day", ",", "calendar", ".", "month_abbr", "[", "now", ".", "month", "]", ",", "now", ".", "year", ")", "(", "retcode", ",", "messages", ")", "=", "mail", ".", "search", "(", "None", ",", "search_string", ")", "if", "retcode", "==", "'OK'", ":", "msg_list", "=", "messages", "[", "0", "]", ".", "split", "(", ")", "# message numbers are separated by spaces, turn into list", "msg_list", ".", "sort", "(", "reverse", "=", "True", ")", "# sort messages descending", "for", "n", ",", "message", "in", "enumerate", "(", "msg_list", ")", ":", "if", "n", ">=", "MAX_EMAILS", ":", "break", "from_elem", "=", "window", ".", "FindElement", "(", "'{}from'", ".", "format", "(", "n", ")", ")", "date_elem", "=", "window", ".", "FindElement", "(", "'{}date'", ".", "format", "(", "n", ")", ")", "from_elem", ".", "Update", "(", "''", ")", "# erase them so you know they're changing", "date_elem", ".", "Update", "(", "''", ")", "window", ".", "Refresh", "(", ")", "typ", ",", "data", "=", "mail", ".", "fetch", "(", "message", ",", "'(RFC822)'", ")", "for", "response_part", "in", "data", ":", "if", "isinstance", "(", "response_part", ",", "tuple", ")", ":", "original", "=", "email", ".", "message_from_bytes", "(", "response_part", "[", "1", "]", ")", "date_str", "=", "original", "[", "'Date'", "]", "[", ":", "22", "]", "from_elem", ".", "Update", "(", "original", "[", "'From'", "]", ")", "date_elem", ".", "Update", "(", "date_str", ")", "window", ".", "Refresh", "(", ")"], "docstring": "Reads late emails from IMAP server and displays them in the Window\n    :param window: window to display emails in\n    :return:", "docstring_tokens": ["Reads", "late", "emails", "from", "IMAP", "server", "and", "displays", "them", "in", "the", "Window", ":", "param", "window", ":", "window", "to", "display", "emails", "in", ":", "return", ":"], "sha": "08184197f5bd4580ab5e5aca28bdda30f87b86fc", "url": "https://github.com/PySimpleGUI/PySimpleGUI/blob/08184197f5bd4580ab5e5aca28bdda30f87b86fc/DemoPrograms/Demo_Desktop_Widget_Email_Notification.py#L67-L101", "partition": "train"}
{"repo": "PySimpleGUI/PySimpleGUI", "path": "PySimpleGUIQt/PySimpleGUIQt.py", "func_name": "convert_tkinter_size_to_Qt", "original_string": "def convert_tkinter_size_to_Qt(size):\n    \"\"\"\n    Converts size in characters to size in pixels\n    :param size:  size in characters, rows\n    :return: size in pixels, pixels\n    \"\"\"\n    qtsize = size\n    if size[1] is not None and size[1] < DEFAULT_PIXEL_TO_CHARS_CUTOFF:        # change from character based size to pixels (roughly)\n        qtsize = size[0]*DEFAULT_PIXELS_TO_CHARS_SCALING[0], size[1]*DEFAULT_PIXELS_TO_CHARS_SCALING[1]\n    return qtsize", "language": "python", "code": "def convert_tkinter_size_to_Qt(size):\n    \"\"\"\n    Converts size in characters to size in pixels\n    :param size:  size in characters, rows\n    :return: size in pixels, pixels\n    \"\"\"\n    qtsize = size\n    if size[1] is not None and size[1] < DEFAULT_PIXEL_TO_CHARS_CUTOFF:        # change from character based size to pixels (roughly)\n        qtsize = size[0]*DEFAULT_PIXELS_TO_CHARS_SCALING[0], size[1]*DEFAULT_PIXELS_TO_CHARS_SCALING[1]\n    return qtsize", "code_tokens": ["def", "convert_tkinter_size_to_Qt", "(", "size", ")", ":", "qtsize", "=", "size", "if", "size", "[", "1", "]", "is", "not", "None", "and", "size", "[", "1", "]", "<", "DEFAULT_PIXEL_TO_CHARS_CUTOFF", ":", "# change from character based size to pixels (roughly)", "qtsize", "=", "size", "[", "0", "]", "*", "DEFAULT_PIXELS_TO_CHARS_SCALING", "[", "0", "]", ",", "size", "[", "1", "]", "*", "DEFAULT_PIXELS_TO_CHARS_SCALING", "[", "1", "]", "return", "qtsize"], "docstring": "Converts size in characters to size in pixels\n    :param size:  size in characters, rows\n    :return: size in pixels, pixels", "docstring_tokens": ["Converts", "size", "in", "characters", "to", "size", "in", "pixels", ":", "param", "size", ":", "size", "in", "characters", "rows", ":", "return", ":", "size", "in", "pixels", "pixels"], "sha": "08184197f5bd4580ab5e5aca28bdda30f87b86fc", "url": "https://github.com/PySimpleGUI/PySimpleGUI/blob/08184197f5bd4580ab5e5aca28bdda30f87b86fc/PySimpleGUIQt/PySimpleGUIQt.py#L3730-L3739", "partition": "train"}
{"repo": "tensorflow/hub", "path": "examples/image_retraining/retrain.py", "func_name": "get_random_distorted_bottlenecks", "original_string": "def get_random_distorted_bottlenecks(\n    sess, image_lists, how_many, category, image_dir, input_jpeg_tensor,\n    distorted_image, resized_input_tensor, bottleneck_tensor):\n  \"\"\"Retrieves bottleneck values for training images, after distortions.\n\n  If we're training with distortions like crops, scales, or flips, we have to\n  recalculate the full model for every image, and so we can't use cached\n  bottleneck values. Instead we find random images for the requested category,\n  run them through the distortion graph, and then the full graph to get the\n  bottleneck results for each.\n\n  Args:\n    sess: Current TensorFlow Session.\n    image_lists: OrderedDict of training images for each label.\n    how_many: The integer number of bottleneck values to return.\n    category: Name string of which set of images to fetch - training, testing,\n    or validation.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    input_jpeg_tensor: The input layer we feed the image data to.\n    distorted_image: The output node of the distortion graph.\n    resized_input_tensor: The input node of the recognition graph.\n    bottleneck_tensor: The bottleneck output layer of the CNN graph.\n\n  Returns:\n    List of bottleneck arrays and their corresponding ground truths.\n  \"\"\"\n  class_count = len(image_lists.keys())\n  bottlenecks = []\n  ground_truths = []\n  for unused_i in range(how_many):\n    label_index = random.randrange(class_count)\n    label_name = list(image_lists.keys())[label_index]\n    image_index = random.randrange(MAX_NUM_IMAGES_PER_CLASS + 1)\n    image_path = get_image_path(image_lists, label_name, image_index, image_dir,\n                                category)\n    if not tf.gfile.Exists(image_path):\n      tf.logging.fatal('File does not exist %s', image_path)\n    jpeg_data = tf.gfile.GFile(image_path, 'rb').read()\n    # Note that we materialize the distorted_image_data as a numpy array before\n    # sending running inference on the image. This involves 2 memory copies and\n    # might be optimized in other implementations.\n    distorted_image_data = sess.run(distorted_image,\n                                    {input_jpeg_tensor: jpeg_data})\n    bottleneck_values = sess.run(bottleneck_tensor,\n                                 {resized_input_tensor: distorted_image_data})\n    bottleneck_values = np.squeeze(bottleneck_values)\n    bottlenecks.append(bottleneck_values)\n    ground_truths.append(label_index)\n  return bottlenecks, ground_truths", "language": "python", "code": "def get_random_distorted_bottlenecks(\n    sess, image_lists, how_many, category, image_dir, input_jpeg_tensor,\n    distorted_image, resized_input_tensor, bottleneck_tensor):\n  \"\"\"Retrieves bottleneck values for training images, after distortions.\n\n  If we're training with distortions like crops, scales, or flips, we have to\n  recalculate the full model for every image, and so we can't use cached\n  bottleneck values. Instead we find random images for the requested category,\n  run them through the distortion graph, and then the full graph to get the\n  bottleneck results for each.\n\n  Args:\n    sess: Current TensorFlow Session.\n    image_lists: OrderedDict of training images for each label.\n    how_many: The integer number of bottleneck values to return.\n    category: Name string of which set of images to fetch - training, testing,\n    or validation.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    input_jpeg_tensor: The input layer we feed the image data to.\n    distorted_image: The output node of the distortion graph.\n    resized_input_tensor: The input node of the recognition graph.\n    bottleneck_tensor: The bottleneck output layer of the CNN graph.\n\n  Returns:\n    List of bottleneck arrays and their corresponding ground truths.\n  \"\"\"\n  class_count = len(image_lists.keys())\n  bottlenecks = []\n  ground_truths = []\n  for unused_i in range(how_many):\n    label_index = random.randrange(class_count)\n    label_name = list(image_lists.keys())[label_index]\n    image_index = random.randrange(MAX_NUM_IMAGES_PER_CLASS + 1)\n    image_path = get_image_path(image_lists, label_name, image_index, image_dir,\n                                category)\n    if not tf.gfile.Exists(image_path):\n      tf.logging.fatal('File does not exist %s', image_path)\n    jpeg_data = tf.gfile.GFile(image_path, 'rb').read()\n    # Note that we materialize the distorted_image_data as a numpy array before\n    # sending running inference on the image. This involves 2 memory copies and\n    # might be optimized in other implementations.\n    distorted_image_data = sess.run(distorted_image,\n                                    {input_jpeg_tensor: jpeg_data})\n    bottleneck_values = sess.run(bottleneck_tensor,\n                                 {resized_input_tensor: distorted_image_data})\n    bottleneck_values = np.squeeze(bottleneck_values)\n    bottlenecks.append(bottleneck_values)\n    ground_truths.append(label_index)\n  return bottlenecks, ground_truths", "code_tokens": ["def", "get_random_distorted_bottlenecks", "(", "sess", ",", "image_lists", ",", "how_many", ",", "category", ",", "image_dir", ",", "input_jpeg_tensor", ",", "distorted_image", ",", "resized_input_tensor", ",", "bottleneck_tensor", ")", ":", "class_count", "=", "len", "(", "image_lists", ".", "keys", "(", ")", ")", "bottlenecks", "=", "[", "]", "ground_truths", "=", "[", "]", "for", "unused_i", "in", "range", "(", "how_many", ")", ":", "label_index", "=", "random", ".", "randrange", "(", "class_count", ")", "label_name", "=", "list", "(", "image_lists", ".", "keys", "(", ")", ")", "[", "label_index", "]", "image_index", "=", "random", ".", "randrange", "(", "MAX_NUM_IMAGES_PER_CLASS", "+", "1", ")", "image_path", "=", "get_image_path", "(", "image_lists", ",", "label_name", ",", "image_index", ",", "image_dir", ",", "category", ")", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "image_path", ")", ":", "tf", ".", "logging", ".", "fatal", "(", "'File does not exist %s'", ",", "image_path", ")", "jpeg_data", "=", "tf", ".", "gfile", ".", "GFile", "(", "image_path", ",", "'rb'", ")", ".", "read", "(", ")", "# Note that we materialize the distorted_image_data as a numpy array before", "# sending running inference on the image. This involves 2 memory copies and", "# might be optimized in other implementations.", "distorted_image_data", "=", "sess", ".", "run", "(", "distorted_image", ",", "{", "input_jpeg_tensor", ":", "jpeg_data", "}", ")", "bottleneck_values", "=", "sess", ".", "run", "(", "bottleneck_tensor", ",", "{", "resized_input_tensor", ":", "distorted_image_data", "}", ")", "bottleneck_values", "=", "np", ".", "squeeze", "(", "bottleneck_values", ")", "bottlenecks", ".", "append", "(", "bottleneck_values", ")", "ground_truths", ".", "append", "(", "label_index", ")", "return", "bottlenecks", ",", "ground_truths"], "docstring": "Retrieves bottleneck values for training images, after distortions.\n\n  If we're training with distortions like crops, scales, or flips, we have to\n  recalculate the full model for every image, and so we can't use cached\n  bottleneck values. Instead we find random images for the requested category,\n  run them through the distortion graph, and then the full graph to get the\n  bottleneck results for each.\n\n  Args:\n    sess: Current TensorFlow Session.\n    image_lists: OrderedDict of training images for each label.\n    how_many: The integer number of bottleneck values to return.\n    category: Name string of which set of images to fetch - training, testing,\n    or validation.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    input_jpeg_tensor: The input layer we feed the image data to.\n    distorted_image: The output node of the distortion graph.\n    resized_input_tensor: The input node of the recognition graph.\n    bottleneck_tensor: The bottleneck output layer of the CNN graph.\n\n  Returns:\n    List of bottleneck arrays and their corresponding ground truths.", "docstring_tokens": ["Retrieves", "bottleneck", "values", "for", "training", "images", "after", "distortions", "."], "sha": "09f45963f6787322967b6fec61459f3ac56fbb27", "url": "https://github.com/tensorflow/hub/blob/09f45963f6787322967b6fec61459f3ac56fbb27/examples/image_retraining/retrain.py#L547-L596", "partition": "train"}
{"repo": "kubernetes-client/python", "path": "kubernetes/client/apis/certificates_v1beta1_api.py", "func_name": "CertificatesV1beta1Api.replace_certificate_signing_request_approval", "original_string": "def replace_certificate_signing_request_approval(self, name, body, **kwargs):\n        \"\"\"\n        replace approval of the specified CertificateSigningRequest\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_certificate_signing_request_approval(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CertificateSigningRequest (required)\n        :param V1beta1CertificateSigningRequest body: (required)\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1beta1CertificateSigningRequest\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_certificate_signing_request_approval_with_http_info(name, body, **kwargs)\n        else:\n            (data) = self.replace_certificate_signing_request_approval_with_http_info(name, body, **kwargs)\n            return data", "language": "python", "code": "def replace_certificate_signing_request_approval(self, name, body, **kwargs):\n        \"\"\"\n        replace approval of the specified CertificateSigningRequest\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_certificate_signing_request_approval(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CertificateSigningRequest (required)\n        :param V1beta1CertificateSigningRequest body: (required)\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1beta1CertificateSigningRequest\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_certificate_signing_request_approval_with_http_info(name, body, **kwargs)\n        else:\n            (data) = self.replace_certificate_signing_request_approval_with_http_info(name, body, **kwargs)\n            return data", "code_tokens": ["def", "replace_certificate_signing_request_approval", "(", "self", ",", "name", ",", "body", ",", "*", "*", "kwargs", ")", ":", "kwargs", "[", "'_return_http_data_only'", "]", "=", "True", "if", "kwargs", ".", "get", "(", "'async_req'", ")", ":", "return", "self", ".", "replace_certificate_signing_request_approval_with_http_info", "(", "name", ",", "body", ",", "*", "*", "kwargs", ")", "else", ":", "(", "data", ")", "=", "self", ".", "replace_certificate_signing_request_approval_with_http_info", "(", "name", ",", "body", ",", "*", "*", "kwargs", ")", "return", "data"], "docstring": "replace approval of the specified CertificateSigningRequest\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_certificate_signing_request_approval(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CertificateSigningRequest (required)\n        :param V1beta1CertificateSigningRequest body: (required)\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1beta1CertificateSigningRequest\n                 If the method is called asynchronously,\n                 returns the request thread.", "docstring_tokens": ["replace", "approval", "of", "the", "specified", "CertificateSigningRequest", "This", "method", "makes", "a", "synchronous", "HTTP", "request", "by", "default", ".", "To", "make", "an", "asynchronous", "HTTP", "request", "please", "pass", "async_req", "=", "True", ">>>", "thread", "=", "api", ".", "replace_certificate_signing_request_approval", "(", "name", "body", "async_req", "=", "True", ")", ">>>", "result", "=", "thread", ".", "get", "()"], "sha": "5e512ff564c244c50cab780d821542ed56aa965a", "url": "https://github.com/kubernetes-client/python/blob/5e512ff564c244c50cab780d821542ed56aa965a/kubernetes/client/apis/certificates_v1beta1_api.py#L1157-L1180", "partition": "train"}
{"repo": "bokeh/bokeh", "path": "bokeh/core/property/wrappers.py", "func_name": "notify_owner", "original_string": "def notify_owner(func):\n    ''' A decorator for mutating methods of property container classes\n    that notifies owners of the property container about mutating changes.\n\n    Args:\n        func (callable) : the container method to wrap in a notification\n\n    Returns:\n        wrapped method\n\n    Examples:\n\n        A ``__setitem__`` could be wrapped like this:\n\n        .. code-block:: python\n\n            # x[i] = y\n            @notify_owner\n            def __setitem__(self, i, y):\n                return super(PropertyValueDict, self).__setitem__(i, y)\n\n    The returned wrapped method will have a docstring indicating what\n    original method it is wrapping.\n\n    '''\n    def wrapper(self, *args, **kwargs):\n        old = self._saved_copy()\n        result = func(self, *args, **kwargs)\n        self._notify_owners(old)\n        return result\n    wrapper.__doc__ = \"Container method ``%s`` instrumented to notify property owners\" % func.__name__\n    return wrapper", "language": "python", "code": "def notify_owner(func):\n    ''' A decorator for mutating methods of property container classes\n    that notifies owners of the property container about mutating changes.\n\n    Args:\n        func (callable) : the container method to wrap in a notification\n\n    Returns:\n        wrapped method\n\n    Examples:\n\n        A ``__setitem__`` could be wrapped like this:\n\n        .. code-block:: python\n\n            # x[i] = y\n            @notify_owner\n            def __setitem__(self, i, y):\n                return super(PropertyValueDict, self).__setitem__(i, y)\n\n    The returned wrapped method will have a docstring indicating what\n    original method it is wrapping.\n\n    '''\n    def wrapper(self, *args, **kwargs):\n        old = self._saved_copy()\n        result = func(self, *args, **kwargs)\n        self._notify_owners(old)\n        return result\n    wrapper.__doc__ = \"Container method ``%s`` instrumented to notify property owners\" % func.__name__\n    return wrapper", "code_tokens": ["def", "notify_owner", "(", "func", ")", ":", "def", "wrapper", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "old", "=", "self", ".", "_saved_copy", "(", ")", "result", "=", "func", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", "self", ".", "_notify_owners", "(", "old", ")", "return", "result", "wrapper", ".", "__doc__", "=", "\"Container method ``%s`` instrumented to notify property owners\"", "%", "func", ".", "__name__", "return", "wrapper"], "docstring": "A decorator for mutating methods of property container classes\n    that notifies owners of the property container about mutating changes.\n\n    Args:\n        func (callable) : the container method to wrap in a notification\n\n    Returns:\n        wrapped method\n\n    Examples:\n\n        A ``__setitem__`` could be wrapped like this:\n\n        .. code-block:: python\n\n            # x[i] = y\n            @notify_owner\n            def __setitem__(self, i, y):\n                return super(PropertyValueDict, self).__setitem__(i, y)\n\n    The returned wrapped method will have a docstring indicating what\n    original method it is wrapping.", "docstring_tokens": ["A", "decorator", "for", "mutating", "methods", "of", "property", "container", "classes", "that", "notifies", "owners", "of", "the", "property", "container", "about", "mutating", "changes", "."], "sha": "dc8cf49e4e4302fd38537ad089ece81fbcca4737", "url": "https://github.com/bokeh/bokeh/blob/dc8cf49e4e4302fd38537ad089ece81fbcca4737/bokeh/core/property/wrappers.py#L97-L128", "partition": "train"}
{"repo": "spyder-ide/spyder", "path": "spyder/plugins/projects/widgets/explorer.py", "func_name": "ExplorerTreeWidget.toggle_hscrollbar", "original_string": "def toggle_hscrollbar(self, checked):\r\n        \"\"\"Toggle horizontal scrollbar\"\"\"\r\n        self.parent_widget.sig_option_changed.emit('show_hscrollbar', checked)\r\n        self.show_hscrollbar = checked\r\n        self.header().setStretchLastSection(not checked)\r\n        self.header().setHorizontalScrollMode(QAbstractItemView.ScrollPerPixel)\r\n        try:\r\n            self.header().setSectionResizeMode(QHeaderView.ResizeToContents)\r\n        except:  # support for qtpy<1.2.0\r\n            self.header().setResizeMode(QHeaderView.ResizeToContents)", "language": "python", "code": "def toggle_hscrollbar(self, checked):\r\n        \"\"\"Toggle horizontal scrollbar\"\"\"\r\n        self.parent_widget.sig_option_changed.emit('show_hscrollbar', checked)\r\n        self.show_hscrollbar = checked\r\n        self.header().setStretchLastSection(not checked)\r\n        self.header().setHorizontalScrollMode(QAbstractItemView.ScrollPerPixel)\r\n        try:\r\n            self.header().setSectionResizeMode(QHeaderView.ResizeToContents)\r\n        except:  # support for qtpy<1.2.0\r\n            self.header().setResizeMode(QHeaderView.ResizeToContents)", "code_tokens": ["def", "toggle_hscrollbar", "(", "self", ",", "checked", ")", ":", "self", ".", "parent_widget", ".", "sig_option_changed", ".", "emit", "(", "'show_hscrollbar'", ",", "checked", ")", "self", ".", "show_hscrollbar", "=", "checked", "self", ".", "header", "(", ")", ".", "setStretchLastSection", "(", "not", "checked", ")", "self", ".", "header", "(", ")", ".", "setHorizontalScrollMode", "(", "QAbstractItemView", ".", "ScrollPerPixel", ")", "try", ":", "self", ".", "header", "(", ")", ".", "setSectionResizeMode", "(", "QHeaderView", ".", "ResizeToContents", ")", "except", ":", "# support for qtpy<1.2.0\r", "self", ".", "header", "(", ")", ".", "setResizeMode", "(", "QHeaderView", ".", "ResizeToContents", ")"], "docstring": "Toggle horizontal scrollbar", "docstring_tokens": ["Toggle", "horizontal", "scrollbar"], "sha": "f76836ce1b924bcc4efd3f74f2960d26a4e528e0", "url": "https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/projects/widgets/explorer.py#L60-L69", "partition": "train"}
{"repo": "spotify/luigi", "path": "luigi/contrib/gcp.py", "func_name": "get_authenticate_kwargs", "original_string": "def get_authenticate_kwargs(oauth_credentials=None, http_=None):\n    \"\"\"Returns a dictionary with keyword arguments for use with discovery\n\n    Prioritizes oauth_credentials or a http client provided by the user\n    If none provided, falls back to default credentials provided by google's command line\n    utilities. If that also fails, tries using httplib2.Http()\n\n    Used by `gcs.GCSClient` and `bigquery.BigQueryClient` to initiate the API Client\n    \"\"\"\n    if oauth_credentials:\n        authenticate_kwargs = {\n            \"credentials\": oauth_credentials\n        }\n    elif http_:\n        authenticate_kwargs = {\n            \"http\": http_\n        }\n    else:\n        # neither http_ or credentials provided\n        try:\n            # try default credentials\n            credentials, _ = google.auth.default()\n            authenticate_kwargs = {\n                \"credentials\": credentials\n            }\n        except google.auth.exceptions.DefaultCredentialsError:\n            # try http using httplib2\n            authenticate_kwargs = {\n                \"http\": httplib2.Http()\n            }\n\n    return authenticate_kwargs", "language": "python", "code": "def get_authenticate_kwargs(oauth_credentials=None, http_=None):\n    \"\"\"Returns a dictionary with keyword arguments for use with discovery\n\n    Prioritizes oauth_credentials or a http client provided by the user\n    If none provided, falls back to default credentials provided by google's command line\n    utilities. If that also fails, tries using httplib2.Http()\n\n    Used by `gcs.GCSClient` and `bigquery.BigQueryClient` to initiate the API Client\n    \"\"\"\n    if oauth_credentials:\n        authenticate_kwargs = {\n            \"credentials\": oauth_credentials\n        }\n    elif http_:\n        authenticate_kwargs = {\n            \"http\": http_\n        }\n    else:\n        # neither http_ or credentials provided\n        try:\n            # try default credentials\n            credentials, _ = google.auth.default()\n            authenticate_kwargs = {\n                \"credentials\": credentials\n            }\n        except google.auth.exceptions.DefaultCredentialsError:\n            # try http using httplib2\n            authenticate_kwargs = {\n                \"http\": httplib2.Http()\n            }\n\n    return authenticate_kwargs", "code_tokens": ["def", "get_authenticate_kwargs", "(", "oauth_credentials", "=", "None", ",", "http_", "=", "None", ")", ":", "if", "oauth_credentials", ":", "authenticate_kwargs", "=", "{", "\"credentials\"", ":", "oauth_credentials", "}", "elif", "http_", ":", "authenticate_kwargs", "=", "{", "\"http\"", ":", "http_", "}", "else", ":", "# neither http_ or credentials provided", "try", ":", "# try default credentials", "credentials", ",", "_", "=", "google", ".", "auth", ".", "default", "(", ")", "authenticate_kwargs", "=", "{", "\"credentials\"", ":", "credentials", "}", "except", "google", ".", "auth", ".", "exceptions", ".", "DefaultCredentialsError", ":", "# try http using httplib2", "authenticate_kwargs", "=", "{", "\"http\"", ":", "httplib2", ".", "Http", "(", ")", "}", "return", "authenticate_kwargs"], "docstring": "Returns a dictionary with keyword arguments for use with discovery\n\n    Prioritizes oauth_credentials or a http client provided by the user\n    If none provided, falls back to default credentials provided by google's command line\n    utilities. If that also fails, tries using httplib2.Http()\n\n    Used by `gcs.GCSClient` and `bigquery.BigQueryClient` to initiate the API Client", "docstring_tokens": ["Returns", "a", "dictionary", "with", "keyword", "arguments", "for", "use", "with", "discovery"], "sha": "c5eca1c3c3ee2a7eb612486192a0da146710a1e9", "url": "https://github.com/spotify/luigi/blob/c5eca1c3c3ee2a7eb612486192a0da146710a1e9/luigi/contrib/gcp.py#L15-L46", "partition": "train"}
{"repo": "spotify/luigi", "path": "luigi/contrib/redshift.py", "func_name": "_CredentialsMixin._credentials", "original_string": "def _credentials(self):\n        \"\"\"\n        Return a credential string for the provided task. If no valid\n        credentials are set, raise a NotImplementedError.\n        \"\"\"\n\n        if self.aws_account_id and self.aws_arn_role_name:\n            return 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(\n                id=self.aws_account_id,\n                role=self.aws_arn_role_name\n            )\n        elif self.aws_access_key_id and self.aws_secret_access_key:\n            return 'aws_access_key_id={key};aws_secret_access_key={secret}{opt}'.format(\n                key=self.aws_access_key_id,\n                secret=self.aws_secret_access_key,\n                opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else ''\n            )\n        else:\n            raise NotImplementedError(\"Missing Credentials. \"\n                                      \"Ensure one of the pairs of auth args below are set \"\n                                      \"in a configuration file, environment variables or by \"\n                                      \"being overridden in the task: \"\n                                      \"'aws_access_key_id' AND 'aws_secret_access_key' OR \"\n                                      \"'aws_account_id' AND 'aws_arn_role_name'\")", "language": "python", "code": "def _credentials(self):\n        \"\"\"\n        Return a credential string for the provided task. If no valid\n        credentials are set, raise a NotImplementedError.\n        \"\"\"\n\n        if self.aws_account_id and self.aws_arn_role_name:\n            return 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(\n                id=self.aws_account_id,\n                role=self.aws_arn_role_name\n            )\n        elif self.aws_access_key_id and self.aws_secret_access_key:\n            return 'aws_access_key_id={key};aws_secret_access_key={secret}{opt}'.format(\n                key=self.aws_access_key_id,\n                secret=self.aws_secret_access_key,\n                opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else ''\n            )\n        else:\n            raise NotImplementedError(\"Missing Credentials. \"\n                                      \"Ensure one of the pairs of auth args below are set \"\n                                      \"in a configuration file, environment variables or by \"\n                                      \"being overridden in the task: \"\n                                      \"'aws_access_key_id' AND 'aws_secret_access_key' OR \"\n                                      \"'aws_account_id' AND 'aws_arn_role_name'\")", "code_tokens": ["def", "_credentials", "(", "self", ")", ":", "if", "self", ".", "aws_account_id", "and", "self", ".", "aws_arn_role_name", ":", "return", "'aws_iam_role=arn:aws:iam::{id}:role/{role}'", ".", "format", "(", "id", "=", "self", ".", "aws_account_id", ",", "role", "=", "self", ".", "aws_arn_role_name", ")", "elif", "self", ".", "aws_access_key_id", "and", "self", ".", "aws_secret_access_key", ":", "return", "'aws_access_key_id={key};aws_secret_access_key={secret}{opt}'", ".", "format", "(", "key", "=", "self", ".", "aws_access_key_id", ",", "secret", "=", "self", ".", "aws_secret_access_key", ",", "opt", "=", "';token={}'", ".", "format", "(", "self", ".", "aws_session_token", ")", "if", "self", ".", "aws_session_token", "else", "''", ")", "else", ":", "raise", "NotImplementedError", "(", "\"Missing Credentials. \"", "\"Ensure one of the pairs of auth args below are set \"", "\"in a configuration file, environment variables or by \"", "\"being overridden in the task: \"", "\"'aws_access_key_id' AND 'aws_secret_access_key' OR \"", "\"'aws_account_id' AND 'aws_arn_role_name'\"", ")"], "docstring": "Return a credential string for the provided task. If no valid\n        credentials are set, raise a NotImplementedError.", "docstring_tokens": ["Return", "a", "credential", "string", "for", "the", "provided", "task", ".", "If", "no", "valid", "credentials", "are", "set", "raise", "a", "NotImplementedError", "."], "sha": "c5eca1c3c3ee2a7eb612486192a0da146710a1e9", "url": "https://github.com/spotify/luigi/blob/c5eca1c3c3ee2a7eb612486192a0da146710a1e9/luigi/contrib/redshift.py#L100-L123", "partition": "train"}
{"repo": "tyiannak/pyAudioAnalysis", "path": "pyAudioAnalysis/audioFeatureExtraction.py", "func_name": "mtFeatureExtractionToFile", "original_string": "def mtFeatureExtractionToFile(fileName, midTermSize, midTermStep, shortTermSize, shortTermStep, outPutFile,\n                              storeStFeatures=False, storeToCSV=False, PLOT=False):\n    \"\"\"\n    This function is used as a wrapper to:\n    a) read the content of a WAV file\n    b) perform mid-term feature extraction on that signal\n    c) write the mid-term feature sequences to a numpy file\n    \"\"\"\n    [fs, x] = audioBasicIO.readAudioFile(fileName)\n    x = audioBasicIO.stereo2mono(x)\n    if storeStFeatures:\n        [mtF, stF, _] = mtFeatureExtraction(x, fs,\n                                         round(fs * midTermSize),\n                                         round(fs * midTermStep),\n                                         round(fs * shortTermSize),\n                                         round(fs * shortTermStep))\n    else:\n        [mtF, _, _] = mtFeatureExtraction(x, fs, round(fs*midTermSize),\n                                       round(fs * midTermStep),\n                                       round(fs * shortTermSize),\n                                       round(fs * shortTermStep))\n    # save mt features to numpy file\n    numpy.save(outPutFile, mtF)\n    if PLOT:\n        print(\"Mid-term numpy file: \" + outPutFile + \".npy saved\")\n    if storeToCSV:\n        numpy.savetxt(outPutFile+\".csv\", mtF.T, delimiter=\",\")\n        if PLOT:\n            print(\"Mid-term CSV file: \" + outPutFile + \".csv saved\")\n\n    if storeStFeatures:\n        # save st features to numpy file\n        numpy.save(outPutFile+\"_st\", stF)\n        if PLOT:\n            print(\"Short-term numpy file: \" + outPutFile + \"_st.npy saved\")\n        if storeToCSV:\n            # store st features to CSV file\n            numpy.savetxt(outPutFile+\"_st.csv\", stF.T, delimiter=\",\")\n            if PLOT:\n                print(\"Short-term CSV file: \" + outPutFile + \"_st.csv saved\")", "language": "python", "code": "def mtFeatureExtractionToFile(fileName, midTermSize, midTermStep, shortTermSize, shortTermStep, outPutFile,\n                              storeStFeatures=False, storeToCSV=False, PLOT=False):\n    \"\"\"\n    This function is used as a wrapper to:\n    a) read the content of a WAV file\n    b) perform mid-term feature extraction on that signal\n    c) write the mid-term feature sequences to a numpy file\n    \"\"\"\n    [fs, x] = audioBasicIO.readAudioFile(fileName)\n    x = audioBasicIO.stereo2mono(x)\n    if storeStFeatures:\n        [mtF, stF, _] = mtFeatureExtraction(x, fs,\n                                         round(fs * midTermSize),\n                                         round(fs * midTermStep),\n                                         round(fs * shortTermSize),\n                                         round(fs * shortTermStep))\n    else:\n        [mtF, _, _] = mtFeatureExtraction(x, fs, round(fs*midTermSize),\n                                       round(fs * midTermStep),\n                                       round(fs * shortTermSize),\n                                       round(fs * shortTermStep))\n    # save mt features to numpy file\n    numpy.save(outPutFile, mtF)\n    if PLOT:\n        print(\"Mid-term numpy file: \" + outPutFile + \".npy saved\")\n    if storeToCSV:\n        numpy.savetxt(outPutFile+\".csv\", mtF.T, delimiter=\",\")\n        if PLOT:\n            print(\"Mid-term CSV file: \" + outPutFile + \".csv saved\")\n\n    if storeStFeatures:\n        # save st features to numpy file\n        numpy.save(outPutFile+\"_st\", stF)\n        if PLOT:\n            print(\"Short-term numpy file: \" + outPutFile + \"_st.npy saved\")\n        if storeToCSV:\n            # store st features to CSV file\n            numpy.savetxt(outPutFile+\"_st.csv\", stF.T, delimiter=\",\")\n            if PLOT:\n                print(\"Short-term CSV file: \" + outPutFile + \"_st.csv saved\")", "code_tokens": ["def", "mtFeatureExtractionToFile", "(", "fileName", ",", "midTermSize", ",", "midTermStep", ",", "shortTermSize", ",", "shortTermStep", ",", "outPutFile", ",", "storeStFeatures", "=", "False", ",", "storeToCSV", "=", "False", ",", "PLOT", "=", "False", ")", ":", "[", "fs", ",", "x", "]", "=", "audioBasicIO", ".", "readAudioFile", "(", "fileName", ")", "x", "=", "audioBasicIO", ".", "stereo2mono", "(", "x", ")", "if", "storeStFeatures", ":", "[", "mtF", ",", "stF", ",", "_", "]", "=", "mtFeatureExtraction", "(", "x", ",", "fs", ",", "round", "(", "fs", "*", "midTermSize", ")", ",", "round", "(", "fs", "*", "midTermStep", ")", ",", "round", "(", "fs", "*", "shortTermSize", ")", ",", "round", "(", "fs", "*", "shortTermStep", ")", ")", "else", ":", "[", "mtF", ",", "_", ",", "_", "]", "=", "mtFeatureExtraction", "(", "x", ",", "fs", ",", "round", "(", "fs", "*", "midTermSize", ")", ",", "round", "(", "fs", "*", "midTermStep", ")", ",", "round", "(", "fs", "*", "shortTermSize", ")", ",", "round", "(", "fs", "*", "shortTermStep", ")", ")", "# save mt features to numpy file", "numpy", ".", "save", "(", "outPutFile", ",", "mtF", ")", "if", "PLOT", ":", "print", "(", "\"Mid-term numpy file: \"", "+", "outPutFile", "+", "\".npy saved\"", ")", "if", "storeToCSV", ":", "numpy", ".", "savetxt", "(", "outPutFile", "+", "\".csv\"", ",", "mtF", ".", "T", ",", "delimiter", "=", "\",\"", ")", "if", "PLOT", ":", "print", "(", "\"Mid-term CSV file: \"", "+", "outPutFile", "+", "\".csv saved\"", ")", "if", "storeStFeatures", ":", "# save st features to numpy file", "numpy", ".", "save", "(", "outPutFile", "+", "\"_st\"", ",", "stF", ")", "if", "PLOT", ":", "print", "(", "\"Short-term numpy file: \"", "+", "outPutFile", "+", "\"_st.npy saved\"", ")", "if", "storeToCSV", ":", "# store st features to CSV file", "numpy", ".", "savetxt", "(", "outPutFile", "+", "\"_st.csv\"", ",", "stF", ".", "T", ",", "delimiter", "=", "\",\"", ")", "if", "PLOT", ":", "print", "(", "\"Short-term CSV file: \"", "+", "outPutFile", "+", "\"_st.csv saved\"", ")"], "docstring": "This function is used as a wrapper to:\n    a) read the content of a WAV file\n    b) perform mid-term feature extraction on that signal\n    c) write the mid-term feature sequences to a numpy file", "docstring_tokens": ["This", "function", "is", "used", "as", "a", "wrapper", "to", ":", "a", ")", "read", "the", "content", "of", "a", "WAV", "file", "b", ")", "perform", "mid", "-", "term", "feature", "extraction", "on", "that", "signal", "c", ")", "write", "the", "mid", "-", "term", "feature", "sequences", "to", "a", "numpy", "file"], "sha": "e3da991e7247492deba50648a4c7c0f41e684af4", "url": "https://github.com/tyiannak/pyAudioAnalysis/blob/e3da991e7247492deba50648a4c7c0f41e684af4/pyAudioAnalysis/audioFeatureExtraction.py#L889-L928", "partition": "train"}
{"repo": "ricequant/rqalpha", "path": "rqalpha/model/base_account.py", "func_name": "BaseAccount.market_value", "original_string": "def market_value(self):\n        \"\"\"\n        [float] \u5e02\u503c\n        \"\"\"\n        return sum(position.market_value for position in six.itervalues(self._positions))", "language": "python", "code": "def market_value(self):\n        \"\"\"\n        [float] \u5e02\u503c\n        \"\"\"\n        return sum(position.market_value for position in six.itervalues(self._positions))", "code_tokens": ["def", "market_value", "(", "self", ")", ":", "return", "sum", "(", "position", ".", "market_value", "for", "position", "in", "six", ".", "itervalues", "(", "self", ".", "_positions", ")", ")"], "docstring": "[float] \u5e02\u503c", "docstring_tokens": ["[", "float", "]", "\u5e02\u503c"], "sha": "ac40a62d4e7eca9494b4d0a14f46facf5616820c", "url": "https://github.com/ricequant/rqalpha/blob/ac40a62d4e7eca9494b4d0a14f46facf5616820c/rqalpha/model/base_account.py#L103-L107", "partition": "train"}
{"repo": "elastic/elasticsearch-py", "path": "elasticsearch/client/__init__.py", "func_name": "Elasticsearch.render_search_template", "original_string": "def render_search_template(self, id=None, body=None, params=None):\n        \"\"\"\n        `<http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-template.html>`_\n\n        :arg id: The id of the stored search template\n        :arg body: The search definition template and its params\n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_render\", \"template\", id), params=params, body=body\n        )", "language": "python", "code": "def render_search_template(self, id=None, body=None, params=None):\n        \"\"\"\n        `<http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-template.html>`_\n\n        :arg id: The id of the stored search template\n        :arg body: The search definition template and its params\n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_render\", \"template\", id), params=params, body=body\n        )", "code_tokens": ["def", "render_search_template", "(", "self", ",", "id", "=", "None", ",", "body", "=", "None", ",", "params", "=", "None", ")", ":", "return", "self", ".", "transport", ".", "perform_request", "(", "\"GET\"", ",", "_make_path", "(", "\"_render\"", ",", "\"template\"", ",", "id", ")", ",", "params", "=", "params", ",", "body", "=", "body", ")"], "docstring": "`<http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-template.html>`_\n\n        :arg id: The id of the stored search template\n        :arg body: The search definition template and its params", "docstring_tokens": ["<http", ":", "//", "www", ".", "elasticsearch", ".", "org", "/", "guide", "/", "en", "/", "elasticsearch", "/", "reference", "/", "current", "/", "search", "-", "template", ".", "html", ">", "_"], "sha": "2aab285c8f506f3863cbdaba3c90a685c510ba00", "url": "https://github.com/elastic/elasticsearch-py/blob/2aab285c8f506f3863cbdaba3c90a685c510ba00/elasticsearch/client/__init__.py#L1662-L1671", "partition": "train"}
{"repo": "Kaggle/kaggle-api", "path": "kaggle/models/dataset_new_request.py", "func_name": "DatasetNewRequest.license_name", "original_string": "def license_name(self, license_name):\n        \"\"\"Sets the license_name of this DatasetNewRequest.\n\n        The license that should be associated with the dataset  # noqa: E501\n\n        :param license_name: The license_name of this DatasetNewRequest.  # noqa: E501\n        :type: str\n        \"\"\"\n        allowed_values = [\"CC0-1.0\", \"CC-BY-SA-4.0\", \"GPL-2.0\", \"ODbL-1.0\", \"CC-BY-NC-SA-4.0\", \"unknown\", \"DbCL-1.0\", \"CC-BY-SA-3.0\", \"copyright-authors\", \"other\", \"reddit-api\", \"world-bank\"]  # noqa: E501\n        if license_name not in allowed_values:\n            raise ValueError(\n                \"Invalid value for `license_name` ({0}), must be one of {1}\"  # noqa: E501\n                .format(license_name, allowed_values)\n            )\n\n        self._license_name = license_name", "language": "python", "code": "def license_name(self, license_name):\n        \"\"\"Sets the license_name of this DatasetNewRequest.\n\n        The license that should be associated with the dataset  # noqa: E501\n\n        :param license_name: The license_name of this DatasetNewRequest.  # noqa: E501\n        :type: str\n        \"\"\"\n        allowed_values = [\"CC0-1.0\", \"CC-BY-SA-4.0\", \"GPL-2.0\", \"ODbL-1.0\", \"CC-BY-NC-SA-4.0\", \"unknown\", \"DbCL-1.0\", \"CC-BY-SA-3.0\", \"copyright-authors\", \"other\", \"reddit-api\", \"world-bank\"]  # noqa: E501\n        if license_name not in allowed_values:\n            raise ValueError(\n                \"Invalid value for `license_name` ({0}), must be one of {1}\"  # noqa: E501\n                .format(license_name, allowed_values)\n            )\n\n        self._license_name = license_name", "code_tokens": ["def", "license_name", "(", "self", ",", "license_name", ")", ":", "allowed_values", "=", "[", "\"CC0-1.0\"", ",", "\"CC-BY-SA-4.0\"", ",", "\"GPL-2.0\"", ",", "\"ODbL-1.0\"", ",", "\"CC-BY-NC-SA-4.0\"", ",", "\"unknown\"", ",", "\"DbCL-1.0\"", ",", "\"CC-BY-SA-3.0\"", ",", "\"copyright-authors\"", ",", "\"other\"", ",", "\"reddit-api\"", ",", "\"world-bank\"", "]", "# noqa: E501", "if", "license_name", "not", "in", "allowed_values", ":", "raise", "ValueError", "(", "\"Invalid value for `license_name` ({0}), must be one of {1}\"", "# noqa: E501", ".", "format", "(", "license_name", ",", "allowed_values", ")", ")", "self", ".", "_license_name", "=", "license_name"], "docstring": "Sets the license_name of this DatasetNewRequest.\n\n        The license that should be associated with the dataset  # noqa: E501\n\n        :param license_name: The license_name of this DatasetNewRequest.  # noqa: E501\n        :type: str", "docstring_tokens": ["Sets", "the", "license_name", "of", "this", "DatasetNewRequest", "."], "sha": "65f14b1386470c5784d4753e491478e7537660d9", "url": "https://github.com/Kaggle/kaggle-api/blob/65f14b1386470c5784d4753e491478e7537660d9/kaggle/models/dataset_new_request.py#L194-L209", "partition": "train"}
{"repo": "dmlc/gluon-nlp", "path": "scripts/sentiment_analysis/sentiment_analysis_cnn.py", "func_name": "train", "original_string": "def train(net, train_data, test_data):\n    \"\"\"Train textCNN model for sentiment analysis.\"\"\"\n    start_pipeline_time = time.time()\n    net, trainer = text_cnn.init(net, vocab, args.model_mode, context, args.lr)\n    random.shuffle(train_data)\n    sp = int(len(train_data)*0.9)\n    train_dataloader = DataLoader(dataset=train_data[:sp],\n                                  batch_size=args.batch_size,\n                                  shuffle=True)\n    val_dataloader = DataLoader(dataset=train_data[sp:],\n                                batch_size=args.batch_size,\n                                shuffle=False)\n    test_dataloader = DataLoader(dataset=test_data,\n                                 batch_size=args.batch_size,\n                                 shuffle=False)\n    # Training/Testing.\n    best_val_acc = 0\n    for epoch in range(args.epochs):\n        # Epoch training stats.\n        start_epoch_time = time.time()\n        epoch_L = 0.0\n        epoch_sent_num = 0\n        epoch_wc = 0\n        # Log interval training stats.\n        start_log_interval_time = time.time()\n        log_interval_wc = 0\n        log_interval_sent_num = 0\n        log_interval_L = 0.0\n        for i, (data, label) in enumerate(train_dataloader):\n            data = mx.nd.transpose(data.as_in_context(context))\n            label = label.as_in_context(context)\n            wc = max_len\n            log_interval_wc += wc\n            epoch_wc += wc\n            log_interval_sent_num += data.shape[1]\n            epoch_sent_num += data.shape[1]\n\n            with autograd.record():\n                output = net(data)\n                L = loss(output, label).mean()\n            L.backward()\n            # Update parameter.\n            trainer.step(1)\n            log_interval_L += L.asscalar()\n            epoch_L += L.asscalar()\n            if (i + 1) % args.log_interval == 0:\n                print('[Epoch %d Batch %d/%d] avg loss %g, throughput %gK wps' % (\n                    epoch, i + 1, len(train_dataloader),\n                    log_interval_L / log_interval_sent_num,\n                    log_interval_wc / 1000 / (time.time() - start_log_interval_time)))\n                # Clear log interval training stats.\n                start_log_interval_time = time.time()\n                log_interval_wc = 0\n                log_interval_sent_num = 0\n                log_interval_L = 0\n        end_epoch_time = time.time()\n        val_avg_L, val_acc = evaluate(net, val_dataloader)\n        print('[Epoch %d] train avg loss %g, '\n              'test acc %.4f, test avg loss %g, throughput %gK wps' % (\n                  epoch, epoch_L / epoch_sent_num,\n                  val_acc, val_avg_L,\n                  epoch_wc / 1000 / (end_epoch_time - start_epoch_time)))\n\n        if val_acc >= best_val_acc:\n            print('Observed Improvement.')\n            best_val_acc = val_acc\n            test_avg_L, test_acc = evaluate(net, test_dataloader)\n\n    print('Test loss %g, test acc %.4f'%(test_avg_L, test_acc))\n    print('Total time cost %.2fs'%(time.time()-start_pipeline_time))\n    return test_acc", "language": "python", "code": "def train(net, train_data, test_data):\n    \"\"\"Train textCNN model for sentiment analysis.\"\"\"\n    start_pipeline_time = time.time()\n    net, trainer = text_cnn.init(net, vocab, args.model_mode, context, args.lr)\n    random.shuffle(train_data)\n    sp = int(len(train_data)*0.9)\n    train_dataloader = DataLoader(dataset=train_data[:sp],\n                                  batch_size=args.batch_size,\n                                  shuffle=True)\n    val_dataloader = DataLoader(dataset=train_data[sp:],\n                                batch_size=args.batch_size,\n                                shuffle=False)\n    test_dataloader = DataLoader(dataset=test_data,\n                                 batch_size=args.batch_size,\n                                 shuffle=False)\n    # Training/Testing.\n    best_val_acc = 0\n    for epoch in range(args.epochs):\n        # Epoch training stats.\n        start_epoch_time = time.time()\n        epoch_L = 0.0\n        epoch_sent_num = 0\n        epoch_wc = 0\n        # Log interval training stats.\n        start_log_interval_time = time.time()\n        log_interval_wc = 0\n        log_interval_sent_num = 0\n        log_interval_L = 0.0\n        for i, (data, label) in enumerate(train_dataloader):\n            data = mx.nd.transpose(data.as_in_context(context))\n            label = label.as_in_context(context)\n            wc = max_len\n            log_interval_wc += wc\n            epoch_wc += wc\n            log_interval_sent_num += data.shape[1]\n            epoch_sent_num += data.shape[1]\n\n            with autograd.record():\n                output = net(data)\n                L = loss(output, label).mean()\n            L.backward()\n            # Update parameter.\n            trainer.step(1)\n            log_interval_L += L.asscalar()\n            epoch_L += L.asscalar()\n            if (i + 1) % args.log_interval == 0:\n                print('[Epoch %d Batch %d/%d] avg loss %g, throughput %gK wps' % (\n                    epoch, i + 1, len(train_dataloader),\n                    log_interval_L / log_interval_sent_num,\n                    log_interval_wc / 1000 / (time.time() - start_log_interval_time)))\n                # Clear log interval training stats.\n                start_log_interval_time = time.time()\n                log_interval_wc = 0\n                log_interval_sent_num = 0\n                log_interval_L = 0\n        end_epoch_time = time.time()\n        val_avg_L, val_acc = evaluate(net, val_dataloader)\n        print('[Epoch %d] train avg loss %g, '\n              'test acc %.4f, test avg loss %g, throughput %gK wps' % (\n                  epoch, epoch_L / epoch_sent_num,\n                  val_acc, val_avg_L,\n                  epoch_wc / 1000 / (end_epoch_time - start_epoch_time)))\n\n        if val_acc >= best_val_acc:\n            print('Observed Improvement.')\n            best_val_acc = val_acc\n            test_avg_L, test_acc = evaluate(net, test_dataloader)\n\n    print('Test loss %g, test acc %.4f'%(test_avg_L, test_acc))\n    print('Total time cost %.2fs'%(time.time()-start_pipeline_time))\n    return test_acc", "code_tokens": ["def", "train", "(", "net", ",", "train_data", ",", "test_data", ")", ":", "start_pipeline_time", "=", "time", ".", "time", "(", ")", "net", ",", "trainer", "=", "text_cnn", ".", "init", "(", "net", ",", "vocab", ",", "args", ".", "model_mode", ",", "context", ",", "args", ".", "lr", ")", "random", ".", "shuffle", "(", "train_data", ")", "sp", "=", "int", "(", "len", "(", "train_data", ")", "*", "0.9", ")", "train_dataloader", "=", "DataLoader", "(", "dataset", "=", "train_data", "[", ":", "sp", "]", ",", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "True", ")", "val_dataloader", "=", "DataLoader", "(", "dataset", "=", "train_data", "[", "sp", ":", "]", ",", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "False", ")", "test_dataloader", "=", "DataLoader", "(", "dataset", "=", "test_data", ",", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "False", ")", "# Training/Testing.", "best_val_acc", "=", "0", "for", "epoch", "in", "range", "(", "args", ".", "epochs", ")", ":", "# Epoch training stats.", "start_epoch_time", "=", "time", ".", "time", "(", ")", "epoch_L", "=", "0.0", "epoch_sent_num", "=", "0", "epoch_wc", "=", "0", "# Log interval training stats.", "start_log_interval_time", "=", "time", ".", "time", "(", ")", "log_interval_wc", "=", "0", "log_interval_sent_num", "=", "0", "log_interval_L", "=", "0.0", "for", "i", ",", "(", "data", ",", "label", ")", "in", "enumerate", "(", "train_dataloader", ")", ":", "data", "=", "mx", ".", "nd", ".", "transpose", "(", "data", ".", "as_in_context", "(", "context", ")", ")", "label", "=", "label", ".", "as_in_context", "(", "context", ")", "wc", "=", "max_len", "log_interval_wc", "+=", "wc", "epoch_wc", "+=", "wc", "log_interval_sent_num", "+=", "data", ".", "shape", "[", "1", "]", "epoch_sent_num", "+=", "data", ".", "shape", "[", "1", "]", "with", "autograd", ".", "record", "(", ")", ":", "output", "=", "net", "(", "data", ")", "L", "=", "loss", "(", "output", ",", "label", ")", ".", "mean", "(", ")", "L", ".", "backward", "(", ")", "# Update parameter.", "trainer", ".", "step", "(", "1", ")", "log_interval_L", "+=", "L", ".", "asscalar", "(", ")", "epoch_L", "+=", "L", ".", "asscalar", "(", ")", "if", "(", "i", "+", "1", ")", "%", "args", ".", "log_interval", "==", "0", ":", "print", "(", "'[Epoch %d Batch %d/%d] avg loss %g, throughput %gK wps'", "%", "(", "epoch", ",", "i", "+", "1", ",", "len", "(", "train_dataloader", ")", ",", "log_interval_L", "/", "log_interval_sent_num", ",", "log_interval_wc", "/", "1000", "/", "(", "time", ".", "time", "(", ")", "-", "start_log_interval_time", ")", ")", ")", "# Clear log interval training stats.", "start_log_interval_time", "=", "time", ".", "time", "(", ")", "log_interval_wc", "=", "0", "log_interval_sent_num", "=", "0", "log_interval_L", "=", "0", "end_epoch_time", "=", "time", ".", "time", "(", ")", "val_avg_L", ",", "val_acc", "=", "evaluate", "(", "net", ",", "val_dataloader", ")", "print", "(", "'[Epoch %d] train avg loss %g, '", "'test acc %.4f, test avg loss %g, throughput %gK wps'", "%", "(", "epoch", ",", "epoch_L", "/", "epoch_sent_num", ",", "val_acc", ",", "val_avg_L", ",", "epoch_wc", "/", "1000", "/", "(", "end_epoch_time", "-", "start_epoch_time", ")", ")", ")", "if", "val_acc", ">=", "best_val_acc", ":", "print", "(", "'Observed Improvement.'", ")", "best_val_acc", "=", "val_acc", "test_avg_L", ",", "test_acc", "=", "evaluate", "(", "net", ",", "test_dataloader", ")", "print", "(", "'Test loss %g, test acc %.4f'", "%", "(", "test_avg_L", ",", "test_acc", ")", ")", "print", "(", "'Total time cost %.2fs'", "%", "(", "time", ".", "time", "(", ")", "-", "start_pipeline_time", ")", ")", "return", "test_acc"], "docstring": "Train textCNN model for sentiment analysis.", "docstring_tokens": ["Train", "textCNN", "model", "for", "sentiment", "analysis", "."], "sha": "4b83eb6bcc8881e5f1081a3675adaa19fac5c0ba", "url": "https://github.com/dmlc/gluon-nlp/blob/4b83eb6bcc8881e5f1081a3675adaa19fac5c0ba/scripts/sentiment_analysis/sentiment_analysis_cnn.py#L114-L184", "partition": "train"}
{"repo": "saltstack/salt", "path": "salt/modules/vsphere.py", "func_name": "_create_scsi_devices", "original_string": "def _create_scsi_devices(scsi_devices):\n    '''\n    Returns a list of vim.vm.device.VirtualDeviceSpec objects representing\n    SCSI controllers\n\n    scsi_devices:\n        List of SCSI device properties\n    '''\n    keys = range(-1000, -1050, -1)\n    scsi_specs = []\n    if scsi_devices:\n        devs = [scsi['adapter'] for scsi in scsi_devices]\n        log.trace('Creating SCSI devices %s', devs)\n        # unitNumber for disk attachment, 0:0 1st 0 is the controller busNumber,\n        # 2nd is the unitNumber\n        for (key, scsi_controller) in zip(keys, scsi_devices):\n            # create the SCSI controller\n            scsi_spec = _apply_scsi_controller(scsi_controller['adapter'],\n                                               scsi_controller['type'],\n                                               scsi_controller['bus_sharing'],\n                                               key,\n                                               scsi_controller['bus_number'],\n                                               'add')\n            scsi_specs.append(scsi_spec)\n    return scsi_specs", "language": "python", "code": "def _create_scsi_devices(scsi_devices):\n    '''\n    Returns a list of vim.vm.device.VirtualDeviceSpec objects representing\n    SCSI controllers\n\n    scsi_devices:\n        List of SCSI device properties\n    '''\n    keys = range(-1000, -1050, -1)\n    scsi_specs = []\n    if scsi_devices:\n        devs = [scsi['adapter'] for scsi in scsi_devices]\n        log.trace('Creating SCSI devices %s', devs)\n        # unitNumber for disk attachment, 0:0 1st 0 is the controller busNumber,\n        # 2nd is the unitNumber\n        for (key, scsi_controller) in zip(keys, scsi_devices):\n            # create the SCSI controller\n            scsi_spec = _apply_scsi_controller(scsi_controller['adapter'],\n                                               scsi_controller['type'],\n                                               scsi_controller['bus_sharing'],\n                                               key,\n                                               scsi_controller['bus_number'],\n                                               'add')\n            scsi_specs.append(scsi_spec)\n    return scsi_specs", "code_tokens": ["def", "_create_scsi_devices", "(", "scsi_devices", ")", ":", "keys", "=", "range", "(", "-", "1000", ",", "-", "1050", ",", "-", "1", ")", "scsi_specs", "=", "[", "]", "if", "scsi_devices", ":", "devs", "=", "[", "scsi", "[", "'adapter'", "]", "for", "scsi", "in", "scsi_devices", "]", "log", ".", "trace", "(", "'Creating SCSI devices %s'", ",", "devs", ")", "# unitNumber for disk attachment, 0:0 1st 0 is the controller busNumber,", "# 2nd is the unitNumber", "for", "(", "key", ",", "scsi_controller", ")", "in", "zip", "(", "keys", ",", "scsi_devices", ")", ":", "# create the SCSI controller", "scsi_spec", "=", "_apply_scsi_controller", "(", "scsi_controller", "[", "'adapter'", "]", ",", "scsi_controller", "[", "'type'", "]", ",", "scsi_controller", "[", "'bus_sharing'", "]", ",", "key", ",", "scsi_controller", "[", "'bus_number'", "]", ",", "'add'", ")", "scsi_specs", ".", "append", "(", "scsi_spec", ")", "return", "scsi_specs"], "docstring": "Returns a list of vim.vm.device.VirtualDeviceSpec objects representing\n    SCSI controllers\n\n    scsi_devices:\n        List of SCSI device properties", "docstring_tokens": ["Returns", "a", "list", "of", "vim", ".", "vm", ".", "device", ".", "VirtualDeviceSpec", "objects", "representing", "SCSI", "controllers"], "sha": "e8541fd6e744ab0df786c0f76102e41631f45d46", "url": "https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/vsphere.py#L8288-L8312", "partition": "train"}
{"repo": "saltstack/salt", "path": "salt/modules/vsphere.py", "func_name": "_create_network_adapters", "original_string": "def _create_network_adapters(network_interfaces, parent=None):\n    '''\n    Returns a list of vim.vm.device.VirtualDeviceSpec objects representing\n    the interfaces to be created for a virtual machine\n\n    network_interfaces\n        List of network interfaces and properties\n\n    parent\n        Parent object reference\n\n    .. code-block: bash\n\n        interfaces:\n          adapter: 'Network adapter 1'\n          name: vlan100\n          switch_type: distributed or standard\n          adapter_type: vmxnet3 or vmxnet, vmxnet2, vmxnet3, e1000, e1000e\n          mac: '00:11:22:33:44:55'\n    '''\n    network_specs = []\n    nics_settings = []\n    keys = range(-4000, -4050, -1)\n    if network_interfaces:\n        devs = [inter['adapter'] for inter in network_interfaces]\n        log.trace('Creating network interfaces %s', devs)\n        for interface, key in zip(network_interfaces, keys):\n            network_spec = _apply_network_adapter_config(\n                key, interface['name'],\n                interface['adapter_type'],\n                interface['switch_type'],\n                network_adapter_label=interface['adapter'],\n                operation='add',\n                connectable=interface['connectable'] if 'connectable' in interface else None,\n                mac=interface['mac'], parent=parent)\n            network_specs.append(network_spec)\n            if 'mapping' in interface:\n                adapter_mapping = _set_network_adapter_mapping(\n                    interface['mapping']['domain'],\n                    interface['mapping']['gateway'],\n                    interface['mapping']['ip_addr'],\n                    interface['mapping']['subnet_mask'],\n                    interface['mac'])\n                nics_settings.append(adapter_mapping)\n    return (network_specs, nics_settings)", "language": "python", "code": "def _create_network_adapters(network_interfaces, parent=None):\n    '''\n    Returns a list of vim.vm.device.VirtualDeviceSpec objects representing\n    the interfaces to be created for a virtual machine\n\n    network_interfaces\n        List of network interfaces and properties\n\n    parent\n        Parent object reference\n\n    .. code-block: bash\n\n        interfaces:\n          adapter: 'Network adapter 1'\n          name: vlan100\n          switch_type: distributed or standard\n          adapter_type: vmxnet3 or vmxnet, vmxnet2, vmxnet3, e1000, e1000e\n          mac: '00:11:22:33:44:55'\n    '''\n    network_specs = []\n    nics_settings = []\n    keys = range(-4000, -4050, -1)\n    if network_interfaces:\n        devs = [inter['adapter'] for inter in network_interfaces]\n        log.trace('Creating network interfaces %s', devs)\n        for interface, key in zip(network_interfaces, keys):\n            network_spec = _apply_network_adapter_config(\n                key, interface['name'],\n                interface['adapter_type'],\n                interface['switch_type'],\n                network_adapter_label=interface['adapter'],\n                operation='add',\n                connectable=interface['connectable'] if 'connectable' in interface else None,\n                mac=interface['mac'], parent=parent)\n            network_specs.append(network_spec)\n            if 'mapping' in interface:\n                adapter_mapping = _set_network_adapter_mapping(\n                    interface['mapping']['domain'],\n                    interface['mapping']['gateway'],\n                    interface['mapping']['ip_addr'],\n                    interface['mapping']['subnet_mask'],\n                    interface['mac'])\n                nics_settings.append(adapter_mapping)\n    return (network_specs, nics_settings)", "code_tokens": ["def", "_create_network_adapters", "(", "network_interfaces", ",", "parent", "=", "None", ")", ":", "network_specs", "=", "[", "]", "nics_settings", "=", "[", "]", "keys", "=", "range", "(", "-", "4000", ",", "-", "4050", ",", "-", "1", ")", "if", "network_interfaces", ":", "devs", "=", "[", "inter", "[", "'adapter'", "]", "for", "inter", "in", "network_interfaces", "]", "log", ".", "trace", "(", "'Creating network interfaces %s'", ",", "devs", ")", "for", "interface", ",", "key", "in", "zip", "(", "network_interfaces", ",", "keys", ")", ":", "network_spec", "=", "_apply_network_adapter_config", "(", "key", ",", "interface", "[", "'name'", "]", ",", "interface", "[", "'adapter_type'", "]", ",", "interface", "[", "'switch_type'", "]", ",", "network_adapter_label", "=", "interface", "[", "'adapter'", "]", ",", "operation", "=", "'add'", ",", "connectable", "=", "interface", "[", "'connectable'", "]", "if", "'connectable'", "in", "interface", "else", "None", ",", "mac", "=", "interface", "[", "'mac'", "]", ",", "parent", "=", "parent", ")", "network_specs", ".", "append", "(", "network_spec", ")", "if", "'mapping'", "in", "interface", ":", "adapter_mapping", "=", "_set_network_adapter_mapping", "(", "interface", "[", "'mapping'", "]", "[", "'domain'", "]", ",", "interface", "[", "'mapping'", "]", "[", "'gateway'", "]", ",", "interface", "[", "'mapping'", "]", "[", "'ip_addr'", "]", ",", "interface", "[", "'mapping'", "]", "[", "'subnet_mask'", "]", ",", "interface", "[", "'mac'", "]", ")", "nics_settings", ".", "append", "(", "adapter_mapping", ")", "return", "(", "network_specs", ",", "nics_settings", ")"], "docstring": "Returns a list of vim.vm.device.VirtualDeviceSpec objects representing\n    the interfaces to be created for a virtual machine\n\n    network_interfaces\n        List of network interfaces and properties\n\n    parent\n        Parent object reference\n\n    .. code-block: bash\n\n        interfaces:\n          adapter: 'Network adapter 1'\n          name: vlan100\n          switch_type: distributed or standard\n          adapter_type: vmxnet3 or vmxnet, vmxnet2, vmxnet3, e1000, e1000e\n          mac: '00:11:22:33:44:55'", "docstring_tokens": ["Returns", "a", "list", "of", "vim", ".", "vm", ".", "device", ".", "VirtualDeviceSpec", "objects", "representing", "the", "interfaces", "to", "be", "created", "for", "a", "virtual", "machine"], "sha": "e8541fd6e744ab0df786c0f76102e41631f45d46", "url": "https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/vsphere.py#L8315-L8359", "partition": "train"}
{"repo": "ray-project/ray", "path": "python/ray/node.py", "func_name": "Node._kill_process_type", "original_string": "def _kill_process_type(self,\n                           process_type,\n                           allow_graceful=False,\n                           check_alive=True,\n                           wait=False):\n        \"\"\"Kill a process of a given type.\n\n        If the process type is PROCESS_TYPE_REDIS_SERVER, then we will kill all\n        of the Redis servers.\n\n        If the process was started in valgrind, then we will raise an exception\n        if the process has a non-zero exit code.\n\n        Args:\n            process_type: The type of the process to kill.\n            allow_graceful (bool): Send a SIGTERM first and give the process\n                time to exit gracefully. If that doesn't work, then use\n                SIGKILL. We usually want to do this outside of tests.\n            check_alive (bool): If true, then we expect the process to be alive\n                and will raise an exception if the process is already dead.\n            wait (bool): If true, then this method will not return until the\n                process in question has exited.\n\n        Raises:\n            This process raises an exception in the following cases:\n                1. The process had already died and check_alive is true.\n                2. The process had been started in valgrind and had a non-zero\n                   exit code.\n        \"\"\"\n        process_infos = self.all_processes[process_type]\n        if process_type != ray_constants.PROCESS_TYPE_REDIS_SERVER:\n            assert len(process_infos) == 1\n        for process_info in process_infos:\n            process = process_info.process\n            # Handle the case where the process has already exited.\n            if process.poll() is not None:\n                if check_alive:\n                    raise Exception(\"Attempting to kill a process of type \"\n                                    \"'{}', but this process is already dead.\"\n                                    .format(process_type))\n                else:\n                    continue\n\n            if process_info.use_valgrind:\n                process.terminate()\n                process.wait()\n                if process.returncode != 0:\n                    message = (\"Valgrind detected some errors in process of \"\n                               \"type {}. Error code {}.\".format(\n                                   process_type, process.returncode))\n                    if process_info.stdout_file is not None:\n                        with open(process_info.stdout_file, \"r\") as f:\n                            message += \"\\nPROCESS STDOUT:\\n\" + f.read()\n                    if process_info.stderr_file is not None:\n                        with open(process_info.stderr_file, \"r\") as f:\n                            message += \"\\nPROCESS STDERR:\\n\" + f.read()\n                    raise Exception(message)\n                continue\n\n            if process_info.use_valgrind_profiler:\n                # Give process signal to write profiler data.\n                os.kill(process.pid, signal.SIGINT)\n                # Wait for profiling data to be written.\n                time.sleep(0.1)\n\n            if allow_graceful:\n                # Allow the process one second to exit gracefully.\n                process.terminate()\n                timer = threading.Timer(1, lambda process: process.kill(),\n                                        [process])\n                try:\n                    timer.start()\n                    process.wait()\n                finally:\n                    timer.cancel()\n\n                if process.poll() is not None:\n                    continue\n\n            # If the process did not exit within one second, force kill it.\n            process.kill()\n            # The reason we usually don't call process.wait() here is that\n            # there's some chance we'd end up waiting a really long time.\n            if wait:\n                process.wait()\n\n        del self.all_processes[process_type]", "language": "python", "code": "def _kill_process_type(self,\n                           process_type,\n                           allow_graceful=False,\n                           check_alive=True,\n                           wait=False):\n        \"\"\"Kill a process of a given type.\n\n        If the process type is PROCESS_TYPE_REDIS_SERVER, then we will kill all\n        of the Redis servers.\n\n        If the process was started in valgrind, then we will raise an exception\n        if the process has a non-zero exit code.\n\n        Args:\n            process_type: The type of the process to kill.\n            allow_graceful (bool): Send a SIGTERM first and give the process\n                time to exit gracefully. If that doesn't work, then use\n                SIGKILL. We usually want to do this outside of tests.\n            check_alive (bool): If true, then we expect the process to be alive\n                and will raise an exception if the process is already dead.\n            wait (bool): If true, then this method will not return until the\n                process in question has exited.\n\n        Raises:\n            This process raises an exception in the following cases:\n                1. The process had already died and check_alive is true.\n                2. The process had been started in valgrind and had a non-zero\n                   exit code.\n        \"\"\"\n        process_infos = self.all_processes[process_type]\n        if process_type != ray_constants.PROCESS_TYPE_REDIS_SERVER:\n            assert len(process_infos) == 1\n        for process_info in process_infos:\n            process = process_info.process\n            # Handle the case where the process has already exited.\n            if process.poll() is not None:\n                if check_alive:\n                    raise Exception(\"Attempting to kill a process of type \"\n                                    \"'{}', but this process is already dead.\"\n                                    .format(process_type))\n                else:\n                    continue\n\n            if process_info.use_valgrind:\n                process.terminate()\n                process.wait()\n                if process.returncode != 0:\n                    message = (\"Valgrind detected some errors in process of \"\n                               \"type {}. Error code {}.\".format(\n                                   process_type, process.returncode))\n                    if process_info.stdout_file is not None:\n                        with open(process_info.stdout_file, \"r\") as f:\n                            message += \"\\nPROCESS STDOUT:\\n\" + f.read()\n                    if process_info.stderr_file is not None:\n                        with open(process_info.stderr_file, \"r\") as f:\n                            message += \"\\nPROCESS STDERR:\\n\" + f.read()\n                    raise Exception(message)\n                continue\n\n            if process_info.use_valgrind_profiler:\n                # Give process signal to write profiler data.\n                os.kill(process.pid, signal.SIGINT)\n                # Wait for profiling data to be written.\n                time.sleep(0.1)\n\n            if allow_graceful:\n                # Allow the process one second to exit gracefully.\n                process.terminate()\n                timer = threading.Timer(1, lambda process: process.kill(),\n                                        [process])\n                try:\n                    timer.start()\n                    process.wait()\n                finally:\n                    timer.cancel()\n\n                if process.poll() is not None:\n                    continue\n\n            # If the process did not exit within one second, force kill it.\n            process.kill()\n            # The reason we usually don't call process.wait() here is that\n            # there's some chance we'd end up waiting a really long time.\n            if wait:\n                process.wait()\n\n        del self.all_processes[process_type]", "code_tokens": ["def", "_kill_process_type", "(", "self", ",", "process_type", ",", "allow_graceful", "=", "False", ",", "check_alive", "=", "True", ",", "wait", "=", "False", ")", ":", "process_infos", "=", "self", ".", "all_processes", "[", "process_type", "]", "if", "process_type", "!=", "ray_constants", ".", "PROCESS_TYPE_REDIS_SERVER", ":", "assert", "len", "(", "process_infos", ")", "==", "1", "for", "process_info", "in", "process_infos", ":", "process", "=", "process_info", ".", "process", "# Handle the case where the process has already exited.", "if", "process", ".", "poll", "(", ")", "is", "not", "None", ":", "if", "check_alive", ":", "raise", "Exception", "(", "\"Attempting to kill a process of type \"", "\"'{}', but this process is already dead.\"", ".", "format", "(", "process_type", ")", ")", "else", ":", "continue", "if", "process_info", ".", "use_valgrind", ":", "process", ".", "terminate", "(", ")", "process", ".", "wait", "(", ")", "if", "process", ".", "returncode", "!=", "0", ":", "message", "=", "(", "\"Valgrind detected some errors in process of \"", "\"type {}. Error code {}.\"", ".", "format", "(", "process_type", ",", "process", ".", "returncode", ")", ")", "if", "process_info", ".", "stdout_file", "is", "not", "None", ":", "with", "open", "(", "process_info", ".", "stdout_file", ",", "\"r\"", ")", "as", "f", ":", "message", "+=", "\"\\nPROCESS STDOUT:\\n\"", "+", "f", ".", "read", "(", ")", "if", "process_info", ".", "stderr_file", "is", "not", "None", ":", "with", "open", "(", "process_info", ".", "stderr_file", ",", "\"r\"", ")", "as", "f", ":", "message", "+=", "\"\\nPROCESS STDERR:\\n\"", "+", "f", ".", "read", "(", ")", "raise", "Exception", "(", "message", ")", "continue", "if", "process_info", ".", "use_valgrind_profiler", ":", "# Give process signal to write profiler data.", "os", ".", "kill", "(", "process", ".", "pid", ",", "signal", ".", "SIGINT", ")", "# Wait for profiling data to be written.", "time", ".", "sleep", "(", "0.1", ")", "if", "allow_graceful", ":", "# Allow the process one second to exit gracefully.", "process", ".", "terminate", "(", ")", "timer", "=", "threading", ".", "Timer", "(", "1", ",", "lambda", "process", ":", "process", ".", "kill", "(", ")", ",", "[", "process", "]", ")", "try", ":", "timer", ".", "start", "(", ")", "process", ".", "wait", "(", ")", "finally", ":", "timer", ".", "cancel", "(", ")", "if", "process", ".", "poll", "(", ")", "is", "not", "None", ":", "continue", "# If the process did not exit within one second, force kill it.", "process", ".", "kill", "(", ")", "# The reason we usually don't call process.wait() here is that", "# there's some chance we'd end up waiting a really long time.", "if", "wait", ":", "process", ".", "wait", "(", ")", "del", "self", ".", "all_processes", "[", "process_type", "]"], "docstring": "Kill a process of a given type.\n\n        If the process type is PROCESS_TYPE_REDIS_SERVER, then we will kill all\n        of the Redis servers.\n\n        If the process was started in valgrind, then we will raise an exception\n        if the process has a non-zero exit code.\n\n        Args:\n            process_type: The type of the process to kill.\n            allow_graceful (bool): Send a SIGTERM first and give the process\n                time to exit gracefully. If that doesn't work, then use\n                SIGKILL. We usually want to do this outside of tests.\n            check_alive (bool): If true, then we expect the process to be alive\n                and will raise an exception if the process is already dead.\n            wait (bool): If true, then this method will not return until the\n                process in question has exited.\n\n        Raises:\n            This process raises an exception in the following cases:\n                1. The process had already died and check_alive is true.\n                2. The process had been started in valgrind and had a non-zero\n                   exit code.", "docstring_tokens": ["Kill", "a", "process", "of", "a", "given", "type", "."], "sha": "4eade036a0505e244c976f36aaa2d64386b5129b", "url": "https://github.com/ray-project/ray/blob/4eade036a0505e244c976f36aaa2d64386b5129b/python/ray/node.py#L493-L579", "partition": "train"}